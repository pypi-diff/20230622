# Comparing `tmp/patroni-3.0.2.tar.gz` & `tmp/patroni-3.0.3.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "patroni-3.0.2.tar", last modified: Fri Mar 24 08:11:49 2023, max compression
+gzip compressed data, was "patroni-3.0.3.tar", last modified: Thu Jun 22 09:35:29 2023, max compression
```

## Comparing `patroni-3.0.2.tar` & `patroni-3.0.3.tar`

### file list

```diff
@@ -1,94 +1,97 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-24 08:11:49.770218 patroni-3.0.2/
--rw-r--r--   0 runner    (1001) docker     (123)     1086 2023-03-24 08:11:03.000000 patroni-3.0.2/LICENSE
--rw-r--r--   0 runner    (1001) docker     (123)       67 2023-03-24 08:11:03.000000 patroni-3.0.2/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (123)    10255 2023-03-24 08:11:49.770218 patroni-3.0.2/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)     8725 2023-03-24 08:11:03.000000 patroni-3.0.2/README.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-24 08:11:49.762217 patroni-3.0.2/patroni/
--rw-r--r--   0 runner    (1001) docker     (123)     1245 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6098 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/__main__.py
--rw-r--r--   0 runner    (1001) docker     (123)    44640 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/api.py
--rw-r--r--   0 runner    (1001) docker     (123)     4661 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/async_executor.py
--rw-r--r--   0 runner    (1001) docker     (123)    21448 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/config.py
--rw-r--r--   0 runner    (1001) docker     (123)    53862 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/ctl.py
--rw-r--r--   0 runner    (1001) docker     (123)     6210 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/daemon.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-24 08:11:49.766218 patroni-3.0.2/patroni/dcs/
--rw-r--r--   0 runner    (1001) docker     (123)    40681 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/dcs/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    26111 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/dcs/consul.py
--rw-r--r--   0 runner    (1001) docker     (123)    33953 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/dcs/etcd.py
--rw-r--r--   0 runner    (1001) docker     (123)    32941 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/dcs/etcd3.py
--rw-r--r--   0 runner    (1001) docker     (123)     2629 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/dcs/exhibitor.py
--rw-r--r--   0 runner    (1001) docker     (123)    56449 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/dcs/kubernetes.py
--rw-r--r--   0 runner    (1001) docker     (123)    17200 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/dcs/raft.py
--rw-r--r--   0 runner    (1001) docker     (123)    21631 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/dcs/zookeeper.py
--rw-r--r--   0 runner    (1001) docker     (123)      659 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/exceptions.py
--rw-r--r--   0 runner    (1001) docker     (123)    96642 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/ha.py
--rw-r--r--   0 runner    (1001) docker     (123)     7631 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/log.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-24 08:11:49.766218 patroni-3.0.2/patroni/postgresql/
--rw-r--r--   0 runner    (1001) docker     (123)    51533 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/postgresql/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    18097 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/postgresql/bootstrap.py
--rw-r--r--   0 runner    (1001) docker     (123)     2056 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/postgresql/callback_executor.py
--rw-r--r--   0 runner    (1001) docker     (123)     4320 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/postgresql/cancellable.py
--rw-r--r--   0 runner    (1001) docker     (123)    16392 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/postgresql/citus.py
--rw-r--r--   0 runner    (1001) docker     (123)    54757 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/postgresql/config.py
--rw-r--r--   0 runner    (1001) docker     (123)     1389 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/postgresql/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2865 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/postgresql/misc.py
--rw-r--r--   0 runner    (1001) docker     (123)    10619 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/postgresql/postmaster.py
--rw-r--r--   0 runner    (1001) docker     (123)    25835 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/postgresql/rewind.py
--rw-r--r--   0 runner    (1001) docker     (123)    21652 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/postgresql/slots.py
--rw-r--r--   0 runner    (1001) docker     (123)    11918 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/postgresql/sync.py
--rw-r--r--   0 runner    (1001) docker     (123)    26508 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/postgresql/validator.py
--rw-r--r--   0 runner    (1001) docker     (123)     1777 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/psycopg.py
--rw-r--r--   0 runner    (1001) docker     (123)      686 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/raft_controller.py
--rw-r--r--   0 runner    (1001) docker     (123)     2714 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/request.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-24 08:11:49.766218 patroni-3.0.2/patroni/scripts/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/scripts/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     3090 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/scripts/aws.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    14394 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/scripts/wale_restore.py
--rw-r--r--   0 runner    (1001) docker     (123)    17212 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    14892 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/validator.py
--rw-r--r--   0 runner    (1001) docker     (123)      128 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/version.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-24 08:11:49.766218 patroni-3.0.2/patroni/watchdog/
--rw-r--r--   0 runner    (1001) docker     (123)       98 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/watchdog/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11946 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/watchdog/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     7961 2023-03-24 08:11:03.000000 patroni-3.0.2/patroni/watchdog/linux.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-24 08:11:49.762217 patroni-3.0.2/patroni.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)    10255 2023-03-24 08:11:49.000000 patroni-3.0.2/patroni.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)     1968 2023-03-24 08:11:49.000000 patroni-3.0.2/patroni.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-03-24 08:11:49.000000 patroni-3.0.2/patroni.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)      230 2023-03-24 08:11:49.000000 patroni-3.0.2/patroni.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (123)      300 2023-03-24 08:11:49.000000 patroni-3.0.2/patroni.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)        8 2023-03-24 08:11:49.000000 patroni-3.0.2/patroni.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (123)       80 2023-03-24 08:11:03.000000 patroni-3.0.2/requirements.dev.txt
--rw-r--r--   0 runner    (1001) docker     (123)      200 2023-03-24 08:11:03.000000 patroni-3.0.2/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)       38 2023-03-24 08:11:49.770218 patroni-3.0.2/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (123)     5718 2023-03-24 08:11:03.000000 patroni-3.0.2/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-24 08:11:49.770218 patroni-3.0.2/tests/
--rw-r--r--   0 runner    (1001) docker     (123)    30291 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_api.py
--rw-r--r--   0 runner    (1001) docker     (123)      756 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_async_executor.py
--rw-r--r--   0 runner    (1001) docker     (123)     2421 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_aws.py
--rw-r--r--   0 runner    (1001) docker     (123)    11799 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_bootstrap.py
--rw-r--r--   0 runner    (1001) docker     (123)     1342 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_callback_executor.py
--rw-r--r--   0 runner    (1001) docker     (123)     1186 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_cancellable.py
--rw-r--r--   0 runner    (1001) docker     (123)     9042 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_citus.py
--rw-r--r--   0 runner    (1001) docker     (123)     6877 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_config.py
--rw-r--r--   0 runner    (1001) docker     (123)    15549 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_consul.py
--rw-r--r--   0 runner    (1001) docker     (123)    36470 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_ctl.py
--rw-r--r--   0 runner    (1001) docker     (123)    16586 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_etcd.py
--rw-r--r--   0 runner    (1001) docker     (123)    13835 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_etcd3.py
--rw-r--r--   0 runner    (1001) docker     (123)     1399 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_exhibitor.py
--rw-r--r--   0 runner    (1001) docker     (123)    76877 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_ha.py
--rw-r--r--   0 runner    (1001) docker     (123)    23873 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_kubernetes.py
--rw-r--r--   0 runner    (1001) docker     (123)     2671 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_log.py
--rw-r--r--   0 runner    (1001) docker     (123)     8202 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_patroni.py
--rw-r--r--   0 runner    (1001) docker     (123)    36203 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_postgresql.py
--rw-r--r--   0 runner    (1001) docker     (123)     7547 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_postmaster.py
--rw-r--r--   0 runner    (1001) docker     (123)     7439 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_raft.py
--rw-r--r--   0 runner    (1001) docker     (123)     1302 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_raft_controller.py
--rw-r--r--   0 runner    (1001) docker     (123)    15850 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_rewind.py
--rw-r--r--   0 runner    (1001) docker     (123)     8861 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_slots.py
--rw-r--r--   0 runner    (1001) docker     (123)     4420 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_sync.py
--rw-r--r--   0 runner    (1001) docker     (123)     3243 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     9687 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_validator.py
--rw-r--r--   0 runner    (1001) docker     (123)     6790 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_wale_restore.py
--rw-r--r--   0 runner    (1001) docker     (123)     8960 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_watchdog.py
--rw-r--r--   0 runner    (1001) docker     (123)    12351 2023-03-24 08:11:03.000000 patroni-3.0.2/tests/test_zookeeper.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-22 09:35:29.793497 patroni-3.0.3/
+-rw-r--r--   0 runner    (1001) docker     (123)     1086 2023-06-22 09:34:34.000000 patroni-3.0.3/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (123)       67 2023-06-22 09:34:34.000000 patroni-3.0.3/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (123)    10213 2023-06-22 09:35:29.793497 patroni-3.0.3/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     8683 2023-06-22 09:34:34.000000 patroni-3.0.3/README.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-22 09:35:29.781497 patroni-3.0.3/patroni/
+-rw-r--r--   0 runner    (1001) docker     (123)     3156 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7100 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/__main__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    80913 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/api.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9059 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/async_executor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6893 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/collections.py
+-rw-r--r--   0 runner    (1001) docker     (123)    27495 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)   101506 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/ctl.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5794 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/daemon.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-22 09:35:29.785497 patroni-3.0.3/patroni/dcs/
+-rw-r--r--   0 runner    (1001) docker     (123)    46749 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/dcs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    28487 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/dcs/consul.py
+-rw-r--r--   0 runner    (1001) docker     (123)    37993 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/dcs/etcd.py
+-rw-r--r--   0 runner    (1001) docker     (123)    38667 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/dcs/etcd3.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2982 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/dcs/exhibitor.py
+-rw-r--r--   0 runner    (1001) docker     (123)    63888 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/dcs/kubernetes.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19589 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/dcs/raft.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23478 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/dcs/zookeeper.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1308 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/exceptions.py
+-rw-r--r--   0 runner    (1001) docker     (123)    99560 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/ha.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15509 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/log.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-22 09:35:29.785497 patroni-3.0.3/patroni/postgresql/
+-rw-r--r--   0 runner    (1001) docker     (123)    58454 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/postgresql/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-22 09:35:29.785497 patroni-3.0.3/patroni/postgresql/available_parameters/
+-rw-r--r--   0 runner    (1001) docker     (123)    35206 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/postgresql/available_parameters/0_postgres.yml
+-rw-r--r--   0 runner    (1001) docker     (123)    21084 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/postgresql/bootstrap.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2165 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/postgresql/callback_executor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4650 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/postgresql/cancellable.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19084 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/postgresql/citus.py
+-rw-r--r--   0 runner    (1001) docker     (123)    58180 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/postgresql/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1809 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/postgresql/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2928 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/postgresql/misc.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11101 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/postgresql/postmaster.py
+-rw-r--r--   0 runner    (1001) docker     (123)    27305 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/postgresql/rewind.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23361 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/postgresql/slots.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13166 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/postgresql/sync.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20981 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/postgresql/validator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4855 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/psycopg.py
+-rw-r--r--   0 runner    (1001) docker     (123)      882 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/raft_controller.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7414 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/request.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-22 09:35:29.785497 patroni-3.0.3/patroni/scripts/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/scripts/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3234 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/scripts/aws.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    14799 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/scripts/wale_restore.py
+-rw-r--r--   0 runner    (1001) docker     (123)    38346 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    42923 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/validator.py
+-rw-r--r--   0 runner    (1001) docker     (123)      128 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/version.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-22 09:35:29.785497 patroni-3.0.3/patroni/watchdog/
+-rw-r--r--   0 runner    (1001) docker     (123)       98 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/watchdog/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12608 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/watchdog/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8514 2023-06-22 09:34:34.000000 patroni-3.0.3/patroni/watchdog/linux.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-22 09:35:29.781497 patroni-3.0.3/patroni.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)    10213 2023-06-22 09:35:29.000000 patroni-3.0.3/patroni.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     2046 2023-06-22 09:35:29.000000 patroni-3.0.3/patroni.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-06-22 09:35:29.000000 patroni-3.0.3/patroni.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      230 2023-06-22 09:35:29.000000 patroni-3.0.3/patroni.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      300 2023-06-22 09:35:29.000000 patroni-3.0.3/patroni.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        8 2023-06-22 09:35:29.000000 patroni-3.0.3/patroni.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       80 2023-06-22 09:34:34.000000 patroni-3.0.3/requirements.dev.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      200 2023-06-22 09:34:34.000000 patroni-3.0.3/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       38 2023-06-22 09:35:29.793497 patroni-3.0.3/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)     5826 2023-06-22 09:34:34.000000 patroni-3.0.3/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-22 09:35:29.793497 patroni-3.0.3/tests/
+-rw-r--r--   0 runner    (1001) docker     (123)    31192 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_api.py
+-rw-r--r--   0 runner    (1001) docker     (123)      756 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_async_executor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2421 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_aws.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13392 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_bootstrap.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1342 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_callback_executor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1186 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_cancellable.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9257 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_citus.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6878 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16072 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_consul.py
+-rw-r--r--   0 runner    (1001) docker     (123)    36966 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_ctl.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16718 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_etcd.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14889 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_etcd3.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1399 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_exhibitor.py
+-rw-r--r--   0 runner    (1001) docker     (123)    78479 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_ha.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24674 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_kubernetes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2671 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_log.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8333 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_patroni.py
+-rw-r--r--   0 runner    (1001) docker     (123)    46321 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_postgresql.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7547 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_postmaster.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7524 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_raft.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1302 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_raft_controller.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15833 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_rewind.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9001 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_slots.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5421 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_sync.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4276 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11919 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_validator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6791 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_wale_restore.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9127 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_watchdog.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12429 2023-06-22 09:34:34.000000 patroni-3.0.3/tests/test_zookeeper.py
```

### Comparing `patroni-3.0.2/LICENSE` & `patroni-3.0.3/LICENSE`

 * *Files identical despite different names*

### Comparing `patroni-3.0.2/PKG-INFO` & `patroni-3.0.3/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: patroni
-Version: 3.0.2
+Version: 3.0.3
 Summary: PostgreSQL High-Available orchestrator and CLI
 Home-page: https://github.com/zalando/patroni
 Author: Alexander Kukushkin, Polina Bungina
 Author-email: akukushkin@microsoft.com, polina.bungina@zalando.de
 License: The MIT License
 Keywords: etcd governor patroni postgresql postgres ha haproxy confd zookeeper exhibitor consul streaming replication kubernetes k8s
 Classifier: Development Status :: 5 - Production/Stable
@@ -41,15 +41,15 @@
 --------------------------------------------------------------------
 
 You can find a version of this documentation that is searchable and also easier to navigate at `patroni.readthedocs.io <https://patroni.readthedocs.io>`__.
 
 
 There are many ways to run high availability with PostgreSQL; for a list, see the `PostgreSQL Documentation <https://wiki.postgresql.org/wiki/Replication,_Clustering,_and_Connection_Pooling>`__.
 
-Patroni is a template for you to create your own customized, high-availability solution using Python and - for maximum accessibility - a distributed configuration store like `ZooKeeper <https://zookeeper.apache.org/>`__, `etcd <https://github.com/coreos/etcd>`__, `Consul <https://github.com/hashicorp/consul>`__ or `Kubernetes <https://kubernetes.io>`__. Database engineers, DBAs, DevOps engineers, and SREs who are looking to quickly deploy HA PostgreSQL in the datacenter-or anywhere else-will hopefully find it useful.
+Patroni is a template for high availability (HA) PostgreSQL solutions using Python. For maximum accessibility, Patroni supports a variety of distributed configuration stores like `ZooKeeper <https://zookeeper.apache.org/>`__, `etcd <https://github.com/coreos/etcd>`__, `Consul <https://github.com/hashicorp/consul>`__ or `Kubernetes <https://kubernetes.io>`__. Database engineers, DBAs, DevOps engineers, and SREs who are looking to quickly deploy HA PostgreSQL in datacenters — or anywhere else — will hopefully find it useful.
 
 We call Patroni a "template" because it is far from being a one-size-fits-all or plug-and-play replication system. It will have its own caveats. Use wisely.
 
 Currently supported PostgreSQL versions: 9.3 to 15.
 
 **Note to Citus users**: Starting from 3.0 Patroni nicely integrates with the `Citus <https://github.com/citusdata/citus>`__ database extension to Postgres. Please check the `Citus support page <https://github.com/zalando/patroni/blob/master/docs/citus.rst>`__ in the Patroni documentation for more info about how to use Patroni high availability together with a Citus distributed cluster.
 
@@ -82,15 +82,15 @@
 
 We report new releases information `here <https://github.com/zalando/patroni/releases>`__.
 
 =========
 Community
 =========
 
-There are two places to connect with the Patroni community: `on github <https://github.com/zalando/patroni>`__, via Issues and PRs, and on channel `#patroni <https://postgresteam.slack.com/archives/C9XPYG92A>`__ in the `PostgreSQL Slack <https://join.slack.com/t/postgresteam/shared_invite/zt-1qj14i9sj-E9WqIFlvcOiHsEk2yFEMjA>`__.  If you're using Patroni, or just interested, please join us.
+There are two places to connect with the Patroni community: `on github <https://github.com/zalando/patroni>`__, via Issues and PRs, and on channel `#patroni <https://postgresteam.slack.com/archives/C9XPYG92A>`__ in the `PostgreSQL Slack <https://pgtreats.info/slack-invite>`__.  If you're using Patroni, or just interested, please join us.
 
 ===================================
 Technical Requirements/Installation
 ===================================
 
 **Pre-requirements for Mac OS**
 
@@ -152,15 +152,15 @@
 exhibitor
     `kazoo` module in order to use Exhibitor as DCS (same dependencies as for Zookeeper)
 kubernetes
     `kubernetes` module in order to use Kubernetes as DCS in Patroni
 raft
     `pysyncobj` module in order to use python Raft implementation as DCS
 aws
-    `boto` in order to use AWS callbacks
+    `boto3` in order to use AWS callbacks
 
 For example, the command in order to install Patroni together with dependencies for Etcd as a DCS and AWS callbacks is:
 
 ::
 
     pip install patroni[etcd,aws]
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `patroni-3.0.2/README.rst` & `patroni-3.0.3/README.rst`

 * *Files 2% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 --------------------------------------------------------------------
 
 You can find a version of this documentation that is searchable and also easier to navigate at `patroni.readthedocs.io <https://patroni.readthedocs.io>`__.
 
 
 There are many ways to run high availability with PostgreSQL; for a list, see the `PostgreSQL Documentation <https://wiki.postgresql.org/wiki/Replication,_Clustering,_and_Connection_Pooling>`__.
 
-Patroni is a template for you to create your own customized, high-availability solution using Python and - for maximum accessibility - a distributed configuration store like `ZooKeeper <https://zookeeper.apache.org/>`__, `etcd <https://github.com/coreos/etcd>`__, `Consul <https://github.com/hashicorp/consul>`__ or `Kubernetes <https://kubernetes.io>`__. Database engineers, DBAs, DevOps engineers, and SREs who are looking to quickly deploy HA PostgreSQL in the datacenter-or anywhere else-will hopefully find it useful.
+Patroni is a template for high availability (HA) PostgreSQL solutions using Python. For maximum accessibility, Patroni supports a variety of distributed configuration stores like `ZooKeeper <https://zookeeper.apache.org/>`__, `etcd <https://github.com/coreos/etcd>`__, `Consul <https://github.com/hashicorp/consul>`__ or `Kubernetes <https://kubernetes.io>`__. Database engineers, DBAs, DevOps engineers, and SREs who are looking to quickly deploy HA PostgreSQL in datacenters — or anywhere else — will hopefully find it useful.
 
 We call Patroni a "template" because it is far from being a one-size-fits-all or plug-and-play replication system. It will have its own caveats. Use wisely.
 
 Currently supported PostgreSQL versions: 9.3 to 15.
 
 **Note to Citus users**: Starting from 3.0 Patroni nicely integrates with the `Citus <https://github.com/citusdata/citus>`__ database extension to Postgres. Please check the `Citus support page <https://github.com/zalando/patroni/blob/master/docs/citus.rst>`__ in the Patroni documentation for more info about how to use Patroni high availability together with a Citus distributed cluster.
 
@@ -45,15 +45,15 @@
 
 We report new releases information `here <https://github.com/zalando/patroni/releases>`__.
 
 =========
 Community
 =========
 
-There are two places to connect with the Patroni community: `on github <https://github.com/zalando/patroni>`__, via Issues and PRs, and on channel `#patroni <https://postgresteam.slack.com/archives/C9XPYG92A>`__ in the `PostgreSQL Slack <https://join.slack.com/t/postgresteam/shared_invite/zt-1qj14i9sj-E9WqIFlvcOiHsEk2yFEMjA>`__.  If you're using Patroni, or just interested, please join us.
+There are two places to connect with the Patroni community: `on github <https://github.com/zalando/patroni>`__, via Issues and PRs, and on channel `#patroni <https://postgresteam.slack.com/archives/C9XPYG92A>`__ in the `PostgreSQL Slack <https://pgtreats.info/slack-invite>`__.  If you're using Patroni, or just interested, please join us.
 
 ===================================
 Technical Requirements/Installation
 ===================================
 
 **Pre-requirements for Mac OS**
 
@@ -115,15 +115,15 @@
 exhibitor
     `kazoo` module in order to use Exhibitor as DCS (same dependencies as for Zookeeper)
 kubernetes
     `kubernetes` module in order to use Kubernetes as DCS in Patroni
 raft
     `pysyncobj` module in order to use python Raft implementation as DCS
 aws
-    `boto` in order to use AWS callbacks
+    `boto3` in order to use AWS callbacks
 
 For example, the command in order to install Patroni together with dependencies for Etcd as a DCS and AWS callbacks is:
 
 ::
 
     pip install patroni[etcd,aws]
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `patroni-3.0.2/patroni/__main__.py` & `patroni-3.0.3/patroni/__main__.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,20 +1,27 @@
 import logging
 import os
 import signal
+import sys
 import time
 
-from patroni.daemon import AbstractPatroniDaemon, abstract_main
+from argparse import Namespace
+from typing import Any, Dict, Optional, TYPE_CHECKING
+
+from patroni.daemon import AbstractPatroniDaemon, abstract_main, get_base_arg_parser
+
+if TYPE_CHECKING:  # pragma: no cover
+    from .config import Config
 
 logger = logging.getLogger(__name__)
 
 
 class Patroni(AbstractPatroniDaemon):
 
-    def __init__(self, config):
+    def __init__(self, config: 'Config') -> None:
         from patroni.api import RestApiServer
         from patroni.dcs import get_dcs
         from patroni.ha import Ha
         from patroni.postgresql import Postgresql
         from patroni.request import PatroniRequest
         from patroni.version import __version__
         from patroni.watchdog import Watchdog
@@ -29,17 +36,17 @@
         self.postgresql = Postgresql(self.config['postgresql'])
         self.api = RestApiServer(self, self.config['restapi'])
         self.request = PatroniRequest(self.config, True)
         self.ha = Ha(self)
 
         self.tags = self.get_tags()
         self.next_run = time.time()
-        self.scheduled_restart = {}
+        self.scheduled_restart: Dict[str, Any] = {}
 
-    def load_dynamic_configuration(self):
+    def load_dynamic_configuration(self) -> None:
         from patroni.exceptions import DCSError
         while True:
             try:
                 cluster = self.dcs.get_cluster()
                 if cluster and cluster.config and cluster.config.data:
                     if self.config.set_dynamic_configuration(cluster.config):
                         self.dcs.reload_config(self.config)
@@ -49,27 +56,27 @@
                         self.dcs.reload_config(self.config)
                         self.watchdog.reload_config(self.config)
                 break
             except DCSError:
                 logger.warning('Can not get cluster from dcs')
                 time.sleep(5)
 
-    def get_tags(self):
+    def get_tags(self) -> Dict[str, Any]:
         return {tag: value for tag, value in self.config.get('tags', {}).items()
                 if tag not in ('clonefrom', 'nofailover', 'noloadbalance', 'nosync') or value}
 
     @property
-    def nofailover(self):
+    def nofailover(self) -> bool:
         return bool(self.tags.get('nofailover', False))
 
     @property
-    def nosync(self):
+    def nosync(self) -> bool:
         return bool(self.tags.get('nosync', False))
 
-    def reload_config(self, sighup=False, local=False):
+    def reload_config(self, sighup: bool = False, local: Optional[bool] = False) -> None:
         try:
             super(Patroni, self).reload_config(sighup, local)
             if local:
                 self.tags = self.get_tags()
                 self.request.reload_config(self.config)
             if local or sighup and self.api.reload_local_certificate():
                 self.api.reload_config(self.config['restapi'])
@@ -83,101 +90,122 @@
     def replicatefrom(self):
         return self.tags.get('replicatefrom')
 
     @property
     def noloadbalance(self):
         return bool(self.tags.get('noloadbalance', False))
 
-    def schedule_next_run(self):
+    def schedule_next_run(self) -> None:
         self.next_run += self.dcs.loop_wait
         current_time = time.time()
         nap_time = self.next_run - current_time
         if nap_time <= 0:
             self.next_run = current_time
             # Release the GIL so we don't starve anyone waiting on async_executor lock
             time.sleep(0.001)
             # Warn user that Patroni is not keeping up
             logger.warning("Loop time exceeded, rescheduling immediately.")
         elif self.ha.watch(nap_time):
             self.next_run = time.time()
 
-    def run(self):
+    def run(self) -> None:
         self.api.start()
         self.next_run = time.time()
         super(Patroni, self).run()
 
-    def _run_cycle(self):
+    def _run_cycle(self) -> None:
         logger.info(self.ha.run_cycle())
 
         if self.dcs.cluster and self.dcs.cluster.config and self.dcs.cluster.config.data \
                 and self.config.set_dynamic_configuration(self.dcs.cluster.config):
             self.reload_config()
 
         if self.postgresql.role != 'uninitialized':
             self.config.save_cache()
 
         self.schedule_next_run()
 
-    def _shutdown(self):
+    def _shutdown(self) -> None:
         try:
             self.api.shutdown()
         except Exception:
             logger.exception('Exception during RestApi.shutdown')
         try:
             self.ha.shutdown()
         except Exception:
             logger.exception('Exception during Ha.shutdown')
 
 
-def patroni_main():
+def patroni_main(configfile: str) -> None:
     from multiprocessing import freeze_support
-    from patroni.validator import schema
 
     freeze_support()
-    abstract_main(Patroni, schema)
+    abstract_main(Patroni, configfile)
 
 
-def main():
-    if os.getpid() != 1:
-        from patroni import check_psycopg
+def process_arguments() -> Namespace:
+    parser = get_base_arg_parser()
+    parser.add_argument('--validate-config', action='store_true', help='Run config validator and exit')
+    args = parser.parse_args()
+
+    if args.validate_config:
+        from patroni.validator import schema
+        from patroni.config import Config, ConfigParseError
+
+        try:
+            Config(args.configfile, validator=schema)
+            sys.exit()
+        except ConfigParseError as e:
+            sys.exit(e.value)
 
-        check_psycopg()
-        return patroni_main()
+    return args
+
+
+def main() -> None:
+    from patroni import check_psycopg
+
+    args = process_arguments()
+
+    check_psycopg()
+
+    if os.getpid() != 1:
+        return patroni_main(args.configfile)
 
     # Patroni started with PID=1, it looks like we are in the container
+    from types import FrameType
     pid = 0
 
     # Looks like we are in a docker, so we will act like init
-    def sigchld_handler(signo, stack_frame):
+    def sigchld_handler(signo: int, stack_frame: Optional[FrameType]) -> None:
         try:
             while True:
                 ret = os.waitpid(-1, os.WNOHANG)
                 if ret == (0, 0):
                     break
                 elif ret[0] != pid:
                     logger.info('Reaped pid=%s, exit status=%s', *ret)
         except OSError:
             pass
 
-    def passtochild(signo, stack_frame):
+    def passtochild(signo: int, stack_frame: Optional[FrameType]):
         if pid:
             os.kill(pid, signo)
 
     if os.name != 'nt':
         signal.signal(signal.SIGCHLD, sigchld_handler)
         signal.signal(signal.SIGHUP, passtochild)
         signal.signal(signal.SIGQUIT, passtochild)
         signal.signal(signal.SIGUSR1, passtochild)
         signal.signal(signal.SIGUSR2, passtochild)
     signal.signal(signal.SIGINT, passtochild)
     signal.signal(signal.SIGABRT, passtochild)
     signal.signal(signal.SIGTERM, passtochild)
 
     import multiprocessing
-    patroni = multiprocessing.Process(target=patroni_main)
+    patroni = multiprocessing.Process(target=patroni_main, args=(args.configfile,))
     patroni.start()
     pid = patroni.pid
     patroni.join()
 
 
 if __name__ == '__main__':
     main()
```

### Comparing `patroni-3.0.2/patroni/api.py` & `patroni-3.0.3/patroni/dcs/etcd3.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,952 +1,944 @@
+from __future__ import absolute_import
 import base64
-import hmac
+import etcd
 import json
 import logging
-import time
-import traceback
-import dateutil.parser
-import datetime
 import os
 import socket
 import sys
+import time
+import urllib3
 
-from http.server import BaseHTTPRequestHandler, HTTPServer
-from ipaddress import ip_address, ip_network
-from socketserver import ThreadingMixIn
-from threading import Thread
-from urllib.parse import urlparse, parse_qs
-
-from . import psycopg
-from .exceptions import PostgresConnectionException, PostgresException
-from .postgresql.misc import postgres_version_to_int
-from .utils import deep_compare, enable_keepalive, parse_bool, patch_config, Retry, \
-    RetryFailedError, parse_int, split_host_port, tzutc, uri, cluster_as_json
+from collections import defaultdict
+from enum import IntEnum
+from urllib3.exceptions import ReadTimeoutError, ProtocolError
+from threading import Condition, Lock, Thread
+from typing import Any, Callable, Collection, Dict, Iterator, List, Optional, Tuple, Type, TYPE_CHECKING, Union
+
+from . import ClusterConfig, Cluster, Failover, Leader, Member, SyncState,\
+    TimelineHistory, catch_return_false_exception, citus_group_re
+from .etcd import AbstractEtcdClientWithFailover, AbstractEtcd, catch_etcd_errors, DnsCachingResolver, Retry
+from ..exceptions import DCSError, PatroniException
+from ..utils import deep_compare, enable_keepalive, iter_response_objects, RetryFailedError, USER_AGENT
 
 logger = logging.getLogger(__name__)
 
 
-class RestApiHandler(BaseHTTPRequestHandler):
+class Etcd3Error(DCSError):
+    pass
 
-    def _write_status_code_only(self, status_code):
-        message = self.responses[status_code][0]
-        self.wfile.write('{0} {1} {2}\r\n\r\n'.format(self.protocol_version, status_code, message).encode('utf-8'))
-        self.log_request(status_code)
-
-    def _write_response(self, status_code, body, content_type='text/html', headers=None):
-        # TODO: try-catch ConnectionResetError: [Errno 104] Connection reset by peer and log it in DEBUG level
-        self.send_response(status_code)
-        headers = headers or {}
-        if content_type:
-            headers['Content-Type'] = content_type
-        for name, value in headers.items():
-            self.send_header(name, value)
-        for name, value in self.server.http_extra_headers.items():
-            self.send_header(name, value)
-        self.end_headers()
-        self.wfile.write(body.encode('utf-8'))
-
-    def _write_json_response(self, status_code, response):
-        self._write_response(status_code, json.dumps(response, default=str), content_type='application/json')
-
-    def check_access(func):
-        """Decorator function to check the source ip, authorization header. or client certificates
-
-        Usage example:
-        @check_access
-        def do_PUT_foo():
-            pass
-        """
 
-        def wrapper(self, *args, **kwargs):
-            if self.server.check_access(self):
-                return func(self, *args, **kwargs)
+class UnsupportedEtcdVersion(PatroniException):
+    pass
 
-        return wrapper
 
-    def _write_status_response(self, status_code, response):
-        patroni = self.server.patroni
-        tags = patroni.ha.get_effective_tags()
-        if tags:
-            response['tags'] = tags
-        if patroni.postgresql.sysid:
-            response['database_system_identifier'] = patroni.postgresql.sysid
-        if patroni.postgresql.pending_restart:
-            response['pending_restart'] = True
-        response['patroni'] = {'version': patroni.version, 'scope': patroni.postgresql.scope}
-        if patroni.scheduled_restart and isinstance(patroni.scheduled_restart, dict):
-            response['scheduled_restart'] = patroni.scheduled_restart.copy()
-            del response['scheduled_restart']['postmaster_start_time']
-            response['scheduled_restart']['schedule'] = (response['scheduled_restart']['schedule']).isoformat()
-        if not patroni.ha.watchdog.is_healthy:
-            response['watchdog_failed'] = True
-        if patroni.ha.is_paused():
-            response['pause'] = True
-        qsize = patroni.logger.queue_size
-        if qsize > patroni.logger.NORMAL_LOG_QUEUE_SIZE:
-            response['logger_queue_size'] = qsize
-            lost = patroni.logger.records_lost
-            if lost:
-                response['logger_records_lost'] = lost
-        self._write_json_response(status_code, response)
-
-    def do_GET(self, write_status_code_only=False):
-        """Default method for processing all GET requests which can not be routed to other methods"""
-
-        path = '/primary' if self.path == '/' else self.path
-        response = self.get_postgresql_status()
-
-        patroni = self.server.patroni
-        cluster = patroni.dcs.cluster
-
-        leader_optime = cluster and cluster.last_lsn or 0
-        replayed_location = response.get('xlog', {}).get('replayed_location', 0)
-        max_replica_lag = parse_int(self.path_query.get('lag', [sys.maxsize])[0], 'B')
-        if max_replica_lag is None:
-            max_replica_lag = sys.maxsize
-        is_lagging = leader_optime and leader_optime > replayed_location + max_replica_lag
-
-        replica_status_code = 200 if not patroni.noloadbalance and not is_lagging and \
-            response.get('role') == 'replica' and response.get('state') == 'running' else 503
-
-        if not cluster and patroni.ha.is_paused():
-            leader_status_code = 200 if response.get('role') in ('master', 'primary', 'standby_leader') else 503
-            primary_status_code = 200 if response.get('role') in ('master', 'primary') else 503
-            standby_leader_status_code = 200 if response.get('role') == 'standby_leader' else 503
-        elif patroni.ha.is_leader():
-            leader_status_code = 200
-            if patroni.ha.is_standby_cluster():
-                primary_status_code = replica_status_code = 503
-                standby_leader_status_code = 200 if response.get('role') in ('replica', 'standby_leader') else 503
-            else:
-                primary_status_code = 200
-                standby_leader_status_code = 503
-        else:
-            leader_status_code = primary_status_code = standby_leader_status_code = 503
+# google.golang.org/grpc/codes
+class GRPCCode(IntEnum):
+    OK = 0
+    Canceled = 1
+    Unknown = 2
+    InvalidArgument = 3
+    DeadlineExceeded = 4
+    NotFound = 5
+    AlreadyExists = 6
+    PermissionDenied = 7
+    ResourceExhausted = 8
+    FailedPrecondition = 9
+    Aborted = 10
+    OutOfRange = 11
+    Unimplemented = 12
+    Internal = 13
+    Unavailable = 14
+    DataLoss = 15
+    Unauthenticated = 16
 
-        status_code = 503
 
-        ignore_tags = False
-        if 'standby_leader' in path or 'standby-leader' in path:
-            status_code = standby_leader_status_code
-            ignore_tags = True
-        elif 'leader' in path:
-            status_code = leader_status_code
-            ignore_tags = True
-        elif 'master' in path or 'primary' in path or 'read-write' in path:
-            status_code = primary_status_code
-            ignore_tags = True
-        elif 'replica' in path:
-            status_code = replica_status_code
-        elif 'read-only' in path and 'sync' not in path:
-            status_code = 200 if 200 in (primary_status_code, standby_leader_status_code) else replica_status_code
-        elif 'health' in path:
-            status_code = 200 if response.get('state') == 'running' else 503
-        elif cluster:  # dcs is available
-            is_synchronous = response.get('sync_standby')
-            if path in ('/sync', '/synchronous') and is_synchronous:
-                status_code = replica_status_code
-            elif path in ('/async', '/asynchronous') and not is_synchronous:
-                status_code = replica_status_code
-            elif path in ('/read-only-sync', '/read-only-synchronous'):
-                if 200 in (primary_status_code, standby_leader_status_code):
-                    status_code = 200
-                elif is_synchronous:
-                    status_code = replica_status_code
-
-        # check for user defined tags in query params
-        if not ignore_tags and status_code == 200:
-            qs_tag_prefix = "tag_"
-            for qs_key, qs_value in self.path_query.items():
-                if not qs_key.startswith(qs_tag_prefix):
-                    continue
-                qs_key = qs_key[len(qs_tag_prefix):]
-                qs_value = qs_value[0]
-                instance_tag_value = patroni.tags.get(qs_key)
-                # tag not registered for instance
-                if instance_tag_value is None:
-                    status_code = 503
-                    break
-                if not isinstance(instance_tag_value, str):
-                    instance_tag_value = str(instance_tag_value).lower()
-                if instance_tag_value != qs_value:
-                    status_code = 503
-                    break
+GRPCcodeToText: Dict[int, str] = {v: k for k, v in GRPCCode.__dict__['_member_map_'].items()}
 
-        if write_status_code_only:  # when haproxy sends OPTIONS request it reads only status code and nothing more
-            self._write_status_code_only(status_code)
-        else:
-            self._write_status_response(status_code, response)
 
-    def do_OPTIONS(self):
-        self.do_GET(write_status_code_only=True)
+class Etcd3Exception(etcd.EtcdException):
+    pass
 
-    def do_HEAD(self):
-        self.do_GET(write_status_code_only=True)
 
-    def do_GET_liveness(self):
-        patroni = self.server.patroni
-        is_primary = patroni.postgresql.role in ('master', 'primary') and patroni.postgresql.is_running()
-        # We can tolerate Patroni problems longer on the replica.
-        # On the primary the liveness probe most likely will start failing only after the leader key expired.
-        # It should not be a big problem because replicas will see that the primary is still alive via REST API call.
-        liveness_threshold = patroni.dcs.ttl * (1 if is_primary else 2)
-
-        # In maintenance mode (pause) we are fine if heartbeat loop stuck.
-        status_code = 200 if patroni.ha.is_paused() or patroni.next_run + liveness_threshold > time.time() else 503
-        self._write_status_code_only(status_code)
-
-    def do_GET_readiness(self):
-        patroni = self.server.patroni
-        if patroni.ha.is_leader():
-            status_code = 200
-        elif patroni.postgresql.state == 'running':
-            status_code = 200 if patroni.dcs.cluster else 503
-        else:
-            status_code = 503
-        self._write_status_code_only(status_code)
+class Etcd3ClientError(Etcd3Exception):
 
-    def do_GET_patroni(self):
-        response = self.get_postgresql_status(True)
-        self._write_status_response(200, response)
-
-    def do_GET_cluster(self):
-        cluster = self.server.patroni.dcs.get_cluster(True)
-        self._write_json_response(200, cluster_as_json(cluster))
-
-    def do_GET_history(self):
-        cluster = self.server.patroni.dcs.cluster or self.server.patroni.dcs.get_cluster()
-        self._write_json_response(200, cluster.history and cluster.history.lines or [])
-
-    def do_GET_config(self):
-        cluster = self.server.patroni.dcs.cluster or self.server.patroni.dcs.get_cluster()
-        if cluster.config:
-            self._write_json_response(200, cluster.config.data)
-        else:
-            self.send_error(502)
+    def __init__(self, code: Optional[int] = None, error: Optional[str] = None, status: Optional[int] = None) -> None:
+        if not hasattr(self, 'error'):
+            self.error = error and error.strip()
+        self.codeText = GRPCcodeToText.get(code) if code is not None else None
+        self.status = status
 
-    def do_GET_metrics(self):
-        postgres = self.get_postgresql_status(True)
-        patroni = self.server.patroni
-        epoch = datetime.datetime(1970, 1, 1, tzinfo=tzutc)
-
-        metrics = []
-
-        scope_label = '{{scope="{0}"}}'.format(patroni.postgresql.scope)
-        metrics.append("# HELP patroni_version Patroni semver without periods.")
-        metrics.append("# TYPE patroni_version gauge")
-        padded_semver = ''.join([x.zfill(2) for x in patroni.version.split('.')])  # 2.0.2 => 020002
-        metrics.append("patroni_version{0} {1}".format(scope_label, padded_semver))
-
-        metrics.append("# HELP patroni_postgres_running Value is 1 if Postgres is running, 0 otherwise.")
-        metrics.append("# TYPE patroni_postgres_running gauge")
-        metrics.append("patroni_postgres_running{0} {1}".format(scope_label, int(postgres['state'] == 'running')))
-
-        metrics.append("# HELP patroni_postmaster_start_time Epoch seconds since Postgres started.")
-        metrics.append("# TYPE patroni_postmaster_start_time gauge")
-        postmaster_start_time = postgres.get('postmaster_start_time')
-        postmaster_start_time = (postmaster_start_time - epoch).total_seconds() if postmaster_start_time else 0
-        metrics.append("patroni_postmaster_start_time{0} {1}".format(scope_label, postmaster_start_time))
-
-        metrics.append("# HELP patroni_master Value is 1 if this node is the leader, 0 otherwise.")
-        metrics.append("# TYPE patroni_master gauge")
-        metrics.append("patroni_master{0} {1}".format(scope_label, int(postgres['role'] in ('master', 'primary'))))
-
-        metrics.append("# HELP patroni_primary Value is 1 if this node is the leader, 0 otherwise.")
-        metrics.append("# TYPE patroni_primary gauge")
-        metrics.append("patroni_primary{0} {1}".format(scope_label, int(postgres['role'] in ('master', 'primary'))))
-
-        metrics.append("# HELP patroni_xlog_location Current location of the Postgres"
-                       " transaction log, 0 if this node is not the leader.")
-        metrics.append("# TYPE patroni_xlog_location counter")
-        metrics.append("patroni_xlog_location{0} {1}".format(scope_label, postgres.get('xlog', {}).get('location', 0)))
-
-        metrics.append("# HELP patroni_standby_leader Value is 1 if this node is the standby_leader, 0 otherwise.")
-        metrics.append("# TYPE patroni_standby_leader gauge")
-        metrics.append("patroni_standby_leader{0} {1}".format(scope_label, int(postgres['role'] == 'standby_leader')))
-
-        metrics.append("# HELP patroni_replica Value is 1 if this node is a replica, 0 otherwise.")
-        metrics.append("# TYPE patroni_replica gauge")
-        metrics.append("patroni_replica{0} {1}".format(scope_label, int(postgres['role'] == 'replica')))
-
-        metrics.append("# HELP patroni_sync_standby Value is 1 if this node is a sync standby replica, 0 otherwise.")
-        metrics.append("# TYPE patroni_sync_standby gauge")
-        metrics.append("patroni_sync_standby{0} {1}".format(scope_label, int(postgres.get('sync_standby', False))))
-
-        metrics.append("# HELP patroni_xlog_received_location Current location of the received"
-                       " Postgres transaction log, 0 if this node is not a replica.")
-        metrics.append("# TYPE patroni_xlog_received_location counter")
-        metrics.append("patroni_xlog_received_location{0} {1}"
-                       .format(scope_label, postgres.get('xlog', {}).get('received_location', 0)))
-
-        metrics.append("# HELP patroni_xlog_replayed_location Current location of the replayed"
-                       " Postgres transaction log, 0 if this node is not a replica.")
-        metrics.append("# TYPE patroni_xlog_replayed_location counter")
-        metrics.append("patroni_xlog_replayed_location{0} {1}"
-                       .format(scope_label, postgres.get('xlog', {}).get('replayed_location', 0)))
-
-        metrics.append("# HELP patroni_xlog_replayed_timestamp Current timestamp of the replayed"
-                       " Postgres transaction log, 0 if null.")
-        metrics.append("# TYPE patroni_xlog_replayed_timestamp gauge")
-        replayed_timestamp = postgres.get('xlog', {}).get('replayed_timestamp')
-        replayed_timestamp = (replayed_timestamp - epoch).total_seconds() if replayed_timestamp else 0
-        metrics.append("patroni_xlog_replayed_timestamp{0} {1}".format(scope_label, replayed_timestamp))
-
-        metrics.append("# HELP patroni_xlog_paused Value is 1 if the Postgres xlog is paused, 0 otherwise.")
-        metrics.append("# TYPE patroni_xlog_paused gauge")
-        metrics.append("patroni_xlog_paused{0} {1}"
-                       .format(scope_label, int(postgres.get('xlog', {}).get('paused', False) is True)))
-
-        metrics.append("# HELP patroni_postgres_server_version Version of Postgres (if running), 0 otherwise.")
-        metrics.append("# TYPE patroni_postgres_server_version gauge")
-        metrics.append("patroni_postgres_server_version {0} {1}".format(scope_label, postgres.get('server_version', 0)))
-
-        metrics.append("# HELP patroni_cluster_unlocked Value is 1 if the cluster is unlocked, 0 if locked.")
-        metrics.append("# TYPE patroni_cluster_unlocked gauge")
-        metrics.append("patroni_cluster_unlocked{0} {1}".format(scope_label, int(postgres.get('cluster_unlocked', 0))))
-
-        metrics.append("# HELP patroni_failsafe_mode_is_active Value is 1 if the cluster is unlocked, 0 if locked.")
-        metrics.append("# TYPE patroni_failsafe_mode_is_active gauge")
-        metrics.append("patroni_failsafe_mode_is_active{0} {1}"
-                       .format(scope_label, int(postgres.get('failsafe_mode_is_active', 0))))
-
-        metrics.append("# HELP patroni_postgres_timeline Postgres timeline of this node (if running), 0 otherwise.")
-        metrics.append("# TYPE patroni_postgres_timeline counter")
-        metrics.append("patroni_postgres_timeline{0} {1}".format(scope_label, postgres.get('timeline', 0)))
-
-        metrics.append("# HELP patroni_dcs_last_seen Epoch timestamp when DCS was last contacted successfully"
-                       " by Patroni.")
-        metrics.append("# TYPE patroni_dcs_last_seen gauge")
-        metrics.append("patroni_dcs_last_seen{0} {1}".format(scope_label, postgres.get('dcs_last_seen', 0)))
-
-        metrics.append("# HELP patroni_pending_restart Value is 1 if the node needs a restart, 0 otherwise.")
-        metrics.append("# TYPE patroni_pending_restart gauge")
-        metrics.append("patroni_pending_restart{0} {1}"
-                       .format(scope_label, int(patroni.postgresql.pending_restart)))
-
-        metrics.append("# HELP patroni_is_paused Value is 1 if auto failover is disabled, 0 otherwise.")
-        metrics.append("# TYPE patroni_is_paused gauge")
-        metrics.append("patroni_is_paused{0} {1}"
-                       .format(scope_label, int(patroni.ha.is_paused())))
-
-        self._write_response(200, '\n'.join(metrics)+'\n', content_type='text/plain')
-
-    def _read_json_content(self, body_is_optional=False):
-        if 'content-length' not in self.headers:
-            return self.send_error(411) if not body_is_optional else {}
-        try:
-            content_length = int(self.headers.get('content-length'))
-            if content_length == 0 and body_is_optional:
-                return {}
-            request = json.loads(self.rfile.read(content_length).decode('utf-8'))
-            if isinstance(request, dict) and (request or body_is_optional):
-                return request
-        except Exception:
-            logger.exception('Bad request')
-        self.send_error(400)
+    def __repr__(self) -> str:
+        return "<{0} error: '{1}', code: {2}>"\
+            .format(self.__class__.__name__, getattr(self, 'error', None), getattr(self, 'code', None))
 
-    @check_access
-    def do_PATCH_config(self):
-        request = self._read_json_content()
-        if request:
-            cluster = self.server.patroni.dcs.get_cluster(True)
-            if not (cluster.config and cluster.config.modify_index):
-                return self.send_error(503)
-            data = cluster.config.data.copy()
-            if patch_config(data, request):
-                value = json.dumps(data, separators=(',', ':'))
-                if not self.server.patroni.dcs.set_config_value(value, cluster.config.index):
-                    return self.send_error(409)
-            self.server.patroni.ha.wakeup()
-            self._write_json_response(200, data)
-
-    @check_access
-    def do_PUT_config(self):
-        request = self._read_json_content()
-        if request:
-            cluster = self.server.patroni.dcs.get_cluster()
-            if not deep_compare(request, cluster.config.data):
-                value = json.dumps(request, separators=(',', ':'))
-                if not self.server.patroni.dcs.set_config_value(value):
-                    return self.send_error(502)
-            self._write_json_response(200, request)
-
-    @check_access
-    def do_POST_reload(self):
-        self.server.patroni.sighup_handler()
-        self._write_response(202, 'reload scheduled')
-
-    def do_GET_failsafe(self):
-        failsafe = self.server.patroni.dcs.failsafe
-        if isinstance(failsafe, dict):
-            self._write_json_response(200, failsafe)
-        else:
-            self.send_error(502)
+    __str__ = __repr__
 
-    @check_access
-    def do_POST_failsafe(self):
-        if self.server.patroni.ha.is_failsafe_mode():
-            request = self._read_json_content()
-            if request:
-                message = self.server.patroni.ha.update_failsafe(request) or 'Accepted'
-                code = 200 if message == 'Accepted' else 500
-                self._write_response(code, message)
-        else:
-            self.send_error(502)
+    def as_dict(self) -> Dict[str, Any]:
+        return {'error': getattr(self, 'error', None), 'code': getattr(self, 'code', None),
+                'codeText': self.codeText, 'status': self.status}
 
-    @check_access
-    def do_POST_sigterm(self):
-        """Only for behave testing on windows"""
-
-        if os.name == 'nt' and os.getenv('BEHAVE_DEBUG'):
-            self.server.patroni.api_sigterm()
-        self._write_response(202, 'shutdown scheduled')
+    @classmethod
+    def get_subclasses(cls) -> Iterator[Type['Etcd3ClientError']]:
+        for subclass in cls.__subclasses__():
+            for subsubclass in subclass.get_subclasses():
+                yield subsubclass
+            yield subclass
 
-    @staticmethod
-    def parse_schedule(schedule, action):
-        """ parses the given schedule and validates at """
-        error = None
-        scheduled_at = None
-        try:
-            scheduled_at = dateutil.parser.parse(schedule)
-            if scheduled_at.tzinfo is None:
-                error = 'Timezone information is mandatory for the scheduled {0}'.format(action)
-                status_code = 400
-            elif scheduled_at < datetime.datetime.now(tzutc):
-                error = 'Cannot schedule {0} in the past'.format(action)
-                status_code = 422
-            else:
-                status_code = None
-        except (ValueError, TypeError):
-            logger.exception('Invalid scheduled %s time: %s', action, schedule)
-            error = 'Unable to parse scheduled timestamp. It should be in an unambiguous format, e.g. ISO 8601'
-            status_code = 422
-        return (status_code, error, scheduled_at)
-
-    @check_access
-    def do_POST_restart(self):
-        status_code = 500
-        data = 'restart failed'
-        request = self._read_json_content(body_is_optional=True)
-        cluster = self.server.patroni.dcs.get_cluster()
-        if request is None:
-            # failed to parse the json
-            return
-        if request:
-            logger.debug("received restart request: {0}".format(request))
-
-        if cluster.is_paused() and 'schedule' in request:
-            self._write_response(status_code, "Can't schedule restart in the paused state")
-            return
-
-        for k in request:
-            if k == 'schedule':
-                (_, data, request[k]) = self.parse_schedule(request[k], "restart")
-                if _:
-                    status_code = _
-                    break
-            elif k == 'role':
-                if request[k] not in ('master', 'primary', 'replica'):
-                    status_code = 400
-                    data = "PostgreSQL role should be either primary or replica"
-                    break
-            elif k == 'postgres_version':
-                try:
-                    postgres_version_to_int(request[k])
-                except PostgresException as e:
-                    status_code = 400
-                    data = e.value
-                    break
-            elif k == 'timeout':
-                request[k] = parse_int(request[k], 's')
-                if request[k] is None or request[k] <= 0:
-                    status_code = 400
-                    data = "Timeout should be a positive number of seconds"
-                    break
-            elif k != 'restart_pending':
-                status_code = 400
-                data = "Unknown filter for the scheduled restart: {0}".format(k)
-                break
+
+class Unknown(Etcd3ClientError):
+    code = GRPCCode.Unknown
+
+
+class InvalidArgument(Etcd3ClientError):
+    code = GRPCCode.InvalidArgument
+
+
+class DeadlineExceeded(Etcd3ClientError):
+    code = GRPCCode.DeadlineExceeded
+    error = "context deadline exceeded"
+
+
+class NotFound(Etcd3ClientError):
+    code = GRPCCode.NotFound
+
+
+class FailedPrecondition(Etcd3ClientError):
+    code = GRPCCode.FailedPrecondition
+
+
+class Unavailable(Etcd3ClientError):
+    code = GRPCCode.Unavailable
+
+
+# https://github.com/etcd-io/etcd/commits/main/api/v3rpc/rpctypes/error.go
+class LeaseNotFound(NotFound):
+    error = "etcdserver: requested lease not found"
+
+
+class UserEmpty(InvalidArgument):
+    error = "etcdserver: user name is empty"
+
+
+class AuthFailed(InvalidArgument):
+    error = "etcdserver: authentication failed, invalid user ID or password"
+
+
+class PermissionDenied(Etcd3ClientError):
+    code = GRPCCode.PermissionDenied
+    error = "etcdserver: permission denied"
+
+
+class AuthNotEnabled(FailedPrecondition):
+    error = "etcdserver: authentication is not enabled"
+
+
+class InvalidAuthToken(Etcd3ClientError):
+    code = GRPCCode.Unauthenticated
+    error = "etcdserver: invalid auth token"
+
+
+errStringToClientError = {getattr(s, 'error'): s for s in Etcd3ClientError.get_subclasses() if hasattr(s, 'error')}
+errCodeToClientError = {getattr(s, 'code'): s for s in Etcd3ClientError.__subclasses__()}
+
+
+def _raise_for_data(data: Union[bytes, str, Dict[str, Union[Any, Dict[str, Any]]]],
+                    status_code: Optional[int] = None) -> Etcd3ClientError:
+    try:
+        if TYPE_CHECKING:  # pragma: no cover
+            assert isinstance(data, dict)
+        data_error: Optional[Dict[str, Any]] = data.get('error') or data.get('Error')
+        if isinstance(data_error, dict):  # streaming response
+            status_code = data_error.get('http_code')
+            code: Optional[int] = data_error['grpc_code']
+            error: str = data_error['message']
         else:
-            if 'schedule' not in request:
-                try:
-                    status, data = self.server.patroni.ha.restart(request)
-                    status_code = 200 if status else 503
-                except Exception:
-                    logger.exception('Exception during restart')
-                    status_code = 400
+            data_code = data.get('code') or data.get('Code')
+            if TYPE_CHECKING:  # pragma: no cover
+                assert not isinstance(data_code, dict)
+            code = data_code
+            error = str(data_error)
+    except Exception:
+        error = str(data)
+        code = GRPCCode.Unknown
+    err = errStringToClientError.get(error) or errCodeToClientError.get(code) or Unknown
+    return err(code, error, status_code)
+
+
+def to_bytes(v: Union[str, bytes]) -> bytes:
+    return v if isinstance(v, bytes) else v.encode('utf-8')
+
+
+def prefix_range_end(v: str) -> bytes:
+    ret = bytearray(to_bytes(v))
+    for i in range(len(ret) - 1, -1, -1):
+        if ret[i] < 0xff:
+            ret[i] += 1
+            break
+    return bytes(ret)
+
+
+def base64_encode(v: Union[str, bytes]) -> str:
+    return base64.b64encode(to_bytes(v)).decode('utf-8')
+
+
+def base64_decode(v: str) -> str:
+    return base64.b64decode(v).decode('utf-8')
+
+
+def build_range_request(key: str, range_end: Union[bytes, str, None] = None) -> Dict[str, Any]:
+    fields = {'key': base64_encode(key)}
+    if range_end:
+        fields['range_end'] = base64_encode(range_end)
+    return fields
+
+
+def _handle_auth_errors(func: Callable[..., Any]) -> Any:
+    def wrapper(self: 'Etcd3Client', *args: Any, **kwargs: Any) -> Any:
+        return self.handle_auth_errors(func, *args, **kwargs)
+    return wrapper
+
+
+class Etcd3Client(AbstractEtcdClientWithFailover):
+
+    ERROR_CLS = Etcd3Error
+
+    def __init__(self, config: Dict[str, Any], dns_resolver: DnsCachingResolver, cache_ttl: int = 300) -> None:
+        self._token = None
+        self._cluster_version: Tuple[int] = tuple()
+        self.version_prefix = '/v3beta'
+        super(Etcd3Client, self).__init__(config, dns_resolver, cache_ttl)
+
+        try:
+            self.authenticate()
+        except AuthFailed as e:
+            logger.fatal('Etcd3 authentication failed: %r', e)
+            sys.exit(1)
+
+    def _get_headers(self) -> Dict[str, str]:
+        headers = urllib3.make_headers(user_agent=USER_AGENT)
+        if self._token and self._cluster_version >= (3, 3, 0):
+            headers['authorization'] = self._token
+        return headers
+
+    def _prepare_request(self, kwargs: Dict[str, Any], params: Optional[Dict[str, Any]] = None,
+                         method: Optional[str] = None) -> Callable[..., urllib3.response.HTTPResponse]:
+        if params is not None:
+            kwargs['body'] = json.dumps(params)
+            kwargs['headers']['Content-Type'] = 'application/json'
+        return self.http.urlopen
+
+    def _handle_server_response(self, response: urllib3.response.HTTPResponse) -> Dict[str, Any]:
+        data: Union[bytes, str] = response.data
+        try:
+            data = data.decode('utf-8')
+            ret: Dict[str, Any] = json.loads(data)
+            if response.status < 400:
+                return ret
+        except (TypeError, ValueError, UnicodeError) as e:
+            if response.status < 400:
+                raise etcd.EtcdException('Server response was not valid JSON: %r' % e)
+            ret = {}
+        raise _raise_for_data(ret or data, response.status)
+
+    def _ensure_version_prefix(self, base_uri: str, **kwargs: Any) -> None:
+        if self.version_prefix != '/v3':
+            response = self.http.urlopen(self._MGET, base_uri + '/version', **kwargs)
+            response = self._handle_server_response(response)
+
+            server_version_str = response['etcdserver']
+            server_version = tuple(int(x) for x in server_version_str.split('.'))
+            cluster_version_str = response['etcdcluster']
+            self._cluster_version = tuple(int(x) for x in cluster_version_str.split('.'))
+
+            if self._cluster_version < (3, 0) or server_version < (3, 0, 4):
+                raise UnsupportedEtcdVersion('Detected Etcd version {0} is lower than 3.0.4'.format(server_version_str))
+
+            if self._cluster_version < (3, 3):
+                if self.version_prefix != '/v3alpha':
+                    if self._cluster_version < (3, 1):
+                        logger.warning('Detected Etcd version %s is lower than 3.1.0, watches are not supported',
+                                       cluster_version_str)
+                    if self.username and self.password:
+                        logger.warning('Detected Etcd version %s is lower than 3.3.0, authentication is not supported',
+                                       cluster_version_str)
+                    self.version_prefix = '/v3alpha'
+            elif self._cluster_version < (3, 4):
+                self.version_prefix = '/v3beta'
             else:
-                if self.server.patroni.ha.schedule_future_restart(request):
-                    data = "Restart scheduled"
-                    status_code = 202
-                else:
-                    data = "Another restart is already scheduled"
-                    status_code = 409
-        self._write_response(status_code, data)
-
-    @check_access
-    def do_DELETE_restart(self):
-        if self.server.patroni.ha.delete_future_restart():
-            data = "scheduled restart deleted"
-            code = 200
+                self.version_prefix = '/v3'
+
+    def _prepare_get_members(self, etcd_nodes: int) -> Dict[str, Any]:
+        kwargs = self._prepare_common_parameters(etcd_nodes)
+        self._prepare_request(kwargs, {})
+        return kwargs
+
+    def _get_members(self, base_uri: str, **kwargs: Any) -> List[str]:
+        self._ensure_version_prefix(base_uri, **kwargs)
+        resp = self.http.urlopen(self._MPOST, base_uri + self.version_prefix + '/cluster/member/list', **kwargs)
+        members = self._handle_server_response(resp)['members']
+        return [url for member in members for url in member.get('clientURLs', [])]
+
+    def call_rpc(self, method: str, fields: Dict[str, Any], retry: Optional[Retry] = None) -> Dict[str, Any]:
+        fields['retry'] = retry
+        return self.api_execute(self.version_prefix + method, self._MPOST, fields)
+
+    def authenticate(self) -> bool:
+        if self._use_proxies and not self._cluster_version:
+            kwargs = self._prepare_common_parameters(1)
+            self._ensure_version_prefix(self._base_uri, **kwargs)
+        if not (self._cluster_version >= (3, 3) and self.username and self.password):
+            return False
+        logger.info('Trying to authenticate on Etcd...')
+        old_token, self._token = self._token, None
+        try:
+            response = self.call_rpc('/auth/authenticate', {'name': self.username, 'password': self.password})
+        except AuthNotEnabled:
+            logger.info('Etcd authentication is not enabled')
+            self._token = None
+        except Exception:
+            self._token = old_token
+            raise
         else:
-            data = "no restarts are scheduled"
-            code = 404
-        self._write_response(code, data)
-
-    @check_access
-    def do_DELETE_switchover(self):
-        failover = self.server.patroni.dcs.get_cluster().failover
-        if failover and failover.scheduled_at:
-            if not self.server.patroni.dcs.manual_failover('', '', index=failover.index):
-                return self.send_error(409)
+            self._token = response.get('token')
+        return old_token != self._token
+
+    def handle_auth_errors(self: 'Etcd3Client', func: Callable[..., Any], *args: Any, **kwargs: Any) -> Any:
+        def retry(ex: Exception) -> Any:
+            if self.username and self.password:
+                self.authenticate()
+                return func(self, *args, **kwargs)
             else:
-                data = "scheduled switchover deleted"
-                code = 200
-        else:
-            data = "no switchover is scheduled"
-            code = 404
-        self._write_response(code, data)
-
-    @check_access
-    def do_POST_reinitialize(self):
-        request = self._read_json_content(body_is_optional=True)
-
-        if request:
-            logger.debug('received reinitialize request: %s', request)
-
-        force = isinstance(request, dict) and parse_bool(request.get('force')) or False
-
-        data = self.server.patroni.ha.reinitialize(force)
-        if data is None:
-            status_code = 200
-            data = 'reinitialize started'
-        else:
-            status_code = 503
-        self._write_response(status_code, data)
+                logger.fatal('Username or password not set, authentication is not possible')
+                raise ex
 
-    def poll_failover_result(self, leader, candidate, action):
-        timeout = max(10, self.server.patroni.dcs.loop_wait)
-        for _ in range(0, timeout*2):
-            time.sleep(1)
+        try:
+            return func(self, *args, **kwargs)
+        except (UserEmpty, PermissionDenied) as e:  # no token provided
+            # PermissionDenied is raised on 3.0 and 3.1
+            if self._cluster_version < (3, 3) and (not isinstance(e, PermissionDenied)
+                                                   or self._cluster_version < (3, 2)):
+                raise UnsupportedEtcdVersion('Authentication is required by Etcd cluster but not '
+                                             'supported on version lower than 3.3.0. Cluster version: '
+                                             '{0}'.format('.'.join(map(str, self._cluster_version))))
+            return retry(e)
+        except InvalidAuthToken as e:
+            logger.error('Invalid auth token: %s', self._token)
+            return retry(e)
+
+    @_handle_auth_errors
+    def range(self, key: str, range_end: Union[bytes, str, None] = None,
+              retry: Optional[Retry] = None) -> Dict[str, Any]:
+        params = build_range_request(key, range_end)
+        params['serializable'] = True  # For better performance. We can tolerate stale reads.
+        return self.call_rpc('/kv/range', params, retry)
+
+    def prefix(self, key: str, retry: Optional[Retry] = None) -> Dict[str, Any]:
+        return self.range(key, prefix_range_end(key), retry)
+
+    @_handle_auth_errors
+    def lease_grant(self, ttl: int, retry: Optional[Retry] = None) -> str:
+        return self.call_rpc('/lease/grant', {'TTL': ttl}, retry)['ID']
+
+    def lease_keepalive(self, ID: str, retry: Optional[Retry] = None) -> Optional[str]:
+        return self.call_rpc('/lease/keepalive', {'ID': ID}, retry).get('result', {}).get('TTL')
+
+    def txn(self, compare: Dict[str, Any], success: Dict[str, Any],
+            failure: Optional[Dict[str, Any]] = None, retry: Optional[Retry] = None) -> Dict[str, Any]:
+        fields = {'compare': [compare], 'success': [success]}
+        if failure:
+            fields['failure'] = [failure]
+        ret = self.call_rpc('/kv/txn', fields, retry)
+        return ret if failure or ret.get('succeeded') else {}
+
+    @_handle_auth_errors
+    def put(self, key: str, value: str, lease: Optional[str] = None, create_revision: Optional[str] = None,
+            mod_revision: Optional[str] = None, retry: Optional[Retry] = None) -> Dict[str, Any]:
+        fields = {'key': base64_encode(key), 'value': base64_encode(value)}
+        if lease:
+            fields['lease'] = lease
+        if create_revision is not None:
+            compare = {'target': 'CREATE', 'create_revision': create_revision}
+        elif mod_revision is not None:
+            compare = {'target': 'MOD', 'mod_revision': mod_revision}
+        else:
+            return self.call_rpc('/kv/put', fields, retry)
+        compare['key'] = fields['key']
+        return self.txn(compare, {'request_put': fields}, retry=retry)
+
+    @_handle_auth_errors
+    def deleterange(self, key: str, range_end: Union[bytes, str, None] = None,
+                    mod_revision: Optional[str] = None, retry: Optional[Retry] = None) -> Dict[str, Any]:
+        fields = build_range_request(key, range_end)
+        if mod_revision is None:
+            return self.call_rpc('/kv/deleterange', fields, retry)
+        compare = {'target': 'MOD', 'mod_revision': mod_revision, 'key': fields['key']}
+        return self.txn(compare, {'request_delete_range': fields}, retry=retry)
+
+    def deleteprefix(self, key: str, retry: Optional[Retry] = None) -> Dict[str, Any]:
+        return self.deleterange(key, prefix_range_end(key), retry=retry)
+
+    def watchrange(self, key: str, range_end: Union[bytes, str, None] = None,
+                   start_revision: Optional[str] = None, filters: Optional[List[Dict[str, Any]]] = None,
+                   read_timeout: Optional[float] = None) -> urllib3.response.HTTPResponse:
+        """returns: response object"""
+        params = build_range_request(key, range_end)
+        if start_revision is not None:
+            params['start_revision'] = start_revision
+        params['filters'] = filters or []
+        kwargs = self._prepare_common_parameters(1, self.read_timeout)
+        request_executor = self._prepare_request(kwargs, {'create_request': params})
+        kwargs.update(timeout=urllib3.Timeout(connect=kwargs['timeout'], read=read_timeout), retries=0)
+        return request_executor(self._MPOST, self._base_uri + self.version_prefix + '/watch', **kwargs)
+
+    def watchprefix(self, key: str, start_revision: Optional[str] = None,
+                    filters: Optional[List[Dict[str, Any]]] = None,
+                    read_timeout: Optional[float] = None) -> urllib3.response.HTTPResponse:
+        return self.watchrange(key, prefix_range_end(key), start_revision, filters, read_timeout)
+
+
+class KVCache(Thread):
+
+    def __init__(self, dcs: 'Etcd3', client: 'PatroniEtcd3Client') -> None:
+        super(KVCache, self).__init__()
+        self.daemon = True
+        self._dcs = dcs
+        self._client = client
+        self.condition = Condition()
+        self._config_key = base64_encode(dcs.config_path)
+        self._leader_key = base64_encode(dcs.leader_path)
+        self._optime_key = base64_encode(dcs.leader_optime_path)
+        self._status_key = base64_encode(dcs.status_path)
+        self._name = base64_encode(getattr(dcs, '_name'))  # pyright
+        self._is_ready = False
+        self._response = None
+        self._response_lock = Lock()
+        self._object_cache = {}
+        self._object_cache_lock = Lock()
+        self.start()
+
+    def set(self, value: Dict[str, Any], overwrite: bool = False) -> Tuple[bool, Optional[Dict[str, Any]]]:
+        with self._object_cache_lock:
+            name = value['key']
+            old_value = self._object_cache.get(name)
+            ret = not old_value or int(old_value['mod_revision']) < int(value['mod_revision'])
+            if ret or overwrite and old_value and old_value['mod_revision'] == value['mod_revision']:
+                self._object_cache[name] = value
+        return ret, old_value
+
+    def delete(self, name: str, mod_revision: str) -> Tuple[bool, Optional[Dict[str, Any]]]:
+        with self._object_cache_lock:
+            old_value = self._object_cache.get(name)
+            ret = old_value and int(old_value['mod_revision']) < int(mod_revision)
+            if ret:
+                del self._object_cache[name]
+        return bool(not old_value or ret), old_value
+
+    def copy(self) -> List[Dict[str, Any]]:
+        with self._object_cache_lock:
+            return [v.copy() for v in self._object_cache.values()]
+
+    def get(self, name: str) -> Optional[Dict[str, Any]]:
+        with self._object_cache_lock:
+            return self._object_cache.get(name)
+
+    def _process_event(self, event: Dict[str, Any]) -> None:
+        kv = event['kv']
+        key = kv['key']
+        if event.get('type') == 'DELETE':
+            success, old_value = self.delete(key, kv['mod_revision'])
+        else:
+            success, old_value = self.set(kv, True)
+
+        if success:
+            old_value = old_value and old_value.get('value')
+            new_value = kv.get('value')
+
+            value_changed = old_value != new_value and \
+                (key == self._leader_key or key in (self._optime_key, self._status_key) and new_value is not None
+                 or key == self._config_key and old_value is not None and new_value is not None)
+
+            if value_changed:
+                logger.debug('%s changed from %s to %s', key, old_value, new_value)
+
+            # We also want to wake up HA loop on replicas if leader optime (or status key) was updated
+            if value_changed and (key not in (self._optime_key, self._status_key)
+                                  or (self.get(self._leader_key) or {}).get('value') != self._name):
+                self._dcs.event.set()
+
+    def _process_message(self, message: Dict[str, Any]) -> None:
+        logger.debug('Received message: %s', message)
+        if 'error' in message:
+            raise _raise_for_data(message)
+        events: List[Dict[str, Any]] = message.get('result', {}).get('events', [])
+        for event in events:
+            self._process_event(event)
+
+    @staticmethod
+    def _finish_response(response: urllib3.response.HTTPResponse) -> None:
+        try:
+            response.close()
+        finally:
+            response.release_conn()
+
+    def _do_watch(self, revision: str) -> None:
+        with self._response_lock:
+            self._response = None
+        # We do most of requests with timeouts. The only exception /watch requests to Etcd v3.
+        # In order to interrupt the /watch request we do socket.shutdown() from the main thread,
+        # which doesn't work on Windows. Therefore we want to use the last resort, `read_timeout`.
+        # Setting it to TTL will help to partially mitigate the problem.
+        # Setting it to lower value is not nice because for idling clusters it will increase
+        # the numbers of interrupts and reconnects.
+        read_timeout = self._dcs.ttl if os.name == 'nt' else None
+        response = self._client.watchprefix(self._dcs.cluster_prefix, revision, read_timeout=read_timeout)
+        with self._response_lock:
+            if self._response is None:
+                self._response = response
+
+        if not self._response:
+            return self._finish_response(response)
+
+        for message in iter_response_objects(response):
+            self._process_message(message)
+
+    def _build_cache(self) -> None:
+        result = self._dcs.retry(self._client.prefix, self._dcs.cluster_prefix)
+        with self._object_cache_lock:
+            self._object_cache = {node['key']: node for node in result.get('kvs', [])}
+        with self.condition:
+            self._is_ready = True
+            self.condition.notify()
+
+        try:
+            self._do_watch(result['header']['revision'])
+        except Exception as e:
+            # Following exceptions are expected on Windows because the /watch request  is done with `read_timeout`
+            if not (os.name == 'nt' and isinstance(e, (ReadTimeoutError, ProtocolError))):
+                logger.error('watchprefix failed: %r', e)
+        finally:
+            with self.condition:
+                self._is_ready = False
+            with self._response_lock:
+                response, self._response = self._response, None
+            if isinstance(response, urllib3.response.HTTPResponse):
+                self._finish_response(response)
+
+    def run(self) -> None:
+        while True:
             try:
-                cluster = self.server.patroni.dcs.get_cluster()
-                if not cluster.is_unlocked() and cluster.leader.name != leader:
-                    if not candidate or candidate == cluster.leader.name:
-                        return 200, 'Successfully {0}ed over to "{1}"'.format(action[:-4], cluster.leader.name)
-                    else:
-                        return 200, '{0}ed over to "{1}" instead of "{2}"'.format(action[:-4].title(),
-                                                                                  cluster.leader.name, candidate)
-                if not cluster.failover:
-                    return 503, action.title() + ' failed'
+                self._build_cache()
             except Exception as e:
-                logger.debug('Exception occurred during polling %s result: %s', action, e)
-        return 503, action.title() + ' status unknown'
+                logger.error('KVCache.run %r', e)
+                time.sleep(1)
 
-    def is_failover_possible(self, cluster, leader, candidate, action):
-        if leader and (not cluster.leader or cluster.leader.name != leader):
-            return 'leader name does not match'
-        if candidate:
-            if action == 'switchover' and cluster.is_synchronous_mode() and candidate not in cluster.sync.members:
-                return 'candidate name does not match with sync_standby'
-            members = [m for m in cluster.members if m.name == candidate]
-            if not members:
-                return 'candidate does not exists'
-        elif cluster.is_synchronous_mode():
-            members = [m for m in cluster.members if m.name in cluster.sync.members]
-            if not members:
-                return action + ' is not possible: can not find sync_standby'
-        else:
-            members = [m for m in cluster.members if m.name != cluster.leader.name and m.api_url]
-            if not members:
-                return action + ' is not possible: cluster does not have members except leader'
-        for st in self.server.patroni.ha.fetch_nodes_statuses(members):
-            if st.failover_limitation() is None:
-                return None
-        return action + ' is not possible: no good candidates have been found'
-
-    @check_access
-    def do_POST_failover(self, action='failover'):
-        request = self._read_json_content()
-        (status_code, data) = (400, '')
-        if not request:
-            return
-
-        leader = request.get('leader')
-        candidate = request.get('candidate') or request.get('member')
-        scheduled_at = request.get('scheduled_at')
-        cluster = self.server.patroni.dcs.get_cluster()
-
-        logger.info("received %s request with leader=%s candidate=%s scheduled_at=%s",
-                    action, leader, candidate, scheduled_at)
-
-        if action == 'failover' and not candidate:
-            data = 'Failover could be performed only to a specific candidate'
-        elif action == 'switchover' and not leader:
-            data = 'Switchover could be performed only from a specific leader'
-
-        if not data and scheduled_at:
-            if not leader:
-                data = 'Scheduled {0} is possible only from a specific leader'.format(action)
-            if not data and cluster.is_paused():
-                data = "Can't schedule {0} in the paused state".format(action)
-            if not data:
-                (status_code, data, scheduled_at) = self.parse_schedule(scheduled_at, action)
-
-        if not data and cluster.is_paused() and not candidate:
-            data = action.title() + ' is possible only to a specific candidate in a paused state'
-
-        if not data and not scheduled_at:
-            data = self.is_failover_possible(cluster, leader, candidate, action)
-            if data:
-                status_code = 412
-
-        if not data:
-            if self.server.patroni.dcs.manual_failover(leader, candidate, scheduled_at=scheduled_at):
-                self.server.patroni.ha.wakeup()
-                if scheduled_at:
-                    data = action.title() + ' scheduled'
-                    status_code = 202
-                else:
-                    status_code, data = self.poll_failover_result(cluster.leader and cluster.leader.name,
-                                                                  candidate, action)
+    def kill_stream(self) -> None:
+        sock = None
+        with self._response_lock:
+            if isinstance(self._response, urllib3.response.HTTPResponse):
+                try:
+                    sock = self._response.connection.sock if self._response.connection else None
+                except Exception:
+                    sock = None
             else:
-                data = 'failed to write {0} key into DCS'.format(action)
-                status_code = 503
-        self._write_response(status_code, data)
-
-    def do_POST_switchover(self):
-        self.do_POST_failover(action='switchover')
-
-    @check_access
-    def do_POST_citus(self):
-        request = self._read_json_content()
-        if not request:
-            return
-
-        patroni = self.server.patroni
-        if patroni.postgresql.citus_handler.is_coordinator() and patroni.ha.is_leader():
-            cluster = patroni.dcs.get_cluster(True)
-            patroni.postgresql.citus_handler.handle_event(cluster, request)
-        self._write_response(200, 'OK')
-
-    def parse_request(self):
-        """Override parse_request method to enrich basic functionality of `BaseHTTPRequestHandler` class
-
-        Original class can only invoke do_GET, do_POST, do_PUT, etc method implementations if they are defined.
-        But we would like to have at least some simple routing mechanism, i.e.:
-        GET /uri1/part2 request should invoke `do_GET_uri1()`
-        POST /other should invoke `do_POST_other()`
+                self._response = False
+        if sock:
+            try:
+                sock.shutdown(socket.SHUT_RDWR)
+                sock.close()
+            except Exception as e:
+                logger.debug('Error on socket.shutdown: %r', e)
+
+    def is_ready(self) -> bool:
+        """Must be called only when holding the lock on `condition`"""
+        return self._is_ready
 
-        If the `do_<REQUEST_METHOD>_<first_part_url>` method does not exists we'll fallback to original behavior."""
 
-        ret = BaseHTTPRequestHandler.parse_request(self)
+class PatroniEtcd3Client(Etcd3Client):
+
+    def __init__(self, *args: Any, **kwargs: Any) -> None:
+        self._kv_cache = None
+        super(PatroniEtcd3Client, self).__init__(*args, **kwargs)
+
+    def configure(self, etcd3: 'Etcd3') -> None:
+        self._etcd3 = etcd3
+
+    def start_watcher(self) -> None:
+        if self._cluster_version >= (3, 1):
+            self._kv_cache = KVCache(self._etcd3, self)
+
+    def _restart_watcher(self) -> None:
+        if self._kv_cache:
+            self._kv_cache.kill_stream()
+
+    def set_base_uri(self, value: str) -> None:
+        super(PatroniEtcd3Client, self).set_base_uri(value)
+        self._restart_watcher()
+
+    def authenticate(self) -> bool:
+        ret = super(PatroniEtcd3Client, self).authenticate()
         if ret:
-            urlpath = urlparse(self.path)
-            self.path = urlpath.path
-            self.path_query = parse_qs(urlpath.query) or {}
-            mname = self.path.lstrip('/').split('/')[0]
-            mname = self.command + ('_' + mname if mname else '')
-            if hasattr(self, 'do_' + mname):
-                self.command = mname
+            self._restart_watcher()
         return ret
 
-    def query(self, sql, *params, **kwargs):
-        if not kwargs.get('retry', False):
-            return self.server.query(sql, *params)
-        retry = Retry(delay=1, retry_exceptions=PostgresConnectionException)
-        return retry(self.server.query, sql, *params)
-
-    def get_postgresql_status(self, retry=False):
-        postgresql = self.server.patroni.postgresql
-        try:
-            cluster = self.server.patroni.dcs.cluster
-
-            if postgresql.state not in ('running', 'restarting', 'starting'):
-                raise RetryFailedError('')
-            stmt = ("SELECT " + postgresql.POSTMASTER_START_TIME + ", " + postgresql.TL_LSN + ","
-                    " pg_catalog.pg_last_xact_replay_timestamp(),"
-                    " pg_catalog.array_to_json(pg_catalog.array_agg(pg_catalog.row_to_json(ri))) "
-                    "FROM (SELECT (SELECT rolname FROM pg_catalog.pg_authid WHERE oid = usesysid) AS usename,"
-                    " application_name, client_addr, w.state, sync_state, sync_priority"
-                    " FROM pg_catalog.pg_stat_get_wal_senders() w, pg_catalog.pg_stat_get_activity(pid)) AS ri")
-
-            row = self.query(stmt.format(postgresql.wal_name, postgresql.lsn_name), retry=retry)[0]
-
-            result = {
-                'state': postgresql.state,
-                'postmaster_start_time': row[0],
-                'role': 'replica' if row[1] == 0 else 'master',
-                'server_version': postgresql.server_version,
-                'xlog': ({
-                    'received_location': row[4] or row[3],
-                    'replayed_location': row[3],
-                    'replayed_timestamp': row[6],
-                    'paused': row[5]} if row[1] == 0 else {
-                    'location': row[2]
-                })
-            }
-
-            if result['role'] == 'replica' and self.server.patroni.ha.is_standby_cluster():
-                result['role'] = postgresql.role
-
-            if result['role'] == 'replica' and cluster and cluster.is_synchronous_mode()\
-                    and cluster.sync and postgresql.name in cluster.sync.members:
-                result['sync_standby'] = True
+    def _wait_cache(self, timeout: float) -> None:
+        stop_time = time.time() + timeout
+        while self._kv_cache and not self._kv_cache.is_ready():
+            timeout = stop_time - time.time()
+            if timeout <= 0:
+                raise RetryFailedError('Exceeded retry deadline')
+            self._kv_cache.condition.wait(timeout)
+
+    def get_cluster(self, path: str) -> List[Dict[str, Any]]:
+        if self._kv_cache and path.startswith(self._etcd3.cluster_prefix):
+            with self._kv_cache.condition:
+                self._wait_cache(self.read_timeout)
+                ret = self._kv_cache.copy()
+        else:
+            ret = self._etcd3.retry(self.prefix, path).get('kvs', [])
+        for node in ret:
+            node.update({'key': base64_decode(node['key']),
+                         'value': base64_decode(node.get('value', '')),
+                         'lease': node.get('lease')})
+        return ret
 
-            if row[1] > 0:
-                result['timeline'] = row[1]
-            else:
-                leader_timeline = None if not cluster or cluster.is_unlocked() else cluster.leader.timeline
-                result['timeline'] = postgresql.replica_cached_timeline(leader_timeline)
+    def call_rpc(self, method: str, fields: Dict[str, Any], retry: Optional[Retry] = None) -> Dict[str, Any]:
+        ret = super(PatroniEtcd3Client, self).call_rpc(method, fields, retry)
 
-            if row[7]:
-                result['replication'] = row[7]
+        if self._kv_cache:
+            value = delete = None
+            # For the 'failure' case we only support a second (nested) transaction that attempts to
+            # update/delete the same keys. Anything more complex than that we don't need and therefore it doesn't
+            # make sense to write a universal response analyzer and we can just check expected JSON path.
+            if method == '/kv/txn'\
+                    and (ret.get('succeeded') or 'failure' in fields and 'request_txn' in fields['failure'][0]
+                         and ret.get('responses', [{'response_txn': {'succeeded': False}}])[0]
+                         .get('response_txn', {}).get('succeeded')):
+                on_success = fields['success'][0]
+                value = on_success.get('request_put')
+                delete = on_success.get('request_delete_range')
+            elif method == '/kv/put' and ret:
+                value = fields
+            elif method == '/kv/deleterange' and ret:
+                delete = fields
+
+            if value:
+                value['mod_revision'] = ret['header']['revision']
+                self._kv_cache.set(value)
+            elif delete and 'range_end' not in delete:
+                self._kv_cache.delete(delete['key'], ret['header']['revision'])
 
-        except (psycopg.Error, RetryFailedError, PostgresConnectionException):
-            state = postgresql.state
-            if state == 'running':
-                logger.exception('get_postgresql_status')
-                state = 'unknown'
-            result = {'state': state, 'role': postgresql.role}
-
-        if not cluster or cluster.is_unlocked():
-            result['cluster_unlocked'] = True
-        if self.server.patroni.ha.failsafe_is_active():
-            result['failsafe_mode_is_active'] = True
-        result['dcs_last_seen'] = self.server.patroni.dcs.last_seen
-        return result
-
-    def handle_one_request(self):
-        self.__start_time = time.time()
-        BaseHTTPRequestHandler.handle_one_request(self)
-
-    def log_message(self, fmt, *args):
-        latency = 1000.0 * (time.time() - self.__start_time)
-        logger.debug("API thread: %s - - %s latency: %0.3f ms", self.client_address[0], fmt % args, latency)
-
-
-class RestApiServer(ThreadingMixIn, HTTPServer, Thread):
-    # On 3.7+ the `ThreadingMixIn` gathers all non-daemon worker threads in order to join on them at server close.
-    daemon_threads = True  # Make worker threads "fire and forget" to prevent a memory leak.
-
-    def __init__(self, patroni, config):
-        self.patroni = patroni
-        self.__listen = None
-        self.__ssl_options = None
-        self.__ssl_serial_number = None
-        self._received_new_cert = False
-        self.reload_config(config)
-        self.daemon = True
+        return ret
+
+
+class Etcd3(AbstractEtcd):
+
+    def __init__(self, config: Dict[str, Any]) -> None:
+        super(Etcd3, self).__init__(config, PatroniEtcd3Client, (DeadlineExceeded, Unavailable, FailedPrecondition))
+        self.__do_not_watch = False
+        self._lease = None
+        self._last_lease_refresh = 0
+
+        self._client.configure(self)
+        if not self._ctl:
+            self._client.start_watcher()
+            self.create_lease()
+
+    @property
+    def _client(self) -> PatroniEtcd3Client:
+        if TYPE_CHECKING:  # pragma: no cover
+            assert isinstance(self._abstract_client, PatroniEtcd3Client)
+        return self._abstract_client
+
+    def set_socket_options(self, sock: socket.socket,
+                           socket_options: Optional[Collection[Tuple[int, int, int]]]) -> None:
+        if TYPE_CHECKING:  # pragma: no cover
+            assert self._retry.deadline is not None
+        enable_keepalive(sock, self.ttl, int(self.loop_wait + self._retry.deadline))
+
+    def set_ttl(self, ttl: int) -> Optional[bool]:
+        self.__do_not_watch = super(Etcd3, self).set_ttl(ttl)
+        if self.__do_not_watch:
+            self._lease = None
+        return None
+
+    def _do_refresh_lease(self, force: bool = False, retry: Optional[Retry] = None) -> bool:
+        if not force and self._lease and self._last_lease_refresh + self._loop_wait > time.time():
+            return False
+
+        if self._lease and not self._client.lease_keepalive(self._lease, retry):
+            self._lease = None
+
+        ret = not self._lease
+        if ret:
+            self._lease = self._client.lease_grant(self._ttl, retry)
+
+        self._last_lease_refresh = time.time()
+        return ret
 
-    def query(self, sql, *params):
-        cursor = None
+    def refresh_lease(self) -> bool:
         try:
-            with self.patroni.postgresql.connection().cursor() as cursor:
-                cursor.execute(sql, params)
-                return [r for r in cursor]
-        except psycopg.Error as e:
-            if cursor and cursor.connection.closed == 0:
-                raise e
-            raise PostgresConnectionException('connection problems')
+            return self.retry(self._do_refresh_lease)
+        except (Etcd3ClientError, RetryFailedError):
+            logger.exception('refresh_lease')
+        raise Etcd3Error('Failed to keepalive/grant lease')
 
-    @staticmethod
-    def _set_fd_cloexec(fd):
-        if os.name != 'nt':
-            import fcntl
-            flags = fcntl.fcntl(fd, fcntl.F_GETFD)
-            fcntl.fcntl(fd, fcntl.F_SETFD, flags | fcntl.FD_CLOEXEC)
-
-    def check_basic_auth_key(self, key):
-        return hmac.compare_digest(self.__auth_key, key.encode('utf-8'))
-
-    def check_auth_header(self, auth_header):
-        if self.__auth_key:
-            if auth_header is None:
-                return 'no auth header received'
-            if not auth_header.startswith('Basic ') or not self.check_basic_auth_key(auth_header[6:]):
-                return 'not authenticated'
+    def create_lease(self) -> None:
+        while not self._lease:
+            try:
+                self.refresh_lease()
+            except Etcd3Error:
+                logger.info('waiting on etcd')
+                time.sleep(5)
+
+    @property
+    def cluster_prefix(self) -> str:
+        return self._base_path + '/' if self.is_citus_coordinator() else self.client_path('')
 
     @staticmethod
-    def __resolve_ips(host, port):
+    def member(node: Dict[str, str]) -> Member:
+        return Member.from_node(node['mod_revision'], os.path.basename(node['key']), node['lease'], node['value'])
+
+    def _cluster_from_nodes(self, nodes: Dict[str, Any]) -> Cluster:
+        # get initialize flag
+        initialize = nodes.get(self._INITIALIZE)
+        initialize = initialize and initialize['value']
+
+        # get global dynamic configuration
+        config = nodes.get(self._CONFIG)
+        config = config and ClusterConfig.from_node(config['mod_revision'], config['value'])
+
+        # get timeline history
+        history = nodes.get(self._HISTORY)
+        history = history and TimelineHistory.from_node(history['mod_revision'], history['value'])
+
+        # get last know leader lsn and slots
+        status = nodes.get(self._STATUS)
+        if status:
+            try:
+                status = json.loads(status['value'])
+                last_lsn = status.get(self._OPTIME)
+                slots = status.get('slots')
+            except Exception:
+                slots = last_lsn = None
+        else:
+            last_lsn = nodes.get(self._LEADER_OPTIME)
+            last_lsn = last_lsn and last_lsn['value']
+            slots = None
+
+        try:
+            last_lsn = int(last_lsn or '')
+        except Exception:
+            last_lsn = 0
+
+        # get list of members
+        members = [self.member(n) for k, n in nodes.items() if k.startswith(self._MEMBERS) and k.count('/') == 1]
+
+        # get leader
+        leader = nodes.get(self._LEADER)
+        if not self._ctl and leader and leader['value'] == self._name and self._lease != leader.get('lease'):
+            logger.warning('I am the leader but not owner of the lease')
+
+        if leader:
+            member = Member(-1, leader['value'], None, {})
+            member = ([m for m in members if m.name == leader['value']] or [member])[0]
+            leader = Leader(leader['mod_revision'], leader['lease'], member)
+
+        # failover key
+        failover = nodes.get(self._FAILOVER)
+        if failover:
+            failover = Failover.from_node(failover['mod_revision'], failover['value'])
+
+        # get synchronization state
+        sync = nodes.get(self._SYNC)
+        sync = SyncState.from_node(sync and sync['mod_revision'], sync and sync['value'])
+
+        # get failsafe topology
+        failsafe = nodes.get(self._FAILSAFE)
+        try:
+            failsafe = json.loads(failsafe['value']) if failsafe else None
+        except Exception:
+            failsafe = None
+
+        return Cluster(initialize, config, leader, last_lsn, members, failover, sync, history, slots, failsafe)
+
+    def _cluster_loader(self, path: str) -> Cluster:
+        nodes = {node['key'][len(path):]: node
+                 for node in self._client.get_cluster(path)
+                 if node['key'].startswith(path)}
+        return self._cluster_from_nodes(nodes)
+
+    def _citus_cluster_loader(self, path: str) -> Dict[int, Cluster]:
+        clusters: Dict[int, Dict[str, Dict[str, Any]]] = defaultdict(dict)
+        path = self._base_path + '/'
+        for node in self._client.get_cluster(path):
+            key = node['key'][len(path):].split('/', 1)
+            if len(key) == 2 and citus_group_re.match(key[0]):
+                clusters[int(key[0])][key[1]] = node
+        return {group: self._cluster_from_nodes(nodes) for group, nodes in clusters.items()}
+
+    def _load_cluster(
+            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]
+    ) -> Union[Cluster, Dict[int, Cluster]]:
+        cluster = None
         try:
-            for _, _, _, _, sa in socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM, socket.IPPROTO_TCP):
-                yield ip_network(sa[0], False)
+            cluster = loader(path)
+        except UnsupportedEtcdVersion:
+            raise
         except Exception as e:
-            logger.error('Failed to resolve %s: %r', host, e)
+            self._handle_exception(e, 'get_cluster', raise_ex=Etcd3Error('Etcd is not responding properly'))
+        self._has_failed = False
+        if TYPE_CHECKING:  # pragma: no cover
+            assert cluster is not None
+        return cluster
 
-    def __members_ips(self):
-        cluster = self.patroni.dcs.cluster
-        if self.__allowlist_include_members and cluster:
-            for cluster in [cluster] + list(cluster.workers.values()):
-                for member in cluster.members:
-                    if member.api_url:
-                        try:
-                            r = urlparse(member.api_url)
-                            host = r.hostname
-                            port = r.port or (443 if r.scheme == 'https' else 80)
-                            for ip in self.__resolve_ips(host, port):
-                                yield ip
-                        except Exception as e:
-                            logger.debug('Failed to parse url %s: %r', member.api_url, e)
-
-    def check_access(self, rh):
-        if self.__allowlist or self.__allowlist_include_members:
-            incoming_ip = ip_address(rh.client_address[0])
-            if not any(incoming_ip in net for net in self.__allowlist + tuple(self.__members_ips())):
-                return rh._write_response(403, 'Access is denied')
-
-        if not hasattr(rh.request, 'getpeercert') or not rh.request.getpeercert():  # valid client cert isn't present
-            if self.__protocol == 'https' and self.__ssl_options.get('verify_client') in ('required', 'optional'):
-                return rh._write_response(403, 'client certificate required')
-
-        reason = self.check_auth_header(rh.headers.get('Authorization'))
-        if reason:
-            headers = {'WWW-Authenticate': 'Basic realm="' + self.patroni.__class__.__name__ + '"'}
-            return rh._write_response(401, reason, headers=headers)
-        return True
+    @catch_etcd_errors
+    def touch_member(self, data: Dict[str, Any]) -> bool:
+        try:
+            self.refresh_lease()
+        except Etcd3Error:
+            return False
 
-    @staticmethod
-    def __has_dual_stack():
-        if hasattr(socket, 'AF_INET6') and hasattr(socket, 'IPPROTO_IPV6') and hasattr(socket, 'IPV6_V6ONLY'):
-            sock = None
-            try:
-                sock = socket.socket(socket.AF_INET6, socket.SOCK_STREAM)
-                sock.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, False)
-                return True
-            except socket.error as e:
-                logger.debug('Error when working with ipv6 socket: %s', e)
-            finally:
-                if sock:
-                    sock.close()
+        cluster = self.cluster
+        member = cluster and cluster.get_member(self._name, fallback_to_leader=False)
+
+        if member and member.session == self._lease and deep_compare(data, member.data):
+            return True
+
+        value = json.dumps(data, separators=(',', ':'))
+        try:
+            return bool(self._client.put(self.member_path, value, self._lease))
+        except LeaseNotFound:
+            self._lease = None
+            logger.error('Our lease disappeared from Etcd, can not "touch_member"')
         return False
 
-    def __httpserver_init(self, host, port):
-        dual_stack = self.__has_dual_stack()
-        if host in ('', '*'):
-            host = None
-
-        info = socket.getaddrinfo(host, port, socket.AF_UNSPEC, socket.SOCK_STREAM, 0, socket.AI_PASSIVE)
-        # in case dual stack is not supported we want IPv4 to be preferred over IPv6
-        info.sort(key=lambda x: x[0] == socket.AF_INET, reverse=not dual_stack)
-
-        self.address_family = info[0][0]
-        try:
-            HTTPServer.__init__(self, info[0][-1][:2], RestApiHandler)
-        except socket.error:
-            logger.error(
-                    "Couldn't start a service on '%s:%s', please check your `restapi.listen` configuration", host, port)
-            raise
+    @catch_etcd_errors
+    def take_leader(self) -> bool:
+        return self.retry(self._client.put, self.leader_path, self._name, self._lease)
+
+    def _do_attempt_to_acquire_leader(self, retry: Retry) -> bool:
+        def _retry(*args: Any, **kwargs: Any) -> Any:
+            kwargs['retry'] = retry
+            return retry(*args, **kwargs)
 
-    def __initialize(self, listen, ssl_options):
         try:
-            host, port = split_host_port(listen, None)
-        except Exception:
-            raise ValueError('Invalid "restapi" config: expected <HOST>:<PORT> for "listen", but got "{0}"'
-                             .format(listen))
+            return _retry(self._client.put, self.leader_path, self._name, self._lease, create_revision='0')
+        except LeaseNotFound:
+            logger.error('Our lease disappeared from Etcd. Will try to get a new one and retry attempt')
+            self._lease = None
+            retry.ensure_deadline(0)
 
-        reloading_config = self.__listen is not None  # changing config in runtime
-        if reloading_config:
-            self.shutdown()
-            # Rely on ThreadingMixIn.server_close() to have all requests terminate before we continue
-            self.server_close()
-
-        self.__listen = listen
-        self.__ssl_options = ssl_options
-        self._received_new_cert = False  # reset to False after reload_config()
-
-        self.__httpserver_init(host, port)
-        Thread.__init__(self, target=self.serve_forever)
-        self._set_fd_cloexec(self.socket)
-
-        # wrap socket with ssl if 'certfile' is defined in a config.yaml
-        # Sometime it's also needed to pass reference to a 'keyfile'.
-        self.__protocol = 'https' if ssl_options.get('certfile') else 'http'
-        if self.__protocol == 'https':
-            import ssl
-            ctx = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH, cafile=ssl_options.get('cafile'))
-            if ssl_options.get('ciphers'):
-                ctx.set_ciphers(ssl_options['ciphers'])
-            ctx.load_cert_chain(certfile=ssl_options['certfile'], keyfile=ssl_options.get('keyfile'),
-                                password=ssl_options.get('keyfile_password'))
-            verify_client = ssl_options.get('verify_client')
-            if verify_client:
-                modes = {'none': ssl.CERT_NONE, 'optional': ssl.CERT_OPTIONAL, 'required': ssl.CERT_REQUIRED}
-                if verify_client in modes:
-                    ctx.verify_mode = modes[verify_client]
-                else:
-                    logger.error('Bad value in the "restapi.verify_client": %s', verify_client)
-            self.__ssl_serial_number = self.get_certificate_serial_number()
-            self.socket = ctx.wrap_socket(self.socket, server_side=True, do_handshake_on_connect=False)
-        if reloading_config:
-            self.start()
-
-    def process_request_thread(self, request, client_address):
-        enable_keepalive(request, 10, 3)
-        if hasattr(request, 'context'):  # SSLSocket
-            request.do_handshake()
-        super(RestApiServer, self).process_request_thread(request, client_address)
+            _retry(self._do_refresh_lease)
 
-    def shutdown_request(self, request):
-        if hasattr(request, 'context'):  # SSLSocket
-            try:
-                request.unwrap()
-            except Exception as e:
-                logger.debug('Failed to shutdown SSL connection: %r', e)
-        super(RestApiServer, self).shutdown_request(request)
+            retry.ensure_deadline(1, Etcd3Error('_do_attempt_to_acquire_leader timeout'))
 
-    def get_certificate_serial_number(self):
-        if self.__ssl_options.get('certfile'):
-            import ssl
-            try:
-                crt = ssl._ssl._test_decode_cert(self.__ssl_options['certfile'])
-                return crt.get('serialNumber')
-            except ssl.SSLError as e:
-                logger.error('Failed to get serial number from certificate %s: %r', self.__ssl_options['certfile'], e)
-
-    def reload_local_certificate(self):
-        if self.__protocol == 'https':
-            on_disk_cert_serial_number = self.get_certificate_serial_number()
-            if on_disk_cert_serial_number != self.__ssl_serial_number:
-                self._received_new_cert = True
-                self.__ssl_serial_number = on_disk_cert_serial_number
-                return True
-
-    def _build_allowlist(self, value):
-        if isinstance(value, list):
-            for v in value:
-                if '/' in v:  # netmask
-                    try:
-                        yield ip_network(v, False)
-                    except Exception as e:
-                        logger.error('Invalid value "%s" in the allowlist: %r', v, e)
-                else:  # ip or hostname, try to resolve it
-                    for ip in self.__resolve_ips(v, 8080):
-                        yield ip
-
-    def reload_config(self, config):
-        if 'listen' not in config:  # changing config in runtime
-            raise ValueError('Can not find "restapi.listen" config')
-
-        self.__allowlist = tuple(self._build_allowlist(config.get('allowlist')))
-        self.__allowlist_include_members = config.get('allowlist_include_members')
-
-        ssl_options = {n: config[n] for n in ('certfile', 'keyfile', 'keyfile_password',
-                                              'cafile', 'ciphers') if n in config}
-
-        self.http_extra_headers = config.get('http_extra_headers') or {}
-        self.http_extra_headers.update((config.get('https_extra_headers') or {}) if ssl_options.get('certfile') else {})
+            return _retry(self._client.put, self.leader_path, self._name, self._lease, create_revision='0')
 
-        if isinstance(config.get('verify_client'), str):
-            ssl_options['verify_client'] = config['verify_client'].lower()
+    @catch_return_false_exception
+    def attempt_to_acquire_leader(self) -> bool:
+        retry = self._retry.copy()
 
-        if self.__listen != config['listen'] or self.__ssl_options != ssl_options or self._received_new_cert:
-            self.__initialize(config['listen'], ssl_options)
+        def _retry(*args: Any, **kwargs: Any) -> Any:
+            kwargs['retry'] = retry
+            return retry(*args, **kwargs)
 
-        self.__auth_key = base64.b64encode(config['auth'].encode('utf-8')) if 'auth' in config else None
-        self.connection_string = uri(self.__protocol, config.get('connect_address') or self.__listen, 'patroni')
+        self._run_and_handle_exceptions(self._do_refresh_lease, retry=_retry)
 
-    @staticmethod
-    def handle_error(request, client_address):
-        logger.warning('Exception happened during processing of request from %s:%s',
-                       client_address[0], client_address[1])
-        logger.warning(traceback.format_exc())
+        retry.ensure_deadline(1, Etcd3Error('attempt_to_acquire_leader timeout'))
+
+        ret = self._run_and_handle_exceptions(self._do_attempt_to_acquire_leader, retry, retry=None)
+        if not ret:
+            logger.info('Could not take out TTL lock')
+        return ret
+
+    @catch_etcd_errors
+    def set_failover_value(self, value: str, version: Optional[str] = None) -> bool:
+        return bool(self._client.put(self.failover_path, value, mod_revision=version))
+
+    @catch_etcd_errors
+    def set_config_value(self, value: str, version: Optional[str] = None) -> bool:
+        return bool(self._client.put(self.config_path, value, mod_revision=version))
+
+    @catch_etcd_errors
+    def _write_leader_optime(self, last_lsn: str) -> bool:
+        return bool(self._client.put(self.leader_optime_path, last_lsn))
+
+    @catch_etcd_errors
+    def _write_status(self, value: str) -> bool:
+        return bool(self._client.put(self.status_path, value))
+
+    @catch_etcd_errors
+    def _write_failsafe(self, value: str) -> bool:
+        return bool(self._client.put(self.failsafe_path, value))
+
+    @catch_return_false_exception
+    def _update_leader(self, leader: Leader) -> bool:
+        retry = self._retry.copy()
+
+        def _retry(*args: Any, **kwargs: Any) -> Any:
+            kwargs['retry'] = retry
+            return retry(*args, **kwargs)
+
+        self._run_and_handle_exceptions(self._do_refresh_lease, True, retry=_retry)
+
+        if self._lease and leader.session != self._lease:
+            retry.ensure_deadline(1, Etcd3Error('update_leader timeout'))
+
+            fields = {'key': base64_encode(self.leader_path), 'value': base64_encode(self._name), 'lease': self._lease}
+            # First we try to update lease on existing leader key "hoping" that we still owning it
+            compare1 = {'key': fields['key'], 'target': 'VALUE', 'value': fields['value']}
+            request_put = {'request_put': fields}
+            # If the first comparison failed we will try to create the new leader key in a transaction
+            compare2 = {'key': fields['key'], 'target': 'CREATE', 'create_revision': '0'}
+            request_txn = {'request_txn': {'compare': [compare2], 'success': [request_put]}}
+            ret = self._run_and_handle_exceptions(self._client.txn, compare1, request_put, request_txn, retry=_retry)
+            return ret.get('succeeded', False)\
+                or ret.get('responses', [{}])[0].get('response_txn', {}).get('succeeded', False)
+        return bool(self._lease)
+
+    @catch_etcd_errors
+    def initialize(self, create_new: bool = True, sysid: str = ""):
+        return self.retry(self._client.put, self.initialize_path, sysid, create_revision='0' if create_new else None)
+
+    @catch_etcd_errors
+    def _delete_leader(self) -> bool:
+        cluster = self.cluster
+        if cluster and isinstance(cluster.leader, Leader) and cluster.leader.name == self._name:
+            return self._client.deleterange(self.leader_path, mod_revision=cluster.leader.version)
+        return True
+
+    @catch_etcd_errors
+    def cancel_initialization(self) -> bool:
+        return self.retry(self._client.deleterange, self.initialize_path)
+
+    @catch_etcd_errors
+    def delete_cluster(self) -> bool:
+        return self.retry(self._client.deleteprefix, self.client_path(''))
+
+    @catch_etcd_errors
+    def set_history_value(self, value: str) -> bool:
+        return bool(self._client.put(self.history_path, value))
+
+    @catch_etcd_errors
+    def set_sync_state_value(self, value: str, version: Optional[str] = None) -> Union[str, bool]:
+        return self.retry(self._client.put, self.sync_path, value, mod_revision=version)\
+            .get('header', {}).get('revision', False)
+
+    @catch_etcd_errors
+    def delete_sync_state(self, version: Optional[str] = None) -> bool:
+        return self.retry(self._client.deleterange, self.sync_path, mod_revision=version)
+
+    def watch(self, leader_version: Optional[str], timeout: float) -> bool:
+        if self.__do_not_watch:
+            self.__do_not_watch = False
+            return True
+
+        # We want to give a bit more time to non-leader nodes to synchronize HA loops
+        if leader_version:
+            timeout += 0.5
+
+        try:
+            return super(Etcd3, self).watch(None, timeout)
+        finally:
+            self.event.clear()
```

### Comparing `patroni-3.0.2/patroni/config.py` & `patroni-3.0.3/patroni/config.py`

 * *Files 22% similar despite different names*

```diff
@@ -3,19 +3,22 @@
 import os
 import shutil
 import tempfile
 import yaml
 
 from collections import defaultdict
 from copy import deepcopy
-from patroni import PATRONI_ENV_PREFIX
-from patroni.exceptions import ConfigParseError
-from patroni.dcs import ClusterConfig
-from patroni.postgresql.config import CaseInsensitiveDict, ConfigHandler
-from patroni.utils import deep_compare, parse_bool, parse_int, patch_config
+from typing import Any, Callable, Collection, Dict, List, Optional, Union, TYPE_CHECKING
+
+from . import PATRONI_ENV_PREFIX
+from .collections import CaseInsensitiveDict
+from .dcs import ClusterConfig, Cluster
+from .exceptions import ConfigParseError
+from .postgresql.config import ConfigHandler
+from .utils import deep_compare, parse_bool, parse_int, patch_config
 
 logger = logging.getLogger(__name__)
 
 _AUTH_ALLOWED_PARAMETERS = (
     'username',
     'password',
     'sslmode',
@@ -26,17 +29,137 @@
     'sslcrl',
     'sslcrldir',
     'gssencmode',
     'channel_binding'
 )
 
 
-def default_validator(conf):
+def default_validator(conf: Dict[str, Any]) -> List[str]:
     if not conf:
         raise ConfigParseError("Config is empty.")
+    return []
+
+
+class GlobalConfig(object):
+
+    """A class that wrapps global configuration and provides convinient methods to access/check values.
+
+    It is instantiated by calling :func:`Config.global_config` method which picks either a
+    configuration from provided :class:`Cluster` object (the most up-to-date) or from the
+    local cache if :class::`ClusterConfig` is not initialized or doesn't have a valid config.
+    """
+
+    def __init__(self, config: Dict[str, Any]) -> None:
+        """Initialize :class:`GlobalConfig` object.
+
+        :param config: current configuration either from
+                       :class:`ClusterConfig` or from :class:`Config.dynamic_configuration`
+        """
+        self.__config = config
+
+    def get(self, name: str) -> Any:
+        """Gets global configuration value by name.
+
+        :param name: parameter name
+        :returns: configuration value or `None` if it is missing
+        """
+        return self.__config.get(name)
+
+    def check_mode(self, mode: str) -> bool:
+        """Checks whether the certain parameter is enabled.
+
+        :param mode: parameter name could be: synchronous_mode, failsafe_mode, pause, check_timeline, and so on
+        :returns: `True` if *mode* is enabled in the global configuration.
+        """
+        return bool(parse_bool(self.__config.get(mode)))
+
+    @property
+    def is_paused(self) -> bool:
+        """:returns: `True` if cluster is in maintenance mode."""
+        return self.check_mode('pause')
+
+    @property
+    def is_synchronous_mode(self) -> bool:
+        """:returns: `True` if synchronous replication is requested."""
+        return self.check_mode('synchronous_mode')
+
+    @property
+    def is_synchronous_mode_strict(self) -> bool:
+        """:returns: `True` if at least one synchronous node is required."""
+        return self.check_mode('synchronous_mode_strict')
+
+    def get_standby_cluster_config(self) -> Union[Dict[str, Any], Any]:
+        """:returns: "standby_cluster" configuration."""
+        return deepcopy(self.get('standby_cluster'))
+
+    @property
+    def is_standby_cluster(self) -> bool:
+        """:returns: `True` if global configuration has a valid "standby_cluster" section."""
+        config = self.get_standby_cluster_config()
+        return isinstance(config, dict) and\
+            bool(config.get('host') or config.get('port') or config.get('restore_command'))
+
+    def get_int(self, name: str, default: int = 0) -> int:
+        """Gets current value from the global configuration and trying to return it as int.
+
+        :param name: name of the parameter
+        :param default: default value if *name* is not in the configuration or invalid
+        :returns: currently configured value from the global configuration or *default* if it is not set or invalid.
+        """
+        ret = parse_int(self.get(name))
+        return default if ret is None else ret
+
+    @property
+    def min_synchronous_nodes(self) -> int:
+        """:returns: the minimal number of synchronous nodes based on whether strict mode is requested or not."""
+        return 1 if self.is_synchronous_mode_strict else 0
+
+    @property
+    def synchronous_node_count(self) -> int:
+        """:returns: currently configured value from the global configuration or 1 if it is not set or invalid."""
+        return max(self.get_int('synchronous_node_count', 1), self.min_synchronous_nodes)
+
+    @property
+    def maximum_lag_on_failover(self) -> int:
+        """:returns: currently configured value from the global configuration or 1048576 if it is not set or invalid."""
+        return self.get_int('maximum_lag_on_failover', 1048576)
+
+    @property
+    def maximum_lag_on_syncnode(self) -> int:
+        """:returns: currently configured value from the global configuration or -1 if it is not set or invalid."""
+        return self.get_int('maximum_lag_on_syncnode', -1)
+
+    @property
+    def primary_start_timeout(self) -> int:
+        """:returns: currently configured value from the global configuration or 300 if it is not set or invalid."""
+        default = 300
+        return self.get_int('primary_start_timeout', default)\
+            if 'primary_start_timeout' in self.__config else self.get_int('master_start_timeout', default)
+
+    @property
+    def primary_stop_timeout(self) -> int:
+        """:returns: currently configured value from the global configuration or 300 if it is not set or invalid."""
+        default = 0
+        return self.get_int('primary_stop_timeout', default)\
+            if 'primary_stop_timeout' in self.__config else self.get_int('master_stop_timeout', default)
+
+
+def get_global_config(cluster: Union[Cluster, None], default: Optional[Dict[str, Any]] = None) -> GlobalConfig:
+    """Instantiates :class:`GlobalConfig` based on the input.
+
+    :param cluster: the currently known cluster state from DCS
+    :param default: default configuration, which will be used if there is no valid *cluster.config*
+    :returns: :class:`GlobalConfig` object
+    """
+    # Try to protect from the case when DCS was wiped out
+    if cluster and cluster.config and cluster.config.modify_version:
+        config = cluster.config.data
+    else:
+        config = default or {}
+    return GlobalConfig(deepcopy(config))
 
 
 class Config(object):
     """
     This class is responsible for:
 
       1) Building and giving access to `effective_configuration` from:
@@ -54,120 +177,105 @@
       4) Mimicking some of the `dict` interfaces to make it possible
          to work with it as with the old `config` object.
     """
 
     PATRONI_CONFIG_VARIABLE = PATRONI_ENV_PREFIX + 'CONFIGURATION'
 
     __CACHE_FILENAME = 'patroni.dynamic.json'
-    __REMAP_KEYS = {
-        'master_start_timeout': 'primary_start_timeout',
-        'master_stop_timeout': 'primary_stop_timeout'
-    }
-    __DEFAULT_CONFIG = {
+    __DEFAULT_CONFIG: Dict[str, Any] = {
         'ttl': 30, 'loop_wait': 10, 'retry_timeout': 10,
-        'maximum_lag_on_failover': 1048576,
-        'maximum_lag_on_syncnode': -1,
-        'check_timeline': False,
-        'primary_start_timeout': 300,
-        'primary_stop_timeout': 0,
-        'synchronous_mode': False,
-        'synchronous_mode_strict': False,
-        'synchronous_node_count': 1,
-        'failsafe_mode': False,
         'standby_cluster': {
             'create_replica_methods': '',
             'host': '',
             'port': '',
             'primary_slot_name': '',
             'restore_command': '',
             'archive_cleanup_command': '',
             'recovery_min_apply_delay': ''
         },
         'postgresql': {
             'bin_dir': '',
             'use_slots': True,
             'parameters': CaseInsensitiveDict({p: v[0] for p, v in ConfigHandler.CMDLINE_OPTIONS.items()
                                                if p not in ('wal_keep_segments', 'wal_keep_size')})
-        },
-        'watchdog': {
-            'mode': 'automatic',
         }
     }
 
-    def __init__(self, configfile, validator=default_validator):
-        self._modify_index = -1
+    def __init__(self, configfile: str,
+                 validator: Optional[Callable[[Dict[str, Any]], List[str]]] = default_validator) -> None:
+        self._modify_version = -1
         self._dynamic_configuration = {}
 
         self.__environment_configuration = self._build_environment_configuration()
 
         # Patroni reads the configuration from the command-line argument if it exists, otherwise from the environment
-        self._config_file = configfile and os.path.exists(configfile) and configfile
+        self._config_file = configfile if configfile and os.path.exists(configfile) else None
         if self._config_file:
             self._local_configuration = self._load_config_file()
         else:
             config_env = os.environ.pop(self.PATRONI_CONFIG_VARIABLE, None)
             self._local_configuration = config_env and yaml.safe_load(config_env) or self.__environment_configuration
+
         if validator:
             errors = validator(self._local_configuration)
             if errors:
                 raise ConfigParseError("\n".join(errors))
 
         self.__effective_configuration = self._build_effective_configuration({}, self._local_configuration)
         self._data_dir = self.__effective_configuration.get('postgresql', {}).get('data_dir', "")
         self._cache_file = os.path.join(self._data_dir, self.__CACHE_FILENAME)
         self._load_cache()
         self._cache_needs_saving = False
 
     @property
-    def config_file(self):
+    def config_file(self) -> Union[str, None]:
         return self._config_file
 
     @property
-    def dynamic_configuration(self):
+    def dynamic_configuration(self) -> Dict[str, Any]:
         return deepcopy(self._dynamic_configuration)
 
-    def check_mode(self, mode):
-        return bool(parse_bool(self._dynamic_configuration.get(mode)))
-
-    def _load_config_path(self, path):
+    def _load_config_path(self, path: str) -> Dict[str, Any]:
         """
         If path is a file, loads the yml file pointed to by path.
         If path is a directory, loads all yml files in that directory in alphabetical order
         """
         if os.path.isfile(path):
             files = [path]
         elif os.path.isdir(path):
             files = [os.path.join(path, f) for f in sorted(os.listdir(path))
                      if (f.endswith('.yml') or f.endswith('.yaml')) and os.path.isfile(os.path.join(path, f))]
         else:
             logger.error('config path %s is neither directory nor file', path)
             raise ConfigParseError('invalid config path')
 
-        overall_config = {}
+        overall_config: Dict[str, Any] = {}
         for fname in files:
             with open(fname) as f:
                 config = yaml.safe_load(f)
                 patch_config(overall_config, config)
         return overall_config
 
-    def _load_config_file(self):
+    def _load_config_file(self) -> Dict[str, Any]:
         """Loads config.yaml from filesystem and applies some values which were set via ENV"""
+        if TYPE_CHECKING:  # pragma: no cover
+            assert self._config_file is not None
         config = self._load_config_path(self._config_file)
         patch_config(config, self.__environment_configuration)
         return config
 
-    def _load_cache(self):
+    def _load_cache(self) -> None:
         if os.path.isfile(self._cache_file):
             try:
                 with open(self._cache_file) as f:
                     self.set_dynamic_configuration(json.load(f))
             except Exception:
                 logger.exception('Exception when loading file: %s', self._cache_file)
 
-    def save_cache(self):
+    def save_cache(self) -> None:
         if self._cache_needs_saving:
             tmpfile = fd = None
             try:
                 (fd, tmpfile) = tempfile.mkstemp(prefix=self.__CACHE_FILENAME, dir=self._data_dir)
                 with os.fdopen(fd, 'w') as f:
                     fd = None
                     json.dump(self.dynamic_configuration, f)
@@ -183,128 +291,131 @@
                 if tmpfile and os.path.exists(tmpfile):
                     try:
                         os.remove(tmpfile)
                     except Exception:
                         logger.error('Can not remove temporary file %s', tmpfile)
 
     # configuration could be either ClusterConfig or dict
-    def set_dynamic_configuration(self, configuration):
+    def set_dynamic_configuration(self, configuration: Union[ClusterConfig, Dict[str, Any]]) -> bool:
         if isinstance(configuration, ClusterConfig):
-            if self._modify_index == configuration.modify_index:
-                return False  # If the index didn't changed there is nothing to do
-            self._modify_index = configuration.modify_index
+            if self._modify_version == configuration.modify_version:
+                return False  # If the version didn't changed there is nothing to do
+            self._modify_version = configuration.modify_version
             configuration = configuration.data
 
         if not deep_compare(self._dynamic_configuration, configuration):
             try:
                 self.__effective_configuration = self._build_effective_configuration(configuration,
                                                                                      self._local_configuration)
                 self._dynamic_configuration = configuration
                 self._cache_needs_saving = True
                 return True
             except Exception:
                 logger.exception('Exception when setting dynamic_configuration')
+        return False
 
-    def reload_local_configuration(self):
+    def reload_local_configuration(self) -> Optional[bool]:
         if self.config_file:
             try:
                 configuration = self._load_config_file()
                 if not deep_compare(self._local_configuration, configuration):
                     new_configuration = self._build_effective_configuration(self._dynamic_configuration, configuration)
                     self._local_configuration = configuration
                     self.__effective_configuration = new_configuration
                     return True
                 else:
                     logger.info('No local configuration items changed.')
             except Exception:
                 logger.exception('Exception when reloading local configuration from %s', self.config_file)
 
     @staticmethod
-    def _process_postgresql_parameters(parameters, is_local=False):
+    def _process_postgresql_parameters(parameters: Dict[str, Any], is_local: bool = False) -> Dict[str, Any]:
         return {name: value for name, value in (parameters or {}).items()
-                if name not in ConfigHandler.CMDLINE_OPTIONS or
-                not is_local and ConfigHandler.CMDLINE_OPTIONS[name][1](value)}
+                if name not in ConfigHandler.CMDLINE_OPTIONS
+                or not is_local and ConfigHandler.CMDLINE_OPTIONS[name][1](value)}
 
-    def _safe_copy_dynamic_configuration(self, dynamic_configuration):
+    def _safe_copy_dynamic_configuration(self, dynamic_configuration: Dict[str, Any]) -> Dict[str, Any]:
         config = deepcopy(self.__DEFAULT_CONFIG)
 
         for name, value in dynamic_configuration.items():
-            # allow copying master_start_timeout->primary_start_timeout when the latter isn't in dynamic_configuration
-            if name in self.__REMAP_KEYS and self.__REMAP_KEYS[name] not in dynamic_configuration:
-                name = self.__REMAP_KEYS[name]
             if name == 'postgresql':
                 for name, value in (value or {}).items():
                     if name == 'parameters':
                         config['postgresql'][name].update(self._process_postgresql_parameters(value))
                     elif name not in ('connect_address', 'proxy_address', 'listen',
                                       'config_dir', 'data_dir', 'pgpass', 'authentication'):
                         config['postgresql'][name] = deepcopy(value)
             elif name == 'standby_cluster':
                 for name, value in (value or {}).items():
                     if name in self.__DEFAULT_CONFIG['standby_cluster']:
                         config['standby_cluster'][name] = deepcopy(value)
             elif name in config:  # only variables present in __DEFAULT_CONFIG allowed to be overridden from DCS
-                if name in ('synchronous_mode', 'synchronous_mode_strict', 'failsafe_mode'):
-                    config[name] = value
-                else:
-                    config[name] = int(value)
+                config[name] = int(value)
         return config
 
     @staticmethod
-    def _build_environment_configuration():
-        ret = defaultdict(dict)
+    def _build_environment_configuration() -> Dict[str, Any]:
+        ret: Dict[str, Any] = defaultdict(dict)
 
-        def _popenv(name):
+        def _popenv(name: str) -> Union[str, None]:
             return os.environ.pop(PATRONI_ENV_PREFIX + name.upper(), None)
 
         for param in ('name', 'namespace', 'scope'):
             value = _popenv(param)
             if value:
                 ret[param] = value
 
-        def _fix_log_env(name, oldname):
+        def _fix_log_env(name: str, oldname: str) -> None:
             value = _popenv(oldname)
             name = PATRONI_ENV_PREFIX + 'LOG_' + name.upper()
             if value and name not in os.environ:
                 os.environ[name] = value
 
         for name, oldname in (('level', 'loglevel'), ('format', 'logformat'), ('dateformat', 'log_datefmt')):
             _fix_log_env(name, oldname)
 
-        def _set_section_values(section, params):
+        def _set_section_values(section: str, params: List[str]) -> None:
             for param in params:
                 value = _popenv(section + '_' + param)
                 if value:
                     ret[section][param] = value
 
         _set_section_values('restapi', ['listen', 'connect_address', 'certfile', 'keyfile', 'keyfile_password',
                                         'cafile', 'ciphers', 'verify_client', 'http_extra_headers',
-                                        'https_extra_headers', 'allowlist', 'allowlist_include_members'])
+                                        'https_extra_headers', 'allowlist', 'allowlist_include_members',
+                                        'request_queue_size'])
         _set_section_values('ctl', ['insecure', 'cacert', 'certfile', 'keyfile', 'keyfile_password'])
         _set_section_values('postgresql', ['listen', 'connect_address', 'proxy_address',
                                            'config_dir', 'data_dir', 'pgpass', 'bin_dir'])
         _set_section_values('log', ['level', 'traceback_level', 'format', 'dateformat', 'max_queue_size',
                                     'dir', 'file_size', 'file_num', 'loggers'])
         _set_section_values('raft', ['data_dir', 'self_addr', 'partner_addrs', 'password', 'bind_addr'])
 
+        for binary in ('pg_ctl', 'initdb', 'pg_controldata', 'pg_basebackup', 'postgres', 'pg_isready', 'pg_rewind'):
+            value = _popenv('POSTGRESQL_BIN_' + binary)
+            if value:
+                ret['postgresql'].setdefault('bin_name', {})[binary] = value
+
         for first, second in (('restapi', 'allowlist_include_members'), ('ctl', 'insecure')):
             value = ret.get(first, {}).pop(second, None)
             if value:
                 value = parse_bool(value)
                 if value is not None:
                     ret[first][second] = value
 
-        for second in ('max_queue_size', 'file_size', 'file_num'):
-            value = ret.get('log', {}).pop(second, None)
-            if value:
-                value = parse_int(value)
-                if value is not None:
-                    ret['log'][second] = value
+        for first, params in (('restapi', ('request_queue_size',)),
+                              ('log', ('max_queue_size', 'file_size', 'file_num'))):
+            for second in params:
+                value = ret.get(first, {}).pop(second, None)
+                if value:
+                    value = parse_int(value)
+                    if value is not None:
+                        ret[first][second] = value
 
-        def _parse_list(value):
+        def _parse_list(value: str) -> Union[List[str], None]:
             if not (value.strip().startswith('-') or '[' in value):
                 value = '[{0}]'.format(value)
             try:
                 return yaml.safe_load(value)
             except Exception:
                 logger.exception('Exception when parsing list %s', value)
                 return None
@@ -312,15 +423,15 @@
         for first, second in (('raft', 'partner_addrs'), ('restapi', 'allowlist')):
             value = ret.get(first, {}).pop(second, None)
             if value:
                 value = _parse_list(value)
                 if value:
                     ret[first][second] = value
 
-        def _parse_dict(value):
+        def _parse_dict(value: str) -> Union[Dict[str, Any], None]:
             if not value.strip().startswith('{'):
                 value = '{{{0}}}'.format(value)
             try:
                 return yaml.safe_load(value)
             except Exception:
                 logger.exception('Exception when parsing dict %s', value)
                 return None
@@ -329,16 +440,16 @@
             for second in params:
                 value = ret.get(first, {}).pop(second, None)
                 if value:
                     value = _parse_dict(value)
                     if value:
                         ret[first][second] = value
 
-        def _get_auth(name, params=None):
-            ret = {}
+        def _get_auth(name: str, params: Optional[Collection[str]] = None) -> Dict[str, str]:
+            ret: Dict[str, str] = {}
             for param in params or _AUTH_ALLOWED_PARAMETERS[:2]:
                 value = _popenv(name + '_' + param)
                 if value:
                     ret[param] = value
             return ret
 
         restapi_auth = _get_auth('restapi')
@@ -399,15 +510,16 @@
                         if options:
                             users[name]['options'] = options
         if users:
             ret['bootstrap']['users'] = users
 
         return ret
 
-    def _build_effective_configuration(self, dynamic_configuration, local_configuration):
+    def _build_effective_configuration(self, dynamic_configuration: Dict[str, Any],
+                                       local_configuration: Dict[str, Union[Dict[str, Any], Any]]) -> Dict[str, Any]:
         config = self._safe_copy_dynamic_configuration(dynamic_configuration)
         for name, value in local_configuration.items():
             if name == 'citus':  # remove invalid citus configuration
                 if isinstance(value, dict) and isinstance(value.get('group'), int)\
                         and isinstance(value.get('database'), str):
                     config[name] = value
             elif name == 'postgresql':
@@ -454,29 +566,35 @@
             dcs = bootstrap.setdefault('dcs', {})
             dcs.setdefault('synchronous_mode', True)
 
         updated_fields = (
             'name',
             'scope',
             'retry_timeout',
-            'synchronous_mode',
-            'synchronous_mode_strict',
-            'synchronous_node_count',
-            'maximum_lag_on_syncnode',
             'citus'
         )
 
         pg_config.update({p: config[p] for p in updated_fields if p in config})
 
         return config
 
-    def get(self, key, default=None):
+    def get(self, key: str, default: Optional[Any] = None) -> Any:
         return self.__effective_configuration.get(key, default)
 
-    def __contains__(self, key):
+    def __contains__(self, key: str) -> bool:
         return key in self.__effective_configuration
 
-    def __getitem__(self, key):
+    def __getitem__(self, key: str) -> Any:
         return self.__effective_configuration[key]
 
-    def copy(self):
+    def copy(self) -> Dict[str, Any]:
         return deepcopy(self.__effective_configuration)
+
+    def get_global_config(self, cluster: Union[Cluster, None]) -> GlobalConfig:
+        """Instantiate :class:`GlobalConfig` based on input.
+
+        Use the configuration from provided *cluster* (the most up-to-date) or from the
+        local cache if *cluster.config* is not initialized or doesn't have a valid config.
+        :param cluster: the currently known cluster state from DCS
+        :returns: :class:`GlobalConfig` object
+        """
+        return get_global_config(cluster, self._dynamic_configuration)
```

### Comparing `patroni-3.0.2/patroni/ctl.py` & `patroni-3.0.3/patroni/postgresql/__init__.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,1335 +1,1283 @@
-'''
-Patroni Control
-'''
-
-import click
-import codecs
-import copy
-import datetime
-import dateutil.parser
-import dateutil.tz
-import difflib
-import io
-import json
 import logging
 import os
-import random
+import re
+import shlex
 import shutil
 import subprocess
-import sys
-import tempfile
 import time
-import yaml
 
-from click import ClickException
-from collections import defaultdict
 from contextlib import contextmanager
-from prettytable import ALL, FRAME, PrettyTable
-from urllib.parse import urlparse
+from copy import deepcopy
+from datetime import datetime
+from dateutil import tz
+from psutil import TimeoutExpired
+from threading import current_thread, Lock
+from typing import Any, Callable, Dict, Generator, List, Optional, Union, Tuple, TYPE_CHECKING
+
+from .bootstrap import Bootstrap
+from .callback_executor import CallbackAction, CallbackExecutor
+from .cancellable import CancellableSubprocess
+from .config import ConfigHandler, mtime
+from .connection import Connection, get_connection_cursor
+from .citus import CitusHandler
+from .misc import parse_history, parse_lsn, postgres_major_version_to_int
+from .postmaster import PostmasterProcess
+from .slots import SlotsHandler
+from .sync import SyncHandler
+from .. import psycopg
+from ..async_executor import CriticalTask
+from ..collections import CaseInsensitiveSet
+from ..dcs import Cluster, Leader, Member
+from ..exceptions import PostgresConnectionException
+from ..utils import Retry, RetryFailedError, polling_loop, data_directory_is_empty, parse_int
+
+if TYPE_CHECKING:  # pragma: no cover
+    from psycopg import Connection as Connection3, Cursor
+    from psycopg2 import connection as connection3, cursor
+    from ..config import GlobalConfig
+
+logger = logging.getLogger(__name__)
+
+STATE_RUNNING = 'running'
+STATE_REJECT = 'rejecting connections'
+STATE_NO_RESPONSE = 'not responding'
+STATE_UNKNOWN = 'unknown'
 
-try:
-    from ydiff import markup_to_pager, PatchStream
-except ImportError:  # pragma: no cover
-    from cdiff import markup_to_pager, PatchStream
-
-from .dcs import get_dcs as _get_dcs
-from .exceptions import PatroniException
-from .postgresql.misc import postgres_version_to_int
-from .utils import cluster_as_json, patch_config, polling_loop, is_standby_cluster
-from .request import PatroniRequest
-from .version import __version__
-
-CONFIG_DIR_PATH = click.get_app_dir('patroni')
-CONFIG_FILE_PATH = os.path.join(CONFIG_DIR_PATH, 'patronictl.yaml')
-DCS_DEFAULTS = {'zookeeper': {'port': 2181, 'template': "zookeeper:\n hosts: ['{host}:{port}']"},
-                'exhibitor': {'port': 8181, 'template': "exhibitor:\n hosts: [{host}]\n port: {port}"},
-                'consul': {'port': 8500, 'template': "consul:\n host: '{host}:{port}'"},
-                'etcd': {'port': 2379, 'template': "etcd:\n host: '{host}:{port}'"}}
-
-
-class PatroniCtlException(ClickException):
-    pass
-
-
-class PatronictlPrettyTable(PrettyTable):
-
-    def __init__(self, header, *args, **kwargs):
-        PrettyTable.__init__(self, *args, **kwargs)
-        self.__table_header = header
-        self.__hline_num = 0
-        self.__hline = None
-
-    def __build_header(self, line):
-        header = self.__table_header[:len(line) - 2]
-        return "".join([line[0], header, line[1 + len(header):]])
-
-    def _stringify_hrule(self, *args, **kwargs):
-        ret = super(PatronictlPrettyTable, self)._stringify_hrule(*args, **kwargs)
-        where = args[1] if len(args) > 1 else kwargs.get('where')
-        if where == 'top_' and self.__table_header:
-            ret = self.__build_header(ret)
-            self.__hline_num += 1
-        return ret
+STOP_POLLING_INTERVAL = 1
 
-    def _is_first_hline(self):
-        return self.__hline_num == 0
 
-    def _set_hline(self, value):
-        self.__hline = value
+@contextmanager
+def null_context():
+    yield
 
-    def _get_hline(self):
-        ret = self.__hline
 
-        # Inject nice table header
-        if self._is_first_hline() and self.__table_header:
-            ret = self.__build_header(ret)
+class Postgresql(object):
 
-        self.__hline_num += 1
-        return ret
+    POSTMASTER_START_TIME = "pg_catalog.pg_postmaster_start_time()"
+    TL_LSN = ("CASE WHEN pg_catalog.pg_is_in_recovery() THEN 0 "
+              "ELSE ('x' || pg_catalog.substr(pg_catalog.pg_{0}file_name("
+              "pg_catalog.pg_current_{0}_{1}()), 1, 8))::bit(32)::int END, "  # primary timeline
+              "CASE WHEN pg_catalog.pg_is_in_recovery() THEN 0 "
+              "ELSE pg_catalog.pg_{0}_{1}_diff(pg_catalog.pg_current_{0}_{1}(), '0/0')::bigint END, "  # write_lsn
+              "pg_catalog.pg_{0}_{1}_diff(pg_catalog.pg_last_{0}_replay_{1}(), '0/0')::bigint, "
+              "pg_catalog.pg_{0}_{1}_diff(COALESCE(pg_catalog.pg_last_{0}_receive_{1}(), '0/0'), '0/0')::bigint, "
+              "pg_catalog.pg_is_in_recovery() AND pg_catalog.pg_is_{0}_replay_paused()")
+
+    def __init__(self, config: Dict[str, Any]) -> None:
+        self.name: str = config['name']
+        self.scope: str = config['scope']
+        self._data_dir: str = config['data_dir']
+        self._database = config.get('database', 'postgres')
+        self._version_file = os.path.join(self._data_dir, 'PG_VERSION')
+        self._pg_control = os.path.join(self._data_dir, 'global', 'pg_control')
+        self.connection_string: str
+        self.proxy_url: Optional[str]
+        self._major_version = self.get_major_version()
+        self._global_config = None
+
+        self._state_lock = Lock()
+        self.set_state('stopped')
+
+        self._pending_restart = False
+        self._connection = Connection()
+        self.citus_handler = CitusHandler(self, config.get('citus'))
+        self.config = ConfigHandler(self, config)
+        self.config.check_directories()
+
+        self._bin_dir = config.get('bin_dir') or ''
+        self.bootstrap = Bootstrap(self)
+        self.bootstrapping = False
+        self.__thread_ident = current_thread().ident
+
+        self.slots_handler = SlotsHandler(self)
+        self.sync_handler = SyncHandler(self)
+
+        self._callback_executor = CallbackExecutor()
+        self.__cb_called = False
+        self.__cb_pending = None
+
+        self.cancellable = CancellableSubprocess()
+
+        self._sysid = ''
+        self.retry = Retry(max_tries=-1, deadline=config['retry_timeout'] / 2.0, max_delay=1,
+                           retry_exceptions=PostgresConnectionException)
+
+        # Retry 'pg_is_in_recovery()' only once
+        self._is_leader_retry = Retry(max_tries=1, deadline=config['retry_timeout'] / 2.0, max_delay=1,
+                                      retry_exceptions=PostgresConnectionException)
+
+        self._role_lock = Lock()
+        self.set_role(self.get_postgres_role_from_data_directory())
+        self._state_entry_timestamp = 0
+
+        self._cluster_info_state = {}
+        self._has_permanent_logical_slots = True
+        self._enforce_hot_standby_feedback = False
+        self._cached_replica_timeline = None
+
+        # Last known running process
+        self._postmaster_proc = None
+
+        if self.is_running():  # we are "joining" already running postgres
+            self.set_state('running')
+            self.set_role('master' if self.is_leader() else 'replica')
+            # postpone writing postgresql.conf for 12+ because recovery parameters are not yet known
+            if self.major_version < 120000 or self.is_leader():
+                self.config.write_postgresql_conf()
+            hba_saved = self.config.replace_pg_hba()
+            ident_saved = self.config.replace_pg_ident()
+            if hba_saved or ident_saved:
+                self.reload()
+        elif self.role in ('master', 'primary'):
+            self.set_role('demoted')
+
+    @property
+    def create_replica_methods(self) -> List[str]:
+        return self.config.get('create_replica_methods', []) or self.config.get('create_replica_method', []) or []
+
+    @property
+    def major_version(self) -> int:
+        return self._major_version
+
+    @property
+    def database(self) -> str:
+        return self._database
+
+    @property
+    def data_dir(self) -> str:
+        return self._data_dir
+
+    @property
+    def callback(self) -> Dict[str, str]:
+        return self.config.get('callbacks', {}) or {}
+
+    @property
+    def wal_dir(self) -> str:
+        return os.path.join(self._data_dir, 'pg_' + self.wal_name)
+
+    @property
+    def wal_name(self) -> str:
+        return 'wal' if self._major_version >= 100000 else 'xlog'
+
+    @property
+    def lsn_name(self) -> str:
+        return 'lsn' if self._major_version >= 100000 else 'location'
+
+    @property
+    def supports_multiple_sync(self) -> bool:
+        """:returns: `True` if Postgres version supports more than one synchronous node."""
+        return self._major_version >= 90600
+
+    @property
+    def cluster_info_query(self) -> str:
+        """Returns the monitoring query with a fixed number of fields.
+
+        The query text is constructed based on current state in DCS and PostgreSQL version:
+        1. function names depend on version. wal/lsn for v10+ and xlog/location for pre v10.
+        2. for primary we query timeline_id (extracted from pg_walfile_name()) and pg_current_wal_lsn()
+        3. for replicas we query pg_last_wal_receive_lsn(), pg_last_wal_replay_lsn(), and  pg_is_wal_replay_paused()
+        4. for v9.6+ we query primary_slot_name and primary_conninfo from pg_stat_get_wal_receiver()
+        5. for v11+ with permanent logical slots we query from pg_replication_slots and aggregate the result
+        6. for standby_leader node running v9.6+ we also query pg_control_checkpoint to fetch timeline_id
+        7. if sync replication is enabled we query pg_stat_replication and aggregate the result.
+           In addition to that we get current values of synchronous_commit and synchronous_standby_names GUCs.
+
+        If some conditions are not satisfied we simply put static values instead. E.g., NULL, 0, '', and so on."""
+
+        extra = ", " + (("pg_catalog.current_setting('synchronous_commit'), "
+                         "pg_catalog.current_setting('synchronous_standby_names'), "
+                         "(SELECT pg_catalog.json_agg(r.*) FROM (SELECT w.pid as pid, application_name, sync_state,"
+                         " pg_catalog.pg_{0}_{1}_diff(write_{1}, '0/0')::bigint AS write_lsn,"
+                         " pg_catalog.pg_{0}_{1}_diff(flush_{1}, '0/0')::bigint AS flush_lsn,"
+                         " pg_catalog.pg_{0}_{1}_diff(replay_{1}, '0/0')::bigint AS replay_lsn "
+                         "FROM pg_catalog.pg_stat_get_wal_senders() w,"
+                         " pg_catalog.pg_stat_get_activity(w.pid)"
+                         " WHERE w.state = 'streaming') r)").format(self.wal_name, self.lsn_name)
+                        if (not self.global_config or self.global_config.is_synchronous_mode)
+                        and self.role in ('master', 'primary', 'promoted') else "'on', '', NULL")
+
+        if self._major_version >= 90600:
+            extra = ("(SELECT pg_catalog.json_agg(s.*) FROM (SELECT slot_name, slot_type as type, datoid::bigint, "
+                     "plugin, catalog_xmin, pg_catalog.pg_wal_lsn_diff(confirmed_flush_lsn, '0/0')::bigint"
+                     " AS confirmed_flush_lsn FROM pg_catalog.pg_get_replication_slots()) AS s)"
+                     if self._has_permanent_logical_slots and self._major_version >= 110000 else "NULL") + extra
+            extra = (", CASE WHEN latest_end_lsn IS NULL THEN NULL ELSE received_tli END,"
+                     " slot_name, conninfo, {0} FROM pg_catalog.pg_stat_get_wal_receiver()").format(extra)
+            if self.role == 'standby_leader':
+                extra = "timeline_id" + extra + ", pg_catalog.pg_control_checkpoint()"
+            else:
+                extra = "0" + extra
+        else:
+            extra = "0, NULL, NULL, NULL, NULL" + extra
 
-    _hrule = property(_get_hline, _set_hline)
+        return ("SELECT " + self.TL_LSN + ", {2}").format(self.wal_name, self.lsn_name, extra)
 
+    @property
+    def available_gucs(self) -> CaseInsensitiveSet:
+        """GUCs available in this Postgres server."""
+        return self._get_gucs()
 
-def parse_dcs(dcs):
-    if dcs is None:
-        return None
-    elif '//' not in dcs:
-        dcs = '//' + dcs
+    def _version_file_exists(self) -> bool:
+        return not self.data_directory_empty() and os.path.isfile(self._version_file)
 
-    parsed = urlparse(dcs)
-    scheme = parsed.scheme
-    port = int(parsed.port) if parsed.port else None
+    def get_major_version(self) -> int:
+        """Reads major version from PG_VERSION file
 
-    if scheme == '':
-        scheme = ([k for k, v in DCS_DEFAULTS.items() if v['port'] == port] or ['etcd'])[0]
-    elif scheme not in DCS_DEFAULTS:
-        raise PatroniCtlException('Unknown dcs scheme: {}'.format(scheme))
+        :returns: major PostgreSQL version in integer format or 0 in case of missing file or errors"""
+        if self._version_file_exists():
+            try:
+                with open(self._version_file) as f:
+                    return postgres_major_version_to_int(f.read().strip())
+            except Exception:
+                logger.exception('Failed to read PG_VERSION from %s', self._data_dir)
+        return 0
+
+    def pgcommand(self, cmd: str) -> str:
+        """Return path to the specified PostgreSQL command.
+
+        .. note::
+            If ``postgresql.bin_name.*cmd*`` was configured by the user then that binary name is used, otherwise the
+            default binary name *cmd* is used.
+
+        :param cmd: the Postgres binary name to get path to.
+
+        :returns: path to Postgres binary named *cmd*.
+        """
+        return os.path.join(self._bin_dir, (self.config.get('bin_name', {}) or {}).get(cmd, cmd))
+
+    def pg_ctl(self, cmd: str, *args: str, **kwargs: Any) -> bool:
+        """Builds and executes pg_ctl command
+
+        :returns: `!True` when return_code == 0, otherwise `!False`"""
+
+        pg_ctl = [self.pgcommand('pg_ctl'), cmd]
+        return subprocess.call(pg_ctl + ['-D', self._data_dir] + list(args), **kwargs) == 0
+
+    def initdb(self, *args: str, **kwargs: Any) -> bool:
+        """Builds and executes the initdb command.
+
+        :param args: List of arguments to be joined into the initdb command.
+        :param kwargs: Keyword arguments to pass to ``subprocess.call``.
+
+        :returns: ``True`` if the result of ``subprocess.call`, the exit code, is ``0``.
+        """
+        initdb = [self.pgcommand('initdb')] + list(args) + [self.data_dir]
+        return subprocess.call(initdb, **kwargs) == 0
+
+    def pg_isready(self) -> str:
+        """Runs pg_isready to see if PostgreSQL is accepting connections.
+
+        :returns: 'ok' if PostgreSQL is up, 'reject' if starting up, 'no_resopnse' if not up."""
+
+        r = self.config.local_connect_kwargs
+        cmd = [self.pgcommand('pg_isready'), '-p', r['port'], '-d', self._database]
+
+        # Host is not set if we are connecting via default unix socket
+        if 'host' in r:
+            cmd.extend(['-h', r['host']])
+
+        # We only need the username because pg_isready does not try to authenticate
+        if 'user' in r:
+            cmd.extend(['-U', r['user']])
+
+        ret = subprocess.call(cmd)
+        return_codes = {0: STATE_RUNNING,
+                        1: STATE_REJECT,
+                        2: STATE_NO_RESPONSE,
+                        3: STATE_UNKNOWN}
+        return return_codes.get(ret, STATE_UNKNOWN)
+
+    def reload_config(self, config: Dict[str, Any], sighup: bool = False) -> None:
+        self.config.reload_config(config, sighup)
+        self._is_leader_retry.deadline = self.retry.deadline = config['retry_timeout'] / 2.0
+
+    @property
+    def pending_restart(self) -> bool:
+        return self._pending_restart
+
+    def set_pending_restart(self, value: bool) -> None:
+        self._pending_restart = value
+
+    @property
+    def sysid(self) -> str:
+        if not self._sysid and not self.bootstrapping:
+            data = self.controldata()
+            self._sysid = data.get('Database system identifier', '')
+        return self._sysid
+
+    def get_postgres_role_from_data_directory(self) -> str:
+        if self.data_directory_empty() or not self.controldata():
+            return 'uninitialized'
+        elif self.config.recovery_conf_exists():
+            return 'replica'
+        else:
+            return 'master'
 
-    default = DCS_DEFAULTS[scheme]
-    return yaml.safe_load(default['template'].format(host=parsed.hostname or 'localhost', port=port or default['port']))
+    @property
+    def server_version(self) -> int:
+        return self._connection.server_version
+
+    def connection(self) -> Union['connection3', 'Connection3[Any]']:
+        return self._connection.get()
+
+    def set_connection_kwargs(self, kwargs: Dict[str, Any]) -> None:
+        self._connection.set_conn_kwargs(kwargs.copy())
+        self.citus_handler.set_conn_kwargs(kwargs.copy())
+
+    def _query(self, sql: str, *params: Any) -> Union['Cursor[Any]', 'cursor']:
+        """We are always using the same cursor, therefore this method is not thread-safe!!!
+        You can call it from different threads only if you are holding explicit `AsyncExecutor` lock,
+        because the main thread is always holding this lock when running HA cycle."""
+        cursor = None
+        try:
+            cursor = self._connection.cursor()
+            cursor.execute(sql.encode('utf-8'), params or None)
+            return cursor
+        except psycopg.Error as e:
+            if cursor and cursor.connection.closed == 0:
+                # When connected via unix socket, psycopg2 can't recoginze 'connection lost'
+                # and leaves `_cursor_holder.connection.closed == 0`, but psycopg2.OperationalError
+                # is still raised (what is correct). It doesn't make sense to continiue with existing
+                # connection and we will close it, to avoid its reuse by the `cursor` method.
+                if isinstance(e, psycopg.OperationalError):
+                    self._connection.close()
+                else:
+                    raise e
+            if self.state == 'restarting':
+                raise RetryFailedError('cluster is being restarted')
+            raise PostgresConnectionException('connection problems')
+
+    def query(self, sql: str, *args: Any, **kwargs: Any) -> Union['Cursor[Any]', 'cursor']:
+        if not kwargs.get('retry', True):
+            return self._query(sql, *args)
+        try:
+            return self.retry(self._query, sql, *args)
+        except RetryFailedError as e:
+            raise PostgresConnectionException(str(e))
+
+    def pg_control_exists(self) -> bool:
+        return os.path.isfile(self._pg_control)
+
+    def data_directory_empty(self) -> bool:
+        if self.pg_control_exists():
+            return False
+        return data_directory_is_empty(self._data_dir)
+
+    def replica_method_options(self, method: str) -> Dict[str, Any]:
+        return deepcopy(self.config.get(method, {}) or {})
+
+    def replica_method_can_work_without_replication_connection(self, method: str) -> bool:
+        return method != 'basebackup' and bool(self.replica_method_options(method).get('no_master')
+                                               or self.replica_method_options(method).get('no_leader'))
+
+    def can_create_replica_without_replication_connection(self, replica_methods: Optional[List[str]]) -> bool:
+        """ go through the replication methods to see if there are ones
+            that does not require a working replication connection.
+        """
+        if replica_methods is None:
+            replica_methods = self.create_replica_methods
+        return any(self.replica_method_can_work_without_replication_connection(m) for m in replica_methods)
+
+    @property
+    def enforce_hot_standby_feedback(self) -> bool:
+        return self._enforce_hot_standby_feedback
+
+    def set_enforce_hot_standby_feedback(self, value: bool) -> None:
+        # If we enable or disable the hot_standby_feedback we need to update postgresql.conf and reload
+        if self._enforce_hot_standby_feedback != value:
+            self._enforce_hot_standby_feedback = value
+            if self.is_running():
+                self.config.write_postgresql_conf()
+                self.reload()
+
+    @property
+    def global_config(self) -> Optional['GlobalConfig']:
+        return self._global_config
+
+    def reset_cluster_info_state(self, cluster: Union[Cluster, None], nofailover: bool = False,
+                                 global_config: Optional['GlobalConfig'] = None) -> None:
+        """Reset monitoring query cache.
+
+        It happens in the beginning of heart-beat loop and on change of `synchronous_standby_names`.
+
+        :param cluster: currently known cluster state from DCS
+        :param nofailover: whether this node could become a new primary.
+                           Important when there are logical permanent replication slots because "nofailover"
+                           node could do cascading replication and should enable `hot_standby_feedback`
+        :param global_config: last known :class:`GlobalConfig` object
+        """
+        self._cluster_info_state = {}
+        if cluster and cluster.config and cluster.config.modify_version:
+            self._has_permanent_logical_slots =\
+                cluster.has_permanent_logical_slots(self.name, nofailover, self.major_version)
+
+            # We want to enable hot_standby_feedback if the replica is supposed
+            # to have a logical slot or in case if it is the cascading replica.
+            self.set_enforce_hot_standby_feedback(
+                self._has_permanent_logical_slots
+                or cluster.should_enforce_hot_standby_feedback(self.name, nofailover, self.major_version))
 
+        if global_config:
+            self._global_config = global_config
 
-def load_config(path, dcs_url):
-    from patroni.config import Config
+    def _cluster_info_state_get(self, name: str) -> Optional[Any]:
+        if not self._cluster_info_state:
+            try:
+                result = self._is_leader_retry(self._query, self.cluster_info_query).fetchone()
+                cluster_info_state = dict(zip(['timeline', 'wal_position', 'replayed_location',
+                                               'received_location', 'replay_paused', 'pg_control_timeline',
+                                               'received_tli', 'slot_name', 'conninfo', 'slots', 'synchronous_commit',
+                                               'synchronous_standby_names', 'pg_stat_replication'], result))
+                if self._has_permanent_logical_slots:
+                    cluster_info_state['slots'] =\
+                        self.slots_handler.process_permanent_slots(cluster_info_state['slots'])
+                self._cluster_info_state = cluster_info_state
+            except RetryFailedError as e:  # SELECT failed two times
+                self._cluster_info_state = {'error': str(e)}
+                if not self.is_starting() and self.pg_isready() == STATE_REJECT:
+                    self.set_state('starting')
+
+        if 'error' in self._cluster_info_state:
+            raise PostgresConnectionException(self._cluster_info_state['error'])
+
+        return self._cluster_info_state.get(name)
+
+    def replayed_location(self) -> Optional[int]:
+        return self._cluster_info_state_get('replayed_location')
+
+    def received_location(self) -> Optional[int]:
+        return self._cluster_info_state_get('received_location')
+
+    def slots(self) -> Dict[str, int]:
+        return self._cluster_info_state_get('slots') or {}
+
+    def primary_slot_name(self) -> Optional[str]:
+        return self._cluster_info_state_get('slot_name')
+
+    def primary_conninfo(self) -> Optional[str]:
+        return self._cluster_info_state_get('conninfo')
+
+    def received_timeline(self) -> Optional[int]:
+        return self._cluster_info_state_get('received_tli')
+
+    def synchronous_commit(self) -> str:
+        """:returns: "synchronous_commit" GUC value."""
+        return self._cluster_info_state_get('synchronous_commit') or 'on'
+
+    def synchronous_standby_names(self) -> str:
+        """:returns: "synchronous_standby_names" GUC value."""
+        return self._cluster_info_state_get('synchronous_standby_names') or ''
+
+    def pg_stat_replication(self) -> List[Dict[str, Any]]:
+        """:returns: a result set of 'SELECT * FROM pg_stat_replication'."""
+        return self._cluster_info_state_get('pg_stat_replication') or []
 
-    if not (os.path.exists(path) and os.access(path, os.R_OK)):
-        if path != CONFIG_FILE_PATH:    # bail if non-default config location specified but file not found / readable
-            raise PatroniCtlException('Provided config file {0} not existing or no read rights.'
-                                      ' Check the -c/--config-file parameter'.format(path))
-        else:
-            logging.debug('Ignoring configuration file "%s". It does not exists or is not readable.', path)
-    else:
-        logging.debug('Loading configuration from file %s', path)
-    config = Config(path, validator=None).copy()
-
-    dcs_url = parse_dcs(dcs_url) or {}
-    if dcs_url:
-        for d in DCS_DEFAULTS:
-            config.pop(d, None)
-        config.update(dcs_url)
-    return config
-
-
-option_format = click.option('--format', '-f', 'fmt', help='Output format (pretty, tsv, json, yaml)', default='pretty')
-option_watchrefresh = click.option('-w', '--watch', type=float, help='Auto update the screen every X seconds')
-option_watch = click.option('-W', is_flag=True, help='Auto update the screen every 2 seconds')
-option_force = click.option('--force', is_flag=True, help='Do not ask for confirmation at any point')
-arg_cluster_name = click.argument('cluster_name', required=False,
-                                  default=lambda: click.get_current_context().obj.get('scope'))
-option_default_citus_group = click.option('--group', required=False, type=int, help='Citus group',
-                                          default=lambda: click.get_current_context().obj.get('citus', {}).get('group'))
-option_citus_group = click.option('--group', required=False, type=int, help='Citus group')
-option_insecure = click.option('-k', '--insecure', is_flag=True, help='Allow connections to SSL sites without certs')
-role_choice = click.Choice(['leader', 'primary', 'standby-leader', 'replica', 'standby', 'any', 'master'])
-
-
-@click.group()
-@click.option('--config-file', '-c', help='Configuration file',
-              envvar='PATRONICTL_CONFIG_FILE', default=CONFIG_FILE_PATH)
-@click.option('--dcs-url', '--dcs', '-d', 'dcs_url', help='The DCS connect url', envvar='DCS_URL')
-@option_insecure
-@click.pass_context
-def ctl(ctx, config_file, dcs_url, insecure):
-    level = 'WARNING'
-    for name in ('LOGLEVEL', 'PATRONI_LOGLEVEL', 'PATRONI_LOG_LEVEL'):
-        level = os.environ.get(name, level)
-    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=level)
-    logging.captureWarnings(True)  # Capture eventual SSL warning
-    ctx.obj = load_config(config_file, dcs_url)
-    # backward compatibility for configuration file where ctl section is not define
-    ctx.obj.setdefault('ctl', {})['insecure'] = ctx.obj.get('ctl', {}).get('insecure') or insecure
-
-
-def get_dcs(config, scope, group):
-    config.update({'scope': scope, 'patronictl': True})
-    if group is not None:
-        config['citus'] = {'group': group}
-    config.setdefault('name', scope)
-    try:
-        dcs = _get_dcs(config)
-        if config.get('citus') and group is None:
-            dcs.get_cluster = dcs._get_citus_cluster
-        return dcs
-    except PatroniException as e:
-        raise PatroniCtlException(str(e))
-
-
-def request_patroni(member, method='GET', endpoint=None, data=None):
-    ctx = click.get_current_context()  # the current click context
-    request_executor = ctx.obj.get('__request_patroni')
-    if not request_executor:
-        request_executor = ctx.obj['__request_patroni'] = PatroniRequest(ctx.obj)
-    return request_executor(member, method, endpoint, data)
-
-
-def print_output(columns, rows, alignment=None, fmt='pretty', header=None, delimiter='\t'):
-    if fmt in {'json', 'yaml', 'yml'}:
-        elements = [{k: v for k, v in zip(columns, r) if not header or str(v)} for r in rows]
-        func = json.dumps if fmt == 'json' else format_config_for_editing
-        click.echo(func(elements))
-    elif fmt in {'pretty', 'tsv', 'topology'}:
-        list_cluster = bool(header and columns and columns[0] == 'Cluster')
-        if list_cluster and 'Tags' in columns:  # we want to format member tags as YAML
-            i = columns.index('Tags')
-            for row in rows:
-                if row[i]:
-                    row[i] = format_config_for_editing(row[i], fmt != 'pretty').strip()
-        if list_cluster and fmt != 'tsv':  # skip cluster name and maybe Citus group if pretty-printing
-            skip_cols = 2 if ' (group: ' in header else 1
-            columns = columns[skip_cols:] if columns else []
-            rows = [row[skip_cols:] for row in rows]
-
-        if fmt == 'tsv':
-            for r in ([columns] if columns else []) + rows:
-                click.echo(delimiter.join(map(str, r)))
-        else:
-            hrules = ALL if any(any(isinstance(c, str) and '\n' in c for c in r) for r in rows) else FRAME
-            table = PatronictlPrettyTable(header, columns, hrules=hrules)
-            table.align = 'l'
-            for k, v in (alignment or {}).items():
-                table.align[k] = v
-            for r in rows:
-                table.add_row(r)
-            click.echo(table)
-
-
-def watching(w, watch, max_count=None, clear=True):
-    """
-    >>> len(list(watching(True, 1, 0)))
-    1
-    >>> len(list(watching(True, 1, 1)))
-    2
-    >>> len(list(watching(True, None, 0)))
-    1
-    """
-
-    if w and not watch:
-        watch = 2
-    if watch and clear:
-        click.clear()
-    yield 0
-
-    if max_count is not None and max_count < 1:
-        return
-
-    counter = 1
-    while watch and counter <= (max_count or counter):
-        time.sleep(watch)
-        counter += 1
-        if clear:
-            click.clear()
-        yield 0
-
-
-def get_all_members(obj, cluster, group, role='leader'):
-    clusters = {0: cluster}
-    if obj.get('citus') and group is None:
-        clusters.update(cluster.workers)
-    if role in ('leader', 'master', 'primary', 'standby-leader'):
-        role = {'primary': 'master', 'standby-leader': 'standby_leader'}.get(role, role)
-        for cluster in clusters.values():
-            if cluster.leader is not None and cluster.leader.name and\
-                    (role == 'leader' or
-                     cluster.leader.data.get('role') != 'master' and role == 'standby_leader' or
-                     cluster.leader.data.get('role') != 'standby_leader' and role == 'master'):
-                yield cluster.leader.member
-        return
-
-    for cluster in clusters.values():
-        leader_name = (cluster.leader.member.name if cluster.leader else None)
-        for m in cluster.members:
-            if role == 'any' or role in ('replica', 'standby') and m.name != leader_name:
-                yield m
-
-
-def get_any_member(obj, cluster, group, role='leader', member=None):
-    for m in get_all_members(obj, cluster, group, role):
-        if member is None or m.name == member:
-            return m
-
-
-def get_all_members_leader_first(cluster):
-    leader_name = cluster.leader.member.name if cluster.leader and cluster.leader.member.api_url else None
-    if leader_name:
-        yield cluster.leader.member
-    for member in cluster.members:
-        if member.api_url and member.name != leader_name:
-            yield member
-
-
-def get_cursor(obj, cluster, group, connect_parameters, role='leader', member=None):
-    member = get_any_member(obj, cluster, group, role=role, member=member)
-    if member is None:
-        return None
-
-    params = member.conn_kwargs(connect_parameters)
-    params.update({'fallback_application_name': 'Patroni ctl', 'connect_timeout': '5'})
-    if 'dbname' in connect_parameters:
-        params['dbname'] = connect_parameters['dbname']
-    else:
-        params.pop('dbname')
-
-    from . import psycopg
-    conn = psycopg.connect(**params)
-    cursor = conn.cursor()
-    if role in ('any', 'leader'):
-        return cursor
-
-    cursor.execute('SELECT pg_catalog.pg_is_in_recovery()')
-    in_recovery = cursor.fetchone()[0]
-
-    if in_recovery and role in ('replica', 'standby', 'standby-leader')\
-            or not in_recovery and role in ('master', 'primary'):
-        return cursor
-
-    conn.close()
-
-    return None
-
-
-def get_members(obj, cluster, cluster_name, member_names, role, force, action, ask_confirmation=True, group=None):
-    members = list(get_all_members(obj, cluster, group, role))
-
-    candidates = {m.name for m in members}
-    if not force or role:
-        if not member_names and not candidates:
-            raise PatroniCtlException('{0} cluster doesn\'t have any members'.format(cluster_name))
-        output_members(obj, cluster, cluster_name, group=group)
-
-    if member_names:
-        member_names = list(set(member_names) & candidates)
-        if not member_names:
-            raise PatroniCtlException('No {0} among provided members'.format(role))
-    elif action != 'reinitialize':
-        member_names = list(candidates)
-
-    if not member_names and not force:
-        member_names = [click.prompt('Which member do you want to {0} [{1}]?'.format(action,
-                        ', '.join(candidates)), type=str, default='')]
-
-    for member_name in member_names:
-        if member_name not in candidates:
-            raise PatroniCtlException('{0} is not a member of cluster'.format(member_name))
-
-    members = [m for m in members if m.name in member_names]
-    if ask_confirmation:
-        confirm_members_action(members, force, action)
-    return members
-
-
-def confirm_members_action(members, force, action, scheduled_at=None):
-    if scheduled_at:
-        if not force:
-            confirm = click.confirm('Are you sure you want to schedule {0} of members {1} at {2}?'
-                                    .format(action, ', '.join([m.name for m in members]), scheduled_at))
-            if not confirm:
-                raise PatroniCtlException('Aborted scheduled {0}'.format(action))
-    else:
-        if not force:
-            confirm = click.confirm('Are you sure you want to {0} members {1}?'
-                                    .format(action, ', '.join([m.name for m in members])))
-            if not confirm:
-                raise PatroniCtlException('Aborted {0}'.format(action))
-
-
-@ctl.command('dsn', help='Generate a dsn for the provided member, defaults to a dsn of the leader')
-@click.option('--role', '-r', help='Give a dsn of any member with this role', type=role_choice, default=None)
-@click.option('--member', '-m', help='Generate a dsn for this member', type=str)
-@arg_cluster_name
-@option_citus_group
-@click.pass_obj
-def dsn(obj, cluster_name, group, role, member):
-    if member is not None:
-        if role is not None:
-            raise PatroniCtlException('--role and --member are mutually exclusive options')
-        role = 'any'
-    if member is None and role is None:
-        role = 'leader'
-
-    cluster = get_dcs(obj, cluster_name, group).get_cluster()
-    m = get_any_member(obj, cluster, group, role=role, member=member)
-    if m is None:
-        raise PatroniCtlException('Can not find a suitable member')
-
-    params = m.conn_kwargs()
-    click.echo('host={host} port={port}'.format(**params))
-
-
-@ctl.command('query', help='Query a Patroni PostgreSQL member')
-@arg_cluster_name
-@option_citus_group
-@click.option('--format', 'fmt', help='Output format (pretty, tsv, json, yaml)', default='tsv')
-@click.option('--file', '-f', 'p_file', help='Execute the SQL commands from this file', type=click.File('rb'))
-@click.option('--password', help='force password prompt', is_flag=True)
-@click.option('-U', '--username', help='database user name', type=str)
-@option_watch
-@option_watchrefresh
-@click.option('--role', '-r', help='The role of the query', type=role_choice, default=None)
-@click.option('--member', '-m', help='Query a specific member', type=str)
-@click.option('--delimiter', help='The column delimiter', default='\t')
-@click.option('--command', '-c', help='The SQL commands to execute')
-@click.option('-d', '--dbname', help='database name to connect to', type=str)
-@click.pass_obj
-def query(
-    obj,
-    cluster_name,
-    group,
-    role,
-    member,
-    w,
-    watch,
-    delimiter,
-    command,
-    p_file,
-    password,
-    username,
-    dbname,
-    fmt='tsv',
-):
-    if member is not None:
-        if role is not None:
-            raise PatroniCtlException('--role and --member are mutually exclusive options')
-        role = 'any'
-    if member is None and role is None:
-        role = 'leader'
-
-    if p_file is not None and command is not None:
-        raise PatroniCtlException('--file and --command are mutually exclusive options')
-
-    if p_file is None and command is None:
-        raise PatroniCtlException('You need to specify either --command or --file')
-
-    connect_parameters = {}
-    if username:
-        connect_parameters['username'] = username
-    if password:
-        connect_parameters['password'] = click.prompt('Password', hide_input=True, type=str)
-    if dbname:
-        connect_parameters['dbname'] = dbname
-
-    if p_file is not None:
-        command = p_file.read()
-
-    dcs = get_dcs(obj, cluster_name, group)
-
-    cursor = None
-    for _ in watching(w, watch, clear=False):
-        if cursor is None:
-            cluster = dcs.get_cluster()
-
-        output, header = query_member(obj, cluster, group, cursor, member, role, command, connect_parameters)
-        print_output(header, output, fmt=fmt, delimiter=delimiter)
-
-
-def query_member(obj, cluster, group, cursor, member, role, command, connect_parameters):
-    from . import psycopg
-    try:
-        if cursor is None:
-            cursor = get_cursor(obj, cluster, group, connect_parameters, role=role, member=member)
-
-        if cursor is None:
-            if member is not None:
-                message = 'No connection to member {0} is available'.format(member)
-            else:
-                message = 'No connection to role={0} is available'.format(role)
-            logging.debug(message)
-            return [[timestamp(0), message]], None
-
-        cursor.execute(command)
-        return cursor.fetchall(), [d.name for d in cursor.description]
-    except psycopg.DatabaseError as de:
-        logging.debug(de)
-        if cursor is not None and not cursor.connection.closed:
-            cursor.connection.close()
-        message = de.diag.sqlstate or str(de)
-        message = message.replace('\n', ' ')
-        return [[timestamp(0), 'ERROR, SQLSTATE: {0}'.format(message)]], None
-
-
-@ctl.command('remove', help='Remove cluster from DCS')
-@click.argument('cluster_name')
-@option_citus_group
-@option_format
-@click.pass_obj
-def remove(obj, cluster_name, group, fmt):
-    dcs = get_dcs(obj, cluster_name, group)
-    cluster = dcs.get_cluster()
-
-    if obj.get('citus') and group is None:
-        raise PatroniCtlException('For Citus clusters the --group must me specified')
-    output_members(obj, cluster, cluster_name, fmt=fmt)
-
-    confirm = click.prompt('Please confirm the cluster name to remove', type=str)
-    if confirm != cluster_name:
-        raise PatroniCtlException('Cluster names specified do not match')
-
-    message = 'Yes I am aware'
-    confirm = \
-        click.prompt('You are about to remove all information in DCS for {0}, please type: "{1}"'.format(cluster_name,
-                     message), type=str)
-    if message != confirm:
-        raise PatroniCtlException('You did not exactly type "{0}"'.format(message))
-
-    if cluster.leader and cluster.leader.name:
-        confirm = click.prompt('This cluster currently is healthy. Please specify the leader name to continue')
-        if confirm != cluster.leader.name:
-            raise PatroniCtlException('You did not specify the current leader of the cluster')
-
-    dcs.delete_cluster()
-
-
-def check_response(response, member_name, action_name, silent_success=False):
-    if response.status >= 400:
-        click.echo('Failed: {0} for member {1}, status code={2}, ({3})'.format(
-            action_name, member_name, response.status, response.data.decode('utf-8')
-        ))
+    def is_leader(self) -> bool:
+        try:
+            return bool(self._cluster_info_state_get('timeline'))
+        except PostgresConnectionException:
+            logger.warning('Failed to determine PostgreSQL state from the connection, falling back to cached role')
+            return bool(self.is_running() and self.role in ('master', 'primary'))
+
+    def replay_paused(self) -> bool:
+        return self._cluster_info_state_get('replay_paused') or False
+
+    def resume_wal_replay(self) -> None:
+        self._query('SELECT pg_catalog.pg_{0}_replay_resume()'.format(self.wal_name))
+
+    def handle_parameter_change(self) -> None:
+        if self.major_version >= 140000 and not self.is_starting() and self.replay_paused():
+            logger.info('Resuming paused WAL replay for PostgreSQL 14+')
+            self.resume_wal_replay()
+
+    def pg_control_timeline(self) -> Optional[int]:
+        try:
+
+            return int(self.controldata().get("Latest checkpoint's TimeLineID", ""))
+        except (TypeError, ValueError):
+            logger.exception('Failed to parse timeline from pg_controldata output')
+
+    def parse_wal_record(self, timeline: str,
+                         lsn: str) -> Union[Tuple[str, str, str, str], Tuple[None, None, None, None]]:
+        out, err = self.waldump(timeline, lsn, 1)
+        if out and not err:
+            match = re.match(r'^rmgr:\s+(.+?)\s+len \(rec/tot\):\s+\d+/\s+\d+, tx:\s+\d+, '
+                             r'lsn: ([0-9A-Fa-f]+/[0-9A-Fa-f]+), prev ([0-9A-Fa-f]+/[0-9A-Fa-f]+), '
+                             r'.*?desc: (.+)', out.decode('utf-8'))
+            if match:
+                return match.groups()
+        return None, None, None, None
+
+    def latest_checkpoint_location(self) -> Optional[int]:
+        """Returns checkpoint location for the cleanly shut down primary.
+           But, if we know that the checkpoint was written to the new WAL
+           due to the archive_mode=on, we will return the LSN of prev wal record (SWITCH)."""
+
+        data = self.controldata()
+        timeline = data.get("Latest checkpoint's TimeLineID")
+        lsn = checkpoint_lsn = data.get('Latest checkpoint location')
+        if data.get('Database cluster state') == 'shut down' and lsn and timeline and checkpoint_lsn:
+            try:
+                checkpoint_lsn = parse_lsn(checkpoint_lsn)
+                rm_name, lsn, prev, desc = self.parse_wal_record(timeline, lsn)
+                desc = str(desc).strip().lower()
+                if rm_name == 'XLOG' and lsn and parse_lsn(lsn) == checkpoint_lsn and prev and\
+                        desc.startswith('checkpoint') and desc.endswith('shutdown'):
+                    _, lsn, _, desc = self.parse_wal_record(timeline, prev)
+                    prev = parse_lsn(prev)
+                    # If the cluster is shutdown with archive_mode=on, WAL is switched before writing the checkpoint.
+                    # In this case we want to take the LSN of previous record (switch) as the last known WAL location.
+                    if lsn and parse_lsn(lsn) == prev and str(desc).strip() in ('xlog switch', 'SWITCH'):
+                        return prev
+            except Exception as e:
+                logger.error('Exception when parsing WAL pg_%sdump output: %r', self.wal_name, e)
+            if isinstance(checkpoint_lsn, int):
+                return checkpoint_lsn
+
+    def is_running(self) -> Optional[PostmasterProcess]:
+        """Returns PostmasterProcess if one is running on the data directory or None. If most recently seen process
+        is running updates the cached process based on pid file."""
+        if self._postmaster_proc:
+            if self._postmaster_proc.is_running():
+                return self._postmaster_proc
+            self._postmaster_proc = None
+
+        # we noticed that postgres was restarted, force syncing of replication slots and check of logical slots
+        self.slots_handler.schedule()
+
+        self._postmaster_proc = PostmasterProcess.from_pidfile(self._data_dir)
+        return self._postmaster_proc
+
+    @property
+    def cb_called(self) -> bool:
+        return self.__cb_called
+
+    def call_nowait(self, cb_type: CallbackAction) -> None:
+        """pick a callback command and call it without waiting for it to finish """
+        if self.bootstrapping:
+            return
+        if cb_type in (CallbackAction.ON_START, CallbackAction.ON_STOP,
+                       CallbackAction.ON_RESTART, CallbackAction.ON_ROLE_CHANGE):
+            self.__cb_called = True
+
+        if self.callback and cb_type in self.callback:
+            cmd = self.callback[cb_type]
+            role = 'master' if self.role == 'promoted' else self.role
+            try:
+                cmd = shlex.split(self.callback[cb_type]) + [cb_type, role, self.scope]
+                self._callback_executor.call(cmd)
+            except Exception:
+                logger.exception('callback %s %r %s %s failed', cmd, cb_type, role, self.scope)
+
+    @property
+    def role(self) -> str:
+        with self._role_lock:
+            return self._role
+
+    def set_role(self, value: str) -> None:
+        with self._role_lock:
+            self._role = value
+
+    @property
+    def state(self) -> str:
+        with self._state_lock:
+            return self._state
+
+    def set_state(self, value: str) -> None:
+        with self._state_lock:
+            self._state = value
+            self._state_entry_timestamp = time.time()
+
+    def time_in_state(self) -> float:
+        return time.time() - self._state_entry_timestamp
+
+    def is_starting(self) -> bool:
+        return self.state == 'starting'
+
+    def wait_for_port_open(self, postmaster: PostmasterProcess, timeout: float) -> bool:
+        """Waits until PostgreSQL opens ports."""
+        for _ in polling_loop(timeout):
+            if self.cancellable.is_cancelled:
+                return False
+
+            if not postmaster.is_running():
+                logger.error('postmaster is not running')
+                self.set_state('start failed')
+                return False
+
+            isready = self.pg_isready()
+            if isready != STATE_NO_RESPONSE:
+                if isready not in [STATE_REJECT, STATE_RUNNING]:
+                    logger.warning("Can't determine PostgreSQL startup status, assuming running")
+                return True
+
+        logger.warning("Timed out waiting for PostgreSQL to start")
         return False
-    elif not silent_success:
-        click.echo('Success: {0} for member {1}'.format(action_name, member_name))
-    return True
 
+    def start(self, timeout: Optional[float] = None, task: Optional[CriticalTask] = None,
+              block_callbacks: bool = False, role: Optional[str] = None,
+              after_start: Optional[Callable[..., Any]] = None) -> Optional[bool]:
+        """Start PostgreSQL
+
+        Waits for postmaster to open ports or terminate so pg_isready can be used to check startup completion
+        or failure.
+
+        :returns: True if start was initiated and postmaster ports are open,
+                  False if start failed, and None if postgres is still starting up"""
+        # make sure we close all connections established against
+        # the former node, otherwise, we might get a stalled one
+        # after kill -9, which would report incorrect data to
+        # patroni.
+        self._connection.close()
+
+        if self.is_running():
+            logger.error('Cannot start PostgreSQL because one is already running.')
+            self.set_state('starting')
+            return True
+
+        if not block_callbacks:
+            self.__cb_pending = CallbackAction.ON_START
+
+        self.set_role(role or self.get_postgres_role_from_data_directory())
+
+        self.set_state('starting')
+        self._pending_restart = False
 
-def parse_scheduled(scheduled):
-    if (scheduled or 'now') != 'now':
         try:
-            scheduled_at = dateutil.parser.parse(scheduled)
-            if scheduled_at.tzinfo is None:
-                scheduled_at = scheduled_at.replace(tzinfo=dateutil.tz.tzlocal())
-        except (ValueError, TypeError):
-            message = 'Unable to parse scheduled timestamp ({0}). It should be in an unambiguous format (e.g. ISO 8601)'
-            raise PatroniCtlException(message.format(scheduled))
-        return scheduled_at
-
-    return None
-
-
-@ctl.command('reload', help='Reload cluster member configuration')
-@click.argument('cluster_name')
-@click.argument('member_names', nargs=-1)
-@option_citus_group
-@click.option('--role', '-r', help='Reload only members with this role', type=role_choice, default='any')
-@option_force
-@click.pass_obj
-def reload(obj, cluster_name, member_names, group, force, role):
-    cluster = get_dcs(obj, cluster_name, group).get_cluster()
-
-    members = get_members(obj, cluster, cluster_name, member_names, role, force, 'reload', group=group)
-
-    for member in members:
-        r = request_patroni(member, 'post', 'reload')
-        if r.status == 200:
-            click.echo('No changes to apply on member {0}'.format(member.name))
-        elif r.status == 202:
-            click.echo('Reload request received for member {0} and will be processed within {1} seconds'.format(
-                member.name, cluster.config.data.get('loop_wait'))
-            )
+            if not self.ensure_major_version_is_known():
+                return None
+            configuration = self.config.effective_configuration
+        except Exception:
+            return None
+
+        self.config.check_directories()
+        self.config.write_postgresql_conf(configuration)
+        self.config.resolve_connection_addresses()
+        self.config.replace_pg_hba()
+        self.config.replace_pg_ident()
+
+        options = ['--{0}={1}'.format(p, configuration[p]) for p in self.config.CMDLINE_OPTIONS
+                   if p in configuration and p not in ('wal_keep_segments', 'wal_keep_size')]
+
+        if self.cancellable.is_cancelled:
+            return False
+
+        with task or null_context():
+            if task and task.is_cancelled:
+                logger.info("PostgreSQL start cancelled.")
+                return False
+
+            self._postmaster_proc = PostmasterProcess.start(self.pgcommand('postgres'),
+                                                            self._data_dir,
+                                                            self.config.postgresql_conf,
+                                                            options)
+
+            if task:
+                task.complete(self._postmaster_proc)
+
+        start_timeout = timeout
+        if not start_timeout:
+            try:
+                start_timeout = float(self.config.get('pg_ctl_timeout', 60) or 0)
+            except ValueError:
+                start_timeout = 60
+
+        # We want postmaster to open ports before we continue
+        if not self._postmaster_proc or not self.wait_for_port_open(self._postmaster_proc, start_timeout):
+            return False
+
+        ret = self.wait_for_startup(start_timeout)
+        if ret is not None:
+            if ret and after_start:
+                after_start()
+            return ret
+        elif timeout is not None:
+            return False
         else:
-            click.echo('Failed: reload for member {0}, status code={1}, ({2})'.format(
-                member.name, r.status, r.data.decode('utf-8'))
-            )
-
-
-@ctl.command('restart', help='Restart cluster member')
-@click.argument('cluster_name')
-@click.argument('member_names', nargs=-1)
-@option_citus_group
-@click.option('--role', '-r', help='Restart only members with this role', type=role_choice, default='any')
-@click.option('--any', 'p_any', help='Restart a single member only', is_flag=True)
-@click.option('--scheduled', help='Timestamp of a scheduled restart in unambiguous format (e.g. ISO 8601)',
-              default=None)
-@click.option('--pg-version', 'version', help='Restart if the PostgreSQL version is less than provided (e.g. 9.5.2)',
-              default=None)
-@click.option('--pending', help='Restart if pending', is_flag=True)
-@click.option('--timeout',
-              help='Return error and fail over if necessary when restarting takes longer than this.')
-@option_force
-@click.pass_obj
-def restart(obj, cluster_name, group, member_names, force, role, p_any, scheduled, version, pending, timeout):
-    cluster = get_dcs(obj, cluster_name, group).get_cluster()
-
-    members = get_members(obj, cluster, cluster_name, member_names, role, force, 'restart', False, group=group)
-    if scheduled is None and not force:
-        next_hour = (datetime.datetime.now() + datetime.timedelta(hours=1)).strftime('%Y-%m-%dT%H:%M')
-        scheduled = click.prompt('When should the restart take place (e.g. ' + next_hour + ') ',
-                                 type=str, default='now')
-
-    scheduled_at = parse_scheduled(scheduled)
-    confirm_members_action(members, force, 'restart', scheduled_at)
-
-    if p_any:
-        random.shuffle(members)
-        members = members[:1]
-
-    if version is None and not force:
-        version = click.prompt('Restart if the PostgreSQL version is less than provided (e.g. 9.5.2) ',
-                               type=str, default='')
-
-    content = {}
-    if pending:
-        content['restart_pending'] = True
+            return None
 
-    if version:
+    def checkpoint(self, connect_kwargs: Optional[Dict[str, Any]] = None,
+                   timeout: Optional[float] = None) -> Optional[str]:
+        check_not_is_in_recovery = connect_kwargs is not None
+        connect_kwargs = connect_kwargs or self.config.local_connect_kwargs
+        for p in ['connect_timeout', 'options']:
+            connect_kwargs.pop(p, None)
+        if timeout:
+            connect_kwargs['connect_timeout'] = timeout
         try:
-            postgres_version_to_int(version)
-        except PatroniException as e:
-            raise PatroniCtlException(e.value)
-
-        content['postgres_version'] = version
-
-    if scheduled_at:
-        if cluster.is_paused():
-            raise PatroniCtlException("Can't schedule restart in the paused state")
-        content['schedule'] = scheduled_at.isoformat()
-
-    if timeout is not None:
-        content['timeout'] = timeout
-
-    for member in members:
-        if 'schedule' in content:
-            if force and member.data.get('scheduled_restart'):
-                r = request_patroni(member, 'delete', 'restart')
-                check_response(r, member.name, 'flush scheduled restart', True)
-
-        r = request_patroni(member, 'post', 'restart', content)
-        if r.status == 200:
-            click.echo('Success: restart on member {0}'.format(member.name))
-        elif r.status == 202:
-            click.echo('Success: restart scheduled on member {0}'.format(member.name))
-        elif r.status == 409:
-            click.echo('Failed: another restart is already scheduled on member {0}'.format(member.name))
-        else:
-            click.echo('Failed: restart for member {0}, status code={1}, ({2})'.format(
-                member.name, r.status, r.data.decode('utf-8'))
-            )
-
-
-@ctl.command('reinit', help='Reinitialize cluster member')
-@click.argument('cluster_name')
-@option_citus_group
-@click.argument('member_names', nargs=-1)
-@option_force
-@click.option('--wait', help='Wait until reinitialization completes', is_flag=True)
-@click.pass_obj
-def reinit(obj, cluster_name, group, member_names, force, wait):
-    cluster = get_dcs(obj, cluster_name, group).get_cluster()
-    members = get_members(obj, cluster, cluster_name, member_names, 'replica', force, 'reinitialize', group=group)
-
-    wait_on_members = []
-    for member in members:
-        body = {'force': force}
-        while True:
-            r = request_patroni(member, 'post', 'reinitialize', body)
-            started = check_response(r, member.name, 'reinitialize')
-            if not started and r.data.endswith(b' already in progress') \
-                    and not force and click.confirm('Do you want to cancel it and reinitialize anyway?'):
-                body['force'] = True
-                continue
-            break
-        if started and wait:
-            wait_on_members.append(member)
-
-    last_display = []
-    while wait_on_members:
-        if wait_on_members != last_display:
-            click.echo('Waiting for reinitialize to complete on: {0}'.format(
-                ", ".join(member.name for member in wait_on_members))
-            )
-            last_display[:] = wait_on_members
-        time.sleep(2)
-        for member in wait_on_members:
-            data = json.loads(request_patroni(member, 'get', 'patroni').data.decode('utf-8'))
-            if data.get('state') != 'creating replica':
-                click.echo('Reinitialize is completed on: {0}'.format(member.name))
-                wait_on_members.remove(member)
-
-
-def _do_failover_or_switchover(obj, action, cluster_name, group, leader, candidate, force, scheduled=None):
-    """
-        We want to trigger a failover or switchover for the specified cluster name.
-
-        We verify that the cluster name, leader name and candidate name are correct.
-        If so, we trigger an action and keep the client up to date.
-    """
-
-    dcs = get_dcs(obj, cluster_name, group)
-    cluster = dcs.get_cluster()
-    click.echo('Current cluster topology')
-    output_members(obj, cluster, cluster_name, group=group)
-
-    if obj.get('citus') and group is None:
-        if force:
-            raise PatroniCtlException('For Citus clusters the --group must me specified')
-        else:
-            group = click.prompt('Citus group', type=int)
-            dcs = get_dcs(obj, cluster_name, group)
-            cluster = dcs.get_cluster()
-
-    if action == 'switchover' and (cluster.leader is None or not cluster.leader.name):
-        raise PatroniCtlException('This cluster has no leader')
-
-    if leader is None:
-        if force or action == 'failover':
-            leader = cluster.leader and cluster.leader.name
+            with get_connection_cursor(**connect_kwargs) as cur:
+                cur.execute("SET statement_timeout = 0")
+                if check_not_is_in_recovery:
+                    cur.execute('SELECT pg_catalog.pg_is_in_recovery()')
+                    row = cur.fetchone()
+                    if not row or row[0]:
+                        return 'is_in_recovery=true'
+                cur.execute('CHECKPOINT')
+        except psycopg.Error:
+            logger.exception('Exception during CHECKPOINT')
+            return 'not accessible or not healty'
+
+    def stop(self, mode: str = 'fast', block_callbacks: bool = False, checkpoint: Optional[bool] = None,
+             on_safepoint: Optional[Callable[..., Any]] = None, on_shutdown: Optional[Callable[[int], Any]] = None,
+             before_shutdown: Optional[Callable[..., Any]] = None, stop_timeout: Optional[int] = None) -> bool:
+        """Stop PostgreSQL
+
+        Supports a callback when a safepoint is reached. A safepoint is when no user backend can return a successful
+        commit to users. Currently this means we wait for user backends to close. But in the future alternate mechanisms
+        could be added.
+
+        :param on_safepoint: This callback is called when no user backends are running.
+        :param on_shutdown: is called when pg_controldata starts reporting `Database cluster state: shut down`
+        :param before_shutdown: is called after running optional CHECKPOINT and before running pg_ctl stop
+        """
+        if checkpoint is None:
+            checkpoint = False if mode == 'immediate' else True
+
+        success, pg_signaled = self._do_stop(mode, block_callbacks, checkpoint, on_safepoint,
+                                             on_shutdown, before_shutdown, stop_timeout)
+        if success:
+            # block_callbacks is used during restart to avoid
+            # running start/stop callbacks in addition to restart ones
+            if not block_callbacks:
+                self.set_state('stopped')
+                if pg_signaled:
+                    self.call_nowait(CallbackAction.ON_STOP)
         else:
-            prompt = 'Standby Leader' if is_standby_cluster(cluster.config) else 'Primary'
-            leader = click.prompt(prompt, type=str, default=cluster.leader.member.name)
+            logger.warning('pg_ctl stop failed')
+            self.set_state('stop failed')
+        return success
+
+    def _do_stop(self, mode: str, block_callbacks: bool, checkpoint: bool,
+                 on_safepoint: Optional[Callable[..., Any]], on_shutdown: Optional[Callable[..., Any]],
+                 before_shutdown: Optional[Callable[..., Any]], stop_timeout: Optional[int]) -> Tuple[bool, bool]:
+        postmaster = self.is_running()
+        if not postmaster:
+            if on_safepoint:
+                on_safepoint()
+            return True, False
+
+        if checkpoint and not self.is_starting():
+            self.checkpoint(timeout=stop_timeout)
+
+        if not block_callbacks:
+            self.set_state('stopping')
+
+        # invoke user-directed before stop script
+        self._before_stop()
+
+        if before_shutdown:
+            before_shutdown()
+
+        # Send signal to postmaster to stop
+        success = postmaster.signal_stop(mode, self.pgcommand('pg_ctl'))
+        if success is not None:
+            if success and on_safepoint:
+                on_safepoint()
+            return success, True
+
+        # We can skip safepoint detection if we don't have a callback
+        if on_safepoint:
+            # Wait for our connection to terminate so we can be sure that no new connections are being initiated
+            self._wait_for_connection_close(postmaster)
+            postmaster.wait_for_user_backends_to_close(stop_timeout)
+            on_safepoint()
+
+        if on_shutdown and mode in ('fast', 'smart'):
+            i = 0
+            # Wait for pg_controldata `Database cluster state:` to change to "shut down"
+            while postmaster.is_running():
+                data = self.controldata()
+                if data.get('Database cluster state', '') == 'shut down':
+                    on_shutdown(self.latest_checkpoint_location())
+                    break
+                elif data.get('Database cluster state', '').startswith('shut down'):  # shut down in recovery
+                    break
+                elif stop_timeout and i >= stop_timeout:
+                    stop_timeout = 0
+                    break
+                time.sleep(STOP_POLLING_INTERVAL)
+                i += STOP_POLLING_INTERVAL
 
-    if leader is not None and cluster.leader and cluster.leader.member.name != leader:
-        raise PatroniCtlException('Member {0} is not the leader of cluster {1}'.format(leader, cluster_name))
+        try:
+            postmaster.wait(timeout=stop_timeout)
+        except TimeoutExpired:
+            logger.warning("Timeout during postmaster stop, aborting Postgres.")
+            if not self.terminate_postmaster(postmaster, mode, stop_timeout):
+                postmaster.wait()
+
+        return True, True
+
+    def terminate_postmaster(self, postmaster: PostmasterProcess, mode: str,
+                             stop_timeout: Optional[int]) -> Optional[bool]:
+        if mode in ['fast', 'smart']:
+            try:
+                success = postmaster.signal_stop('immediate', self.pgcommand('pg_ctl'))
+                if success:
+                    return True
+                postmaster.wait(timeout=stop_timeout)
+                return True
+            except TimeoutExpired:
+                pass
+        logger.warning("Sending SIGKILL to Postmaster and its children")
+        return postmaster.signal_kill()
+
+    def terminate_starting_postmaster(self, postmaster: PostmasterProcess) -> None:
+        """Terminates a postmaster that has not yet opened ports or possibly even written a pid file. Blocks
+        until the process goes away."""
+        postmaster.signal_stop('immediate', self.pgcommand('pg_ctl'))
+        postmaster.wait()
 
-    # excluding members with nofailover tag
-    candidate_names = [str(m.name) for m in cluster.members if m.name != leader and not m.nofailover]
-    # We sort the names for consistent output to the client
-    candidate_names.sort()
-
-    if not candidate_names:
-        raise PatroniCtlException('No candidates found to {0} to'.format(action))
-
-    if candidate is None and not force:
-        candidate = click.prompt('Candidate ' + str(candidate_names), type=str, default='')
-
-    if action == 'failover' and not candidate:
-        raise PatroniCtlException('Failover could be performed only to a specific candidate')
-
-    if candidate == leader:
-        raise PatroniCtlException(action.title() + ' target and source are the same.')
-
-    if candidate and candidate not in candidate_names:
-        raise PatroniCtlException('Member {0} does not exist in cluster {1}'.format(candidate, cluster_name))
-
-    scheduled_at_str = None
-    scheduled_at = None
-
-    if action == 'switchover':
-        if scheduled is None and not force:
-            next_hour = (datetime.datetime.now() + datetime.timedelta(hours=1)).strftime('%Y-%m-%dT%H:%M')
-            scheduled = click.prompt('When should the switchover take place (e.g. ' + next_hour + ' ) ',
-                                     type=str, default='now')
-
-        scheduled_at = parse_scheduled(scheduled)
-        if scheduled_at:
-            if cluster.is_paused():
-                raise PatroniCtlException("Can't schedule switchover in the paused state")
-            scheduled_at_str = scheduled_at.isoformat()
-
-    failover_value = {'leader': leader, 'candidate': candidate, 'scheduled_at': scheduled_at_str}
-
-    logging.debug(failover_value)
-
-    # By now we have established that the leader exists and the candidate exists
-    if not force:
-        demote_msg = ', demoting current leader ' + leader if leader else ''
-        if scheduled_at_str:
-            if not click.confirm('Are you sure you want to schedule {0} of cluster {1} at {2}{3}?'
-                                 .format(action, cluster_name, scheduled_at_str, demote_msg)):
-                raise PatroniCtlException('Aborting scheduled ' + action)
-        else:
-            if not click.confirm('Are you sure you want to {0} cluster {1}{2}?'
-                                 .format(action, cluster_name, demote_msg)):
-                raise PatroniCtlException('Aborting ' + action)
-
-    r = None
-    try:
-        member = cluster.leader.member if cluster.leader else cluster.get_member(candidate, False)
-
-        r = request_patroni(member, 'post', action, failover_value)
-
-        # probably old patroni, which doesn't support switchover yet
-        if r.status == 501 and action == 'switchover' and b'Server does not support this operation' in r.data:
-            r = request_patroni(member, 'post', 'failover', failover_value)
-
-        if r.status in (200, 202):
-            logging.debug(r)
-            cluster = dcs.get_cluster()
-            logging.debug(cluster)
-            click.echo('{0} {1}'.format(timestamp(), r.data.decode('utf-8')))
+    def _wait_for_connection_close(self, postmaster: PostmasterProcess) -> None:
+        try:
+            with self.connection().cursor() as cur:
+                while postmaster.is_running():  # Need a timeout here?
+                    cur.execute("SELECT 1")
+                    time.sleep(STOP_POLLING_INTERVAL)
+        except psycopg.Error:
+            pass
+
+    def reload(self, block_callbacks: bool = False) -> bool:
+        ret = self.pg_ctl('reload')
+        if ret and not block_callbacks:
+            self.call_nowait(CallbackAction.ON_RELOAD)
+        return ret
+
+    def check_for_startup(self) -> bool:
+        """Checks PostgreSQL status and returns if PostgreSQL is in the middle of startup."""
+        return self.is_starting() and not self.check_startup_state_changed()
+
+    def check_startup_state_changed(self) -> bool:
+        """Checks if PostgreSQL has completed starting up or failed or still starting.
+
+        Should only be called when state == 'starting'
+
+        :returns: True if state was changed from 'starting'
+        """
+        ready = self.pg_isready()
+
+        if ready == STATE_REJECT:
+            return False
+        elif ready == STATE_NO_RESPONSE:
+            ret = not self.is_running()
+            if ret:
+                self.set_state('start failed')
+                self.slots_handler.schedule(False)  # TODO: can remove this?
+                self.config.save_configuration_files(True)  # TODO: maybe remove this?
+            return ret
         else:
-            click.echo('{0} failed, details: {1}, {2}'.format(action.title(), r.status, r.data.decode('utf-8')))
-            return
-    except Exception:
-        logging.exception(r)
-        logging.warning('Failing over to DCS')
-        click.echo('{0} Could not {1} using Patroni api, falling back to DCS'.format(timestamp(), action))
-        dcs.manual_failover(leader, candidate, scheduled_at=scheduled_at)
-
-    output_members(obj, cluster, cluster_name, group=group)
-
-
-@ctl.command('failover', help='Failover to a replica')
-@arg_cluster_name
-@option_citus_group
-@click.option('--leader', '--primary', '--master', 'leader', help='The name of the current leader', default=None)
-@click.option('--candidate', help='The name of the candidate', default=None)
-@option_force
-@click.pass_obj
-def failover(obj, cluster_name, group, leader, candidate, force):
-    action = 'switchover' if leader else 'failover'
-    _do_failover_or_switchover(obj, action, cluster_name, group, leader, candidate, force)
-
-
-@ctl.command('switchover', help='Switchover to a replica')
-@arg_cluster_name
-@option_citus_group
-@click.option('--leader', '--primary', '--master', 'leader', help='The name of the current leader', default=None)
-@click.option('--candidate', help='The name of the candidate', default=None)
-@click.option('--scheduled', help='Timestamp of a scheduled switchover in unambiguous format (e.g. ISO 8601)',
-              default=None)
-@option_force
-@click.pass_obj
-def switchover(obj, cluster_name, group, leader, candidate, force, scheduled):
-    _do_failover_or_switchover(obj, 'switchover', cluster_name, group, leader, candidate, force, scheduled)
-
-
-def generate_topology(level, member, topology):
-    members = topology.get(member['name'], [])
-
-    if level > 0:
-        member['name'] = '{0}+ {1}'.format((' ' * (level - 1) * 2), member['name'])
-
-    if member['name']:
-        yield member
-
-    for member in members:
-        for member in generate_topology(level + 1, member, topology):
-            yield member
-
-
-def topology_sort(members):
-    topology = defaultdict(list)
-    leader = next((m for m in members if m['role'].endswith('leader')), {'name': None})
-    replicas = set(member['name'] for member in members if not member['role'].endswith('leader'))
-    for member in members:
-        if not member['role'].endswith('leader'):
-            parent = member.get('tags', {}).get('replicatefrom')
-            parent = parent if parent and parent != member['name'] and parent in replicas else leader['name']
-            topology[parent].append(member)
-    for member in generate_topology(0, leader, topology):
-        yield member
-
-
-def get_cluster_service_info(cluster):
-    service_info = []
-    if cluster.get('pause'):
-        service_info.append('Maintenance mode: on')
-
-    if 'scheduled_switchover' in cluster:
-        info = 'Switchover scheduled at: ' + cluster['scheduled_switchover']['at']
-        for name in ('from', 'to'):
-            if name in cluster['scheduled_switchover']:
-                info += '\n{0:>24}: {1}'.format(name, cluster['scheduled_switchover'][name])
-        service_info.append(info)
-    return service_info
-
-
-def output_members(obj, cluster, name, extended=False, fmt='pretty', group=None):
-    rows = []
-    logging.debug(cluster)
-
-    initialize = {None: 'uninitialized', '': 'initializing'}.get(cluster.initialize, cluster.initialize)
-    columns = ['Cluster', 'Member', 'Host', 'Role', 'State', 'TL', 'Lag in MB']
-
-    clusters = {group or 0: cluster_as_json(cluster)}
-
-    is_citus_cluster = obj.get('citus')
-    if is_citus_cluster:
-        columns.insert(1, 'Group')
-        if group is None:
-            clusters.update({g: cluster_as_json(c) for g, c in cluster.workers.items()})
-
-    all_members = [m for c in clusters.values() for m in c['members'] if 'host' in m]
-
-    for c in ('Pending restart', 'Scheduled restart', 'Tags'):
-        if extended or any(m.get(c.lower().replace(' ', '_')) for m in all_members):
-            columns.append(c)
-
-    # Show Host as 'host:port' if somebody is running on non-standard port or two nodes are running on the same host
-    append_port = any('port' in m and m['port'] != 5432 for m in all_members) or\
-        len(set(m['host'] for m in all_members)) < len(all_members)
-
-    sort = topology_sort if fmt == 'topology' else iter
-    for g, cluster in sorted(clusters.items()):
-        for member in sort(cluster['members']):
-            logging.debug(member)
-
-            lag = member.get('lag', '')
-            member.update(cluster=name, member=member['name'], group=g,
-                          host=member.get('host', ''), tl=member.get('timeline', ''),
-                          role=member['role'].replace('_', ' ').title(),
-                          lag_in_mb=round(lag/1024/1024) if isinstance(lag, int) else lag,
-                          pending_restart='*' if member.get('pending_restart') else '')
-
-            if append_port and member['host'] and member.get('port'):
-                member['host'] = ':'.join([member['host'], str(member['port'])])
-
-            if 'scheduled_restart' in member:
-                value = member['scheduled_restart']['schedule']
-                if 'postgres_version' in member['scheduled_restart']:
-                    value += ' if version < {0}'.format(member['scheduled_restart']['postgres_version'])
-                member['scheduled_restart'] = value
-
-            rows.append([member.get(n.lower().replace(' ', '_'), '') for n in columns])
-
-    title = 'Citus cluster' if is_citus_cluster else 'Cluster'
-    group_title = '' if group is None else 'group: {0}, '.format(group)
-    title_details = group_title and ' ({0}{1})'.format(group_title, initialize)
-    title = ' {0}: {1}{2} '.format(title, name, title_details)
-    print_output(columns, rows, {'Group': 'r', 'Lag in MB': 'r', 'TL': 'r'}, fmt, title)
-
-    if fmt not in ('pretty', 'topology'):  # Omit service info when using machine-readable formats
-        return
-
-    for g, cluster in sorted(clusters.items()):
-        service_info = get_cluster_service_info(cluster)
-        if service_info:
-            if is_citus_cluster and group is None:
-                click.echo('Citus group: {0}'.format(g))
-            click.echo(' ' + '\n '.join(service_info))
-
-
-@ctl.command('list', help='List the Patroni members for a given Patroni')
-@click.argument('cluster_names', nargs=-1)
-@option_citus_group
-@click.option('--extended', '-e', help='Show some extra information', is_flag=True)
-@click.option('--timestamp', '-t', 'ts', help='Print timestamp', is_flag=True)
-@option_format
-@option_watch
-@option_watchrefresh
-@click.pass_obj
-def members(obj, cluster_names, group, fmt, watch, w, extended, ts):
-    if not cluster_names:
-        if 'scope' in obj:
-            cluster_names = [obj['scope']]
-        if not cluster_names:
-            return logging.warning('Listing members: No cluster names were provided')
-
-    for _ in watching(w, watch):
-        if ts:
-            click.echo(timestamp(0))
-
-        for cluster_name in cluster_names:
-            dcs = get_dcs(obj, cluster_name, group)
-
-            cluster = dcs.get_cluster()
-            output_members(obj, cluster, cluster_name, extended, fmt, group)
-
-
-@ctl.command('topology', help='Prints ASCII topology for given cluster')
-@click.argument('cluster_names', nargs=-1)
-@option_citus_group
-@option_watch
-@option_watchrefresh
-@click.pass_obj
-@click.pass_context
-def topology(ctx, obj, cluster_names, group, watch, w):
-    ctx.forward(members, fmt='topology')
-
-
-def timestamp(precision=6):
-    return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:precision - 7]
-
-
-@ctl.command('flush', help='Discard scheduled events')
-@click.argument('cluster_name')
-@option_citus_group
-@click.argument('member_names', nargs=-1)
-@click.argument('target', type=click.Choice(['restart', 'switchover']))
-@click.option('--role', '-r', help='Flush only members with this role', type=role_choice, default='any')
-@option_force
-@click.pass_obj
-def flush(obj, cluster_name, group, member_names, force, role, target):
-    dcs = get_dcs(obj, cluster_name, group)
-    cluster = dcs.get_cluster()
-
-    if target == 'restart':
-        for member in get_members(obj, cluster, cluster_name, member_names, role, force, 'flush', group=group):
-            if member.data.get('scheduled_restart'):
-                r = request_patroni(member, 'delete', 'restart')
-                check_response(r, member.name, 'flush scheduled restart')
-            else:
-                click.echo('No scheduled restart for member {0}'.format(member.name))
-    elif target == 'switchover':
-        failover = cluster.failover
-        if not failover or not failover.scheduled_at:
-            return click.echo('No pending scheduled switchover')
-        for member in get_all_members_leader_first(cluster):
+            if ready != STATE_RUNNING:
+                # Bad configuration or unexpected OS error. No idea of PostgreSQL status.
+                # Let the main loop of run cycle clean up the mess.
+                logger.warning("%s status returned from pg_isready",
+                               "Unknown" if ready == STATE_UNKNOWN else "Invalid")
+            self.set_state('running')
+            self.slots_handler.schedule()
+            self.config.save_configuration_files(True)
+            # TODO: __cb_pending can be None here after PostgreSQL restarts on its own. Do we want to call the callback?
+            # Previously we didn't even notice.
+            action = self.__cb_pending or CallbackAction.ON_START
+            self.call_nowait(action)
+            self.__cb_pending = None
+
+            return True
+
+    def wait_for_startup(self, timeout: float = 0) -> Optional[bool]:
+        """Waits for PostgreSQL startup to complete or fail.
+
+        :returns: True if start was successful, False otherwise"""
+        if not self.is_starting():
+            # Should not happen
+            logger.warning("wait_for_startup() called when not in starting state")
+
+        while not self.check_startup_state_changed():
+            if self.cancellable.is_cancelled or timeout and self.time_in_state() > timeout:
+                return None
+            time.sleep(1)
+
+        return self.state == 'running'
+
+    def restart(self, timeout: Optional[float] = None, task: Optional[CriticalTask] = None,
+                block_callbacks: bool = False, role: Optional[str] = None,
+                before_shutdown: Optional[Callable[..., Any]] = None,
+                after_start: Optional[Callable[..., Any]] = None) -> Optional[bool]:
+        """Restarts PostgreSQL.
+
+        When timeout parameter is set the call will block either until PostgreSQL has started, failed to start or
+        timeout arrives.
+
+        :returns: True when restart was successful and timeout did not expire when waiting.
+        """
+        self.set_state('restarting')
+        if not block_callbacks:
+            self.__cb_pending = CallbackAction.ON_RESTART
+        ret = self.stop(block_callbacks=True, before_shutdown=before_shutdown)\
+            and self.start(timeout, task, True, role, after_start)
+        if not ret and not self.is_starting():
+            self.set_state('restart failed ({0})'.format(self.state))
+        return ret
+
+    def is_healthy(self) -> bool:
+        if not self.is_running():
+            logger.warning('Postgresql is not running.')
+            return False
+        return True
+
+    def get_guc_value(self, name: str) -> Optional[str]:
+        cmd = [self.pgcommand('postgres'), '-D', self._data_dir, '-C', name,
+               '--config-file={}'.format(self.config.postgresql_conf)]
+        try:
+            data = subprocess.check_output(cmd)
+            if data:
+                return data.decode('utf-8').strip()
+        except Exception as e:
+            logger.error('Failed to execute %s: %r', cmd, e)
+
+    def controldata(self) -> Dict[str, str]:
+        """ return the contents of pg_controldata, or non-True value if pg_controldata call failed """
+        # Don't try to call pg_controldata during backup restore
+        if self._version_file_exists() and self.state != 'creating replica':
             try:
-                r = request_patroni(member, 'delete', 'switchover')
-                if r.status in (200, 404):
-                    prefix = 'Success' if r.status == 200 else 'Failed'
-                    return click.echo('{0}: {1}'.format(prefix, r.data.decode('utf-8')))
-            except Exception as err:
-                logging.warning(str(err))
-                logging.warning('Member %s is not accessible', member.name)
-
-            click.echo('Failed: member={0}, status_code={1}, ({2})'.format(
-                member.name, r.status, r.data.decode('utf-8')))
-
-        logging.warning('Failing over to DCS')
-        click.echo('{0} Could not find any accessible member of cluster {1}'.format(timestamp(), cluster_name))
-        dcs.manual_failover('', '', index=failover.index)
-
-
-def wait_until_pause_is_applied(dcs, paused, old_cluster):
-    click.echo("'{0}' request sent, waiting until it is recognized by all nodes".format(paused and 'pause' or 'resume'))
-    old = {m.name: m.index for m in old_cluster.members if m.api_url}
-    loop_wait = old_cluster.config.data.get('loop_wait', dcs.loop_wait)
-
-    for _ in polling_loop(loop_wait + 1):
-        cluster = dcs.get_cluster()
-        if all(m.data.get('pause', False) == paused for m in cluster.members if m.name in old):
-            break
-    else:
-        remaining = [m.name for m in cluster.members if m.data.get('pause', False) != paused
-                     and m.name in old and old[m.name] != m.index]
-        if remaining:
-            return click.echo("{0} members didn't recognized pause state after {1} seconds"
-                              .format(', '.join(remaining), loop_wait))
-    return click.echo('Success: cluster management is {0}'.format(paused and 'paused' or 'resumed'))
-
-
-def toggle_pause(config, cluster_name, group, paused, wait):
-    dcs = get_dcs(config, cluster_name, group)
-    cluster = dcs.get_cluster()
-    if cluster.is_paused() == paused:
-        raise PatroniCtlException('Cluster is {0} paused'.format(paused and 'already' or 'not'))
+                env = {**os.environ, 'LANG': 'C', 'LC_ALL': 'C'}
+                data = subprocess.check_output([self.pgcommand('pg_controldata'), self._data_dir], env=env)
+                if data:
+                    data = filter(lambda e: ':' in e, data.decode('utf-8').splitlines())
+                    # pg_controldata output depends on major version. Some of parameters are prefixed by 'Current '
+                    return {k.replace('Current ', '', 1): v.strip() for k, v in map(lambda e: e.split(':', 1), data)}
+            except subprocess.CalledProcessError:
+                logger.exception("Error when calling pg_controldata")
+        return {}
+
+    def waldump(self, timeline: Union[int, str], lsn: str, limit: int) -> Tuple[Optional[bytes], Optional[bytes]]:
+        cmd = self.pgcommand('pg_{0}dump'.format(self.wal_name))
+        env = {**os.environ, 'LANG': 'C', 'LC_ALL': 'C', 'PGDATA': self._data_dir}
+        try:
+            waldump = subprocess.Popen([cmd, '-t', str(timeline), '-s', lsn, '-n', str(limit)],
+                                       stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)
+            out, err = waldump.communicate()
+            waldump.wait()
+            return out, err
+        except Exception as e:
+            logger.error('Failed to execute `%s -t %s -s %s -n %s`: %r', cmd, timeline, lsn, limit, e)
+            return None, None
+
+    @contextmanager
+    def get_replication_connection_cursor(self, host: Optional[str] = None, port: int = 5432,
+                                          **kwargs: Any) -> Generator[Union['cursor', 'Cursor[Any]'], None, None]:
+        conn_kwargs = self.config.replication.copy()
+        conn_kwargs.update(host=host, port=int(port) if port else None, user=conn_kwargs.pop('username'),
+                           connect_timeout=3, replication=1, options='-c statement_timeout=2000')
+        with get_connection_cursor(**conn_kwargs) as cur:
+            yield cur
 
-    for member in get_all_members_leader_first(cluster):
+    def get_replica_timeline(self) -> Optional[int]:
         try:
-            r = request_patroni(member, 'patch', 'config', {'pause': paused or None})
-        except Exception as err:
-            logging.warning(str(err))
-            logging.warning('Member %s is not accessible', member.name)
-            continue
-
-        if r.status == 200:
-            if wait:
-                wait_until_pause_is_applied(dcs, paused, cluster)
+            with self.get_replication_connection_cursor(**self.config.local_replication_address) as cur:
+                cur.execute('IDENTIFY_SYSTEM')
+                row = cur.fetchone()
+                return row[1] if row else None
+        except Exception:
+            logger.exception('Can not fetch local timeline and lsn from replication connection')
+
+    def replica_cached_timeline(self, primary_timeline: Optional[int]) -> Optional[int]:
+        if not self._cached_replica_timeline or not primary_timeline\
+                or self._cached_replica_timeline != primary_timeline:
+            self._cached_replica_timeline = self.get_replica_timeline()
+        return self._cached_replica_timeline
+
+    def get_primary_timeline(self) -> int:
+        """:returns: current timeline if postgres is running as a primary or 0."""
+        return self._cluster_info_state_get('timeline') or 0
+
+    def get_history(self, timeline: int) -> List[Union[Tuple[int, int, str], Tuple[int, int, str, str, str]]]:
+        history_path = os.path.join(self.wal_dir, '{0:08X}.history'.format(timeline))
+        history_mtime = mtime(history_path)
+        history: List[Union[Tuple[int, int, str], Tuple[int, int, str, str, str]]] = []
+        if history_mtime:
+            try:
+                with open(history_path, 'r') as f:
+                    history_content = f.read()
+                history = list(parse_history(history_content))
+                if history[-1][0] == timeline - 1:
+                    history_mtime = datetime.fromtimestamp(history_mtime).replace(tzinfo=tz.tzlocal())
+                    history[-1] = history[-1][:3] + (history_mtime.isoformat(), self.name)
+            except Exception:
+                logger.exception('Failed to read and parse %s', (history_path,))
+        return history
+
+    def follow(self, member: Union[Leader, Member, None], role: str = 'replica',
+               timeout: Optional[float] = None, do_reload: bool = False) -> Optional[bool]:
+        """Reconfigure postgres to follow a new member or use different recovery parameters.
+
+        Method may call `on_role_change` callback if role is changing.
+
+        :param member: The member to follow
+        :param role: The desired role, normally 'replica', but could also be a 'standby_leader'
+        :param timeout: start timeout, how long should the `start()` method wait for postgres accepting connections
+        :param do_reload: indicates that after updating postgresql.conf we just need to do a reload instead of restart
+
+        :returns: True - if restart/reload were successfully performed,
+                  False - if restart/reload failed
+                  None - if nothing was done or if Postgres is still in starting state after `timeout` seconds."""
+
+        if not self.ensure_major_version_is_known():
+            return None
+
+        recovery_params = self.config.build_recovery_params(member)
+        self.config.write_recovery_conf(recovery_params)
+
+        # When we demoting the primary or standby_leader to replica or promoting replica to a standby_leader
+        # and we know for sure that postgres was already running before, we will only execute on_role_change
+        # callback and prevent execution of on_restart/on_start callback.
+        # If the role remains the same (replica or standby_leader), we will execute on_start or on_restart
+        change_role = self.cb_called and (self.role in ('master', 'primary', 'demoted')
+                                          or not {'standby_leader', 'replica'} - {self.role, role})
+        if change_role:
+            self.__cb_pending = CallbackAction.NOOP
+
+        ret = True
+        if self.is_running():
+            if do_reload:
+                self.config.write_postgresql_conf()
+                ret = self.reload(block_callbacks=change_role)
+                if ret and change_role:
+                    self.set_role(role)
             else:
-                click.echo('Success: cluster management is {0}'.format(paused and 'paused' or 'resumed'))
+                ret = self.restart(block_callbacks=change_role, role=role)
         else:
-            click.echo('Failed: {0} cluster management status code={1}, ({2})'.format(
-                       paused and 'pause' or 'resume', r.status, r.data.decode('utf-8')))
-        break
-    else:
-        raise PatroniCtlException('Can not find accessible cluster member')
-
-
-@ctl.command('pause', help='Disable auto failover')
-@arg_cluster_name
-@option_default_citus_group
-@click.pass_obj
-@click.option('--wait', help='Wait until pause is applied on all nodes', is_flag=True)
-def pause(obj, cluster_name, group, wait):
-    return toggle_pause(obj, cluster_name, group, True, wait)
-
-
-@ctl.command('resume', help='Resume auto failover')
-@arg_cluster_name
-@option_default_citus_group
-@click.option('--wait', help='Wait until pause is cleared on all nodes', is_flag=True)
-@click.pass_obj
-def resume(obj, cluster_name, group, wait):
-    return toggle_pause(obj, cluster_name, group, False, wait)
+            ret = self.start(timeout=timeout, block_callbacks=change_role, role=role) or None
+
+        if change_role:
+            # TODO: postpone this until start completes, or maybe do even earlier
+            self.call_nowait(CallbackAction.ON_ROLE_CHANGE)
+        return ret
 
+    def _wait_promote(self, wait_seconds: int) -> Optional[bool]:
+        for _ in polling_loop(wait_seconds):
+            data = self.controldata()
+            if data.get('Database cluster state') == 'in production':
+                self.set_role('master')
+                return True
+
+    def _pre_promote(self) -> bool:
+        """
+        Runs a fencing script after the leader lock is acquired but before the replica is promoted.
+        If the script exits with a non-zero code, promotion does not happen and the leader key is removed from DCS.
+        """
+
+        cmd = self.config.get('pre_promote')
+        if not cmd:
+            return True
+
+        ret = self.cancellable.call(shlex.split(cmd))
+        if ret is not None:
+            logger.info('pre_promote script `%s` exited with %s', cmd, ret)
+        return ret == 0
+
+    def _before_stop(self) -> None:
+        """Synchronously run a script prior to stopping postgres."""
+
+        cmd = self.config.get('before_stop')
+        if cmd:
+            self._do_before_stop(cmd)
 
-@contextmanager
-def temporary_file(contents, suffix='', prefix='tmp'):
-    """Creates a temporary file with specified contents that persists for the context.
+    def _do_before_stop(self, cmd: str) -> None:
+        try:
+            ret = self.cancellable.call(shlex.split(cmd))
+            if ret is not None:
+                logger.info('before_stop script `%s` exited with %s', cmd, ret)
+        except Exception as e:
+            logger.error('Exception when calling `%s`: %r', cmd, e)
+
+    def promote(self, wait_seconds: int, task: CriticalTask, before_promote: Optional[Callable[..., Any]] = None,
+                on_success: Optional[Callable[..., Any]] = None) -> Optional[bool]:
+        if self.role in ('promoted', 'master', 'primary'):
+            return True
+
+        ret = self._pre_promote()
+        with task:
+            if task.is_cancelled:
+                return False
+            task.complete(ret)
+
+        if ret is False:
+            return False
+
+        if self.cancellable.is_cancelled:
+            logger.info("PostgreSQL promote cancelled.")
+            return False
 
-    :param contents: binary string that will be written to the file.
-    :param prefix: will be prefixed to the filename.
-    :param suffix: will be appended to the filename.
-    :returns path of the created file.
-    """
-    tmp = tempfile.NamedTemporaryFile(suffix=suffix, prefix=prefix, delete=False)
-    with tmp:
-        tmp.write(contents)
-
-    try:
-        yield tmp.name
-    finally:
-        os.unlink(tmp.name)
-
-
-def show_diff(before_editing, after_editing):
-    """Shows a diff between two strings.
-
-    If the output is to a tty the diff will be colored. Inputs are expected to be unicode strings.
-    """
-    def listify(string):
-        return [line + '\n' for line in string.rstrip('\n').split('\n')]
-
-    unified_diff = difflib.unified_diff(listify(before_editing), listify(after_editing))
-
-    if sys.stdout.isatty():
-        buf = io.StringIO()
-        for line in unified_diff:
-            buf.write(str(line))
-        buf.seek(0)
-
-        class opts:
-            side_by_side = False
-            width = 80
-            tab_width = 8
-            wrap = True
-            pager = next(
-                (
-                    os.path.basename(p)
-                    for p in (os.environ.get('PAGER'), "less", "more")
-                    if p is not None and shutil.which(p)
-                ),
-                None,
-            )
-            pager_options = None
-
-        if opts.pager is None:
-            raise PatroniCtlException(
-                'No pager could be found. Either set PAGER environment variable with '
-                'your pager or install either "less" or "more" in the host.'
-            )
-
-        # if we end up selecting "less" as "pager" then we set "pager" attribute
-        # to "None". "less" is the default pager for "ydiff" module, and that
-        # module adds some command-line options to "less" when "pager" is "None"
-        if opts.pager == 'less':
-            opts.pager = None
-
-        markup_to_pager(PatchStream(buf), opts)
-    else:
-        for line in unified_diff:
-            click.echo(line.rstrip('\n'))
-
-
-def format_config_for_editing(data, default_flow_style=False):
-    """Formats configuration as YAML for human consumption.
-
-    :param data: configuration as nested dictionaries
-    :returns unicode YAML of the configuration"""
-    return yaml.safe_dump(data, default_flow_style=default_flow_style, encoding=None, allow_unicode=True, width=200)
-
-
-def apply_config_changes(before_editing, data, kvpairs):
-    """Applies config changes specified as a list of key-value pairs.
-
-    Keys are interpreted as dotted paths into the configuration data structure. Except for paths beginning with
-    `postgresql.parameters` where rest of the path is used directly to allow for PostgreSQL GUCs containing dots.
-    Values are interpreted as YAML values.
-
-    :param before_editing: human representation before editing
-    :param data: configuration datastructure
-    :param kvpairs: list of strings containing key value pairs separated by =
-    :returns tuple of human readable and parsed datastructure after changes
-    """
-    changed_data = copy.deepcopy(data)
-
-    def set_path_value(config, path, value, prefix=()):
-        # Postgresql GUCs can't be nested, but can contain dots so we re-flatten the structure for this case
-        if prefix == ('postgresql', 'parameters'):
-            path = ['.'.join(path)]
-
-        key = path[0]
-        if len(path) == 1:
-            if value is None:
-                config.pop(key, None)
-            else:
-                config[key] = value
-        else:
-            if not isinstance(config.get(key), dict):
-                config[key] = {}
-            set_path_value(config[key], path[1:], value, prefix + (key,))
-            if config[key] == {}:
-                del config[key]
-
-    for pair in kvpairs:
-        if not pair or "=" not in pair:
-            raise PatroniCtlException("Invalid parameter setting {0}".format(pair))
-        key_path, value = pair.split("=", 1)
-        set_path_value(changed_data, key_path.strip().split("."), yaml.safe_load(value))
-
-    return format_config_for_editing(changed_data), changed_data
-
-
-def apply_yaml_file(data, filename):
-    """Applies changes from a YAML file to configuration
-
-    :param data: configuration datastructure
-    :param filename: name of the YAML file, - is taken to mean standard input
-    :returns tuple of human readable and parsed datastructure after changes
-    """
-    changed_data = copy.deepcopy(data)
-
-    if filename == '-':
-        new_options = yaml.safe_load(sys.stdin)
-    else:
-        with open(filename) as fd:
-            new_options = yaml.safe_load(fd)
-
-    patch_config(changed_data, new_options)
-
-    return format_config_for_editing(changed_data), changed_data
-
-
-def invoke_editor(before_editing, cluster_name):
-    """Starts editor command to edit configuration in human readable format
-
-    :param before_editing: human representation before editing
-    :returns tuple of human readable and parsed datastructure after changes
-    """
-
-    editor_cmd = os.environ.get('EDITOR')
-    if not editor_cmd:
-        for editor in ('editor', 'vi'):
-            editor_cmd = shutil.which(editor)
-            if editor_cmd:
-                logging.debug('Setting fallback editor_cmd=%s', editor)
-                break
-    if not editor_cmd:
-        raise PatroniCtlException('EDITOR environment variable is not set. editor or vi are not available')
-
-    with temporary_file(contents=before_editing.encode('utf-8'),
-                        suffix='.yaml',
-                        prefix='{0}-config-'.format(cluster_name)) as tmpfile:
-        ret = subprocess.call([editor_cmd, tmpfile])
+        if before_promote is not None:
+            before_promote()
+
+        self.slots_handler.on_promote()
+        self.citus_handler.schedule_cache_rebuild()
+
+        ret = self.pg_ctl('promote', '-W')
         if ret:
-            raise PatroniCtlException("Editor exited with return code {0}".format(ret))
+            self.set_role('promoted')
+            if on_success is not None:
+                on_success()
+            self.call_nowait(CallbackAction.ON_ROLE_CHANGE)
+            ret = self._wait_promote(wait_seconds)
+        return ret
 
-        with codecs.open(tmpfile, encoding='utf-8') as fd:
-            after_editing = fd.read()
+    @staticmethod
+    def _wal_position(is_leader: bool, wal_position: int,
+                      received_location: Optional[int], replayed_location: Optional[int]) -> int:
+        return wal_position if is_leader else max(received_location or 0, replayed_location or 0)
+
+    def timeline_wal_position(self) -> Tuple[int, int, Optional[int]]:
+        # This method could be called from different threads (simultaneously with some other `_query` calls).
+        # If it is called not from main thread we will create a new cursor to execute statement.
+        if current_thread().ident == self.__thread_ident:
+            timeline = self._cluster_info_state_get('timeline') or 0
+            wal_position = self._cluster_info_state_get('wal_position') or 0
+            replayed_location = self.replayed_location()
+            received_location = self.received_location()
+            pg_control_timeline = self._cluster_info_state_get('pg_control_timeline')
+        else:
+            with self.connection().cursor() as cursor:
+                cursor.execute(self.cluster_info_query.encode('utf-8'))
+                row = cursor.fetchone()
+                if TYPE_CHECKING:  # pragma: no cover
+                    assert row is not None
+                (timeline, wal_position, replayed_location, received_location, _, pg_control_timeline) = row[:6]
 
-        return after_editing, yaml.safe_load(after_editing)
+        wal_position = self._wal_position(bool(timeline), wal_position, received_location, replayed_location)
+        return (timeline, wal_position, pg_control_timeline)
 
+    def postmaster_start_time(self) -> Optional[str]:
+        try:
+            query = "SELECT " + self.POSTMASTER_START_TIME
+            if current_thread().ident == self.__thread_ident:
+                row = self.query(query).fetchone()
+            else:
+                with self.connection().cursor() as cursor:
+                    cursor.execute(query)
+                    row = cursor.fetchone()
+            return row[0].isoformat(sep=' ') if row else None
+        except psycopg.Error:
+            return None
+
+    def last_operation(self) -> int:
+        return self._wal_position(self.is_leader(), self._cluster_info_state_get('wal_position') or 0,
+                                  self.received_location(), self.replayed_location())
+
+    def configure_server_parameters(self) -> None:
+        self._major_version = self.get_major_version()
+        self.config.setup_server_parameters()
+
+    def ensure_major_version_is_known(self) -> bool:
+        """Calls configure_server_parameters() if `_major_version` is not known
+
+        :returns: `True` if `_major_version` is set, otherwise `False`"""
+
+        if not self._major_version:
+            self.configure_server_parameters()
+        return self._major_version > 0
+
+    def pg_wal_realpath(self) -> Dict[str, str]:
+        """Returns a dict containing the symlink (key) and target (value) for the wal directory"""
+        links: Dict[str, str] = {}
+        for pg_wal_dir in ('pg_xlog', 'pg_wal'):
+            pg_wal_path = os.path.join(self._data_dir, pg_wal_dir)
+            if os.path.exists(pg_wal_path) and os.path.islink(pg_wal_path):
+                pg_wal_realpath = os.path.realpath(pg_wal_path)
+                links[pg_wal_path] = pg_wal_realpath
+        return links
+
+    def pg_tblspc_realpaths(self) -> Dict[str, str]:
+        """Returns a dict containing the symlink (key) and target (values) for the tablespaces"""
+        links: Dict[str, str] = {}
+        pg_tblsp_dir = os.path.join(self._data_dir, 'pg_tblspc')
+        if os.path.exists(pg_tblsp_dir):
+            for tsdn in os.listdir(pg_tblsp_dir):
+                pg_tsp_path = os.path.join(pg_tblsp_dir, tsdn)
+                if parse_int(tsdn) and os.path.islink(pg_tsp_path):
+                    pg_tsp_rpath = os.path.realpath(pg_tsp_path)
+                    links[pg_tsp_path] = pg_tsp_rpath
+        return links
 
-@ctl.command('edit-config', help="Edit cluster configuration")
-@arg_cluster_name
-@option_default_citus_group
-@click.option('--quiet', '-q', is_flag=True, help='Do not show changes')
-@click.option('--set', '-s', 'kvpairs', multiple=True,
-              help='Set specific configuration value. Can be specified multiple times')
-@click.option('--pg', '-p', 'pgkvpairs', multiple=True,
-              help='Set specific PostgreSQL parameter value. Shorthand for -s postgresql.parameters. '
-                   'Can be specified multiple times')
-@click.option('--apply', 'apply_filename', help='Apply configuration from file. Use - for stdin.')
-@click.option('--replace', 'replace_filename', help='Apply configuration from file, replacing existing configuration.'
-              ' Use - for stdin.')
-@option_force
-@click.pass_obj
-def edit_config(obj, cluster_name, group, force, quiet, kvpairs, pgkvpairs, apply_filename, replace_filename):
-    dcs = get_dcs(obj, cluster_name, group)
-    cluster = dcs.get_cluster()
-
-    before_editing = format_config_for_editing(cluster.config.data)
-
-    after_editing = None  # Serves as a flag if any changes were requested
-    changed_data = cluster.config.data
-
-    if replace_filename:
-        after_editing, changed_data = apply_yaml_file({}, replace_filename)
-
-    if apply_filename:
-        after_editing, changed_data = apply_yaml_file(changed_data, apply_filename)
-
-    if kvpairs or pgkvpairs:
-        all_pairs = list(kvpairs) + ['postgresql.parameters.'+v.lstrip() for v in pgkvpairs]
-        after_editing, changed_data = apply_config_changes(before_editing, changed_data, all_pairs)
-
-    # If no changes were specified on the command line invoke editor
-    if after_editing is None:
-        after_editing, changed_data = invoke_editor(before_editing, cluster_name)
-
-    if cluster.config.data == changed_data:
-        if not quiet:
-            click.echo("Not changed")
-        return
-
-    if not quiet:
-        show_diff(before_editing, after_editing)
-
-    if (apply_filename == '-' or replace_filename == '-') and not force:
-        click.echo("Use --force option to apply changes")
-        return
-
-    if force or click.confirm('Apply these changes?'):
-        if not dcs.set_config_value(json.dumps(changed_data), cluster.config.index):
-            raise PatroniCtlException("Config modification aborted due to concurrent changes")
-        click.echo("Configuration changed")
-
-
-@ctl.command('show-config', help="Show cluster configuration")
-@arg_cluster_name
-@option_default_citus_group
-@click.pass_obj
-def show_config(obj, cluster_name, group):
-    cluster = get_dcs(obj, cluster_name, group).get_cluster()
-
-    click.echo(format_config_for_editing(cluster.config.data))
-
-
-@ctl.command('version', help='Output version of patronictl command or a running Patroni instance')
-@click.argument('cluster_name', required=False)
-@click.argument('member_names', nargs=-1)
-@option_citus_group
-@click.pass_obj
-def version(obj, cluster_name, group, member_names):
-    click.echo("patronictl version {0}".format(__version__))
-
-    if not cluster_name:
-        return
-
-    click.echo("")
-    cluster = get_dcs(obj, cluster_name, group).get_cluster()
-    for m in get_all_members(obj, cluster, group, 'any'):
-        if m.api_url:
-            if not member_names or m.name in member_names:
-                try:
-                    response = request_patroni(m)
-                    data = json.loads(response.data.decode('utf-8'))
-                    version = data.get('patroni', {}).get('version')
-                    pg_version = data.get('server_version')
-                    pg_version_str = " PostgreSQL {0}".format(format_pg_version(pg_version)) if pg_version else ""
-                    click.echo("{0}: Patroni {1}{2}".format(m.name, version, pg_version_str))
-                except Exception as e:
-                    click.echo("{0}: failed to get version: {1}".format(m.name, e))
-
-
-@ctl.command('history', help="Show the history of failovers/switchovers")
-@arg_cluster_name
-@option_default_citus_group
-@option_format
-@click.pass_obj
-def history(obj, cluster_name, group, fmt):
-    cluster = get_dcs(obj, cluster_name, group).get_cluster()
-    history = cluster.history and cluster.history.lines or []
-    table_header_row = ['TL', 'LSN', 'Reason', 'Timestamp', 'New Leader']
-    for line in history:
-        if len(line) < len(table_header_row):
-            add_column_num = len(table_header_row) - len(line)
-            for _ in range(add_column_num):
-                line.append('')
-    print_output(table_header_row, history, {'TL': 'r', 'LSN': 'r'}, fmt)
-
-
-def format_pg_version(version):
-    if version < 100000:
-        return "{0}.{1}.{2}".format(version // 10000, version // 100 % 100, version % 100)
-    else:
-        return "{0}.{1}".format(version // 10000, version % 100)
+    def move_data_directory(self) -> None:
+        if os.path.isdir(self._data_dir) and not self.is_running():
+            try:
+                postfix = 'failed'
+
+                # let's see if the wal directory is a symlink, in this case we
+                # should move the target
+                for (source, pg_wal_realpath) in self.pg_wal_realpath().items():
+                    logger.info('renaming WAL directory and updating symlink: %s', pg_wal_realpath)
+                    new_name = '{0}.{1}'.format(pg_wal_realpath, postfix)
+                    if os.path.exists(new_name):
+                        shutil.rmtree(new_name)
+                    os.rename(pg_wal_realpath, new_name)
+                    os.unlink(source)
+                    os.symlink(new_name, source)
+
+                # Move user defined tablespace directory
+                for (source, pg_tsp_rpath) in self.pg_tblspc_realpaths().items():
+                    logger.info('renaming user defined tablespace directory and updating symlink: %s', pg_tsp_rpath)
+                    new_name = '{0}.{1}'.format(pg_tsp_rpath, postfix)
+                    if os.path.exists(new_name):
+                        shutil.rmtree(new_name)
+                    os.rename(pg_tsp_rpath, new_name)
+                    os.unlink(source)
+                    os.symlink(new_name, source)
+
+                new_name = '{0}.{1}'.format(self._data_dir, postfix)
+                logger.info('renaming data directory to %s', new_name)
+                if os.path.exists(new_name):
+                    shutil.rmtree(new_name)
+                os.rename(self._data_dir, new_name)
+            except OSError:
+                logger.exception("Could not rename data directory %s", self._data_dir)
+
+    def remove_data_directory(self) -> None:
+        self.set_role('uninitialized')
+        logger.info('Removing data directory: %s', self._data_dir)
+        try:
+            if os.path.islink(self._data_dir):
+                os.unlink(self._data_dir)
+            elif not os.path.exists(self._data_dir):
+                return
+            elif os.path.isfile(self._data_dir):
+                os.remove(self._data_dir)
+            elif os.path.isdir(self._data_dir):
+
+                # let's see if wal directory is a symlink, in this case we
+                # should clean the target
+                for pg_wal_realpath in self.pg_wal_realpath().values():
+                    logger.info('Removing WAL directory: %s', pg_wal_realpath)
+                    shutil.rmtree(pg_wal_realpath)
+
+                # Remove user defined tablespace directories
+                for pg_tsp_rpath in self.pg_tblspc_realpaths().values():
+                    logger.info('Removing user defined tablespace directory: %s', pg_tsp_rpath)
+                    shutil.rmtree(pg_tsp_rpath, ignore_errors=True)
+
+                shutil.rmtree(self._data_dir)
+        except (IOError, OSError):
+            logger.exception('Could not remove data directory %s', self._data_dir)
+            self.move_data_directory()
+
+    def schedule_sanity_checks_after_pause(self) -> None:
+        """
+            After coming out of pause we have to:
+            1. configure server parameters if necessary
+            2. sync replication slots, because it might happen that slots were removed
+            3. get new 'Database system identifier' to make sure that it wasn't changed
+        """
+        self.ensure_major_version_is_known()
+        self.slots_handler.schedule()
+        self.citus_handler.schedule_cache_rebuild()
+        self._sysid = ''
+
+    def _get_gucs(self) -> CaseInsensitiveSet:
+        """Get all available GUCs based on ``postgres --describe-config`` output.
+
+        :returns: all available GUCs in the local Postgres server.
+        """
+        cmd = [self.pgcommand('postgres'), '--describe-config']
+        return CaseInsensitiveSet({
+            line.split('\t')[0] for line in subprocess.check_output(cmd).decode('utf-8').strip().split('\n')
+        })
```

### Comparing `patroni-3.0.2/patroni/daemon.py` & `patroni-3.0.3/patroni/daemon.py`

 * *Files 6% similar despite different names*

```diff
@@ -2,23 +2,40 @@
 
 This module implements abstraction classes and functions for creating and managing daemon processes in Patroni.
 Currently it is only used for the main "Thread" of ``patroni`` and ``patroni_raft_controller`` commands.
 """
 from __future__ import print_function
 
 import abc
+import argparse
 import os
 import signal
 import sys
 
 from threading import Lock
-from typing import Any, Optional, Type
+from typing import Any, Optional, Type, TYPE_CHECKING
 
-from .config import Config
-from .validator import Schema
+if TYPE_CHECKING:  # pragma: no cover
+    from .config import Config
+
+
+def get_base_arg_parser() -> argparse.ArgumentParser:
+    """Create a basic argument parser with the arguments used for both patroni and raft controller daemon.
+
+    :returns: 'argparse.ArgumentParser' object
+    """
+    from .config import Config
+    from .version import __version__
+
+    parser = argparse.ArgumentParser()
+    parser.add_argument('--version', action='version', version='%(prog)s {0}'.format(__version__))
+    parser.add_argument('configfile', nargs='?', default='',
+                        help='Patroni may also read the configuration from the {0} environment variable'
+                        .format(Config.PATRONI_CONFIG_VARIABLE))
+    return parser
 
 
 class AbstractPatroniDaemon(abc.ABC):
     """A Patroni daemon process.
 
     .. note::
 
@@ -26,15 +43,15 @@
         to determine what it should do in each execution cycle, and :func:`_shutdown` to determine what it should do
         when shutting down.
 
     :ivar logger: log handler used by this daemon.
     :ivar config: configuration options for this daemon.
     """
 
-    def __init__(self, config: Config) -> None:
+    def __init__(self, config: 'Config') -> None:
         """Set up signal handlers, logging handler and configuration.
 
         :param config: configuration options for this daemon.
         """
         from patroni.log import PatroniLogger
 
         self.setup_signal_handlers()
@@ -90,15 +107,15 @@
 
     @property
     def received_sigterm(self) -> bool:
         """If daemon was signaled with SIGTERM."""
         with self._sigterm_lock:
             return self._received_sigterm
 
-    def reload_config(self, sighup: Optional[bool] = False, local: Optional[bool] = False) -> None:
+    def reload_config(self, sighup: bool = False, local: Optional[bool] = False) -> None:
         """Reload configuration.
 
         :param sighup: if it is related to a SIGHUP signal.
                        The sighup parameter could be used in the method overridden in a child class.
         :param local: will be ``True`` if there are changes in the local configuration file.
         """
         if local:
@@ -136,49 +153,25 @@
         """
         with self._sigterm_lock:
             self._received_sigterm = True
         self._shutdown()
         self.logger.shutdown()
 
 
-def abstract_main(cls: Type[AbstractPatroniDaemon], validator: Optional[Schema] = None) -> None:
+def abstract_main(cls: Type[AbstractPatroniDaemon], configfile: str) -> None:
     """Create the main entry point of a given daemon process.
 
-    Expose a basic argument parser, parse the command-line arguments, and run the given daemon process.
-
     :param cls: a class that should inherit from :class:`AbstractPatroniDaemon`.
-    :param validator: used to validate the daemon configuration schema, if requested by the user through
-        ``--validate-config`` CLI option.
+    :param configfile:
     """
-    import argparse
-
     from .config import Config, ConfigParseError
-    from .version import __version__
-
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--version', action='version', version='%(prog)s {0}'.format(__version__))
-    if validator:
-        parser.add_argument('--validate-config', action='store_true', help='Run config validator and exit')
-    parser.add_argument('configfile', nargs='?', default='',
-                        help='Patroni may also read the configuration from the {0} environment variable'
-                        .format(Config.PATRONI_CONFIG_VARIABLE))
-    args = parser.parse_args()
-    validate_config = validator and args.validate_config
     try:
-        if validate_config:
-            Config(args.configfile, validator=validator)
-            sys.exit()
-
-        config = Config(args.configfile)
+        config = Config(configfile)
     except ConfigParseError as e:
-        if e.value:
-            print(e.value, file=sys.stderr)
-        if not validate_config:
-            parser.print_help()
-        sys.exit(1)
+        sys.exit(e.value)
 
     controller = cls(config)
     try:
         controller.run()
     except KeyboardInterrupt:
         pass
     finally:
```

### Comparing `patroni-3.0.2/patroni/dcs/__init__.py` & `patroni-3.0.3/patroni/dcs/__init__.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,52 +1,55 @@
 import abc
 import dateutil.parser
+import datetime
 import importlib
 import inspect
 import json
 import logging
 import os
 import pkgutil
 import re
 import sys
 import time
 
-from collections import defaultdict, namedtuple
+from collections import defaultdict
 from copy import deepcopy
 from random import randint
 from threading import Event, Lock
-from typing import Any, Dict, List, Optional, Union
+from typing import Any, Callable, Collection, Dict, List, NamedTuple, Optional, Set, Tuple, Union, TYPE_CHECKING
 from urllib.parse import urlparse, urlunparse, parse_qsl
 
 from ..exceptions import PatroniFatalException
-from ..utils import deep_compare, parse_bool, uri
+from ..utils import deep_compare, uri
+if TYPE_CHECKING:  # pragma: no cover
+    from ..config import Config
 
 CITUS_COORDINATOR_GROUP_ID = 0
 citus_group_re = re.compile('^(0|[1-9][0-9]*)$')
 slot_name_re = re.compile('^[a-z0-9_]{1,63}$')
 logger = logging.getLogger(__name__)
 
 
-def slot_name_from_member_name(member_name):
+def slot_name_from_member_name(member_name: str) -> str:
     """Translate member name to valid PostgreSQL slot name.
 
     PostgreSQL replication slot names must be valid PostgreSQL names. This function maps the wider space of
     member names to valid PostgreSQL names. Names are lowercased, dashes and periods common in hostnames
     are replaced with underscores, other characters are encoded as their unicode codepoint. Name is truncated
     to 64 characters. Multiple different member names may map to a single slot name."""
 
-    def replace_char(match):
+    def replace_char(match: Any) -> str:
         c = match.group(0)
         return '_' if c in '-.' else "u{:04d}".format(ord(c))
 
     slot_name = re.sub('[^a-z0-9_]', replace_char, member_name.lower())
     return slot_name[0:63]
 
 
-def parse_connection_string(value):
+def parse_connection_string(value: str) -> Tuple[str, Union[str, None]]:
     """Original Governor stores connection strings for each cluster members if a following format:
         postgres://{username}:{password}@{connect_address}/postgres
     Since each of our patroni instances provides own REST API endpoint it's good to store this information
     in DCS among with postgresql connection string. In order to not introduce new keys and be compatible with
     original Governor we decided to extend original connection string in a following way:
         postgres://{username}:{password}@{connect_address}/postgres?application_name={api_url}
     This way original Governor could use such connection string as it is, because of feature of `libpq` library.
@@ -55,38 +58,38 @@
 
     scheme, netloc, path, params, query, fragment = urlparse(value)
     conn_url = urlunparse((scheme, netloc, path, params, '', fragment))
     api_url = ([v for n, v in parse_qsl(query) if n == 'application_name'] or [None])[0]
     return conn_url, api_url
 
 
-def dcs_modules():
+def dcs_modules() -> List[str]:
     """Get names of DCS modules, depending on execution environment. If being packaged with PyInstaller,
     modules aren't discoverable dynamically by scanning source directory because `FrozenImporter` doesn't
     implement `iter_modules` method. But it is still possible to find all potential DCS modules by
     iterating through `toc`, which contains list of all "frozen" resources."""
 
     dcs_dirname = os.path.dirname(__file__)
     module_prefix = __package__ + '.'
 
     if getattr(sys, 'frozen', False):
-        toc = set()
+        toc: Set[str] = set()
         # dcs_dirname may contain a dot, which causes pkgutil.iter_importers()
         # to misinterpret the path as a package name. This can be avoided
         # altogether by not passing a path at all, because PyInstaller's
         # FrozenImporter is a singleton and registered as top-level finder.
         for importer in pkgutil.iter_importers():
             if hasattr(importer, 'toc'):
-                toc |= importer.toc
+                toc |= getattr(importer, 'toc')
         return [module for module in toc if module.startswith(module_prefix) and module.count('.') == 2]
     else:
         return [module_prefix + name for _, name, is_pkg in pkgutil.iter_modules([dcs_dirname]) if not is_pkg]
 
 
-def get_dcs(config):
+def get_dcs(config: Union['Config', Dict[str, Any]]) -> 'AbstractDCS':
     modules = dcs_modules()
 
     for module_name in modules:
         name = module_name.split('.')[-1]
         if name in config:  # we will try to import only modules which have configuration section in the config file
             try:
                 module = importlib.import_module(module_name)
@@ -99,79 +102,86 @@
                         # From citus section we only need "group" parameter, but will propagate everything just in case.
                         if isinstance(config.get('citus'), dict):
                             config[name].update(config['citus'])
                         return item(config[name])
             except ImportError:
                 logger.debug('Failed to import %s', module_name)
 
-    available_implementations = []
+    available_implementations: List[str] = []
     for module_name in modules:
         name = module_name.split('.')[-1]
         try:
             module = importlib.import_module(module_name)
             available_implementations.extend(name for key, item in module.__dict__.items() if key.lower() == name
                                              and inspect.isclass(item) and issubclass(item, AbstractDCS))
         except ImportError:
             logger.info('Failed to import %s', module_name)
     raise PatroniFatalException("""Can not find suitable configuration of distributed configuration store
 Available implementations: """ + ', '.join(sorted(set(available_implementations))))
 
 
-class Member(namedtuple('Member', 'index,name,session,data')):
+_Version = Union[int, str]
+_Session = Union[int, float, str, None]
 
+
+class Member(NamedTuple):
     """Immutable object (namedtuple) which represents single member of PostgreSQL cluster.
     Consists of the following fields:
-    :param index: modification index of a given member key in a Configuration Store
+    :param version: modification version of a given member key in a Configuration Store
     :param name: name of PostgreSQL cluster member
     :param session: either session id or just ttl in seconds
     :param data: arbitrary data i.e. conn_url, api_url, xlog location, state, role, tags, etc...
 
     There are two mandatory keys in a data:
     conn_url: connection string containing host, user and password which could be used to access this member.
-    api_url: REST API url of patroni instance"""
+    api_url: REST API url of patroni instance
+    """
+    version: _Version
+    name: str
+    session: _Session
+    data: Dict[str, Any]
 
     @staticmethod
-    def from_node(index, name, session, data):
+    def from_node(version: _Version, name: str, session: _Session, value: str) -> 'Member':
         """
         >>> Member.from_node(-1, '', '', '{"conn_url": "postgres://foo@bar/postgres"}') is not None
         True
         >>> Member.from_node(-1, '', '', '{')
-        Member(index=-1, name='', session='', data={})
+        Member(version=-1, name='', session='', data={})
         """
-        if data.startswith('postgres'):
-            conn_url, api_url = parse_connection_string(data)
+        if value.startswith('postgres'):
+            conn_url, api_url = parse_connection_string(value)
             data = {'conn_url': conn_url, 'api_url': api_url}
         else:
             try:
-                data = json.loads(data)
-                if not isinstance(data, dict):
-                    data = {}
-            except (TypeError, ValueError):
-                data = {}
-        return Member(index, name, session, data)
+                data = json.loads(value)
+                assert isinstance(data, dict)
+            except (AssertionError, TypeError, ValueError):
+                data: Dict[str, Any] = {}
+        return Member(version, name, session, data)
 
     @property
-    def conn_url(self):
+    def conn_url(self) -> Optional[str]:
         conn_url = self.data.get('conn_url')
         if conn_url:
             return conn_url
 
         conn_kwargs = self.data.get('conn_kwargs')
         if conn_kwargs:
             conn_url = uri('postgresql', (conn_kwargs.get('host'), conn_kwargs.get('port', 5432)))
             self.data['conn_url'] = conn_url
             return conn_url
 
-    def conn_kwargs(self, auth=None):
+    def conn_kwargs(self, auth: Union[Any, Dict[str, Any], None] = None) -> Dict[str, Any]:
         defaults = {
             "host": None,
             "port": None,
             "dbname": None
         }
-        ret = self.data.get('conn_kwargs')
+        ret: Optional[Dict[str, Any]] = self.data.get('conn_kwargs')
         if ret:
             defaults.update(ret)
             ret = defaults
         else:
             conn_url = self.conn_url
             if not conn_url:
                 return {}  # due to the invalid conn_url we don't care about authentication parameters
@@ -187,109 +197,116 @@
         if auth and isinstance(auth, dict):
             ret.update({k: v for k, v in auth.items() if v is not None})
             if 'username' in auth:
                 ret['user'] = ret.pop('username')
         return ret
 
     @property
-    def api_url(self):
+    def api_url(self) -> Optional[str]:
         return self.data.get('api_url')
 
     @property
-    def tags(self):
+    def tags(self) -> Dict[str, Any]:
         return self.data.get('tags', {})
 
     @property
-    def nofailover(self):
+    def nofailover(self) -> bool:
         return self.tags.get('nofailover', False)
 
     @property
-    def replicatefrom(self):
+    def replicatefrom(self) -> Optional[str]:
         return self.tags.get('replicatefrom')
 
     @property
-    def clonefrom(self):
+    def clonefrom(self) -> bool:
         return self.tags.get('clonefrom', False) and bool(self.conn_url)
 
     @property
-    def state(self):
+    def state(self) -> str:
         return self.data.get('state', 'unknown')
 
     @property
-    def is_running(self):
+    def is_running(self) -> bool:
         return self.state == 'running'
 
     @property
-    def version(self):
+    def patroni_version(self) -> Optional[Tuple[int, ...]]:
         version = self.data.get('version')
         if version:
             try:
                 return tuple(map(int, version.split('.')))
             except Exception:
                 logger.debug('Failed to parse Patroni version %s', version)
 
 
 class RemoteMember(Member):
     """Represents a remote member (typically a primary) for a standby cluster"""
-    def __new__(cls, name, data):
-        return super(RemoteMember, cls).__new__(cls, None, name, None, data)
+
+    @classmethod
+    def from_name_and_data(cls, name: str, data: Dict[str, Any]) -> 'RemoteMember':
+        return super(RemoteMember, cls).__new__(cls, -1, name, None, data)
 
     @staticmethod
-    def allowed_keys():
+    def allowed_keys() -> Tuple[str, ...]:
         return ('primary_slot_name',
                 'create_replica_methods',
                 'restore_command',
                 'archive_cleanup_command',
                 'recovery_min_apply_delay',
                 'no_replication_slot')
 
-    def __getattr__(self, name):
+    def __getattr__(self, name: str) -> Any:
         if name in RemoteMember.allowed_keys():
             return self.data.get(name)
 
 
-class Leader(namedtuple('Leader', 'index,session,member')):
-
+class Leader(NamedTuple):
     """Immutable object (namedtuple) which represents leader key.
+
     Consists of the following fields:
-    :param index: modification index of a leader key in a Configuration Store
+    :param version: modification version of a leader key in a Configuration Store
     :param session: either session id or just ttl in seconds
-    :param member: reference to a `Member` object which represents current leader (see `Cluster.members`)"""
+    :param member: reference to a `Member` object which represents current leader (see `Cluster.members`)
+    """
+    version: _Version
+    session: _Session
+    member: Member
 
     @property
-    def name(self):
+    def name(self) -> str:
         return self.member.name
 
-    def conn_kwargs(self, auth=None):
+    def conn_kwargs(self, auth: Optional[Dict[str, str]] = None) -> Dict[str, str]:
         return self.member.conn_kwargs(auth)
 
     @property
-    def conn_url(self):
+    def conn_url(self) -> Optional[str]:
         return self.member.conn_url
 
     @property
-    def data(self):
+    def data(self) -> Dict[str, Any]:
         return self.member.data
 
     @property
-    def timeline(self):
+    def timeline(self) -> Optional[int]:
         return self.data.get('timeline')
 
     @property
-    def checkpoint_after_promote(self):
+    def checkpoint_after_promote(self) -> Optional[bool]:
         """
         >>> Leader(1, '', Member.from_node(1, '', '', '{"version":"z"}')).checkpoint_after_promote
+
         """
-        version = self.member.version
+        version = self.member.patroni_version
         # 1.5.6 is the last version which doesn't expose checkpoint_after_promote: false
         if version and version > (1, 5, 6):
             return self.data.get('role') in ('master', 'primary') and 'checkpoint_after_promote' not in self.data
 
 
-class Failover(namedtuple('Failover', 'index,leader,candidate,scheduled_at')):
+class Failover(NamedTuple):
 
     """
     >>> 'Failover' in str(Failover.from_node(1, '{"leader": "cluster_leader"}'))
     True
     >>> 'Failover' in str(Failover.from_node(1, {"leader": "cluster_leader"}))
     True
     >>> 'Failover' in str(Failover.from_node(1, '{"leader": "cluster_leader", "member": "cluster_candidate"}'))
@@ -302,84 +319,93 @@
     >>> Failover.from_node(1, None) is None
     False
     >>> Failover.from_node(1, '{}') is None
     False
     >>> 'abc' in Failover.from_node(1, 'abc:def')
     True
     """
+    version: _Version
+    leader: Optional[str]
+    candidate: Optional[str]
+    scheduled_at: Optional[datetime.datetime]
+
     @staticmethod
-    def from_node(index, value):
+    def from_node(version: _Version, value: Union[str, Dict[str, str]]) -> 'Failover':
         if isinstance(value, dict):
-            data = value
+            data: Dict[str, Any] = value
         elif value:
             try:
                 data = json.loads(value)
-                if not isinstance(data, dict):
-                    data = {}
+                assert isinstance(data, dict)
+            except AssertionError:
+                data = {}
             except ValueError:
                 t = [a.strip() for a in value.split(':')]
                 leader = t[0]
                 candidate = t[1] if len(t) > 1 else None
-                return Failover(index, leader, candidate, None) if leader or candidate else None
+                return Failover(version, leader, candidate, None)
         else:
             data = {}
 
         if data.get('scheduled_at'):
             data['scheduled_at'] = dateutil.parser.parse(data['scheduled_at'])
 
-        return Failover(index, data.get('leader'), data.get('member'), data.get('scheduled_at'))
+        return Failover(version, data.get('leader'), data.get('member'), data.get('scheduled_at'))
 
-    def __len__(self):
+    def __len__(self) -> int:
         return int(bool(self.leader)) + int(bool(self.candidate))
 
 
-class ClusterConfig(namedtuple('ClusterConfig', 'index,data,modify_index')):
+class ClusterConfig(NamedTuple):
+    version: _Version
+    data: Dict[str, Any]
+    modify_version: _Version
 
     @staticmethod
-    def from_node(index, data, modify_index=None):
+    def from_node(version: _Version, value: str, modify_version: Optional[_Version] = None) -> 'ClusterConfig':
         """
         >>> ClusterConfig.from_node(1, '{') is None
         False
         """
 
         try:
-            data = json.loads(data)
-        except (TypeError, ValueError):
-            data = None
-            modify_index = 0
-        if not isinstance(data, dict):
-            data = {}
-        return ClusterConfig(index, data, index if modify_index is None else modify_index)
+            data = json.loads(value)
+            assert isinstance(data, dict)
+        except (AssertionError, TypeError, ValueError):
+            data: Dict[str, Any] = {}
+            modify_version = 0
+        return ClusterConfig(version, data, version if modify_version is None else modify_version)
 
     @property
-    def permanent_slots(self):
-        return isinstance(self.data, dict) and (
-                self.data.get('permanent_replication_slots') or
-                self.data.get('permanent_slots') or self.data.get('slots')
-        ) or {}
+    def permanent_slots(self) -> Dict[str, Any]:
+        return self.data.get('permanent_replication_slots')\
+            or self.data.get('permanent_slots') or self.data.get('slots') or {}
 
     @property
-    def ignore_slots_matchers(self):
-        return isinstance(self.data, dict) and self.data.get('ignore_slots') or []
+    def ignore_slots_matchers(self) -> List[Dict[str, Any]]:
+        return self.data.get('ignore_slots') or []
 
     @property
-    def max_timelines_history(self):
+    def max_timelines_history(self) -> int:
         return self.data.get('max_timelines_history', 0)
 
 
-class SyncState(namedtuple('SyncState', 'index,leader,sync_standby')):
+class SyncState(NamedTuple):
     """Immutable object (namedtuple) which represents last observed synhcronous replication state
 
-    :param index: modification index of a synchronization key in a Configuration Store
+    :param version: modification version of a synchronization key in a Configuration Store
     :param leader: reference to member that was leader
     :param sync_standby: synchronous standby list (comma delimited) which are last synchronized to leader
     """
+    version: Optional[_Version]
+    leader: Optional[str]
+    sync_standby: Optional[str]
 
     @staticmethod
-    def from_node(index: Union[str, int], value: Union[str, Dict[str, Any]]) -> 'SyncState':
+    def from_node(version: Optional[_Version], value: Union[str, Dict[str, Any], None]) -> 'SyncState':
         """
         >>> SyncState.from_node(1, None).leader is None
         True
         >>> SyncState.from_node(1, '{}').leader is None
         True
         >>> SyncState.from_node(1, '{').leader is None
         True
@@ -389,177 +415,208 @@
         True
         >>> SyncState.from_node(1, {"leader": "leader"}).leader == "leader"
         True
         """
         try:
             if value and isinstance(value, str):
                 value = json.loads(value)
-            if not isinstance(value, dict):
-                return SyncState.empty(index)
-            return SyncState(index, value.get('leader'), value.get('sync_standby'))
-        except (TypeError, ValueError):
-            return SyncState.empty(index)
+            assert isinstance(value, dict)
+            return SyncState(version, value.get('leader'), value.get('sync_standby'))
+        except (AssertionError, TypeError, ValueError):
+            return SyncState.empty(version)
 
     @staticmethod
-    def empty(index: Optional[Union[str, int]] = '') -> 'SyncState':
-        return SyncState(index, None, '')
+    def empty(version: Optional[_Version] = None) -> 'SyncState':
+        return SyncState(version, None, None)
 
     @property
     def is_empty(self) -> bool:
-        """:returns: True if /sync key doesn't have a leader"""
-        return self.leader is None
+        """:returns: True if /sync key is not valid (doesn't have a leader)."""
+        return not self.leader
+
+    @staticmethod
+    def _str_to_list(value: str) -> List[str]:
+        """Splits a string by comma and returns list of strings.
+
+        :param value: a comma separated string
+        :returns: list of non-empty strings after splitting an input value by comma
+        """
+        return list(filter(lambda a: a, [s.strip() for s in value.split(',')]))
 
     @property
     def members(self) -> List[str]:
-        """:returns: sync_standby as list"""
-        return list(filter(lambda a: a, [s.strip() for s in self.sync_standby.split(',')])) if self.sync_standby else []
+        """:returns: sync_standby as list."""
+        return self._str_to_list(self.sync_standby) if not self.is_empty and self.sync_standby else []
 
-    def matches(self, name: str) -> bool:
-        """:returns: True if a node name matches one of the nodes in the sync state (including leader)
+    def matches(self, name: Optional[str], check_leader: bool = False) -> bool:
+        """Checks if node is presented in the /sync state.
 
+        Since PostgreSQL does case-insensitive checks for synchronous_standby_name we do it also.
+        :param name: name of the node
+        :param check_leader: by default the name is searched in members, check_leader=True will include leader to list
+        :returns: `True` if the /sync key not :func:`is_empty` and a given name is among presented in the sync state
         >>> s = SyncState(1, 'foo', 'bar,zoo')
         >>> s.matches('foo')
+        False
+        >>> s.matches('fOo', True)
         True
-        >>> s.matches('bar')
+        >>> s.matches('Bar')
         True
-        >>> s.matches('zoo')
+        >>> s.matches('zoO')
         True
         >>> s.matches('baz')
         False
         >>> s.matches(None)
         False
-        >>> SyncState(1, None, None).matches('foo')
+        >>> SyncState.empty(1).matches('foo')
         False
         """
-        return name is not None and name in [self.leader] + self.members
+        ret = False
+        if name and not self.is_empty:
+            search_str = (self.sync_standby or '') + (',' + (self.leader or '') if check_leader else '')
+            ret = name.lower() in self._str_to_list(search_str.lower())
+        return ret
+
+    def leader_matches(self, name: Optional[str]) -> bool:
+        """:returns: `True` if name is matching the `SyncState.leader` value."""
+        return bool(name and not self.is_empty and name.lower() == (self.leader or '').lower())
 
 
-class TimelineHistory(namedtuple('TimelineHistory', 'index,value,lines')):
+_HistoryTuple = Union[Tuple[int, int, str], Tuple[int, int, str, str], Tuple[int, int, str, str, str]]
+
+
+class TimelineHistory(NamedTuple):
     """Object representing timeline history file"""
+    version: _Version
+    value: Any
+    lines: List[_HistoryTuple]
 
     @staticmethod
-    def from_node(index, value):
+    def from_node(version: _Version, value: str) -> 'TimelineHistory':
         """
         >>> h = TimelineHistory.from_node(1, 2)
         >>> h.lines
         []
         """
         try:
             lines = json.loads(value)
-        except (TypeError, ValueError):
-            lines = None
-        if not isinstance(lines, list):
-            lines = []
-        return TimelineHistory(index, value, lines)
-
+            assert isinstance(lines, list)
+        except (AssertionError, TypeError, ValueError):
+            lines: List[_HistoryTuple] = []
+        return TimelineHistory(version, value, lines)
 
-class Cluster(namedtuple('Cluster', 'initialize,config,leader,last_lsn,members,'
-                                    'failover,sync,history,slots,failsafe,workers')):
 
+class Cluster(NamedTuple):
     """Immutable object (namedtuple) which represents PostgreSQL cluster.
     Consists of the following fields:
     :param initialize: shows whether this cluster has initialization key stored in DC or not.
     :param config: global dynamic configuration, reference to `ClusterConfig` object
     :param leader: `Leader` object which represents current leader of the cluster
     :param last_lsn: int or long object containing position of last known leader LSN.
         This value is stored in the `/status` key or `/optime/leader` (legacy) key
     :param members: list of Member object, all PostgreSQL cluster members including leader
     :param failover: reference to `Failover` object
     :param sync: reference to `SyncState` object, last observed synchronous replication state.
     :param history: reference to `TimelineHistory` object
     :param slots: state of permanent logical replication slots on the primary in the format: {"slot_name": int}
     :param failsafe: failsafe topology. Node is allowed to become the leader only if its name is found in this list.
-    :param workers: workers of the Citus cluster, optional. Format: {int(group): Cluster()}"""
-
-    def __new__(cls, *args):
-        # Make workers argument optional
-        if len(cls._fields) == len(args) + 1:
-            args = args + ({},)
-        return super(Cluster, cls).__new__(cls, *args)
+    :param workers: workers of the Citus cluster, optional. Format: {int(group): Cluster()}
+    """
+    initialize: Optional[str]
+    config: Optional[ClusterConfig]
+    leader: Optional[Leader]
+    last_lsn: int
+    members: List[Member]
+    failover: Optional[Failover]
+    sync: SyncState
+    history: Optional[TimelineHistory]
+    slots: Optional[Dict[str, int]]
+    failsafe: Optional[Dict[str, str]]
+    workers: Dict[int, 'Cluster'] = {}
 
     @staticmethod
-    def empty():
+    def empty() -> 'Cluster':
         return Cluster(None, None, None, 0, [], None, SyncState.empty(), None, None, None)
 
+    def is_empty(self):
+        return self.initialize is None and self.config is None and self.leader is None and self.last_lsn == 0\
+            and self.members == [] and self.failover is None and self.sync.version is None\
+            and self.history is None and self.slots is None and self.failsafe is None and self.workers == {}
+
+    def __len__(self) -> int:
+        return int(not self.is_empty())
+
     @property
-    def leader_name(self):
+    def leader_name(self) -> Optional[str]:
         return self.leader and self.leader.name
 
-    def is_unlocked(self):
+    def is_unlocked(self) -> bool:
         return not self.leader_name
 
-    def has_member(self, member_name):
+    def has_member(self, member_name: str) -> bool:
         return any(m for m in self.members if m.name == member_name)
 
-    def get_member(self, member_name, fallback_to_leader=True):
+    def get_member(self, member_name: str, fallback_to_leader: bool = True) -> Union[Member, Leader, None]:
         return ([m for m in self.members if m.name == member_name] or [self.leader if fallback_to_leader else None])[0]
 
-    def get_clone_member(self, exclude):
-        exclude = [exclude] + [self.leader.name] if self.leader else []
+    def get_clone_member(self, exclude_name: str) -> Union[Member, Leader, None]:
+        exclude = [exclude_name] + ([self.leader.name] if self.leader else [])
         candidates = [m for m in self.members if m.clonefrom and m.is_running and m.name not in exclude]
         return candidates[randint(0, len(candidates) - 1)] if candidates else self.leader
 
-    def check_mode(self, mode):
-        return bool(self.config and parse_bool(self.config.data.get(mode)))
-
-    def is_paused(self):
-        return self.check_mode('pause')
-
-    def is_synchronous_mode(self):
-        return self.check_mode('synchronous_mode')
-
     @property
-    def __permanent_slots(self):
+    def __permanent_slots(self) -> Dict[str, Union[Dict[str, Any], Any]]:
         return self.config and self.config.permanent_slots or {}
 
     @property
-    def __permanent_physical_slots(self):
+    def __permanent_physical_slots(self) -> Dict[str, Any]:
         return {name: value for name, value in self.__permanent_slots.items()
                 if not value or isinstance(value, dict) and value.get('type', 'physical') == 'physical'}
 
     @property
-    def __permanent_logical_slots(self):
+    def __permanent_logical_slots(self) -> Dict[str, Any]:
         return {name: value for name, value in self.__permanent_slots.items() if isinstance(value, dict)
                 and value.get('type', 'logical') == 'logical' and value.get('database') and value.get('plugin')}
 
     @property
-    def use_slots(self):
-        return self.config and (self.config.data.get('postgresql') or {}).get('use_slots', True)
+    def use_slots(self) -> bool:
+        return bool(self.config and (self.config.data.get('postgresql') or {}).get('use_slots', True))
 
-    def get_replication_slots(self, my_name, role, nofailover, major_version, show_error=False):
+    def get_replication_slots(self, my_name: str, role: str, nofailover: bool,
+                              major_version: int, show_error: bool = False) -> Dict[str, Dict[str, Any]]:
         # if the replicatefrom tag is set on the member - we should not create the replication slot for it on
         # the current primary, because that member would replicate from elsewhere. We still create the slot if
         # the replicatefrom destination member is currently not a member of the cluster (fallback to the
         # primary), or if replicatefrom destination member happens to be the current primary
         use_slots = self.use_slots
         if role in ('master', 'primary', 'standby_leader'):
-            slot_members = [m.name for m in self.members if use_slots and m.name != my_name and
-                            (m.replicatefrom is None or m.replicatefrom == my_name or
-                             not self.has_member(m.replicatefrom))]
+            slot_members = [m.name for m in self.members if use_slots and m.name != my_name
+                            and (m.replicatefrom is None or m.replicatefrom == my_name
+                                 or not self.has_member(m.replicatefrom))]
             permanent_slots = self.__permanent_slots if use_slots and \
                 role in ('master', 'primary') else self.__permanent_physical_slots
         else:
             # only manage slots for replicas that replicate from this one, except for the leader among them
-            slot_members = [m.name for m in self.members if use_slots and
-                            m.replicatefrom == my_name and m.name != self.leader_name]
+            slot_members = [m.name for m in self.members if use_slots
+                            and m.replicatefrom == my_name and m.name != self.leader_name]
             permanent_slots = self.__permanent_logical_slots if use_slots and not nofailover else {}
 
         slots = {slot_name_from_member_name(name): {'type': 'physical'} for name in slot_members}
 
         if len(slots) < len(slot_members):
             # Find which names are conflicting for a nicer error message
-            slot_conflicts = defaultdict(list)
+            slot_conflicts: Dict[str, List[str]] = defaultdict(list)
             for name in slot_members:
                 slot_conflicts[slot_name_from_member_name(name)].append(name)
             logger.error("Following cluster members share a replication slot name: %s",
                          "; ".join("{} map to {}".format(", ".join(v), k)
                                    for k, v in slot_conflicts.items() if len(v) > 1))
 
         # "merge" replication slots for members with permanent_replication_slots
-        disabled_permanent_logical_slots = []
+        disabled_permanent_logical_slots: List[str] = []
         for name, value in permanent_slots.items():
             if not slot_name_re.match(name):
                 logger.error("Invalid permanent replication slot name '%s'", name)
                 logger.error("Slot name may only contain lower case letters, numbers, and the underscore chars")
                 continue
 
             value = deepcopy(value) if value else {'type': 'physical'}
@@ -572,35 +629,35 @@
                     if name != slot_name_from_member_name(my_name):
                         slots[name] = value
                     continue
                 elif value['type'] == 'logical' and value.get('database') and value.get('plugin'):
                     if major_version < 110000:
                         disabled_permanent_logical_slots.append(name)
                     elif name in slots:
-                        logger.error("Permanent logical replication slot {'%s': %s} is conflicting with" +
+                        logger.error("Permanent logical replication slot {'%s': %s} is conflicting with"
                                      " physical replication slot for cluster member", name, value)
                     else:
                         slots[name] = value
                     continue
 
             logger.error("Bad value for slot '%s' in permanent_slots: %s", name, permanent_slots[name])
 
         if disabled_permanent_logical_slots and show_error:
             logger.error("Permanent logical replication slots supported by Patroni only starting from PostgreSQL 11. "
                          "Following slots will not be created: %s.", disabled_permanent_logical_slots)
 
         return slots
 
-    def has_permanent_logical_slots(self, my_name, nofailover, major_version=110000):
+    def has_permanent_logical_slots(self, my_name: str, nofailover: bool, major_version: int = 110000) -> bool:
         if major_version < 110000:
             return False
         slots = self.get_replication_slots(my_name, 'replica', nofailover, major_version).values()
         return any(v for v in slots if v.get("type") == "logical")
 
-    def should_enforce_hot_standby_feedback(self, my_name, nofailover, major_version):
+    def should_enforce_hot_standby_feedback(self, my_name: str, nofailover: bool, major_version: int) -> bool:
         """
         The hot_standby_feedback must be enabled if the current replica has logical slots
         or it is working as a cascading replica for the other node that has logical slots.
         """
 
         if major_version < 110000:
             return False
@@ -609,26 +666,27 @@
             return True
 
         if self.use_slots:
             members = [m for m in self.members if m.replicatefrom == my_name and m.name != self.leader_name]
             return any(self.should_enforce_hot_standby_feedback(m.name, m.nofailover, major_version) for m in members)
         return False
 
-    def get_my_slot_name_on_primary(self, my_name, replicatefrom):
+    def get_my_slot_name_on_primary(self, my_name: str, replicatefrom: Optional[str]) -> str:
         """
         P <-- I <-- L
         In case of cascading replication we have to check not our physical slot,
         but slot of the replica that connects us to the primary.
         """
 
         m = self.get_member(replicatefrom, False) if replicatefrom else None
-        return self.get_my_slot_name_on_primary(m.name, m.replicatefrom) if m else slot_name_from_member_name(my_name)
+        return self.get_my_slot_name_on_primary(m.name, m.replicatefrom)\
+            if isinstance(m, Member) else slot_name_from_member_name(my_name)
 
     @property
-    def timeline(self):
+    def timeline(self) -> int:
         """
         >>> Cluster(0, 0, 0, 0, 0, 0, 0, 0, 0, None).timeline
         0
         >>> Cluster(0, 0, 0, 0, 0, 0, 0, TimelineHistory.from_node(1, '[]'), 0, None).timeline
         1
         >>> Cluster(0, 0, 0, 0, 0, 0, 0, TimelineHistory.from_node(1, '[["a"]]'), 0, None).timeline
         0
@@ -640,24 +698,24 @@
                 except Exception:
                     logger.error('Failed to parse cluster history from DCS: %s', self.history.lines)
             elif self.history.value == '[]':
                 return 1
         return 0
 
     @property
-    def min_version(self):
-        return next(iter(sorted(filter(lambda v: v, [m.version for m in self.members])) + [None]))
+    def min_version(self) -> Optional[Tuple[int, ...]]:
+        return next(iter(sorted(m.patroni_version for m in self.members if m.patroni_version)), None)
 
 
 class ReturnFalseException(Exception):
     pass
 
 
-def catch_return_false_exception(func):
-    def wrapper(*args, **kwargs):
+def catch_return_false_exception(func: Callable[..., Any]) -> Any:
+    def wrapper(*args: Any, **kwargs: Any):
         try:
             return func(*args, **kwargs)
         except ReturnFalseException:
             return False
 
     return wrapper
 
@@ -672,373 +730,403 @@
     _MEMBERS = 'members/'
     _OPTIME = 'optime'
     _STATUS = 'status'  # JSON, contains "leader_lsn" and confirmed_flush_lsn of logical "slots" on the leader
     _LEADER_OPTIME = _OPTIME + '/' + _LEADER  # legacy
     _SYNC = 'sync'
     _FAILSAFE = 'failsafe'
 
-    def __init__(self, config):
+    def __init__(self, config: Dict[str, Any]) -> None:
         """
         :param config: dict, reference to config section of selected DCS.
             i.e.: `zookeeper` for zookeeper, `etcd` for etcd, etc...
         """
         self._name = config['name']
         self._base_path = re.sub('/+', '/', '/'.join(['', config.get('namespace', 'service'), config['scope']]))
         self._citus_group = str(config['group']) if isinstance(config.get('group'), int) else None
         self._set_loop_wait(config.get('loop_wait', 10))
 
         self._ctl = bool(config.get('patronictl', False))
-        self._cluster = None
-        self._cluster_valid_till = 0
+        self._cluster: Optional[Cluster] = None
+        self._cluster_valid_till: float = 0
         self._cluster_thread_lock = Lock()
-        self._last_lsn = ''
-        self._last_seen = 0
-        self._last_status = {}
-        self._last_failsafe = {}
+        self._last_lsn: int = 0
+        self._last_seen: int = 0
+        self._last_status: Dict[str, Any] = {}
+        self._last_failsafe: Optional[Dict[str, str]] = {}
         self.event = Event()
 
-    def client_path(self, path):
+    def client_path(self, path: str) -> str:
         components = [self._base_path]
         if self._citus_group:
             components.append(self._citus_group)
         components.append(path.lstrip('/'))
         return '/'.join(components)
 
     @property
-    def initialize_path(self):
+    def initialize_path(self) -> str:
         return self.client_path(self._INITIALIZE)
 
     @property
-    def config_path(self):
+    def config_path(self) -> str:
         return self.client_path(self._CONFIG)
 
     @property
-    def members_path(self):
+    def members_path(self) -> str:
         return self.client_path(self._MEMBERS)
 
     @property
-    def member_path(self):
+    def member_path(self) -> str:
         return self.client_path(self._MEMBERS + self._name)
 
     @property
-    def leader_path(self):
+    def leader_path(self) -> str:
         return self.client_path(self._LEADER)
 
     @property
-    def failover_path(self):
+    def failover_path(self) -> str:
         return self.client_path(self._FAILOVER)
 
     @property
-    def history_path(self):
+    def history_path(self) -> str:
         return self.client_path(self._HISTORY)
 
     @property
-    def status_path(self):
+    def status_path(self) -> str:
         return self.client_path(self._STATUS)
 
     @property
-    def leader_optime_path(self):
+    def leader_optime_path(self) -> str:
         return self.client_path(self._LEADER_OPTIME)
 
     @property
-    def sync_path(self):
+    def sync_path(self) -> str:
         return self.client_path(self._SYNC)
 
     @property
-    def failsafe_path(self):
+    def failsafe_path(self) -> str:
         return self.client_path(self._FAILSAFE)
 
     @abc.abstractmethod
-    def set_ttl(self, ttl):
+    def set_ttl(self, ttl: int) -> Optional[bool]:
         """Set the new ttl value for leader key"""
 
+    @property
     @abc.abstractmethod
-    def ttl(self):
+    def ttl(self) -> int:
         """Get new ttl value"""
 
     @abc.abstractmethod
-    def set_retry_timeout(self, retry_timeout):
+    def set_retry_timeout(self, retry_timeout: int) -> None:
         """Set the new value for retry_timeout"""
 
-    def _set_loop_wait(self, loop_wait):
+    def _set_loop_wait(self, loop_wait: int) -> None:
         self._loop_wait = loop_wait
 
-    def reload_config(self, config):
+    def reload_config(self, config: Union['Config', Dict[str, Any]]) -> None:
         self._set_loop_wait(config['loop_wait'])
         self.set_ttl(config['ttl'])
         self.set_retry_timeout(config['retry_timeout'])
 
     @property
-    def loop_wait(self):
+    def loop_wait(self) -> int:
         return self._loop_wait
 
     @property
-    def last_seen(self):
+    def last_seen(self) -> int:
         return self._last_seen
 
     @abc.abstractmethod
-    def _cluster_loader(self, path):
+    def _cluster_loader(self, path: Any) -> Cluster:
         """Load and build the `Cluster` object from DCS, which
         represents a single Patroni cluster.
 
         :param path: the path in DCS where to load Cluster(s) from.
         :returns: `Cluster`"""
 
-    def _citus_cluster_loader(self, path):
+    @abc.abstractmethod
+    def _citus_cluster_loader(self, path: Any) -> Union[Cluster, Dict[int, Cluster]]:
         """Load and build `Cluster` onjects from DCS that represent all
         Patroni clusters from a single Citus cluster.
 
         :param path: the path in DCS where to load Cluster(s) from.
         :returns: all Citus groups as `dict`, with group ids as keys"""
 
     @abc.abstractmethod
-    def _load_cluster(self, path, loader):
+    def _load_cluster(
+            self, path: str, loader: Callable[[Any], Union[Cluster, Dict[int, Cluster]]]
+    ) -> Union[Cluster, Dict[int, Cluster]]:
         """Internally this method should call the `loader` method that
         will build `Cluster` object which represents current state and
         topology of the cluster in DCS. This method supposed to be
         called only by `get_cluster` method.
 
         :param path: the path in DCS where to load Cluster(s) from.
         :param loader: one of `_cluster_loader` or `_citus_cluster_loader`
         :raise: `~DCSError` in case of communication problems with DCS.
         If the current node was running as a primary and exception
         raised, instance would be demoted."""
 
-    def _bypass_caches(self):
+    def _bypass_caches(self) -> None:
         """Used only in zookeeper"""
 
-    def is_citus_coordinator(self):
+    def __get_patroni_cluster(self, path: Optional[str] = None) -> Cluster:
+        if path is None:
+            path = self.client_path('')
+        cluster = self._load_cluster(path, self._cluster_loader)
+        if TYPE_CHECKING:  # pragma: no cover
+            assert isinstance(cluster, Cluster)
+        return cluster
+
+    def is_citus_coordinator(self) -> bool:
         return self._citus_group == str(CITUS_COORDINATOR_GROUP_ID)
 
-    def get_citus_coordinator(self):
+    def get_citus_coordinator(self) -> Optional[Cluster]:
         try:
-            path = '{0}/{1}/'.format(self._base_path, CITUS_COORDINATOR_GROUP_ID)
-            return self._load_cluster(path, self._cluster_loader)
+            return self.__get_patroni_cluster('{0}/{1}/'.format(self._base_path, CITUS_COORDINATOR_GROUP_ID))
         except Exception as e:
             logger.error('Failed to load Citus coordinator cluster from %s: %r', self.__class__.__name__, e)
 
-    def _get_citus_cluster(self):
+    def _get_citus_cluster(self) -> Cluster:
         groups = self._load_cluster(self._base_path + '/', self._citus_cluster_loader)
         if isinstance(groups, Cluster):  # Zookeeper could return a cached version
             cluster = groups
         else:
-            assert isinstance(groups, dict)
             cluster = groups.pop(CITUS_COORDINATOR_GROUP_ID, Cluster.empty())
             cluster.workers.update(groups)
         return cluster
 
-    def get_cluster(self, force=False):
+    def get_cluster(self, force: bool = False) -> Cluster:
         if force:
             self._bypass_caches()
         try:
-            cluster = self._get_citus_cluster() if self.is_citus_coordinator()\
-                else self._load_cluster(self.client_path(''), self._cluster_loader)
+            cluster = self._get_citus_cluster() if self.is_citus_coordinator() else self.__get_patroni_cluster()
         except Exception:
             self.reset_cluster()
             raise
 
         self._last_seen = int(time.time())
         self._last_status = {self._OPTIME: cluster.last_lsn, 'slots': cluster.slots}
         self._last_failsafe = cluster.failsafe
 
         with self._cluster_thread_lock:
             self._cluster = cluster
             self._cluster_valid_till = time.time() + self.ttl
             return cluster
 
     @property
-    def cluster(self):
+    def cluster(self) -> Optional[Cluster]:
         with self._cluster_thread_lock:
             return self._cluster if self._cluster_valid_till > time.time() else None
 
-    def reset_cluster(self):
+    def reset_cluster(self) -> None:
         with self._cluster_thread_lock:
             self._cluster = None
             self._cluster_valid_till = 0
 
     @abc.abstractmethod
-    def _write_leader_optime(self, last_lsn):
+    def _write_leader_optime(self, last_lsn: str) -> bool:
         """write current WAL LSN into `/optime/leader` key in DCS
 
         :param last_lsn: absolute WAL LSN in bytes
         :returns: `!True` on success."""
 
-    def write_leader_optime(self, last_lsn):
+    def write_leader_optime(self, last_lsn: int) -> None:
         self.write_status({self._OPTIME: last_lsn})
 
     @abc.abstractmethod
-    def _write_status(self, value):
+    def _write_status(self, value: str) -> bool:
         """write current WAL LSN and confirmed_flush_lsn of permanent slots into the `/status` key in DCS
 
         :param value: status serialized in JSON forman
         :returns: `!True` on success."""
 
-    def write_status(self, value):
+    def write_status(self, value: Dict[str, Any]) -> None:
         if not deep_compare(self._last_status, value) and self._write_status(json.dumps(value, separators=(',', ':'))):
             self._last_status = value
         cluster = self.cluster
         min_version = cluster and cluster.min_version
         if min_version and min_version < (2, 1, 0) and self._last_lsn != value[self._OPTIME]:
             self._last_lsn = value[self._OPTIME]
             self._write_leader_optime(str(value[self._OPTIME]))
 
     @abc.abstractmethod
-    def _write_failsafe(self, value):
+    def _write_failsafe(self, value: str) -> bool:
         """Write current cluster topology to DCS that will be used by failsafe mechanism (if enabled).
 
         :param value: failsafe topology serialized in JSON format
         :returns: `!True` on success."""
 
-    def write_failsafe(self, value):
+    def write_failsafe(self, value: Dict[str, str]) -> None:
         if not (isinstance(self._last_failsafe, dict) and deep_compare(self._last_failsafe, value))\
                 and self._write_failsafe(json.dumps(value, separators=(',', ':'))):
             self._last_failsafe = value
 
     @property
-    def failsafe(self):
+    def failsafe(self) -> Optional[Dict[str, str]]:
         return self._last_failsafe
 
     @abc.abstractmethod
-    def _update_leader(self):
+    def _update_leader(self, leader: Leader) -> bool:
         """Update leader key (or session) ttl
 
-        :returns: `!True` if leader key (or session) has been updated successfully.
+        :param leader: a reference to a current leader key object
+        :returns: `!True` if leader key (or session) has been updated successfully
 
         You have to use CAS (Compare And Swap) operation in order to update leader key,
         for example for etcd `prevValue` parameter must be used.
         If update fails due to DCS not being accessible or because it is not able to
         process requests (hopefuly temporary), the ~DCSError exception should be raised."""
 
-    def update_leader(self, last_lsn, slots=None, failsafe=None):
+    def update_leader(self, leader: Leader, last_lsn: Optional[int],
+                      slots: Optional[Dict[str, int]] = None, failsafe: Optional[Dict[str, str]] = None) -> bool:
         """Update leader key (or session) ttl and optime/leader
 
         :param last_lsn: absolute WAL LSN in bytes
         :param slots: dict with permanent slots confirmed_flush_lsn
         :returns: `!True` if leader key (or session) has been updated successfully."""
 
-        ret = self._update_leader()
+        ret = self._update_leader(leader)
         if ret and last_lsn:
-            status = {self._OPTIME: last_lsn}
+            status: Dict[str, Any] = {self._OPTIME: last_lsn}
             if slots:
                 status['slots'] = slots
             self.write_status(status)
 
         if ret and failsafe is not None:
             self.write_failsafe(failsafe)
 
         return ret
 
     @abc.abstractmethod
-    def attempt_to_acquire_leader(self):
+    def attempt_to_acquire_leader(self) -> bool:
         """Attempt to acquire leader lock
         This method should create `/leader` key with value=`~self._name`
         :returns: `!True` if key has been created successfully.
 
         Key must be created atomically. In case if key already exists it should not be
         overwritten and `!False` must be returned.
 
         If key creation fails due to DCS not being accessible or because it is not able to
         process requests (hopefuly temporary), the ~DCSError exception should be raised"""
 
     @abc.abstractmethod
-    def set_failover_value(self, value, index=None):
+    def set_failover_value(self, value: str, version: Optional[Any] = None) -> bool:
         """Create or update `/failover` key"""
 
-    def manual_failover(self, leader, candidate, scheduled_at=None, index=None):
+    def manual_failover(self, leader: Optional[str], candidate: Optional[str],
+                        scheduled_at: Optional[datetime.datetime] = None, version: Optional[Any] = None) -> bool:
         failover_value = {}
         if leader:
             failover_value['leader'] = leader
 
         if candidate:
             failover_value['member'] = candidate
 
         if scheduled_at:
             failover_value['scheduled_at'] = scheduled_at.isoformat()
-        return self.set_failover_value(json.dumps(failover_value, separators=(',', ':')), index)
+        return self.set_failover_value(json.dumps(failover_value, separators=(',', ':')), version)
 
     @abc.abstractmethod
-    def set_config_value(self, value, index=None):
+    def set_config_value(self, value: str, version: Optional[Any] = None) -> bool:
         """Create or update `/config` key"""
 
     @abc.abstractmethod
-    def touch_member(self, data):
+    def touch_member(self, data: Dict[str, Any]) -> bool:
         """Update member key in DCS.
         This method should create or update key with the name = '/members/' + `~self._name`
         and value = data in a given DCS.
 
         :param data: information about instance (including connection strings)
         :param ttl: ttl for member key, optional parameter. If it is None `~self.member_ttl will be used`
         :returns: `!True` on success otherwise `!False`
         """
 
     @abc.abstractmethod
-    def take_leader(self):
+    def take_leader(self) -> bool:
         """This method should create leader key with value = `~self._name` and ttl=`~self.ttl`
         Since it could be called only on initial cluster bootstrap it could create this key regardless,
         overwriting the key if necessary."""
 
     @abc.abstractmethod
-    def initialize(self, create_new=True, sysid=""):
+    def initialize(self, create_new: bool = True, sysid: str = "") -> bool:
         """Race for cluster initialization.
 
         :param create_new: False if the key should already exist (in the case we are setting the system_id)
         :param sysid: PostgreSQL cluster system identifier, if specified, is written to the key
         :returns: `!True` if key has been created successfully.
 
         this method should create atomically initialize key and return `!True`
         otherwise it should return `!False`"""
 
     @abc.abstractmethod
-    def _delete_leader(self):
+    def _delete_leader(self) -> bool:
         """Remove leader key from DCS.
         This method should remove leader key if current instance is the leader"""
 
-    def delete_leader(self, last_lsn=None):
+    def delete_leader(self, last_lsn: Optional[int] = None) -> bool:
         """Update optime/leader and voluntarily remove leader key from DCS.
         This method should remove leader key if current instance is the leader.
         :param last_lsn: latest checkpoint location in bytes"""
 
         if last_lsn:
             self.write_status({self._OPTIME: last_lsn})
         return self._delete_leader()
 
     @abc.abstractmethod
-    def cancel_initialization(self):
+    def cancel_initialization(self) -> bool:
         """ Removes the initialize key for a cluster """
 
     @abc.abstractmethod
-    def delete_cluster(self):
+    def delete_cluster(self) -> bool:
         """Delete cluster from DCS"""
 
     @staticmethod
-    def sync_state(leader, sync_standby):
-        """Build sync_state dict
-           sync_standby dictionary key being kept for backward compatibility
+    def sync_state(leader: Optional[str], sync_standby: Optional[Collection[str]]) -> Dict[str, Any]:
+        """Build sync_state dict.
+        The sync_standby key being kept for backward compatibility.
+        :param leader: name of the leader node that manages /sync key
+        :param sync_standby: collection of currently known synchronous standby node names
+        :returns: dictionary that later could be serialized to JSON or saved directly to DCS
+        """
+        return {'leader': leader, 'sync_standby': ','.join(sorted(sync_standby)) if sync_standby else None}
+
+    def write_sync_state(self, leader: Optional[str], sync_standby: Optional[Collection[str]],
+                         version: Optional[Any] = None) -> Optional[SyncState]:
+        """Write the new synchronous state to DCS.
+        Calls :func:`sync_state` method to build a dict and than calls DCS specific :func:`set_sync_state_value` method.
+        :param leader: name of the leader node that manages /sync key
+        :param sync_standby: collection of currently known synchronous standby node names
+        :param version: for conditional update of the key/object
+        :returns: the new :class:`SyncState` object or None
         """
-        return {'leader': leader, 'sync_standby': sync_standby and ','.join(sorted(sync_standby)) or None}
-
-    def write_sync_state(self, leader, sync_standby, index=None):
         sync_value = self.sync_state(leader, sync_standby)
-        return self.set_sync_state_value(json.dumps(sync_value, separators=(',', ':')), index)
+        ret = self.set_sync_state_value(json.dumps(sync_value, separators=(',', ':')), version)
+        if not isinstance(ret, bool):
+            return SyncState.from_node(ret, sync_value)
 
     @abc.abstractmethod
-    def set_history_value(self, value):
+    def set_history_value(self, value: str) -> bool:
         """"""
 
     @abc.abstractmethod
-    def set_sync_state_value(self, value, index=None):
-        """"""
+    def set_sync_state_value(self, value: str, version: Optional[Any] = None) -> Union[Any, bool]:
+        """Set synchronous state in DCS, should be implemented in the child class.
+
+        :param value: the new value of /sync key
+        :param version: for conditional update of the key/object
+        :returns: version of the new object or `False` in case of error
+        """
 
     @abc.abstractmethod
-    def delete_sync_state(self, index=None):
+    def delete_sync_state(self, version: Optional[Any] = None) -> bool:
         """"""
 
-    def watch(self, leader_index, timeout):
+    def watch(self, leader_version: Optional[Any], timeout: float) -> bool:
         """If the current node is a leader it should just sleep.
         Any other node should watch for changes of leader key with a given timeout
 
-        :param leader_index: index of a leader key
+        :param leader_version: version of a leader key
         :param timeout: timeout in seconds
         :returns: `!True` if you would like to reschedule the next run of ha cycle"""
 
         self.event.wait(timeout)
         return self.event.is_set()
```

### Comparing `patroni-3.0.2/patroni/dcs/consul.py` & `patroni-3.0.3/patroni/dcs/consul.py`

 * *Files 7% similar despite different names*

```diff
@@ -4,24 +4,27 @@
 import os
 import re
 import socket
 import ssl
 import time
 import urllib3
 
-from collections import defaultdict, namedtuple
+from collections import defaultdict
 from consul import ConsulException, NotFound, base
 from http.client import HTTPException
 from urllib3.exceptions import HTTPError
 from urllib.parse import urlencode, urlparse, quote
+from typing import Any, Callable, Dict, List, Mapping, NamedTuple, Optional, Union, Tuple, TYPE_CHECKING
 
 from . import AbstractDCS, Cluster, ClusterConfig, Failover, Leader, Member, SyncState,\
-        TimelineHistory, ReturnFalseException, catch_return_false_exception, citus_group_re
+    TimelineHistory, ReturnFalseException, catch_return_false_exception, citus_group_re
 from ..exceptions import DCSError
 from ..utils import deep_compare, parse_bool, Retry, RetryFailedError, split_host_port, uri, USER_AGENT
+if TYPE_CHECKING:  # pragma: no cover
+    from ..config import Config
 
 logger = logging.getLogger(__name__)
 
 
 class ConsulError(DCSError):
     pass
 
@@ -34,20 +37,25 @@
     """Session TTL is too small or too big"""
 
 
 class InvalidSession(ConsulException):
     """invalid session"""
 
 
-Response = namedtuple('Response', 'code,headers,body,content')
+class Response(NamedTuple):
+    code: int
+    headers: Union[Mapping[str, str], Mapping[bytes, bytes], None]
+    body: str
+    content: bytes
 
 
 class HTTPClient(object):
 
-    def __init__(self, host='127.0.0.1', port=8500, token=None, scheme='http', verify=True, cert=None, ca_cert=None):
+    def __init__(self, host: str = '127.0.0.1', port: int = 8500, token: Optional[str] = None, scheme: str = 'http',
+                 verify: bool = True, cert: Optional[str] = None, ca_cert: Optional[str] = None) -> None:
         self.token = token
         self._read_timeout = 10
         self.base_uri = uri(scheme, (host, port))
         kwargs = {}
         if cert:
             if isinstance(cert, tuple):
                 # Key and cert are separate
@@ -55,171 +63,177 @@
                 kwargs['key_file'] = cert[1]
             else:
                 # combined certificate
                 kwargs['cert_file'] = cert
         if ca_cert:
             kwargs['ca_certs'] = ca_cert
         kwargs['cert_reqs'] = ssl.CERT_REQUIRED if verify or ca_cert else ssl.CERT_NONE
-        self.http = urllib3.PoolManager(num_pools=10, maxsize=10, **kwargs)
-        self._ttl = None
+        self.http = urllib3.PoolManager(num_pools=10, maxsize=10, headers={}, **kwargs)
+        self._ttl = 30
 
-    def set_read_timeout(self, timeout):
-        self._read_timeout = timeout/3.0
+    def set_read_timeout(self, timeout: float) -> None:
+        self._read_timeout = timeout / 3.0
 
     @property
-    def ttl(self):
+    def ttl(self) -> int:
         return self._ttl
 
-    def set_ttl(self, ttl):
+    def set_ttl(self, ttl: int) -> bool:
         ret = self._ttl != ttl
         self._ttl = ttl
         return ret
 
     @staticmethod
-    def response(response):
+    def response(response: urllib3.response.HTTPResponse) -> Response:
         content = response.data
         body = content.decode('utf-8')
         if response.status == 500:
             msg = '{0} {1}'.format(response.status, body)
             if body.startswith('Invalid Session TTL'):
                 raise InvalidSessionTTL(msg)
             elif body.startswith('invalid session'):
                 raise InvalidSession(msg)
             else:
                 raise ConsulInternalError(msg)
         return Response(response.status, response.headers, body, content)
 
-    def uri(self, path, params=None):
+    def uri(self, path: str,
+            params: Union[None, Dict[str, Any], List[Tuple[str, Any]], Tuple[Tuple[str, Any], ...]] = None) -> str:
         return '{0}{1}{2}'.format(self.base_uri, path, params and '?' + urlencode(params) or '')
 
-    def __getattr__(self, method):
+    def __getattr__(self, method: str) -> Callable[[Callable[[Response], Union[bool, Any, Tuple[str, Any]]],
+                                                    str, Union[None, Dict[str, Any], List[Tuple[str, Any]]],
+                                                    str, Optional[Dict[str, str]]], Union[bool, Any, Tuple[str, Any]]]:
         if method not in ('get', 'post', 'put', 'delete'):
             raise AttributeError("HTTPClient instance has no attribute '{0}'".format(method))
 
-        def wrapper(callback, path, params=None, data='', headers=None):
+        def wrapper(callback: Callable[[Response], Union[bool, Any, Tuple[str, Any]]], path: str,
+                    params: Union[None, Dict[str, Any], List[Tuple[str, Any]]] = None, data: str = '',
+                    headers: Optional[Dict[str, str]] = None) -> Union[bool, Any, Tuple[str, Any]]:
             # python-consul doesn't allow to specify ttl smaller then 10 seconds
             # because session_ttl_min defaults to 10s, so we have to do this ugly dirty hack...
             if method == 'put' and path == '/v1/session/create':
                 ttl = '"ttl": "{0}s"'.format(self._ttl)
                 if not data or data == '{}':
                     data = '{' + ttl + '}'
                 else:
                     data = data[:-1] + ', ' + ttl + '}'
             if isinstance(params, list):  # starting from v1.1.0 python-consul switched from `dict` to `list` for params
                 params = {k: v for k, v in params}
-            kwargs = {'retries': 0, 'preload_content': False, 'body': data}
+            kwargs: Dict[str, Any] = {'retries': 0, 'preload_content': False, 'body': data}
             if method == 'get' and isinstance(params, dict) and 'index' in params:
                 timeout = float(params['wait'][:-1]) if 'wait' in params else 300
                 # According to the documentation a small random amount of additional wait time is added to the
                 # supplied maximum wait time to spread out the wake up time of any concurrent requests. This adds
                 # up to wait / 16 additional time to the maximum duration. Since our goal is actually getting a
                 # response rather read timeout we will add to the timeout a slightly bigger value.
-                kwargs['timeout'] = timeout + max(timeout/15.0, 1)
+                kwargs['timeout'] = timeout + max(timeout / 15.0, 1)
             else:
                 kwargs['timeout'] = self._read_timeout
             kwargs['headers'] = (headers or {}).copy()
             kwargs['headers'].update(urllib3.make_headers(user_agent=USER_AGENT))
             token = params.pop('token', self.token) if isinstance(params, dict) else self.token
             if token:
                 kwargs['headers']['X-Consul-Token'] = token
             return callback(self.response(self.http.request(method.upper(), self.uri(path, params), **kwargs)))
         return wrapper
 
 
 class ConsulClient(base.Consul):
 
-    def __init__(self, *args, **kwargs):
+    def __init__(self, *args: Any, **kwargs: Any) -> None:
         self._cert = kwargs.pop('cert', None)
         self._ca_cert = kwargs.pop('ca_cert', None)
         self.token = kwargs.get('token')
         super(ConsulClient, self).__init__(*args, **kwargs)
 
-    def http_connect(self, *args, **kwargs):
+    def http_connect(self, *args: Any, **kwargs: Any) -> HTTPClient:
         kwargs.update(dict(zip(['host', 'port', 'scheme', 'verify'], args)))
         if self._cert:
             kwargs['cert'] = self._cert
         if self._ca_cert:
             kwargs['ca_cert'] = self._ca_cert
         if self.token:
             kwargs['token'] = self.token
         return HTTPClient(**kwargs)
 
-    def connect(self, *args, **kwargs):
+    def connect(self, *args: Any, **kwargs: Any) -> HTTPClient:
         return self.http_connect(*args, **kwargs)
 
-    def reload_config(self, config):
+    def reload_config(self, config: Dict[str, Any]) -> None:
         self.http.token = self.token = config.get('token')
         self.consistency = config.get('consistency', 'default')
         self.dc = config.get('dc')
 
 
-def catch_consul_errors(func):
-    def wrapper(*args, **kwargs):
+def catch_consul_errors(func: Callable[..., Any]) -> Callable[..., Any]:
+    def wrapper(*args: Any, **kwargs: Any) -> Any:
         try:
             return func(*args, **kwargs)
         except (RetryFailedError, ConsulException, HTTPException, HTTPError, socket.error, socket.timeout):
             return False
     return wrapper
 
 
-def force_if_last_failed(func):
-    def wrapper(*args, **kwargs):
-        if wrapper.last_result is False:
+def force_if_last_failed(func: Callable[..., Any]) -> Callable[..., Any]:
+    def wrapper(*args: Any, **kwargs: Any) -> Any:
+        if getattr(wrapper, 'last_result', None) is False:
             kwargs['force'] = True
-        wrapper.last_result = func(*args, **kwargs)
-        return wrapper.last_result
+        last_result = func(*args, **kwargs)
+        setattr(wrapper, 'last_result', last_result)
+        return last_result
 
-    wrapper.last_result = None
+    setattr(wrapper, 'last_result', None)
     return wrapper
 
 
-def service_name_from_scope_name(scope_name):
+def service_name_from_scope_name(scope_name: str) -> str:
     """Translate scope name to service name which can be used in dns.
 
     230 = 253 - len('replica.') - len('.service.consul')
     """
 
-    def replace_char(match):
+    def replace_char(match: Any) -> str:
         c = match.group(0)
         return '-' if c in '. _' else "u{:04d}".format(ord(c))
 
     service_name = re.sub(r'[^a-z0-9\-]', replace_char, scope_name.lower())
     return service_name[0:230]
 
 
 class Consul(AbstractDCS):
 
-    def __init__(self, config):
+    def __init__(self, config: Dict[str, Any]) -> None:
         super(Consul, self).__init__(config)
         self._base_path = self._base_path[1:]
         self._scope = config['scope']
         self._session = None
         self.__do_not_watch = False
         self._retry = Retry(deadline=config['retry_timeout'], max_delay=1, max_tries=-1,
                             retry_exceptions=(ConsulInternalError, HTTPException,
                                               HTTPError, socket.error, socket.timeout))
 
-        kwargs = {}
         if 'url' in config:
-            r = urlparse(config['url'])
+            url: str = config['url']
+            r = urlparse(url)
             config.update({'scheme': r.scheme, 'host': r.hostname, 'port': r.port or 8500})
         elif 'host' in config:
             host, port = split_host_port(config.get('host', '127.0.0.1:8500'), 8500)
             config['host'] = host
             if 'port' not in config:
                 config['port'] = int(port)
 
         if config.get('cacert'):
             config['ca_cert'] = config.pop('cacert')
 
         if config.get('key') and config.get('cert'):
             config['cert'] = (config['cert'], config['key'])
 
         config_keys = ('host', 'port', 'token', 'scheme', 'cert', 'ca_cert', 'dc', 'consistency')
-        kwargs = {p: config.get(p) for p in config_keys if config.get(p)}
+        kwargs: Dict[str, Any] = {p: config.get(p) for p in config_keys if config.get(p)}
 
         verify = config.get('verify')
         if not isinstance(verify, bool):
             verify = parse_bool(verify)
         if isinstance(verify, bool):
             kwargs['verify'] = verify
 
@@ -236,63 +250,65 @@
             self._set_service_name()
         self._service_check_interval = config.get('service_check_interval', '5s')
         self._service_check_tls_server_name = config.get('service_check_tls_server_name', None)
         if not self._ctl:
             self.create_session()
         self._previous_loop_token = self._client.token
 
-    def retry(self, *args, **kwargs):
-        return self._retry.copy()(*args, **kwargs)
+    def retry(self, method: Callable[..., Any], *args: Any, **kwargs: Any) -> Any:
+        return self._retry.copy()(method, *args, **kwargs)
 
-    def create_session(self):
+    def create_session(self) -> None:
         while not self._session:
             try:
                 self.refresh_session()
             except ConsulError:
                 logger.info('waiting on consul')
                 time.sleep(5)
 
-    def reload_config(self, config):
+    def reload_config(self, config: Union['Config', Dict[str, Any]]) -> None:
         super(Consul, self).reload_config(config)
 
         consul_config = config.get('consul', {})
         self._client.reload_config(consul_config)
         self._previous_loop_service_tags = self._service_tags
-        self._service_tags = sorted(consul_config.get('service_tags', []))
+        self._service_tags: List[str] = consul_config.get('service_tags', [])
+        self._service_tags.sort()
 
         should_register_service = consul_config.get('register_service', False)
         if should_register_service and not self._register_service:
             self._set_service_name()
 
         self._previous_loop_register_service = self._register_service
         self._register_service = should_register_service
 
-    def set_ttl(self, ttl):
-        if self._client.http.set_ttl(ttl/2.0):  # Consul multiplies the TTL by 2x
+    def set_ttl(self, ttl: int) -> Optional[bool]:
+        if self._client.http.set_ttl(ttl / 2.0):  # Consul multiplies the TTL by 2x
             self._session = None
             self.__do_not_watch = True
+        return None
 
     @property
-    def ttl(self):
+    def ttl(self) -> int:
         return self._client.http.ttl * 2  # we multiply the value by 2 because it was divided in the `set_ttl()` method
 
-    def set_retry_timeout(self, retry_timeout):
+    def set_retry_timeout(self, retry_timeout: int) -> None:
         self._retry.deadline = retry_timeout
         self._client.http.set_read_timeout(retry_timeout)
 
-    def adjust_ttl(self):
+    def adjust_ttl(self) -> None:
         try:
             settings = self._client.agent.self()
-            min_ttl = (settings['Config']['SessionTTLMin'] or 10000000000)/1000000000.0
+            min_ttl = (settings['Config']['SessionTTLMin'] or 10000000000) / 1000000000.0
             logger.warning('Changing Session TTL from %s to %s', self._client.http.ttl, min_ttl)
             self._client.http.set_ttl(min_ttl)
         except Exception:
             logger.exception('adjust_ttl')
 
-    def _do_refresh_session(self, force=False):
+    def _do_refresh_session(self, force: bool = False) -> bool:
         """:returns: `!True` if it had to create new session"""
         if not force and self._session and self._last_session_refresh + self._loop_wait > time.time():
             return False
 
         if self._session:
             try:
                 self._client.session.renew(self._session)
@@ -308,26 +324,26 @@
                 logger.exception('session.create')
                 self.adjust_ttl()
                 raise
 
         self._last_session_refresh = time.time()
         return ret
 
-    def refresh_session(self):
+    def refresh_session(self) -> bool:
         try:
             return self.retry(self._do_refresh_session)
         except (ConsulException, RetryFailedError):
             logger.exception('refresh_session')
         raise ConsulError('Failed to renew/create session')
 
     @staticmethod
-    def member(node):
+    def member(node: Dict[str, str]) -> Member:
         return Member.from_node(node['ModifyIndex'], os.path.basename(node['Key']), node.get('Session'), node['Value'])
 
-    def _cluster_from_nodes(self, nodes):
+    def _cluster_from_nodes(self, nodes: Dict[str, Any]) -> Cluster:
         # get initialize flag
         initialize = nodes.get(self._INITIALIZE)
         initialize = initialize and initialize['Value']
 
         # get global dynamic configuration
         config = nodes.get(self._CONFIG)
         config = config and ClusterConfig.from_node(config['ModifyIndex'], config['Value'])
@@ -347,15 +363,15 @@
                 slots = last_lsn = None
         else:
             last_lsn = nodes.get(self._LEADER_OPTIME)
             last_lsn = last_lsn and last_lsn['Value']
             slots = None
 
         try:
-            last_lsn = int(last_lsn)
+            last_lsn = int(last_lsn or '')
         except Exception:
             last_lsn = 0
 
         # get list of members
         members = [self.member(n) for k, n in nodes.items() if k.startswith(self._MEMBERS) and k.count('/') == 1]
 
         # get leader
@@ -380,46 +396,48 @@
         try:
             failsafe = json.loads(failsafe['Value']) if failsafe else None
         except Exception:
             failsafe = None
 
         return Cluster(initialize, config, leader, last_lsn, members, failover, sync, history, slots, failsafe)
 
-    def _cluster_loader(self, path):
+    def _cluster_loader(self, path: str) -> Cluster:
         _, results = self.retry(self._client.kv.get, path, recurse=True)
         if results is None:
             raise NotFound
         nodes = {}
         for node in results:
             node['Value'] = (node['Value'] or b'').decode('utf-8')
             nodes[node['Key'][len(path):]] = node
 
         return self._cluster_from_nodes(nodes)
 
-    def _citus_cluster_loader(self, path):
+    def _citus_cluster_loader(self, path: str) -> Dict[int, Cluster]:
         _, results = self.retry(self._client.kv.get, path, recurse=True)
-        clusters = defaultdict(dict)
+        clusters: Dict[int, Dict[str, Cluster]] = defaultdict(dict)
         for node in results or []:
             key = node['Key'][len(path):].split('/', 1)
             if len(key) == 2 and citus_group_re.match(key[0]):
                 node['Value'] = (node['Value'] or b'').decode('utf-8')
                 clusters[int(key[0])][key[1]] = node
         return {group: self._cluster_from_nodes(nodes) for group, nodes in clusters.items()}
 
-    def _load_cluster(self, path, loader):
+    def _load_cluster(
+            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]
+    ) -> Union[Cluster, Dict[int, Cluster]]:
         try:
             return loader(path)
         except NotFound:
             return Cluster.empty()
         except Exception:
             logger.exception('get_cluster')
             raise ConsulError('Consul is not responding properly')
 
     @catch_consul_errors
-    def touch_member(self, data):
+    def touch_member(self, data: Dict[str, Any]) -> bool:
         cluster = self.cluster
         member = cluster and cluster.get_member(self._name, fallback_to_leader=False)
 
         try:
             create_member = self.refresh_session()
         except DCSError:
             return False
@@ -443,38 +461,40 @@
         except InvalidSession:
             self._session = None
             logger.error('Our session disappeared from Consul, can not "touch_member"')
         except Exception:
             logger.exception('touch_member')
         return False
 
-    def _set_service_name(self):
+    def _set_service_name(self) -> None:
         self._service_name = service_name_from_scope_name(self._scope)
         if self._scope != self._service_name:
             logger.warning('Using %s as consul service name instead of scope name %s', self._service_name, self._scope)
 
     @catch_consul_errors
-    def register_service(self, service_name, **kwargs):
+    def register_service(self, service_name: str, **kwargs: Any) -> bool:
         logger.info('Register service %s, params %s', service_name, kwargs)
         return self._client.agent.service.register(service_name, **kwargs)
 
     @catch_consul_errors
-    def deregister_service(self, service_id):
+    def deregister_service(self, service_id: str) -> bool:
         logger.info('Deregister service %s', service_id)
         # service_id can contain special characters, but is used as part of uri in deregister request
         service_id = quote(service_id)
         return self._client.agent.service.deregister(service_id)
 
-    def _update_service(self, data):
+    def _update_service(self, data: Dict[str, Any]) -> Optional[bool]:
         service_name = self._service_name
         role = data['role'].replace('_', '-')
         state = data['state']
-        api_parts = urlparse(data['api_url'])
+        api_url: str = data['api_url']
+        api_parts = urlparse(api_url)
         api_parts = api_parts._replace(path='/{0}'.format(role))
-        conn_parts = urlparse(data['conn_url'])
+        conn_url: str = data['conn_url']
+        conn_parts = urlparse(conn_url)
         check = base.Check.http(api_parts.geturl(), self._service_check_interval,
                                 deregister='{0}s'.format(self._client.http.ttl * 10))
         if self._service_check_tls_server_name is not None:
             check['TLSServerName'] = self._service_check_tls_server_name
         tags = self._service_tags[:]
         tags.append(role)
         if role == 'master':
@@ -502,15 +522,15 @@
             if state != 'running':
                 return
             return self.register_service(service_name, **params)
 
         logger.warning('Could not register service: unknown role type %s', role)
 
     @force_if_last_failed
-    def update_service(self, old_data, new_data, force=False):
+    def update_service(self, old_data: Dict[str, Any], new_data: Dict[str, Any], force: bool = False) -> Optional[bool]:
         update = False
 
         for key in ['role', 'api_url', 'conn_url', 'state']:
             if key not in new_data:
                 logger.warning('Could not register service: not enough params in member data')
                 return
             if old_data.get(key) != new_data[key]:
@@ -519,146 +539,145 @@
         if (
             force or update or self._register_service != self._previous_loop_register_service
             or self._service_tags != self._previous_loop_service_tags
             or self._client.token != self._previous_loop_token
         ):
             return self._update_service(new_data)
 
-    def _do_attempt_to_acquire_leader(self, retry):
+    def _do_attempt_to_acquire_leader(self, retry: Retry) -> bool:
         try:
             return retry(self._client.kv.put, self.leader_path, self._name, acquire=self._session)
         except InvalidSession:
             logger.error('Our session disappeared from Consul. Will try to get a new one and retry attempt')
             self._session = None
-            retry.deadline = retry.stoptime - time.time()
+            retry.ensure_deadline(0)
 
             retry(self._do_refresh_session)
 
-            retry.deadline = retry.stoptime - time.time()
-            if retry.deadline < 1:
-                raise ConsulError('_do_attempt_to_acquire_leader timeout')
+            retry.ensure_deadline(1, ConsulError('_do_attempt_to_acquire_leader timeout'))
 
             return retry(self._client.kv.put, self.leader_path, self._name, acquire=self._session)
 
     @catch_return_false_exception
-    def attempt_to_acquire_leader(self):
+    def attempt_to_acquire_leader(self) -> bool:
         retry = self._retry.copy()
         self._run_and_handle_exceptions(self._do_refresh_session, retry=retry)
 
-        retry.deadline = retry.stoptime - time.time()
-        if retry.deadline < 1:
-            raise ConsulError('attempt_to_acquire_leader timeout')
+        retry.ensure_deadline(1, ConsulError('attempt_to_acquire_leader timeout'))
 
         ret = self._run_and_handle_exceptions(self._do_attempt_to_acquire_leader, retry, retry=None)
         if not ret:
             logger.info('Could not take out TTL lock')
 
         return ret
 
-    def take_leader(self):
+    def take_leader(self) -> bool:
         return self.attempt_to_acquire_leader()
 
     @catch_consul_errors
-    def set_failover_value(self, value, index=None):
-        return self._client.kv.put(self.failover_path, value, cas=index)
+    def set_failover_value(self, value: str, version: Optional[int] = None) -> bool:
+        return self._client.kv.put(self.failover_path, value, cas=version)
 
     @catch_consul_errors
-    def set_config_value(self, value, index=None):
-        return self._client.kv.put(self.config_path, value, cas=index)
+    def set_config_value(self, value: str, version: Optional[int] = None) -> bool:
+        return self._client.kv.put(self.config_path, value, cas=version)
 
     @catch_consul_errors
-    def _write_leader_optime(self, last_lsn):
+    def _write_leader_optime(self, last_lsn: str) -> bool:
         return self._client.kv.put(self.leader_optime_path, last_lsn)
 
     @catch_consul_errors
-    def _write_status(self, value):
+    def _write_status(self, value: str) -> bool:
         return self._client.kv.put(self.status_path, value)
 
     @catch_consul_errors
-    def _write_failsafe(self, value):
+    def _write_failsafe(self, value: str) -> bool:
         return self._client.kv.put(self.failsafe_path, value)
 
     @staticmethod
-    def _run_and_handle_exceptions(method, *args, **kwargs):
+    def _run_and_handle_exceptions(method: Callable[..., Any], *args: Any, **kwargs: Any) -> Any:
         retry = kwargs.pop('retry', None)
         try:
             return retry(method, *args, **kwargs) if retry else method(*args, **kwargs)
         except (RetryFailedError, InvalidSession, HTTPException, HTTPError, socket.error, socket.timeout) as e:
             raise ConsulError(e)
         except ConsulException:
             raise ReturnFalseException
 
     @catch_return_false_exception
-    def _update_leader(self):
+    def _update_leader(self, leader: Leader) -> bool:
         retry = self._retry.copy()
 
         self._run_and_handle_exceptions(self._do_refresh_session, True, retry=retry)
 
-        if self._session:
-            cluster = self.cluster
-            leader_session = cluster and isinstance(cluster.leader, Leader) and cluster.leader.session
-            if leader_session != self._session:
-                retry.deadline = retry.stoptime - time.time()
-                if retry.deadline < 1:
-                    raise ConsulError('update_leader timeout')
-                logger.warning('Recreating the leader key due to session mismatch')
-                if cluster.leader:
-                    self._run_and_handle_exceptions(self._client.kv.delete, self.leader_path, cas=cluster.leader.index)
-
-                retry.deadline = retry.stoptime - time.time()
-                if retry.deadline < 0.5:
-                    raise ConsulError('update_leader timeout')
-                self._run_and_handle_exceptions(self._client.kv.put, self.leader_path,
-                                                self._name, acquire=self._session)
+        if self._session and leader.session != self._session:
+            retry.ensure_deadline(1, ConsulError('update_leader timeout'))
+
+            logger.warning('Recreating the leader key due to session mismatch')
+            self._run_and_handle_exceptions(self._client.kv.delete, self.leader_path, cas=leader.version)
+
+            retry.ensure_deadline(0.5, ConsulError('update_leader timeout'))
+
+            self._run_and_handle_exceptions(self._client.kv.put, self.leader_path, self._name, acquire=self._session)
 
         return bool(self._session)
 
     @catch_consul_errors
-    def initialize(self, create_new=True, sysid=''):
+    def initialize(self, create_new: bool = True, sysid: str = '') -> bool:
         kwargs = {'cas': 0} if create_new else {}
         return self.retry(self._client.kv.put, self.initialize_path, sysid, **kwargs)
 
     @catch_consul_errors
-    def cancel_initialization(self):
+    def cancel_initialization(self) -> bool:
         return self.retry(self._client.kv.delete, self.initialize_path)
 
     @catch_consul_errors
-    def delete_cluster(self):
+    def delete_cluster(self) -> bool:
         return self.retry(self._client.kv.delete, self.client_path(''), recurse=True)
 
     @catch_consul_errors
-    def set_history_value(self, value):
+    def set_history_value(self, value: str) -> bool:
         return self._client.kv.put(self.history_path, value)
 
     @catch_consul_errors
-    def _delete_leader(self):
+    def _delete_leader(self) -> bool:
         cluster = self.cluster
-        if cluster and isinstance(cluster.leader, Leader) and cluster.leader.name == self._name:
-            return self._client.kv.delete(self.leader_path, cas=cluster.leader.index)
+        if cluster and isinstance(cluster.leader, Leader) and\
+                cluster.leader.name == self._name and isinstance(cluster.leader.version, int):
+            return self._client.kv.delete(self.leader_path, cas=cluster.leader.version)
+        return True
 
     @catch_consul_errors
-    def set_sync_state_value(self, value, index=None):
-        return self.retry(self._client.kv.put, self.sync_path, value, cas=index)
+    def set_sync_state_value(self, value: str, version: Optional[int] = None) -> Union[int, bool]:
+        retry = self._retry.copy()
+        ret = retry(self._client.kv.put, self.sync_path, value, cas=version)
+        if ret:  # We have no other choise, only read after write :(
+            if not retry.ensure_deadline(0.5):
+                return False
+            _, ret = self.retry(self._client.kv.get, self.sync_path)
+            if ret and (ret.get('Value') or b'').decode('utf-8') == value:
+                return ret['ModifyIndex']
+        return False
 
     @catch_consul_errors
-    def delete_sync_state(self, index=None):
-        return self.retry(self._client.kv.delete, self.sync_path, cas=index)
+    def delete_sync_state(self, version: Optional[int] = None) -> bool:
+        return self.retry(self._client.kv.delete, self.sync_path, cas=version)
 
-    def watch(self, leader_index, timeout):
+    def watch(self, leader_version: Optional[int], timeout: float) -> bool:
         self._last_session_refresh = 0
         if self.__do_not_watch:
             self.__do_not_watch = False
             return True
 
-        if leader_index:
+        if leader_version:
             end_time = time.time() + timeout
             while timeout >= 1:
                 try:
-                    idx, _ = self._client.kv.get(self.leader_path, index=leader_index, wait=str(timeout) + 's')
-                    return str(idx) != str(leader_index)
+                    idx, _ = self._client.kv.get(self.leader_path, index=leader_version, wait=str(timeout) + 's')
+                    return str(idx) != str(leader_version)
                 except (ConsulException, HTTPException, HTTPError, socket.error, socket.timeout):
                     logger.exception('watch')
 
                 timeout = end_time - time.time()
 
         try:
             return super(Consul, self).watch(None, timeout)
```

### Comparing `patroni-3.0.2/patroni/dcs/exhibitor.py` & `patroni-3.0.3/patroni/dcs/exhibitor.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,72 +1,79 @@
 import json
 import logging
 import random
 import time
 
-from patroni.dcs.zookeeper import ZooKeeper
-from patroni.request import get as requests_get
-from patroni.utils import uri
+from typing import Any, Callable, Dict, List, Union
+
+from . import Cluster
+from .zookeeper import ZooKeeper
+from ..request import get as requests_get
+from ..utils import uri
 
 logger = logging.getLogger(__name__)
 
 
 class ExhibitorEnsembleProvider(object):
 
     TIMEOUT = 3.1
 
-    def __init__(self, hosts, port, uri_path='/exhibitor/v1/cluster/list', poll_interval=300):
+    def __init__(self, hosts: List[str], port: int,
+                 uri_path: str = '/exhibitor/v1/cluster/list', poll_interval: int = 300) -> None:
         self._exhibitor_port = port
         self._uri_path = uri_path
         self._poll_interval = poll_interval
-        self._exhibitors = hosts
+        self._exhibitors: List[str] = hosts
         self._boot_exhibitors = hosts
         self._zookeeper_hosts = ''
         self._next_poll = None
         while not self.poll():
             logger.info('waiting on exhibitor')
             time.sleep(5)
 
-    def poll(self):
+    def poll(self) -> bool:
         if self._next_poll and self._next_poll > time.time():
             return False
 
         json = self._query_exhibitors(self._exhibitors)
         if not json:
             json = self._query_exhibitors(self._boot_exhibitors)
 
         if isinstance(json, dict) and 'servers' in json and 'port' in json:
             self._next_poll = time.time() + self._poll_interval
-            zookeeper_hosts = ','.join([h + ':' + str(json['port']) for h in sorted(json['servers'])])
+            servers: List[str] = json['servers']
+            zookeeper_hosts = ','.join([h + ':' + str(json['port']) for h in sorted(servers)])
             if self._zookeeper_hosts != zookeeper_hosts:
                 logger.info('ZooKeeper connection string has changed: %s => %s', self._zookeeper_hosts, zookeeper_hosts)
                 self._zookeeper_hosts = zookeeper_hosts
                 self._exhibitors = json['servers']
                 return True
         return False
 
-    def _query_exhibitors(self, exhibitors):
+    def _query_exhibitors(self, exhibitors: List[str]) -> Union[Dict[str, Any], Any]:
         random.shuffle(exhibitors)
         for host in exhibitors:
             try:
                 response = requests_get(uri('http', (host, self._exhibitor_port), self._uri_path), timeout=self.TIMEOUT)
                 return json.loads(response.data.decode('utf-8'))
             except Exception:
                 logging.debug('Request to %s failed', host)
         return None
 
     @property
-    def zookeeper_hosts(self):
+    def zookeeper_hosts(self) -> str:
         return self._zookeeper_hosts
 
 
 class Exhibitor(ZooKeeper):
 
-    def __init__(self, config):
+    def __init__(self, config: Dict[str, Any]) -> None:
         interval = config.get('poll_interval', 300)
         self._ensemble_provider = ExhibitorEnsembleProvider(config['hosts'], config['port'], poll_interval=interval)
         super(Exhibitor, self).__init__({**config, 'hosts': self._ensemble_provider.zookeeper_hosts})
 
-    def _load_cluster(self, path, loader):
+    def _load_cluster(
+            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]
+    ) -> Union[Cluster, Dict[int, Cluster]]:
         if self._ensemble_provider.poll():
             self._client.set_hosts(self._ensemble_provider.zookeeper_hosts)
         return super(Exhibitor, self)._load_cluster(path, loader)
```

### Comparing `patroni-3.0.2/patroni/dcs/kubernetes.py` & `patroni-3.0.3/patroni/dcs/kubernetes.py`

 * *Files 10% similar despite different names*

```diff
@@ -11,96 +11,99 @@
 import time
 import urllib3
 import yaml
 
 from collections import defaultdict
 from copy import deepcopy
 from http.client import HTTPException
-from threading import Condition, Lock, Thread
-from typing import Any, Dict, List, Optional
 from urllib3.exceptions import HTTPError
+from threading import Condition, Lock, Thread
+from typing import Any, Callable, Collection, Dict, List, Optional, Tuple, Type, Union, TYPE_CHECKING
 
 from . import AbstractDCS, Cluster, ClusterConfig, Failover, Leader, Member, SyncState,\
-        TimelineHistory, CITUS_COORDINATOR_GROUP_ID, citus_group_re
+    TimelineHistory, CITUS_COORDINATOR_GROUP_ID, citus_group_re
 from ..exceptions import DCSError
 from ..utils import deep_compare, iter_response_objects, keepalive_socket_options,\
-        Retry, RetryFailedError, tzutc, uri, USER_AGENT
+    Retry, RetryFailedError, tzutc, uri, USER_AGENT
+if TYPE_CHECKING:  # pragma: no cover
+    from ..config import Config
 
 logger = logging.getLogger(__name__)
 
 KUBE_CONFIG_DEFAULT_LOCATION = os.environ.get('KUBECONFIG', '~/.kube/config')
 SERVICE_HOST_ENV_NAME = 'KUBERNETES_SERVICE_HOST'
 SERVICE_PORT_ENV_NAME = 'KUBERNETES_SERVICE_PORT'
 SERVICE_TOKEN_FILENAME = '/var/run/secrets/kubernetes.io/serviceaccount/token'
 SERVICE_CERT_FILENAME = '/var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
-__temp_files = []
+__temp_files: List[str] = []
 
 
 class KubernetesError(DCSError):
     pass
 
 
-def _cleanup_temp_files():
+def _cleanup_temp_files() -> None:
     global __temp_files
     for temp_file in __temp_files:
         try:
             os.remove(temp_file)
         except OSError:
             pass
     __temp_files = []
 
 
-def _create_temp_file(content):
+def _create_temp_file(content: bytes) -> str:
     if len(__temp_files) == 0:
         atexit.register(_cleanup_temp_files)
 
     fd, name = tempfile.mkstemp()
     os.write(fd, content)
     os.close(fd)
     __temp_files.append(name)
     return name
 
 
 # this function does the same mapping of snake_case => camelCase for > 97% of cases as autogenerated swagger code
-def to_camel_case(value):
+def to_camel_case(value: str) -> str:
     reserved = {'api', 'apiv3', 'cidr', 'cpu', 'csi', 'id', 'io', 'ip', 'ipc', 'pid', 'tls', 'uri', 'url', 'uuid'}
     words = value.split('_')
     return words[0] + ''.join(w.upper() if w in reserved else w.title() for w in words[1:])
 
 
 class K8sConfig(object):
 
     class ConfigException(Exception):
         pass
 
-    def __init__(self):
-        self.pool_config = {'maxsize': 10, 'num_pools': 10}  # configuration for urllib3.PoolManager
+    def __init__(self) -> None:
+        self.pool_config: Dict[str, Any] = {'maxsize': 10, 'num_pools': 10}  # urllib3.PoolManager config
         self._token_expires_at = datetime.datetime.max
+        self._headers: Dict[str, str] = {}
         self._make_headers()
 
-    def _set_token(self, token):
+    def _set_token(self, token: str) -> None:
         self._headers['authorization'] = 'Bearer ' + token
 
-    def _make_headers(self, token=None, **kwargs):
+    def _make_headers(self, token: Optional[str] = None, **kwargs: Any) -> None:
         self._headers = urllib3.make_headers(user_agent=USER_AGENT, **kwargs)
         if token:
             self._set_token(token)
 
-    def _read_token_file(self):
+    def _read_token_file(self) -> str:
         if not os.path.isfile(SERVICE_TOKEN_FILENAME):
             raise self.ConfigException('Service token file does not exists.')
         with open(SERVICE_TOKEN_FILENAME) as f:
             token = f.read()
             if not token:
                 raise self.ConfigException('Token file exists but empty.')
             self._token_expires_at = datetime.datetime.now() + self._token_refresh_interval
             return token
 
-    def load_incluster_config(self, ca_certs=SERVICE_CERT_FILENAME,
-                              token_refresh_interval=datetime.timedelta(minutes=1)):
+    def load_incluster_config(self, ca_certs: str = SERVICE_CERT_FILENAME,
+                              token_refresh_interval: datetime.timedelta = datetime.timedelta(minutes=1)) -> None:
         if SERVICE_HOST_ENV_NAME not in os.environ or SERVICE_PORT_ENV_NAME not in os.environ:
             raise self.ConfigException('Service host/port is not set.')
         if not os.environ[SERVICE_HOST_ENV_NAME] or not os.environ[SERVICE_PORT_ENV_NAME]:
             raise self.ConfigException('Service host/port is set but empty.')
 
         if not os.path.isfile(ca_certs):
             raise self.ConfigException('Service certificate file does not exists.')
@@ -110,85 +113,94 @@
         self.pool_config['ca_certs'] = ca_certs
         self._token_refresh_interval = token_refresh_interval
         token = self._read_token_file()
         self._make_headers(token=token)
         self._server = uri('https', (os.environ[SERVICE_HOST_ENV_NAME], os.environ[SERVICE_PORT_ENV_NAME]))
 
     @staticmethod
-    def _get_by_name(config, section, name):
+    def _get_by_name(config: Dict[str, List[Dict[str, Any]]], section: str, name: str) -> Optional[Dict[str, Any]]:
         for c in config[section + 's']:
             if c['name'] == name:
                 return c[section]
 
-    def _pool_config_from_file_or_data(self, config, file_key_name, pool_key_name):
+    def _pool_config_from_file_or_data(self, config: Dict[str, str], file_key_name: str, pool_key_name: str) -> None:
         data_key_name = file_key_name + '-data'
         if data_key_name in config:
             self.pool_config[pool_key_name] = _create_temp_file(base64.b64decode(config[data_key_name]))
         elif file_key_name in config:
             self.pool_config[pool_key_name] = config[file_key_name]
 
-    def load_kube_config(self, context=None):
+    def load_kube_config(self, context: Optional[str] = None) -> None:
         with open(os.path.expanduser(KUBE_CONFIG_DEFAULT_LOCATION)) as f:
-            config = yaml.safe_load(f)
+            config: Dict[str, Any] = yaml.safe_load(f)
 
-        context = self._get_by_name(config, 'context', context or config['current-context'])
-        cluster = self._get_by_name(config, 'cluster', context['cluster'])
-        user = self._get_by_name(config, 'user', context['user'])
+        context = context or config['current-context']
+        context_value = self._get_by_name(config, 'context', context)
+        if TYPE_CHECKING:  # pragma: no cover
+            assert isinstance(context_value, dict)
+        cluster = self._get_by_name(config, 'cluster', context_value['cluster'])
+        if TYPE_CHECKING:  # pragma: no cover
+            assert isinstance(cluster, dict)
+        user = self._get_by_name(config, 'user', context_value['user'])
+        if TYPE_CHECKING:  # pragma: no cover
+            assert isinstance(user, dict)
 
         self._server = cluster['server'].rstrip('/')
         if self._server.startswith('https'):
             self._pool_config_from_file_or_data(user, 'client-certificate', 'cert_file')
             self._pool_config_from_file_or_data(user, 'client-key', 'key_file')
             self._pool_config_from_file_or_data(cluster, 'certificate-authority', 'ca_certs')
             self.pool_config['cert_reqs'] = 'CERT_NONE' if cluster.get('insecure-skip-tls-verify') else 'CERT_REQUIRED'
         if user.get('token'):
             self._make_headers(token=user['token'])
         elif 'username' in user and 'password' in user:
-            self._headers = self._make_headers(basic_auth=':'.join((user['username'],  user['password'])))
+            self._make_headers(basic_auth=':'.join((user['username'], user['password'])))
 
     @property
-    def server(self):
+    def server(self) -> str:
         return self._server
 
     @property
-    def headers(self):
+    def headers(self) -> Dict[str, str]:
         if self._token_expires_at <= datetime.datetime.now():
             try:
                 self._set_token(self._read_token_file())
             except Exception as e:
                 logger.error('Failed to refresh service account token: %r', e)
         return self._headers.copy()
 
 
 class K8sObject(object):
 
-    def __init__(self, kwargs):
+    def __init__(self, kwargs: Dict[str, Any]) -> None:
         self._dict = {k: self._wrap(k, v) for k, v in kwargs.items()}
 
-    def get(self, name, default=None):
+    def get(self, name: str, default: Optional[Any] = None) -> Optional[Any]:
         return self._dict.get(name, default)
 
-    def __getattr__(self, name):
+    def __getattr__(self, name: str) -> Any:
         return self.get(to_camel_case(name))
 
     @classmethod
-    def _wrap(cls, parent, value):
+    def _wrap(cls, parent: Optional[str], value: Any) -> Any:
         if isinstance(value, dict):
+            data_dict: Dict[str, Any] = value
             # we know that `annotations` and `labels` are dicts and therefore don't want to convert them into K8sObject
-            return value if parent in {'annotations', 'labels'} and \
-                    all(isinstance(v, str) for v in value.values()) else cls(value)
+            return data_dict if parent in {'annotations', 'labels'} and \
+                all(isinstance(v, str) for v in data_dict.values()) else cls(data_dict)
         elif isinstance(value, list):
-            return [cls._wrap(None, v) for v in value]
+            data_list: List[Any] = value
+            return [cls._wrap(None, v) for v in data_list]
         else:
             return value
 
-    def to_dict(self):
+    def to_dict(self) -> Dict[str, Any]:
         return self._dict
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return json.dumps(self, indent=4, default=lambda o: o.to_dict())
 
 
 class K8sException(Exception):
     pass
 
 
@@ -197,21 +209,22 @@
 
 
 class K8sClient(object):
 
     class rest(object):
 
         class ApiException(Exception):
-            def __init__(self, status=None, reason=None, http_resp=None):
+            def __init__(self, status: Optional[int] = None, reason: Optional[str] = None,
+                         http_resp: Optional[urllib3.HTTPResponse] = None) -> None:
                 self.status = http_resp.status if http_resp else status
                 self.reason = http_resp.reason if http_resp else reason
                 self.body = http_resp.data if http_resp else None
-                self.headers = http_resp.getheaders() if http_resp else None
+                self.headers = http_resp.headers if http_resp else None
 
-            def __str__(self):
+            def __str__(self) -> str:
                 error_message = "({0})\nReason: {1}\n".format(self.status, self.reason)
                 if self.headers:
                     error_message += "HTTP response headers: {0}\n".format(self.headers)
                 if self.body:
                     error_message += "HTTP response body: {0}\n".format(self.body)
                 return error_message
 
@@ -228,67 +241,71 @@
             self.set_api_servers_cache_ttl(10)
             self.set_read_timeout(10)
             try:
                 self._load_api_servers_cache()
             except K8sException:
                 pass
 
-        def set_read_timeout(self, timeout):
+        def set_read_timeout(self, timeout: Union[int, float]) -> None:
             self._read_timeout = timeout
 
-        def set_api_servers_cache_ttl(self, ttl):
+        def set_api_servers_cache_ttl(self, ttl: int) -> None:
             self._api_servers_cache_ttl = ttl - 0.5
 
-        def set_base_uri(self, value):
+        def set_base_uri(self, value: str) -> None:
             logger.info('Selected new K8s API server endpoint %s', value)
             # We will connect by IP of the K8s master node which is not listed as alternative name
             self.pool_manager.connection_pool_kw['assert_hostname'] = False
             self._base_uri = value
 
         @staticmethod
-        def _handle_server_response(response, _preload_content):
+        def _handle_server_response(response: urllib3.HTTPResponse,
+                                    _preload_content: bool) -> Union[urllib3.HTTPResponse, K8sObject]:
             if response.status not in range(200, 206):
                 raise k8s_client.rest.ApiException(http_resp=response)
             return K8sObject(json.loads(response.data.decode('utf-8'))) if _preload_content else response
 
         @staticmethod
-        def _make_headers(headers):
+        def _make_headers(headers: Optional[Dict[str, str]]) -> Dict[str, str]:
             ret = k8s_config.headers
             ret.update(headers or {})
             return ret
 
         @property
-        def api_servers_cache(self):
+        def api_servers_cache(self) -> List[str]:
             base_uri, cache = self._base_uri, self._api_servers_cache
             return ([base_uri] if base_uri in cache else []) + [machine for machine in cache if machine != base_uri]
 
-        def _get_api_servers(self, api_servers_cache):
+        def _get_api_servers(self, api_servers_cache: List[str]) -> List[str]:
             _, per_node_timeout, per_node_retries = self._calculate_timeouts(len(api_servers_cache))
-            kwargs = {'headers': self._make_headers({}), 'preload_content': True, 'retries': per_node_retries,
-                      'timeout': urllib3.Timeout(connect=max(1, per_node_timeout/2.0), total=per_node_timeout)}
+            headers = self._make_headers({})
+            kwargs = {'preload_content': True, 'retries': per_node_retries,
+                      'timeout': urllib3.Timeout(connect=max(1.0, per_node_timeout / 2.0), total=per_node_timeout)}
             path = self._API_URL_PREFIX + 'default/endpoints/kubernetes'
             for base_uri in api_servers_cache:
                 try:
-                    response = self.pool_manager.request('GET', base_uri + path, **kwargs)
+                    response = self.pool_manager.request('GET', base_uri + path, headers=headers, **kwargs)
                     endpoint = self._handle_server_response(response, True)
+                    if TYPE_CHECKING:  # pragma: no cover
+                        assert isinstance(endpoint, K8sObject)
                     for subset in endpoint.subsets:
                         for port in subset.ports:
                             if port.name == 'https' and port.protocol == 'TCP':
                                 addresses = [uri('https', (a.ip, port.port)) for a in subset.addresses]
                                 if addresses:
                                     random.shuffle(addresses)
                                     return addresses
                 except Exception as e:
                     if isinstance(e, k8s_client.rest.ApiException) and e.status == 403:
                         raise
                     self.pool_manager.clear()
                     logger.error('Failed to get "kubernetes" endpoint from %s: %r', base_uri, e)
             raise K8sConnectionFailed('No more K8s API server nodes in the cluster')
 
-        def _refresh_api_servers_cache(self, updating_cache=False):
+        def _refresh_api_servers_cache(self, updating_cache: Optional[bool] = False) -> None:
             if self._bypass_api_service:
                 try:
                     api_servers_cache = [k8s_config.server] if updating_cache else self.api_servers_cache
                     self._api_servers_cache = self._get_api_servers(api_servers_cache)
                     if updating_cache:
                         self.pool_manager.clear()
                 except k8s_client.rest.ApiException:  # 403 Permission denied
@@ -305,24 +322,24 @@
             else:
                 self._api_servers_cache = [k8s_config.server]
 
             if self._base_uri not in self._api_servers_cache:
                 self.set_base_uri(self._api_servers_cache[0])
             self._api_servers_cache_updated = time.time()
 
-        def refresh_api_servers_cache(self):
+        def refresh_api_servers_cache(self) -> None:
             if self._bypass_api_service and time.time() - self._api_servers_cache_updated > self._api_servers_cache_ttl:
                 self._refresh_api_servers_cache()
 
-        def _load_api_servers_cache(self):
+        def _load_api_servers_cache(self) -> None:
             self._update_api_servers_cache = True
             self._refresh_api_servers_cache(True)
             self._update_api_servers_cache = False
 
-        def _calculate_timeouts(self, api_servers, timeout=None):
+        def _calculate_timeouts(self, api_servers: int, timeout: Optional[float] = None) -> Tuple[int, float, int]:
             """Calculate a request timeout and number of retries per single K8s API server node.
             In case if the timeout per node is too small (less than one second) we will reduce the number of nodes.
             For the cluster with only one API server node we will try to do 1 retry.
             No retries for clusters with 2 or more API server nodes. We better rely on switching to a different node."""
 
             per_node_timeout = timeout = float(timeout or self._read_timeout)
 
@@ -340,15 +357,16 @@
                     break
                 # if the timeout per one node is to small try to reduce number of nodes
                 api_servers -= 1
                 max_retries = 1
 
             return api_servers, per_node_timeout, per_node_retries - 1
 
-        def _do_http_request(self, retry, api_servers_cache, method, path, **kwargs):
+        def _do_http_request(self, retry: Optional[Retry], api_servers_cache: List[str],
+                             method: str, path: str, **kwargs: Any) -> urllib3.HTTPResponse:
             some_request_failed = False
             for i, base_uri in enumerate(api_servers_cache):
                 if i > 0:
                     logger.info('Retrying on %s', base_uri)
                 try:
                     response = self.pool_manager.request(method, base_uri + path, **kwargs)
                     if some_request_failed:
@@ -363,76 +381,84 @@
                             self.set_base_uri(api_servers_cache[i + 1])
                         raise K8sException('{0} {1} request failed'.format(method, path))
                     logger.error('Request to server %s failed: %r', base_uri, e)
                     some_request_failed = True
 
             raise K8sConnectionFailed('No more API server nodes in the cluster')
 
-        def request(self, retry, method, path, timeout=None, **kwargs):
+        def request(
+                self, retry: Optional[Retry], method: str, path: str,
+                timeout: Union[int, float, Tuple[Union[int, float], Union[int, float]], urllib3.Timeout, None] = None,
+                **kwargs: Any) -> urllib3.HTTPResponse:
             if self._update_api_servers_cache:
                 self._load_api_servers_cache()
 
             api_servers_cache = self.api_servers_cache
             api_servers = len(api_servers_cache)
 
             if timeout:
                 if isinstance(timeout, (int, float)):
                     timeout = urllib3.Timeout(total=timeout)
                 elif isinstance(timeout, tuple) and len(timeout) == 2:
                     timeout = urllib3.Timeout(connect=timeout[0], read=timeout[1])
                 retries = 0
             else:
                 _, timeout, retries = self._calculate_timeouts(api_servers)
-                timeout = urllib3.Timeout(connect=max(1, timeout/2.0), total=timeout)
+                timeout = urllib3.Timeout(connect=max(1.0, timeout / 2.0), total=timeout)
             kwargs.update(retries=retries, timeout=timeout)
 
             while True:
                 try:
                     return self._do_http_request(retry, api_servers_cache, method, path, **kwargs)
                 except K8sConnectionFailed as ex:
                     try:
                         self._load_api_servers_cache()
                         api_servers_cache = self.api_servers_cache
                         api_servers = len(api_servers_cache)
                     except Exception as e:
                         logger.debug('Failed to update list of K8s master nodes: %r', e)
 
+                    if TYPE_CHECKING:  # pragma: no cover
+                        assert isinstance(retry, Retry)  # K8sConnectionFailed is raised only if retry is not None!
                     sleeptime = retry.sleeptime
                     remaining_time = (retry.stoptime or time.time()) - sleeptime - time.time()
                     nodes, timeout, retries = self._calculate_timeouts(api_servers, remaining_time)
                     if nodes == 0:
                         self._update_api_servers_cache = True
                         raise ex
                     retry.sleep_func(sleeptime)
                     retry.update_delay()
                     # We still have some time left. Partially reduce `api_servers_cache` and retry request
-                    kwargs.update(timeout=urllib3.Timeout(connect=max(1, timeout/2.0), total=timeout), retries=retries)
+                    kwargs.update(timeout=urllib3.Timeout(connect=max(1.0, timeout / 2.0), total=timeout),
+                                  retries=retries)
                     api_servers_cache = api_servers_cache[:nodes]
 
-        def call_api(self, method, path, headers=None, body=None, _retry=None,
-                     _preload_content=True, _request_timeout=None, **kwargs):
+        def call_api(self, method: str, path: str, headers: Optional[Dict[str, str]] = None,
+                     body: Optional[Any] = None, _retry: Optional[Retry] = None, _preload_content: bool = True,
+                     _request_timeout: Optional[float] = None, **kwargs: Any) -> Union[urllib3.HTTPResponse, K8sObject]:
             headers = self._make_headers(headers)
             fields = {to_camel_case(k): v for k, v in kwargs.items()}  # resource_version => resourceVersion
             body = json.dumps(body, default=lambda o: o.to_dict()) if body is not None else None
 
             response = self.request(_retry, method, self._API_URL_PREFIX + path, headers=headers, fields=fields,
                                     body=body, preload_content=_preload_content, timeout=_request_timeout)
 
             return self._handle_server_response(response, _preload_content)
 
     class CoreV1Api(object):
 
-        def __init__(self, api_client=None):
+        def __init__(self, api_client: Optional['K8sClient.ApiClient'] = None) -> None:
             self._api_client = api_client or k8s_client.ApiClient()
 
-        def __getattr__(self, func):  # `func` name pattern: (action)_namespaced_(kind)
+        def __getattr__(self, func: str) -> Callable[..., Any]:
+            # `func` name pattern: (action)_namespaced_(kind)
             action, kind = func.split('_namespaced_')  # (read|list|create|patch|replace|delete|delete_collection)
             kind = kind.replace('_', '') + ('s' * int(kind[-1] != 's'))  # plural, single word
 
-            def wrapper(*args, **kwargs):
+            def wrapper(*args: Any, **kwargs: Any) -> Union[urllib3.HTTPResponse, K8sObject]:
                 method = {'read': 'GET', 'list': 'GET', 'create': 'POST',
                           'replace': 'PUT'}.get(action, action.split('_')[0]).upper()
 
                 if action == 'create' or len(args) == 1:  # namespace is a first argument and name in not in arguments
                     path = '/'.join([args[0], kind])
                 else:  # name, namespace followed by optional body
                     path = '/'.join([args[1], kind, args[0]])
@@ -449,43 +475,43 @@
                     body = None
 
                 return self._api_client.call_api(method, path, headers, body, **kwargs)
             return wrapper
 
     class _K8sObjectTemplate(K8sObject):
         """The template for objects which we create locally, e.g. k8s_client.V1ObjectMeta & co"""
-        def __init__(self, **kwargs):
+        def __init__(self, **kwargs: Any) -> None:
             self._dict = {to_camel_case(k): v for k, v in kwargs.items()}
 
-    def __init__(self):
-        self.__cls_cache = {}
+    def __init__(self) -> None:
+        self.__cls_cache: Dict[str, Type['K8sClient._K8sObjectTemplate']] = {}
         self.__cls_lock = Lock()
 
-    def __getattr__(self, name):
+    def __getattr__(self, name: str) -> Type['K8sClient._K8sObjectTemplate']:
         with self.__cls_lock:
             if name not in self.__cls_cache:
                 self.__cls_cache[name] = type(name, (self._K8sObjectTemplate,), {})
         return self.__cls_cache[name]
 
 
 k8s_client = K8sClient()
 k8s_config = K8sConfig()
 
 
 class KubernetesRetriableException(k8s_client.rest.ApiException):
 
-    def __init__(self, orig):
+    def __init__(self, orig: K8sClient.rest.ApiException) -> None:
         super(KubernetesRetriableException, self).__init__(orig.status, orig.reason)
         self.body = orig.body
         self.headers = orig.headers
 
     @property
-    def sleeptime(self):
+    def sleeptime(self) -> Optional[int]:
         try:
-            return int(self.headers['retry-after'])
+            return int((self.headers or {}).get('retry-after', ''))
         except Exception:
             return None
 
 
 class CoreV1ApiProxy(object):
     """Proxy class to work with k8s_client.CoreV1Api() object"""
 
@@ -493,131 +519,146 @@
 
     def __init__(self, use_endpoints: Optional[bool] = False, bypass_api_service: Optional[bool] = False) -> None:
         self._api_client = k8s_client.ApiClient(bypass_api_service)
         self._core_v1_api = k8s_client.CoreV1Api(self._api_client)
         self._use_endpoints = bool(use_endpoints)
         self._retriable_http_codes = set(self._DEFAULT_RETRIABLE_HTTP_CODES)
 
-    def configure_timeouts(self, loop_wait, retry_timeout, ttl):
+    def configure_timeouts(self, loop_wait: int, retry_timeout: Union[int, float], ttl: int) -> None:
         # Normally every loop_wait seconds we should have receive something from the socket.
         # If we didn't received anything after the loop_wait + retry_timeout it is a time
         # to start worrying (send keepalive messages). Finally, the connection should be
         # considered as dead if we received nothing from the socket after the ttl seconds.
         self._api_client.pool_manager.connection_pool_kw['socket_options'] = \
-                list(keepalive_socket_options(ttl, int(loop_wait + retry_timeout)))
+            list(keepalive_socket_options(ttl, int(loop_wait + retry_timeout)))
         self._api_client.set_read_timeout(retry_timeout)
         self._api_client.set_api_servers_cache_ttl(loop_wait)
 
     def configure_retriable_http_codes(self, retriable_http_codes: List[int]) -> None:
         self._retriable_http_codes = self._DEFAULT_RETRIABLE_HTTP_CODES | set(retriable_http_codes)
 
-    def refresh_api_servers_cache(self):
+    def refresh_api_servers_cache(self) -> None:
         self._api_client.refresh_api_servers_cache()
 
-    def __getattr__(self, func: str):
+    def __getattr__(self, func: str) -> Callable[..., Any]:
         """Intercepts calls to `CoreV1Api` methods.
 
         Handles two important cases:
         1. Depending on whether Patroni is configured to work with `ConfigMaps` or `Endpoints`
            it remaps "virtual" method names from `*_kind` to `*_endpoints` or `*_config_map`.
         2. It handles HTTP error codes and raises `KubernetesRetriableException`
            if the given error is supposed to be handled with retry."""
 
         if func.endswith('_kind'):
             func = func[:-4] + ('endpoints' if self._use_endpoints else 'config_map')
 
-        def wrapper(*args, **kwargs):
+        def wrapper(*args: Any, **kwargs: Any) -> Any:
             try:
                 return getattr(self._core_v1_api, func)(*args, **kwargs)
             except k8s_client.rest.ApiException as e:
                 if e.status in self._retriable_http_codes or e.headers and 'retry-after' in e.headers:
                     raise KubernetesRetriableException(e)
                 raise
         return wrapper
 
     @property
-    def use_endpoints(self):
+    def use_endpoints(self) -> bool:
         return self._use_endpoints
 
 
-def catch_kubernetes_errors(func):
-    def wrapper(self, *args, **kwargs):
+def _run_and_handle_exceptions(method: Callable[..., Any], *args: Any, **kwargs: Any) -> Any:
+    try:
+        return method(*args, **kwargs)
+    except k8s_client.rest.ApiException as e:
+        if e.status == 403:
+            logger.exception('Permission denied')
+        elif e.status != 409:  # Object exists or conflict in resource_version
+            logger.exception('Unexpected error from Kubernetes API')
+        return False
+    except (RetryFailedError, K8sException) as e:
+        raise KubernetesError(e)
+
+
+def catch_kubernetes_errors(func: Callable[..., Any]) -> Callable[..., Any]:
+    def wrapper(self: 'Kubernetes', *args: Any, **kwargs: Any) -> Any:
         try:
-            return self._run_and_handle_exceptions(func, self, *args, **kwargs)
+            return _run_and_handle_exceptions(func, self, *args, **kwargs)
         except KubernetesError:
             return False
     return wrapper
 
 
 class ObjectCache(Thread):
 
-    def __init__(self, dcs, func, retry, condition, name=None):
-        Thread.__init__(self)
+    def __init__(self, dcs: 'Kubernetes', func: Callable[..., Any], retry: Retry,
+                 condition: Condition, name: Optional[str] = None) -> None:
+        super(ObjectCache, self).__init__()
         self.daemon = True
         self._dcs = dcs
         self._func = func
         self._retry = retry
         self._condition = condition
         self._name = name  # name of this pod
         self._is_ready = False
-        self._response = None  # needs to be accessible from the `kill_stream()` method
+        self._response: Union[urllib3.HTTPResponse, bool, None] = None  # needs to be accessible from the `kill_stream`
         self._response_lock = Lock()  # protect the `self._response` from concurrent access
-        self._object_cache = {}
+        self._object_cache: Dict[str, K8sObject] = {}
         self._object_cache_lock = Lock()
-        self._annotations_map = {self._dcs.leader_path: self._dcs._LEADER, self._dcs.config_path: self._dcs._CONFIG}
+        self._annotations_map = {self._dcs.leader_path: getattr(self._dcs, '_LEADER'),
+                                 self._dcs.config_path: getattr(self._dcs, '_CONFIG')}  # pyright
         self.start()
 
-    def _list(self):
+    def _list(self) -> K8sObject:
         try:
             return self._func(_retry=self._retry.copy())
         except Exception:
             time.sleep(1)
             raise
 
-    def _watch(self, resource_version):
+    def _watch(self, resource_version: str) -> urllib3.HTTPResponse:
         return self._func(_request_timeout=(self._retry.deadline, urllib3.Timeout.DEFAULT_TIMEOUT),
                           _preload_content=False, watch=True, resource_version=resource_version)
 
-    def set(self, name, value):
+    def set(self, name: str, value: K8sObject) -> Tuple[bool, Optional[K8sObject]]:
         with self._object_cache_lock:
             old_value = self._object_cache.get(name)
             ret = not old_value or int(old_value.metadata.resource_version) < int(value.metadata.resource_version)
             if ret:
                 self._object_cache[name] = value
         return ret, old_value
 
-    def delete(self, name, resource_version):
+    def delete(self, name: str, resource_version: str) -> Tuple[bool, Optional[K8sObject]]:
         with self._object_cache_lock:
             old_value = self._object_cache.get(name)
             ret = old_value and int(old_value.metadata.resource_version) < int(resource_version)
             if ret:
                 del self._object_cache[name]
-        return not old_value or ret, old_value
+        return bool(not old_value or ret), old_value
 
-    def copy(self):
+    def copy(self) -> Dict[str, K8sObject]:
         with self._object_cache_lock:
             return self._object_cache.copy()
 
-    def get(self, name):
+    def get(self, name: str) -> Optional[K8sObject]:
         with self._object_cache_lock:
             return self._object_cache.get(name)
 
-    def _process_event(self, event):
+    def _process_event(self, event: Dict[str, Union[Any, Dict[str, Union[Any, Dict[str, Any]]]]]) -> None:
         ev_type = event['type']
         obj = event['object']
         name = obj['metadata']['name']
 
+        new_value = None
         if ev_type in ('ADDED', 'MODIFIED'):
             obj = K8sObject(obj)
             success, old_value = self.set(name, obj)
             if success:
                 new_value = (obj.metadata.annotations or {}).get(self._annotations_map.get(name))
         elif ev_type == 'DELETED':
             success, old_value = self.delete(name, obj['metadata']['resourceVersion'])
-            new_value = None
         else:
             return logger.warning('Unexpected event type: %s', ev_type)
 
         if success and obj.get('kind') != 'Pod':
             if old_value:
                 old_value = (old_value.metadata.annotations or {}).get(self._annotations_map.get(name))
 
@@ -628,21 +669,21 @@
                 logger.debug('%s changed from %s to %s', name, old_value, new_value)
 
             # Do not wake up HA loop if we run as leader and received leader object update event
             if value_changed or name == self._dcs.leader_path and self._name != new_value:
                 self._dcs.event.set()
 
     @staticmethod
-    def _finish_response(response):
+    def _finish_response(response: urllib3.HTTPResponse) -> None:
         try:
             response.close()
         finally:
             response.release_conn()
 
-    def _do_watch(self, resource_version):
+    def _do_watch(self, resource_version: str) -> None:
         with self._response_lock:
             self._response = None
         response = self._watch(resource_version)
         with self._response_lock:
             if self._response is None:
                 self._response = response
 
@@ -650,321 +691,319 @@
             return self._finish_response(response)
 
         for event in iter_response_objects(response):
             if event['object'].get('code') == 410:
                 break
             self._process_event(event)
 
-    def _build_cache(self):
+    def _build_cache(self) -> None:
         objects = self._list()
         with self._object_cache_lock:
             self._object_cache = {item.metadata.name: item for item in objects.items}
         with self._condition:
             self._is_ready = True
             self._condition.notify()
 
         try:
             self._do_watch(objects.metadata.resource_version)
         finally:
             with self._condition:
                 self._is_ready = False
             with self._response_lock:
                 response, self._response = self._response, None
-            if response:
+            if isinstance(response, urllib3.HTTPResponse):
                 self._finish_response(response)
 
-    def kill_stream(self):
+    def kill_stream(self) -> None:
         sock = None
         with self._response_lock:
-            if self._response:
+            if isinstance(self._response, urllib3.HTTPResponse):
                 try:
-                    sock = self._response.connection.sock
+                    sock = self._response.connection.sock if self._response.connection else None
                 except Exception:
                     sock = None
             else:
                 self._response = False
         if sock:
             try:
                 sock.shutdown(socket.SHUT_RDWR)
                 sock.close()
             except Exception as e:
                 logger.debug('Error on socket.shutdown: %r', e)
 
-    def run(self):
+    def run(self) -> None:
         while True:
             try:
                 self._build_cache()
             except Exception as e:
                 logger.error('ObjectCache.run %r', e)
 
-    def is_ready(self):
+    def is_ready(self) -> bool:
         """Must be called only when holding the lock on `_condition`"""
         return self._is_ready
 
 
 class Kubernetes(AbstractDCS):
 
     _CITUS_LABEL = 'citus-group'
 
-    def __init__(self, config):
+    def __init__(self, config: Dict[str, Any]) -> None:
         self._labels = deepcopy(config['labels'])
         self._labels[config.get('scope_label', 'cluster-name')] = config['scope']
         self._label_selector = ','.join('{0}={1}'.format(k, v) for k, v in self._labels.items())
         self._namespace = config.get('namespace') or 'default'
         self._role_label = config.get('role_label', 'role')
         self._ca_certs = os.environ.get('PATRONI_KUBERNETES_CACERT', config.get('cacert')) or SERVICE_CERT_FILENAME
         super(Kubernetes, self).__init__({**config, 'namespace': ''})
         if self._citus_group:
             self._labels[self._CITUS_LABEL] = self._citus_group
 
         self._retry = Retry(deadline=config['retry_timeout'], max_delay=1, max_tries=-1,
                             retry_exceptions=KubernetesRetriableException)
-        self._ttl = None
+        self._ttl = int(config.get('ttl') or 30)
         try:
             k8s_config.load_incluster_config(ca_certs=self._ca_certs)
         except k8s_config.ConfigException:
             k8s_config.load_kube_config(context=config.get('context', 'kind-kind'))
 
-        self.__my_pod = None
-        self.__ips = [] if config.get('patronictl') else [config.get('pod_ip')]
-        self.__ports = []
-        for p in config.get('ports', [{}]):
-            port = {'port': int(p.get('port', '5432'))}
+        pod_ip = config.get('pod_ip')
+        self.__ips: List[str] = [] if config.get('patronictl') or not isinstance(pod_ip, str) else [pod_ip]
+        self.__ports: List[K8sObject] = []
+        ports: List[Dict[str, Any]] = config.get('ports', [{}])
+        for p in ports:
+            port: Dict[str, Any] = {'port': int(p.get('port', '5432'))}
             port.update({n: p[n] for n in ('name', 'protocol') if p.get(n)})
             self.__ports.append(k8s_client.V1EndpointPort(**port))
 
         bypass_api_service = not config.get('patronictl') and config.get('bypass_api_service')
         self._api = CoreV1ApiProxy(config.get('use_endpoints'), bypass_api_service)
         self._should_create_config_service = self._api.use_endpoints
         self.reload_config(config)
         # leader_observed_record, leader_resource_version, and leader_observed_time are used only for leader race!
-        self._leader_observed_record = {}
+        self._leader_observed_record: Dict[str, str] = {}
         self._leader_observed_time = None
         self._leader_resource_version = None
         self.__do_not_watch = False
 
         self._condition = Condition()
 
         pods_func = functools.partial(self._api.list_namespaced_pod, self._namespace,
                                       label_selector=self._label_selector)
         self._pods = ObjectCache(self, pods_func, self._retry, self._condition)
 
         kinds_func = functools.partial(self._api.list_namespaced_kind, self._namespace,
                                        label_selector=self._label_selector)
         self._kinds = ObjectCache(self, kinds_func, self._retry, self._condition, self._name)
 
-    def retry(self, *args, **kwargs):
+    def retry(self, method: Callable[..., Any], *args: Any, **kwargs: Any) -> Any:
         retry = self._retry.copy()
         kwargs['_retry'] = retry
-        return retry(*args, **kwargs)
-
-    @staticmethod
-    def _run_and_handle_exceptions(method, *args, **kwargs):
-        try:
-            return method(*args, **kwargs)
-        except k8s_client.rest.ApiException as e:
-            if e.status == 403:
-                logger.exception('Permission denied')
-            elif e.status != 409:  # Object exists or conflict in resource_version
-                logger.exception('Unexpected error from Kubernetes API')
-            return False
-        except (RetryFailedError, K8sException) as e:
-            raise KubernetesError(e)
+        return retry(method, *args, **kwargs)
 
-    def client_path(self, path):
+    def client_path(self, path: str) -> str:
         return super(Kubernetes, self).client_path(path)[1:].replace('/', '-')
 
     @property
-    def leader_path(self):
+    def leader_path(self) -> str:
         return super(Kubernetes, self).leader_path[:-7 if self._api.use_endpoints else None]
 
-    def set_ttl(self, ttl):
+    def set_ttl(self, ttl: int) -> Optional[bool]:
         ttl = int(ttl)
         self.__do_not_watch = self._ttl != ttl
         self._ttl = ttl
+        return None
 
     @property
-    def ttl(self):
+    def ttl(self) -> int:
         return self._ttl
 
-    def set_retry_timeout(self, retry_timeout):
+    def set_retry_timeout(self, retry_timeout: int) -> None:
         self._retry.deadline = retry_timeout
 
-    def reload_config(self, config: Dict[str, Any]) -> None:
+    def reload_config(self, config: Union['Config', Dict[str, Any]]) -> None:
         """Handles dynamic config changes.
 
         Either cause by changes in the local configuration file + SIGHUP or by changes of dynamic configuration"""
 
         super(Kubernetes, self).reload_config(config)
+        if TYPE_CHECKING:  # pragma: no cover
+            assert self._retry.deadline is not None
         self._api.configure_timeouts(self.loop_wait, self._retry.deadline, self.ttl)
 
         # retriable_http_codes supposed to be either int, list of integers or comma-separated string with integers.
         retriable_http_codes = config.get('retriable_http_codes', [])
         if not isinstance(retriable_http_codes, list):
             retriable_http_codes = [c.strip() for c in str(retriable_http_codes).split(',')]
 
         try:
             self._api.configure_retriable_http_codes([int(c) for c in retriable_http_codes])
         except Exception as e:
             logger.warning('Invalid value of retriable_http_codes = %s: %r', config['retriable_http_codes'], e)
 
     @staticmethod
-    def member(pod):
+    def member(pod: K8sObject) -> Member:
         annotations = pod.metadata.annotations or {}
         member = Member.from_node(pod.metadata.resource_version, pod.metadata.name, None, annotations.get('status', ''))
         member.data['pod_labels'] = pod.metadata.labels
         return member
 
-    def _wait_caches(self, stop_time):
+    def _wait_caches(self, stop_time: float) -> None:
         while not (self._pods.is_ready() and self._kinds.is_ready()):
             timeout = stop_time - time.time()
             if timeout <= 0:
                 raise RetryFailedError('Exceeded retry deadline')
             self._condition.wait(timeout)
 
-    def _cluster_from_nodes(self, group, nodes, pods):
+    def _cluster_from_nodes(self, group: str, nodes: Dict[str, K8sObject], pods: Collection[K8sObject]) -> Cluster:
         members = [self.member(pod) for pod in pods]
         path = self._base_path[1:] + '-'
         if group:
             path += group + '-'
 
         config = nodes.get(path + self._CONFIG)
         metadata = config and config.metadata
         annotations = metadata and metadata.annotations or {}
 
         # get initialize flag
         initialize = annotations.get(self._INITIALIZE)
 
         # get global dynamic configuration
-        config = ClusterConfig.from_node(metadata and metadata.resource_version,
-                                         annotations.get(self._CONFIG) or '{}',
-                                         metadata.resource_version if self._CONFIG in annotations else 0)
+        config = metadata and ClusterConfig.from_node(metadata.resource_version,
+                                                      annotations.get(self._CONFIG) or '{}',
+                                                      metadata.resource_version if self._CONFIG in annotations else 0)
 
         # get timeline history
-        history = TimelineHistory.from_node(metadata and metadata.resource_version,
-                                            annotations.get(self._HISTORY) or '[]')
+        history = metadata and TimelineHistory.from_node(metadata.resource_version,
+                                                         annotations.get(self._HISTORY) or '[]')
 
         leader_path = path[:-1] if self._api.use_endpoints else path + self._LEADER
         leader = nodes.get(leader_path)
         metadata = leader and leader.metadata
         if leader_path == self.leader_path:  # We want to memorize leader_resource_version only for our cluster
             self._leader_resource_version = metadata.resource_version if metadata else None
-        annotations = metadata and metadata.annotations or {}
+        annotations: Dict[str, str] = metadata and metadata.annotations or {}
 
         # get last known leader lsn
-        last_lsn = annotations.get(self._OPTIME)
         try:
-            last_lsn = 0 if last_lsn is None else int(last_lsn)
+            last_lsn = int(annotations.get(self._OPTIME, ''))
         except Exception:
             last_lsn = 0
 
         # get permanent slots state (confirmed_flush_lsn)
         slots = annotations.get('slots')
         try:
-            slots = slots and json.loads(slots)
+            slots = json.loads(annotations.get('slots', ''))
         except Exception:
             slots = None
 
         # get failsafe topology
-        failsafe = annotations.get(self._FAILSAFE)
         try:
-            failsafe = json.loads(failsafe) if failsafe else None
+            failsafe = json.loads(annotations.get(self._FAILSAFE, ''))
         except Exception:
             failsafe = None
 
         # get leader
-        leader_record = {n: annotations.get(n) for n in (self._LEADER, 'acquireTime',
-                         'ttl', 'renewTime', 'transitions') if n in annotations}
+        leader_record: Dict[str, str] = {n: annotations[n] for n in (self._LEADER, 'acquireTime',
+                                         'ttl', 'renewTime', 'transitions') if n in annotations}
         # We want to memorize leader_observed_record and update leader_observed_time only for our cluster
         if leader_path == self.leader_path and (leader_record or self._leader_observed_record)\
                 and leader_record != self._leader_observed_record:
             self._leader_observed_record = leader_record
             self._leader_observed_time = time.time()
 
         leader = leader_record.get(self._LEADER)
         try:
-            ttl = int(leader_record.get('ttl')) or self._ttl
+            ttl = int(leader_record.get('ttl', self._ttl)) or self._ttl
         except (TypeError, ValueError):
             ttl = self._ttl
 
         # We want to check validity of the leader record only for our own cluster
         if leader_path == self.leader_path and\
                 not (metadata and self._leader_observed_time and self._leader_observed_time + ttl >= time.time()):
             leader = None
 
         if metadata:
-            member = Member(-1, leader, None, {})
+            member = Member(-1, leader or '', None, {})
             member = ([m for m in members if m.name == leader] or [member])[0]
             leader = Leader(metadata.resource_version, None, member)
+        else:
+            leader = None
 
         # failover key
         failover = nodes.get(path + self._FAILOVER)
         metadata = failover and failover.metadata
-        failover = Failover.from_node(metadata and metadata.resource_version,
-                                      metadata and (metadata.annotations or {}).copy())
+        failover = metadata and Failover.from_node(metadata.resource_version,
+                                                   (metadata.annotations or {}).copy())
 
         # get synchronization state
         sync = nodes.get(path + self._SYNC)
         metadata = sync and sync.metadata
-        sync = SyncState.from_node(metadata and metadata.resource_version,  metadata and metadata.annotations)
+        sync = SyncState.from_node(metadata and metadata.resource_version, metadata and metadata.annotations)
 
         return Cluster(initialize, config, leader, last_lsn, members, failover, sync, history, slots, failsafe)
 
-    def _cluster_loader(self, path):
-        return self._cluster_from_nodes(path['group'], path['nodes'], path['pods'])
+    def _cluster_loader(self, path: Dict[str, Any]) -> Cluster:
+        return self._cluster_from_nodes(path['group'], path['nodes'], path['pods'].values())
 
-    def _citus_cluster_loader(self, path):
-        clusters = defaultdict(lambda: {'pods': [], 'nodes': {}})
+    def _citus_cluster_loader(self, path: Dict[str, Any]) -> Dict[int, Cluster]:
+        clusters: Dict[str, Dict[str, Dict[str, K8sObject]]] = defaultdict(lambda: defaultdict(dict))
 
-        for pod in path['pods']:
+        for name, pod in path['pods'].items():
             group = pod.metadata.labels.get(self._CITUS_LABEL)
             if group and citus_group_re.match(group):
-                clusters[group]['pods'].append(pod)
+                clusters[group]['pods'][name] = pod
 
         for name, kind in path['nodes'].items():
             group = kind.metadata.labels.get(self._CITUS_LABEL)
             if group and citus_group_re.match(group):
                 clusters[group]['nodes'][name] = kind
-        return {int(group): self._cluster_from_nodes(group, value['nodes'], value['pods'])
+        return {int(group): self._cluster_from_nodes(group, value['nodes'], value['pods'].values())
                 for group, value in clusters.items()}
 
-    def __load_cluster(self, group, loader):
+    def __load_cluster(
+            self, group: Optional[str], loader: Callable[[Dict[str, Any]], Union[Cluster, Dict[int, Cluster]]]
+    ) -> Union[Cluster, Dict[int, Cluster]]:
+        if TYPE_CHECKING:  # pragma: no cover
+            assert self._retry.deadline is not None
         stop_time = time.time() + self._retry.deadline
         self._api.refresh_api_servers_cache()
         try:
             with self._condition:
                 self._wait_caches(stop_time)
-                pods = [pod for pod in self._pods.copy().values()
-                        if not group or pod.metadata.labels.get(self._CITUS_LABEL) == group]
+                pods = {name: pod for name, pod in self._pods.copy().items()
+                        if not group or pod.metadata.labels.get(self._CITUS_LABEL) == group}
                 nodes = {name: kind for name, kind in self._kinds.copy().items()
                          if not group or kind.metadata.labels.get(self._CITUS_LABEL) == group}
             return loader({'group': group, 'pods': pods, 'nodes': nodes})
         except Exception:
             logger.exception('get_cluster')
             raise KubernetesError('Kubernetes API is not responding properly')
 
-    def _load_cluster(self, path, loader):
+    def _load_cluster(
+            self, path: str, loader: Callable[[Any], Union[Cluster, Dict[int, Cluster]]]
+    ) -> Union[Cluster, Dict[int, Cluster]]:
         group = self._citus_group if path == self.client_path('') else None
         return self.__load_cluster(group, loader)
 
-    def get_citus_coordinator(self):
+    def get_citus_coordinator(self) -> Optional[Cluster]:
         try:
-            return self.__load_cluster(str(CITUS_COORDINATOR_GROUP_ID), self._cluster_loader)
+            ret = self.__load_cluster(str(CITUS_COORDINATOR_GROUP_ID), self._cluster_loader)
+            if TYPE_CHECKING:  # pragma: no cover
+                assert isinstance(ret, Cluster)
+            return ret
         except Exception as e:
             logger.error('Failed to load Citus coordinator cluster from Kubernetes: %r', e)
 
     @staticmethod
-    def compare_ports(p1, p2):
+    def compare_ports(p1: K8sObject, p2: K8sObject) -> bool:
         return p1.name == p2.name and p1.port == p2.port and (p1.protocol or 'TCP') == (p2.protocol or 'TCP')
 
     @staticmethod
-    def subsets_changed(last_observed_subsets, ip, ports):
+    def subsets_changed(last_observed_subsets: List[K8sObject], ip: str, ports: List[K8sObject]) -> bool:
         """
-        >>> Kubernetes.subsets_changed([], None, [])
-        True
         >>> ip = '1.2.3.4'
         >>> a = [k8s_client.V1EndpointAddress(ip=ip)]
         >>> s = [k8s_client.V1EndpointSubset(addresses=a)]
         >>> Kubernetes.subsets_changed(s, '1.2.3.5', [])
         True
         >>> s = [k8s_client.V1EndpointSubset(addresses=a, ports=[k8s_client.V1EndpointPort(protocol='TCP', port=1)])]
         >>> Kubernetes.subsets_changed(s, '1.2.3.4', [k8s_client.V1EndpointPort(port=5432)])
@@ -990,24 +1029,24 @@
             return not Kubernetes.compare_ports(last_observed_subsets[0].ports[0], ports[0])
         observed_ports = {p.name: p for p in last_observed_subsets[0].ports}
         for p in ports:
             if p.name not in observed_ports or not Kubernetes.compare_ports(p, observed_ports.pop(p.name)):
                 return True
         return False
 
-    def __target_ref(self, leader_ip, latest_subsets, pod):
+    def __target_ref(self, leader_ip: str, latest_subsets: List[K8sObject], pod: K8sObject) -> K8sObject:
         # we want to re-use existing target_ref if possible
         for subset in latest_subsets:
             for address in subset.addresses or []:
                 if address.ip == leader_ip and address.target_ref and address.target_ref.name == self._name:
                     return address.target_ref
         return k8s_client.V1ObjectReference(kind='Pod', uid=pod.metadata.uid, namespace=self._namespace,
                                             name=self._name, resource_version=pod.metadata.resource_version)
 
-    def _map_subsets(self, endpoints, ips):
+    def _map_subsets(self, endpoints: Dict[str, Any], ips: List[str]) -> None:
         leader = self._kinds.get(self.leader_path)
         latest_subsets = leader and leader.subsets or []
         if not ips:
             # We want to have subsets empty
             if latest_subsets:
                 endpoints['subsets'] = []
             return
@@ -1017,130 +1056,137 @@
         # don't touch subsets if our (leader) ip is unknown or subsets is valid
         if leader_ip and self.subsets_changed(latest_subsets, leader_ip, self.__ports):
             kwargs = {'hostname': pod.spec.hostname, 'node_name': pod.spec.node_name,
                       'target_ref': self.__target_ref(leader_ip, latest_subsets, pod)} if pod else {}
             address = k8s_client.V1EndpointAddress(ip=leader_ip, **kwargs)
             endpoints['subsets'] = [k8s_client.V1EndpointSubset(addresses=[address], ports=self.__ports)]
 
-    def _patch_or_create(self, name, annotations, resource_version=None, patch=False, retry=None, ips=None):
+    def _patch_or_create(self, name: str, annotations: Dict[str, Any],
+                         resource_version: Optional[str] = None, patch: bool = False,
+                         retry: Optional[Callable[..., Any]] = None, ips: Optional[List[str]] = None) -> K8sObject:
         metadata = {'namespace': self._namespace, 'name': name, 'labels': self._labels, 'annotations': annotations}
         if patch or resource_version:
             if resource_version is not None:
                 metadata['resource_version'] = resource_version
             func = functools.partial(self._api.patch_namespaced_kind, name)
+            metadata['annotations'] = annotations
         else:
             func = functools.partial(self._api.create_namespaced_kind)
             # skip annotations with null values
-            metadata['annotations'] = {k: v for k, v in metadata['annotations'].items() if v is not None}
+            metadata['annotations'] = {k: v for k, v in annotations.items() if v is not None}
 
         metadata = k8s_client.V1ObjectMeta(**metadata)
         if ips is not None and self._api.use_endpoints:
             endpoints = {'metadata': metadata}
             self._map_subsets(endpoints, ips)
             body = k8s_client.V1Endpoints(**endpoints)
         else:
             body = k8s_client.V1ConfigMap(metadata=metadata)
         ret = retry(func, self._namespace, body) if retry else func(self._namespace, body)
         if ret:
             self._kinds.set(name, ret)
         return ret
 
     @catch_kubernetes_errors
-    def patch_or_create(self, name, annotations, resource_version=None, patch=False, retry=True, ips=None):
-        if retry is True:
-            retry = self.retry
+    def patch_or_create(self, name: str, annotations: Dict[str, Any], resource_version: Optional[str] = None,
+                        patch: bool = False, retry: bool = True, ips: Optional[List[str]] = None) -> K8sObject:
         try:
-            return self._patch_or_create(name, annotations, resource_version, patch, retry, ips)
+            return self._patch_or_create(name, annotations, resource_version, patch, self.retry if retry else None, ips)
         except k8s_client.rest.ApiException as e:
             if e.status == 409 and resource_version:  # Conflict in resource_version
                 # Terminate watchers, it could be a sign that K8s API is in a failed state
                 self._kinds.kill_stream()
                 self._pods.kill_stream()
             raise e
 
-    def patch_or_create_config(self, annotations, resource_version=None, patch=False, retry=True):
+    def patch_or_create_config(self, annotations: Dict[str, Any],
+                               resource_version: Optional[str] = None, patch: bool = False, retry: bool = True) -> bool:
         # SCOPE-config endpoint requires corresponding service otherwise it might be "cleaned" by k8s master
         if self._api.use_endpoints and not patch and not resource_version:
             self._should_create_config_service = True
             self._create_config_service()
-        return self.patch_or_create(self.config_path, annotations, resource_version, patch, retry)
+        return bool(self.patch_or_create(self.config_path, annotations, resource_version, patch, retry))
 
-    def _create_config_service(self):
+    def _create_config_service(self) -> None:
         metadata = k8s_client.V1ObjectMeta(namespace=self._namespace, name=self.config_path, labels=self._labels)
         body = k8s_client.V1Service(metadata=metadata, spec=k8s_client.V1ServiceSpec(cluster_ip='None'))
         try:
             if not self._api.create_namespaced_service(self._namespace, body):
                 return
         except Exception as e:
             # 409 - service already exists, 403 - creation forbidden
             if not isinstance(e, k8s_client.rest.ApiException) or e.status not in (409, 403):
                 return logger.exception('create_config_service failed')
         self._should_create_config_service = False
 
-    def _write_leader_optime(self, last_lsn):
+    def _write_leader_optime(self, last_lsn: str) -> bool:
         """Unused"""
+        raise NotImplementedError  # pragma: no cover
 
-    def _write_status(self, value):
+    def _write_status(self, value: str) -> bool:
         """Unused"""
+        raise NotImplementedError  # pragma: no cover
 
-    def _write_failsafe(self, value):
+    def _write_failsafe(self, value: str) -> bool:
         """Unused"""
+        raise NotImplementedError  # pragma: no cover
 
-    def _update_leader(self):
+    def _update_leader(self, leader: Leader) -> bool:
         """Unused"""
+        raise NotImplementedError  # pragma: no cover
 
-    def _update_leader_with_retry(self, annotations, resource_version, ips):
+    def _update_leader_with_retry(self, annotations: Dict[str, Any],
+                                  resource_version: Optional[str], ips: List[str]) -> bool:
         retry = self._retry.copy()
 
-        def _retry(*args, **kwargs):
+        def _retry(*args: Any, **kwargs: Any) -> Any:
             kwargs['_retry'] = retry
             return retry(*args, **kwargs)
 
         try:
-            return self._patch_or_create(self.leader_path, annotations, resource_version, ips=ips, retry=_retry)
+            return bool(self._patch_or_create(self.leader_path, annotations, resource_version, ips=ips, retry=_retry))
         except k8s_client.rest.ApiException as e:
             if e.status == 409:
                 logger.warning('Concurrent update of %s', self.leader_path)
             else:
                 logger.exception('Permission denied' if e.status == 403 else 'Unexpected error from Kubernetes API')
                 return False
         except (RetryFailedError, K8sException) as e:
             raise KubernetesError(e)
 
         # if we are here, that means update failed with 409
-        retry.deadline = retry.stoptime - time.time()
-        if retry.deadline < 1:
+        if not retry.ensure_deadline(1):
             return False  # No time for retry. Tell ha.py that we have to demote due to failed update.
 
         # Try to get the latest version directly from K8s API instead of relying on async cache
         try:
             kind = _retry(self._api.read_namespaced_kind, self.leader_path, self._namespace)
         except (RetryFailedError, K8sException) as e:
             raise KubernetesError(e)
         except Exception as e:
             logger.error('Failed to get the leader object "%s": %r', self.leader_path, e)
             return False
 
         self._kinds.set(self.leader_path, kind)
 
-        retry.deadline = retry.stoptime - time.time()
-        if retry.deadline < 0.5:
+        if not retry.ensure_deadline(0.5):
             return False
 
         kind_annotations = kind and kind.metadata.annotations or {}
         kind_resource_version = kind and kind.metadata.resource_version
 
         # There is different leader or resource_version in cache didn't change
         if kind and (kind_annotations.get(self._LEADER) != self._name or kind_resource_version == resource_version):
             return False
 
-        return self._run_and_handle_exceptions(self._patch_or_create, self.leader_path, annotations,
-                                               kind_resource_version, ips=ips, retry=_retry)
+        return bool(_run_and_handle_exceptions(self._patch_or_create, self.leader_path, annotations,
+                                               kind_resource_version, ips=ips, retry=_retry))
 
-    def update_leader(self, last_lsn, slots=None, failsafe=None):
+    def update_leader(self, leader: Leader, last_lsn: Optional[int],
+                      slots: Optional[Dict[str, int]] = None, failsafe: Optional[Dict[str, str]] = None) -> bool:
         kind = self._kinds.get(self.leader_path)
         kind_annotations = kind and kind.metadata.annotations or {}
 
         if kind and kind_annotations.get(self._LEADER) != self._name:
             return False
 
         now = datetime.datetime.now(tzutc).isoformat()
@@ -1154,130 +1200,160 @@
 
         if failsafe is not None:
             annotations[self._FAILSAFE] = json.dumps(failsafe, separators=(',', ':')) if failsafe else None
 
         resource_version = kind and kind.metadata.resource_version
         return self._update_leader_with_retry(annotations, resource_version, self.__ips)
 
-    def attempt_to_acquire_leader(self):
+    def attempt_to_acquire_leader(self) -> bool:
         now = datetime.datetime.now(tzutc).isoformat()
         annotations = {self._LEADER: self._name, 'ttl': str(self._ttl),
                        'renewTime': now, 'acquireTime': now, 'transitions': '0'}
         if self._leader_observed_record:
             try:
-                transitions = int(self._leader_observed_record.get('transitions'))
+                transitions = int(self._leader_observed_record.get('transitions', ''))
             except (TypeError, ValueError):
                 transitions = 0
 
             if self._leader_observed_record.get(self._LEADER) != self._name:
                 transitions += 1
             else:
                 annotations['acquireTime'] = self._leader_observed_record.get('acquireTime') or now
             annotations['transitions'] = str(transitions)
-        ips = [] if self._api.use_endpoints else None
+        ips: Optional[List[str]] = [] if self._api.use_endpoints else None
 
         try:
-            ret = self._patch_or_create(self.leader_path, annotations,
-                                        self._leader_resource_version, retry=self.retry, ips=ips)
+            ret = bool(self._patch_or_create(self.leader_path, annotations,
+                                             self._leader_resource_version, retry=self.retry, ips=ips))
         except k8s_client.rest.ApiException as e:
             if e.status == 409 and self._leader_resource_version:  # Conflict in resource_version
                 # Terminate watchers, it could be a sign that K8s API is in a failed state
                 self._kinds.kill_stream()
                 self._pods.kill_stream()
             ret = False
         except (RetryFailedError, K8sException) as e:
             raise KubernetesError(e)
 
         if not ret:
             logger.info('Could not take out TTL lock')
         return ret
 
-    def take_leader(self):
+    def take_leader(self) -> bool:
         return self.attempt_to_acquire_leader()
 
-    def set_failover_value(self, value, index=None):
+    def set_failover_value(self, value: str, version: Optional[str] = None) -> bool:
         """Unused"""
+        raise NotImplementedError  # pragma: no cover
 
-    def manual_failover(self, leader, candidate, scheduled_at=None, index=None):
+    def manual_failover(self, leader: Optional[str], candidate: Optional[str],
+                        scheduled_at: Optional[datetime.datetime] = None, version: Optional[str] = None) -> bool:
         annotations = {'leader': leader or None, 'member': candidate or None,
                        'scheduled_at': scheduled_at and scheduled_at.isoformat()}
-        patch = bool(self.cluster and isinstance(self.cluster.failover, Failover) and self.cluster.failover.index)
-        return self.patch_or_create(self.failover_path, annotations, index, bool(index or patch), False)
+        patch = bool(self.cluster and isinstance(self.cluster.failover, Failover) and self.cluster.failover.version)
+        return bool(self.patch_or_create(self.failover_path, annotations, version, bool(version or patch), False))
 
     @property
-    def _config_resource_version(self):
+    def _config_resource_version(self) -> Optional[str]:
         config = self._kinds.get(self.config_path)
         return config and config.metadata.resource_version
 
-    def set_config_value(self, value, index=None):
-        return self.patch_or_create_config({self._CONFIG: value}, index, bool(self._config_resource_version), False)
+    def set_config_value(self, value: str, version: Optional[str] = None) -> bool:
+        return self.patch_or_create_config({self._CONFIG: value}, version, bool(self._config_resource_version), False)
 
     @catch_kubernetes_errors
-    def touch_member(self, data):
+    def touch_member(self, data: Dict[str, Any]) -> bool:
         cluster = self.cluster
         if cluster and cluster.leader and cluster.leader.name == self._name:
             role = 'master'
         elif data['state'] == 'running' and data['role'] not in ('master', 'primary'):
             role = data['role']
         else:
             role = None
 
         member = cluster and cluster.get_member(self._name, fallback_to_leader=False)
         pod_labels = member and member.data.pop('pod_labels', None)
-        ret = pod_labels is not None and pod_labels.get(self._role_label) == role and deep_compare(data, member.data)
+        ret = member and pod_labels is not None\
+            and pod_labels.get(self._role_label) == role and deep_compare(data, member.data)
 
         if not ret:
             metadata = {'namespace': self._namespace, 'name': self._name, 'labels': {self._role_label: role},
                         'annotations': {'status': json.dumps(data, separators=(',', ':'))}}
             body = k8s_client.V1Pod(metadata=k8s_client.V1ObjectMeta(**metadata))
             ret = self._api.patch_namespaced_pod(self._name, self._namespace, body)
             if ret:
                 self._pods.set(self._name, ret)
         if self._should_create_config_service:
             self._create_config_service()
-        return ret
+        return bool(ret)
 
-    def initialize(self, create_new=True, sysid=""):
+    def initialize(self, create_new: bool = True, sysid: str = "") -> bool:
         cluster = self.cluster
-        resource_version = cluster.config.index if cluster and cluster.config and cluster.config.index else None
+        resource_version = str(cluster.config.version)\
+            if cluster and cluster.config and cluster.config.version else None
         return self.patch_or_create_config({self._INITIALIZE: sysid}, resource_version)
 
-    def _delete_leader(self):
+    def _delete_leader(self) -> bool:
         """Unused"""
+        raise NotImplementedError  # pragma: no cover
 
-    def delete_leader(self, last_lsn=None):
+    def delete_leader(self, last_lsn: Optional[int] = None) -> bool:
+        ret = False
         kind = self._kinds.get(self.leader_path)
         if kind and (kind.metadata.annotations or {}).get(self._LEADER) == self._name:
-            annotations = {self._LEADER: None}
+            annotations: Dict[str, Optional[str]] = {self._LEADER: None}
             if last_lsn:
                 annotations[self._OPTIME] = str(last_lsn)
-            self.patch_or_create(self.leader_path, annotations, kind.metadata.resource_version, True, False, [])
+            ret = self.patch_or_create(self.leader_path, annotations, kind.metadata.resource_version, True, False, [])
             self.reset_cluster()
+        return ret
 
-    def cancel_initialization(self):
+    def cancel_initialization(self) -> bool:
         return self.patch_or_create_config({self._INITIALIZE: None}, None, True)
 
     @catch_kubernetes_errors
-    def delete_cluster(self):
-        self.retry(self._api.delete_collection_namespaced_kind, self._namespace, label_selector=self._label_selector)
+    def delete_cluster(self) -> bool:
+        return bool(self.retry(self._api.delete_collection_namespaced_kind,
+                               self._namespace, label_selector=self._label_selector))
 
-    def set_history_value(self, value):
+    def set_history_value(self, value: str) -> bool:
         return self.patch_or_create_config({self._HISTORY: value}, None, bool(self._config_resource_version), False)
 
-    def set_sync_state_value(self, value, index=None):
+    def set_sync_state_value(self, value: str, version: Optional[str] = None) -> bool:
         """Unused"""
+        raise NotImplementedError  # pragma: no cover
 
-    def write_sync_state(self, leader, sync_standby, index=None):
-        return self.patch_or_create(self.sync_path, self.sync_state(leader, sync_standby), index, False)
-
-    def delete_sync_state(self, index=None):
-        return self.write_sync_state(None, None, index)
+    def write_sync_state(self, leader: Optional[str], sync_standby: Optional[Collection[str]],
+                         version: Optional[str] = None) -> Optional[SyncState]:
+        """Prepare and write annotations to $SCOPE-sync Endpoint or ConfigMap.
+
+        :param leader: name of the leader node that manages /sync key
+        :param sync_standby: collection of currently known synchronous standby node names
+        :param version: last known `resource_version` for conditional update of the object
+        :returns: the new :class:`SyncState` object or None
+        """
+        sync_state = self.sync_state(leader, sync_standby)
+        ret = self.patch_or_create(self.sync_path, sync_state, version, False)
+        if not isinstance(ret, bool):
+            return SyncState.from_node(ret.metadata.resource_version, sync_state)
+
+    def delete_sync_state(self, version: Optional[str] = None) -> bool:
+        """Patch annotations of $SCOPE-sync Endpoint or ConfigMap with empty values.
+
+        Effectively it removes "leader" and "sync_standby" annotations from the object.
+        :param version: last known `resource_version` for conditional update of the object
+        :returns: `True` if "delete" was successful
+        """
+        return self.write_sync_state(None, None, version=version) is not None
 
-    def watch(self, leader_index, timeout):
+    def watch(self, leader_version: Optional[str], timeout: float) -> bool:
         if self.__do_not_watch:
             self.__do_not_watch = False
             return True
 
+        # We want to give a bit more time to non-leader nodes to synchronize HA loops
+        if leader_version:
+            timeout += 0.5
+
         try:
-            return super(Kubernetes, self).watch(None, timeout + 0.5)
+            return super(Kubernetes, self).watch(None, timeout)
         finally:
             self.event.clear()
```

### Comparing `patroni-3.0.2/patroni/dcs/raft.py` & `patroni-3.0.3/patroni/dcs/raft.py`

 * *Files 26% similar despite different names*

```diff
@@ -6,69 +6,76 @@
 
 from collections import defaultdict
 from pysyncobj import SyncObj, SyncObjConf, replicated, FAIL_REASON
 from pysyncobj.dns_resolver import globalDnsResolver
 from pysyncobj.node import TCPNode
 from pysyncobj.transport import TCPTransport, CONNECTION_STATE
 from pysyncobj.utility import TcpUtility
+from typing import Any, Callable, Collection, Dict, List, Optional, Set, Union, TYPE_CHECKING
 
 from . import AbstractDCS, ClusterConfig, Cluster, Failover, Leader, Member, SyncState, TimelineHistory, citus_group_re
 from ..exceptions import DCSError
 from ..utils import validate_directory
+if TYPE_CHECKING:  # pragma: no cover
+    from ..config import Config
 
 logger = logging.getLogger(__name__)
 
 
 class RaftError(DCSError):
     pass
 
 
 class _TCPTransport(TCPTransport):
 
-    def __init__(self, syncObj, selfNode, otherNodes):
+    def __init__(self, syncObj: 'DynMemberSyncObj', selfNode: Optional[TCPNode],
+                 otherNodes: Collection[TCPNode]) -> None:
         super(_TCPTransport, self).__init__(syncObj, selfNode, otherNodes)
         self.setOnUtilityMessageCallback('members', syncObj.getMembers)
 
-    def _connectIfNecessarySingle(self, node):
+    def _connectIfNecessarySingle(self, node: TCPNode) -> bool:
         try:
             return super(_TCPTransport, self)._connectIfNecessarySingle(node)
         except Exception as e:
             logger.debug('Connection to %s failed: %r', node, e)
             return False
 
 
-def resolve_host(self):
+def resolve_host(self: TCPNode) -> Optional[str]:
     return globalDnsResolver().resolve(self.host)
 
 
 setattr(TCPNode, 'ip', property(resolve_host))
 
 
 class SyncObjUtility(object):
 
-    def __init__(self, otherNodes, conf, retry_timeout=10):
+    def __init__(self, otherNodes: Collection[Union[str, TCPNode]], conf: SyncObjConf, retry_timeout: int = 10) -> None:
         self._nodes = otherNodes
-        self._utility = TcpUtility(conf.password, retry_timeout/max(1, len(otherNodes)))
+        self._utility = TcpUtility(conf.password, retry_timeout / max(1, len(otherNodes)))
+        self.__node = next(iter(otherNodes), None)
 
-    def executeCommand(self, command):
+    def executeCommand(self, command: List[Any]) -> Any:
         try:
-            return self._utility.executeCommand(self.__node, command)
+            if self.__node:
+                return self._utility.executeCommand(self.__node, command)
         except Exception:
             return None
 
-    def getMembers(self):
+    def getMembers(self) -> Optional[List[str]]:
         for self.__node in self._nodes:
             response = self.executeCommand(['members'])
             if response:
                 return [member['addr'] for member in response]
 
 
 class DynMemberSyncObj(SyncObj):
 
-    def __init__(self, selfAddress, partnerAddrs, conf, retry_timeout=10):
+    def __init__(self, selfAddress: Optional[str], partnerAddrs: Collection[str],
+                 conf: SyncObjConf, retry_timeout: int = 10) -> None:
         self.__early_apply_local_log = selfAddress is not None
         self.applied_local_log = False
 
         utility = SyncObjUtility(partnerAddrs, conf, retry_timeout)
         members = utility.getMembers()
         add_self = members and selfAddress not in members
 
@@ -77,40 +84,41 @@
         super(DynMemberSyncObj, self).__init__(selfAddress, partnerAddrs, conf, transportClass=_TCPTransport)
 
         if add_self:
             thread = threading.Thread(target=utility.executeCommand, args=(['add', selfAddress],))
             thread.daemon = True
             thread.start()
 
-    def getMembers(self, args, callback):
+    def getMembers(self, args: Any, callback: Callable[[Any, Any], Any]) -> None:
         callback([{'addr': node.id, 'leader': node == self._getLeader(), 'status': CONNECTION_STATE.CONNECTED
-                   if self.isNodeConnected(node) else CONNECTION_STATE.DISCONNECTED} for node in self.otherNodes] +
-                 [{'addr': self.selfNode.id, 'leader': self._isLeader(), 'status': CONNECTION_STATE.CONNECTED}], None)
+                   if self.isNodeConnected(node) else CONNECTION_STATE.DISCONNECTED} for node in self.otherNodes]
+                 + [{'addr': self.selfNode.id, 'leader': self._isLeader(), 'status': CONNECTION_STATE.CONNECTED}], None)
 
-    def _onTick(self, timeToWait=0.0):
+    def _onTick(self, timeToWait: float = 0.0):
         super(DynMemberSyncObj, self)._onTick(timeToWait)
 
         # The SyncObj calls onReady callback only when cluster got the leader and is ready for writes.
         # In some cases for us it is safe to "signal" the Raft object when the local log is fully applied.
         # We are using the `applied_local_log` property for that, but not calling the callback function.
         if self.__early_apply_local_log and not self.applied_local_log and self.raftLastApplied == self.raftCommitIndex:
             self.applied_local_log = True
 
 
 class KVStoreTTL(DynMemberSyncObj):
 
-    def __init__(self, on_ready, on_set, on_delete, **config):
+    def __init__(self, on_ready: Optional[Callable[..., Any]], on_set: Optional[Callable[[str, Dict[str, Any]], None]],
+                 on_delete: Optional[Callable[[str], None]], **config: Any) -> None:
         self.__thread = None
         self.__on_set = on_set
         self.__on_delete = on_delete
-        self.__limb = {}
+        self.__limb: Dict[str, Dict[str, Any]] = {}
         self.set_retry_timeout(int(config.get('retry_timeout') or 10))
 
         self_addr = config.get('self_addr')
-        partner_addrs = set(config.get('partner_addrs', []))
+        partner_addrs: Set[str] = set(config.get('partner_addrs', []))
         if config.get('patronictl'):
             if self_addr:
                 partner_addrs.add(self_addr)
             self_addr = None
 
         # Create raft data_dir if necessary
         raft_data_dir = config.get('data_dir', '')
@@ -124,30 +132,30 @@
                            bindAddress=config.get('bind_addr'), dnsFailCacheTime=(config.get('loop_wait') or 10),
                            dnsCacheTime=(config.get('ttl') or 30), commandsWaitLeader=config.get('commandsWaitLeader'),
                            fullDumpFile=(file_template + '.dump' if self_addr else None),
                            journalFile=(file_template + '.journal' if self_addr else None),
                            onReady=on_ready, dynamicMembershipChange=True)
 
         super(KVStoreTTL, self).__init__(self_addr, partner_addrs, conf, self.__retry_timeout)
-        self.__data = {}
+        self.__data: Dict[str, Dict[str, Any]] = {}
 
     @staticmethod
-    def __check_requirements(old_value, **kwargs):
-        return ('prevExist' not in kwargs or bool(kwargs['prevExist']) == bool(old_value)) and \
-            ('prevValue' not in kwargs or old_value and old_value['value'] == kwargs['prevValue']) and \
-            (not kwargs.get('prevIndex') or old_value and old_value['index'] == kwargs['prevIndex'])
+    def __check_requirements(old_value: Dict[str, Any], **kwargs: Any) -> bool:
+        return bool(('prevExist' not in kwargs or bool(kwargs['prevExist']) == bool(old_value))
+                    and ('prevValue' not in kwargs or old_value and old_value['value'] == kwargs['prevValue'])
+                    and (kwargs.get('prevIndex') is None or old_value and old_value['index'] == kwargs['prevIndex']))
 
-    def set_retry_timeout(self, retry_timeout):
+    def set_retry_timeout(self, retry_timeout: int) -> None:
         self.__retry_timeout = retry_timeout
 
-    def retry(self, func, *args, **kwargs):
+    def retry(self, func: Callable[..., Any], *args: Any, **kwargs: Any) -> Any:
         event = threading.Event()
         ret = {'result': None, 'error': -1}
 
-        def callback(result, error):
+        def callback(result: Any, error: Any) -> None:
             ret.update(result=result, error=error)
             event.set()
 
         kwargs['callback'] = callback
         timeout = kwargs.pop('timeout', None) or self.__retry_timeout
         deadline = timeout and time.time() + timeout
 
@@ -163,168 +171,169 @@
                 timeout = deadline - time.time()
                 if timeout <= 0:
                     raise RaftError('timeout')
             time.sleep(1)
         return False
 
     @replicated
-    def _set(self, key, value, **kwargs):
+    def _set(self, key: str, value: Dict[str, Any], **kwargs: Any) -> Union[bool, Dict[str, Any]]:
         old_value = self.__data.get(key, {})
         if not self.__check_requirements(old_value, **kwargs):
             return False
 
         if old_value and old_value['created'] != value['created']:
             value['created'] = value['updated']
         value['index'] = self.raftLastApplied + 1
 
         self.__data[key] = value
         if self.__on_set:
             self.__on_set(key, value)
-        return True
+        return value
 
-    def set(self, key, value, ttl=None, handle_raft_error=True, **kwargs):
+    def set(self, key: str, value: str, ttl: Optional[int] = None,
+            handle_raft_error: bool = True, **kwargs: Any) -> Union[bool, Dict[str, Any]]:
         old_value = self.__data.get(key, {})
         if not self.__check_requirements(old_value, **kwargs):
             return False
 
-        value = {'value': value, 'updated': time.time()}
-        value['created'] = old_value.get('created', value['updated'])
+        data: Dict[str, Any] = {'value': value, 'updated': time.time()}
+        data['created'] = old_value.get('created', data['updated'])
         if ttl:
-            value['expire'] = value['updated'] + ttl
+            data['expire'] = data['updated'] + ttl
         try:
-            return self.retry(self._set, key, value, **kwargs)
+            return self.retry(self._set, key, data, **kwargs)
         except RaftError:
             if not handle_raft_error:
                 raise
             return False
 
-    def __pop(self, key):
+    def __pop(self, key: str) -> None:
         self.__data.pop(key)
         if self.__on_delete:
             self.__on_delete(key)
 
     @replicated
-    def _delete(self, key, recursive=False, **kwargs):
+    def _delete(self, key: str, recursive: bool = False, **kwargs: Any) -> bool:
         if recursive:
             for k in list(self.__data.keys()):
                 if k.startswith(key):
                     self.__pop(k)
         elif not self.__check_requirements(self.__data.get(key, {}), **kwargs):
             return False
         else:
             self.__pop(key)
         return True
 
-    def delete(self, key, recursive=False, **kwargs):
+    def delete(self, key: str, recursive: bool = False, **kwargs: Any) -> bool:
         if not recursive and not self.__check_requirements(self.__data.get(key, {}), **kwargs):
             return False
         try:
             return self.retry(self._delete, key, recursive=recursive, **kwargs)
         except RaftError:
             return False
 
     @staticmethod
-    def __values_match(old, new):
+    def __values_match(old: Dict[str, Any], new: Dict[str, Any]) -> bool:
         return all(old.get(n) == new.get(n) for n in ('created', 'updated', 'expire', 'value'))
 
     @replicated
-    def _expire(self, key, value, callback=None):
+    def _expire(self, key: str, value: Dict[str, Any], callback: Optional[Callable[..., Any]] = None) -> None:
         current = self.__data.get(key)
         if current and self.__values_match(current, value):
             self.__pop(key)
 
-    def __expire_keys(self):
+    def __expire_keys(self) -> None:
         for key, value in self.__data.items():
             if value and 'expire' in value and value['expire'] <= time.time() and \
                     not (key in self.__limb and self.__values_match(self.__limb[key], value)):
                 self.__limb[key] = value
 
-                def callback(*args):
+                def callback(*args: Any) -> None:
                     if key in self.__limb and self.__values_match(self.__limb[key], value):
                         self.__limb.pop(key)
                 self._expire(key, value, callback=callback)
 
-    def get(self, key, recursive=False):
+    def get(self, key: str, recursive: bool = False) -> Union[None, Dict[str, Any], Dict[str, Dict[str, Any]]]:
         if not recursive:
             return self.__data.get(key)
         return {k: v for k, v in self.__data.items() if k.startswith(key)}
 
-    def _onTick(self, timeToWait=0.0):
+    def _onTick(self, timeToWait: float = 0.0) -> None:
         super(KVStoreTTL, self)._onTick(timeToWait)
 
         if self._isLeader():
             self.__expire_keys()
         else:
             self.__limb.clear()
 
-    def _autoTickThread(self):
+    def _autoTickThread(self) -> None:
         self.__destroying = False
         while not self.__destroying:
             self.doTick(self.conf.autoTickPeriod)
 
-    def startAutoTick(self):
+    def startAutoTick(self) -> None:
         self.__thread = threading.Thread(target=self._autoTickThread)
         self.__thread.daemon = True
         self.__thread.start()
 
-    def destroy(self):
+    def destroy(self) -> None:
         if self.__thread:
             self.__destroying = True
             self.__thread.join()
         super(KVStoreTTL, self).destroy()
 
 
 class Raft(AbstractDCS):
 
-    def __init__(self, config):
+    def __init__(self, config: Dict[str, Any]) -> None:
         super(Raft, self).__init__(config)
         self._ttl = int(config.get('ttl') or 30)
 
         ready_event = threading.Event()
         self._sync_obj = KVStoreTTL(ready_event.set, self._on_set, self._on_delete, commandsWaitLeader=False, **config)
         self._sync_obj.startAutoTick()
 
         while True:
             ready_event.wait(5)
             if ready_event.is_set() or self._sync_obj.applied_local_log:
                 break
             else:
                 logger.info('waiting on raft')
 
-    def _on_set(self, key, value):
+    def _on_set(self, key: str, value: Dict[str, Any]) -> None:
         leader = (self._sync_obj.get(self.leader_path) or {}).get('value')
         if key == value['created'] == value['updated'] and \
                 (key.startswith(self.members_path) or key == self.leader_path and leader != self._name) or \
                 key in (self.leader_optime_path, self.status_path) and leader != self._name or \
                 key in (self.config_path, self.sync_path):
             self.event.set()
 
-    def _on_delete(self, key):
+    def _on_delete(self, key: str) -> None:
         if key == self.leader_path:
             self.event.set()
 
-    def set_ttl(self, ttl):
+    def set_ttl(self, ttl: int) -> Optional[bool]:
         self._ttl = ttl
 
     @property
-    def ttl(self):
+    def ttl(self) -> int:
         return self._ttl
 
-    def set_retry_timeout(self, retry_timeout):
+    def set_retry_timeout(self, retry_timeout: int) -> None:
         self._sync_obj.set_retry_timeout(retry_timeout)
 
-    def reload_config(self, config):
+    def reload_config(self, config: Union['Config', Dict[str, Any]]) -> None:
         super(Raft, self).reload_config(config)
         globalDnsResolver().setTimeouts(self.ttl, self.loop_wait)
 
     @staticmethod
-    def member(key, value):
+    def member(key: str, value: Dict[str, Any]) -> Member:
         return Member.from_node(value['index'], os.path.basename(key), None, value['value'])
 
-    def _cluster_from_nodes(self, nodes):
+    def _cluster_from_nodes(self, nodes: Dict[str, Any]) -> Cluster:
         # get initialize flag
         initialize = nodes.get(self._INITIALIZE)
         initialize = initialize and initialize['value']
 
         # get global dynamic configuration
         config = nodes.get(self._CONFIG)
         config = config and ClusterConfig.from_node(config['index'], config['value'])
@@ -344,15 +353,15 @@
                 slots = last_lsn = None
         else:
             last_lsn = nodes.get(self._LEADER_OPTIME)
             last_lsn = last_lsn and last_lsn['value']
             slots = None
 
         try:
-            last_lsn = int(last_lsn)
+            last_lsn = int(last_lsn or '')
         except Exception:
             last_lsn = 0
 
         # get list of members
         members = [self.member(k, n) for k, n in nodes.items() if k.startswith(self._MEMBERS) and k.count('/') == 1]
 
         # get leader
@@ -376,84 +385,90 @@
         try:
             failsafe = json.loads(failsafe['value']) if failsafe else None
         except Exception:
             failsafe = None
 
         return Cluster(initialize, config, leader, last_lsn, members, failover, sync, history, slots, failsafe)
 
-    def _cluster_loader(self, path):
+    def _cluster_loader(self, path: str) -> Cluster:
         response = self._sync_obj.get(path, recursive=True)
         if not response:
             return Cluster.empty()
         nodes = {key[len(path):]: value for key, value in response.items()}
         return self._cluster_from_nodes(nodes)
 
-    def _citus_cluster_loader(self, path):
-        clusters = defaultdict(dict)
+    def _citus_cluster_loader(self, path: str) -> Dict[int, Cluster]:
+        clusters: Dict[int, Dict[str, Any]] = defaultdict(dict)
         response = self._sync_obj.get(path, recursive=True)
-        for key, value in response.items():
+        for key, value in (response or {}).items():
             key = key[len(path):].split('/', 1)
             if len(key) == 2 and citus_group_re.match(key[0]):
                 clusters[int(key[0])][key[1]] = value
         return {group: self._cluster_from_nodes(nodes) for group, nodes in clusters.items()}
 
-    def _load_cluster(self, path, loader):
+    def _load_cluster(
+            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]
+    ) -> Union[Cluster, Dict[int, Cluster]]:
         return loader(path)
 
-    def _write_leader_optime(self, last_lsn):
-        return self._sync_obj.set(self.leader_optime_path, last_lsn, timeout=1)
+    def _write_leader_optime(self, last_lsn: str) -> bool:
+        return self._sync_obj.set(self.leader_optime_path, last_lsn, timeout=1) is not False
 
-    def _write_status(self, value):
-        return self._sync_obj.set(self.status_path, value, timeout=1)
+    def _write_status(self, value: str) -> bool:
+        return self._sync_obj.set(self.status_path, value, timeout=1) is not False
 
-    def _write_failsafe(self, value):
-        return self._sync_obj.set(self.failsafe_path, value, timeout=1)
+    def _write_failsafe(self, value: str) -> bool:
+        return self._sync_obj.set(self.failsafe_path, value, timeout=1) is not False
 
-    def _update_leader(self):
+    def _update_leader(self, leader: Leader) -> bool:
         ret = self._sync_obj.set(self.leader_path, self._name, ttl=self._ttl,
-                                 handle_raft_error=False, prevValue=self._name)
+                                 handle_raft_error=False, prevValue=self._name) is not False
         if not ret and self._sync_obj.get(self.leader_path) is None:
             ret = self.attempt_to_acquire_leader()
         return ret
 
-    def attempt_to_acquire_leader(self):
-        return self._sync_obj.set(self.leader_path, self._name, ttl=self._ttl, handle_raft_error=False, prevExist=False)
+    def attempt_to_acquire_leader(self) -> bool:
+        return self._sync_obj.set(self.leader_path, self._name, ttl=self._ttl,
+                                  handle_raft_error=False, prevExist=False) is not False
 
-    def set_failover_value(self, value, index=None):
-        return self._sync_obj.set(self.failover_path, value, prevIndex=index)
+    def set_failover_value(self, value: str, version: Optional[int] = None) -> bool:
+        return self._sync_obj.set(self.failover_path, value, prevIndex=version) is not False
 
-    def set_config_value(self, value, index=None):
-        return self._sync_obj.set(self.config_path, value, prevIndex=index)
+    def set_config_value(self, value: str, version: Optional[int] = None) -> bool:
+        return self._sync_obj.set(self.config_path, value, prevIndex=version) is not False
 
-    def touch_member(self, data):
-        data = json.dumps(data, separators=(',', ':'))
-        return self._sync_obj.set(self.member_path, data, self._ttl, timeout=2)
+    def touch_member(self, data: Dict[str, Any]) -> bool:
+        value = json.dumps(data, separators=(',', ':'))
+        return self._sync_obj.set(self.member_path, value, self._ttl, timeout=2) is not False
 
-    def take_leader(self):
-        return self._sync_obj.set(self.leader_path, self._name, ttl=self._ttl)
+    def take_leader(self) -> bool:
+        return self._sync_obj.set(self.leader_path, self._name, ttl=self._ttl) is not False
 
-    def initialize(self, create_new=True, sysid=''):
-        return self._sync_obj.set(self.initialize_path, sysid, prevExist=(not create_new))
+    def initialize(self, create_new: bool = True, sysid: str = '') -> bool:
+        return self._sync_obj.set(self.initialize_path, sysid, prevExist=(not create_new)) is not False
 
-    def _delete_leader(self):
+    def _delete_leader(self) -> bool:
         return self._sync_obj.delete(self.leader_path, prevValue=self._name, timeout=1)
 
-    def cancel_initialization(self):
+    def cancel_initialization(self) -> bool:
         return self._sync_obj.delete(self.initialize_path)
 
-    def delete_cluster(self):
+    def delete_cluster(self) -> bool:
         return self._sync_obj.delete(self.client_path(''), recursive=True)
 
-    def set_history_value(self, value):
-        return self._sync_obj.set(self.history_path, value)
+    def set_history_value(self, value: str) -> bool:
+        return self._sync_obj.set(self.history_path, value) is not False
 
-    def set_sync_state_value(self, value, index=None):
-        return self._sync_obj.set(self.sync_path, value, prevIndex=index)
+    def set_sync_state_value(self, value: str, version: Optional[int] = None) -> Union[int, bool]:
+        ret = self._sync_obj.set(self.sync_path, value, prevIndex=version)
+        if isinstance(ret, dict):
+            return ret['index']
+        return ret
 
-    def delete_sync_state(self, index=None):
-        return self._sync_obj.delete(self.sync_path, prevIndex=index)
+    def delete_sync_state(self, version: Optional[int] = None) -> bool:
+        return self._sync_obj.delete(self.sync_path, prevIndex=version)
 
-    def watch(self, leader_index, timeout):
+    def watch(self, leader_version: Optional[int], timeout: float) -> bool:
         try:
-            return super(Raft, self).watch(leader_index, timeout)
+            return super(Raft, self).watch(leader_version, timeout)
         finally:
             self.event.clear()
```

### Comparing `patroni-3.0.2/patroni/dcs/zookeeper.py` & `patroni-3.0.3/patroni/dcs/zookeeper.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,60 +1,64 @@
 import json
 import logging
 import select
+import socket
 import time
 
 from kazoo.client import KazooClient, KazooState, KazooRetry
 from kazoo.exceptions import ConnectionClosedError, NoNodeError, NodeExistsError, SessionExpiredError
-from kazoo.handlers.threading import SequentialThreadingHandler
-from kazoo.protocol.states import KeeperState
+from kazoo.handlers.threading import AsyncResult, SequentialThreadingHandler
+from kazoo.protocol.states import KeeperState, WatchedEvent, ZnodeStat
 from kazoo.retry import RetryFailedError
-from kazoo.security import make_acl
+from kazoo.security import ACL, make_acl
+from typing import Any, Callable, Dict, List, Optional, Union, Tuple, TYPE_CHECKING
 
 from . import AbstractDCS, ClusterConfig, Cluster, Failover, Leader, Member, SyncState, TimelineHistory, citus_group_re
 from ..exceptions import DCSError
 from ..utils import deep_compare
+if TYPE_CHECKING:  # pragma: no cover
+    from ..config import Config
 
 logger = logging.getLogger(__name__)
 
 
 class ZooKeeperError(DCSError):
     pass
 
 
 class PatroniSequentialThreadingHandler(SequentialThreadingHandler):
 
-    def __init__(self, connect_timeout):
+    def __init__(self, connect_timeout: Union[int, float]) -> None:
         super(PatroniSequentialThreadingHandler, self).__init__()
         self.set_connect_timeout(connect_timeout)
 
-    def set_connect_timeout(self, connect_timeout):
-        self._connect_timeout = max(1.0, connect_timeout/2.0)  # try to connect to zookeeper node during loop_wait/2
+    def set_connect_timeout(self, connect_timeout: Union[int, float]) -> None:
+        self._connect_timeout = max(1.0, connect_timeout / 2.0)  # try to connect to zookeeper node during loop_wait/2
 
-    def create_connection(self, *args, **kwargs):
+    def create_connection(self, *args: Any, **kwargs: Any) -> socket.socket:
         """This method is trying to establish connection with one of the zookeeper nodes.
            Somehow strategy "fail earlier and retry more often" works way better comparing to
            the original strategy "try to connect with specified timeout".
            Since we want to try connect to zookeeper more often (with the smaller connect_timeout),
            he have to override `create_connection` method in the `SequentialThreadingHandler`
            class (which is used by `kazoo.Client`).
 
         :param args: always contains `tuple(host, port)` as the first element and could contain
                      `connect_timeout` (negotiated session timeout) as the second element."""
 
-        args = list(args)
-        if len(args) == 0:  # kazoo 2.6.0 slightly changed the way how it calls create_connection method
-            kwargs['timeout'] = max(self._connect_timeout, kwargs.get('timeout', self._connect_timeout*10)/10.0)
-        elif len(args) == 1:
-            args.append(self._connect_timeout)
+        args_list: List[Any] = list(args)
+        if len(args_list) == 0:  # kazoo 2.6.0 slightly changed the way how it calls create_connection method
+            kwargs['timeout'] = max(self._connect_timeout, kwargs.get('timeout', self._connect_timeout * 10) / 10.0)
+        elif len(args_list) == 1:
+            args_list.append(self._connect_timeout)
         else:
-            args[1] = max(self._connect_timeout, args[1]/10.0)
-        return super(PatroniSequentialThreadingHandler, self).create_connection(*args, **kwargs)
+            args_list[1] = max(self._connect_timeout, args_list[1] / 10.0)
+        return super(PatroniSequentialThreadingHandler, self).create_connection(*args_list, **kwargs)
 
-    def select(self, *args, **kwargs):
+    def select(self, *args: Any, **kwargs: Any) -> Any:
         """
         Python 3.XY may raise following exceptions if select/poll are called with an invalid socket:
         - `ValueError`: because fd == -1
         - `TypeError`: Invalid file descriptor: -1 (starting from kazoo 2.9)
         Python 2.7 may raise the `IOError` instead of `socket.error` (starting from kazoo 2.9)
 
         When it is appropriate we map these exceptions to `socket.error`.
@@ -64,142 +68,143 @@
             return super(PatroniSequentialThreadingHandler, self).select(*args, **kwargs)
         except (TypeError, ValueError) as e:
             raise select.error(9, str(e))
 
 
 class PatroniKazooClient(KazooClient):
 
-    def _call(self, request, async_object):
+    def _call(self, request: Tuple[Any], async_object: AsyncResult) -> Optional[bool]:
         # Before kazoo==2.7.0 it wasn't possible to send requests to zookeeper if
         # the connection is in the SUSPENDED state and Patroni was strongly relying on it.
         # The https://github.com/python-zk/kazoo/pull/588 changed it, and now such requests are queued.
         # We override the `_call()` method in order to keep the old behavior.
 
         if self._state == KeeperState.CONNECTING:
             async_object.set_exception(SessionExpiredError())
             return False
         return super(PatroniKazooClient, self)._call(request, async_object)
 
 
 class ZooKeeper(AbstractDCS):
 
-    def __init__(self, config):
+    def __init__(self, config: Dict[str, Any]) -> None:
         super(ZooKeeper, self).__init__(config)
 
         hosts = config.get('hosts', [])
         if isinstance(hosts, list):
             hosts = ','.join(hosts)
 
         mapping = {'use_ssl': 'use_ssl', 'verify': 'verify_certs', 'cacert': 'ca',
                    'cert': 'certfile', 'key': 'keyfile', 'key_password': 'keyfile_password'}
         kwargs = {v: config[k] for k, v in mapping.items() if k in config}
 
         if 'set_acls' in config:
-            kwargs['default_acl'] = []
+            default_acl: List[ACL] = []
             for principal, permissions in config['set_acls'].items():
                 normalizedPermissions = [p.upper() for p in permissions]
-                kwargs['default_acl'].append(make_acl(scheme='x509',
-                                                      credential=principal,
-                                                      read='READ' in normalizedPermissions,
-                                                      write='WRITE' in normalizedPermissions,
-                                                      create='CREATE' in normalizedPermissions,
-                                                      delete='DELETE' in normalizedPermissions,
-                                                      admin='ADMIN' in normalizedPermissions,
-                                                      all='ALL' in normalizedPermissions))
+                default_acl.append(make_acl(scheme='x509',
+                                            credential=principal,
+                                            read='READ' in normalizedPermissions,
+                                            write='WRITE' in normalizedPermissions,
+                                            create='CREATE' in normalizedPermissions,
+                                            delete='DELETE' in normalizedPermissions,
+                                            admin='ADMIN' in normalizedPermissions,
+                                            all='ALL' in normalizedPermissions))
+            kwargs['default_acl'] = default_acl
 
         self._client = PatroniKazooClient(hosts, handler=PatroniSequentialThreadingHandler(config['retry_timeout']),
                                           timeout=config['ttl'], connection_retry=KazooRetry(max_delay=1, max_tries=-1,
                                           sleep_func=time.sleep), command_retry=KazooRetry(max_delay=1, max_tries=-1,
                                           deadline=config['retry_timeout'], sleep_func=time.sleep), **kwargs)
         self._client.add_listener(self.session_listener)
 
-        self._fetch_cluster = True
-        self._fetch_status = True
-        self.__last_member_data = None
+        self._fetch_cluster: bool = True
+        self._fetch_status: bool = True
+        self.__last_member_data: Optional[Dict[str, Any]] = None
 
         self._orig_kazoo_connect = self._client._connection._connect
         self._client._connection._connect = self._kazoo_connect
 
         self._client.start()
 
-    def _kazoo_connect(self, *args):
+    def _kazoo_connect(self, *args: Any) -> Tuple[Union[int, float], Union[int, float]]:
         """Kazoo is using Ping's to determine health of connection to zookeeper. If there is no
         response on Ping after Ping interval (1/2 from read_timeout) it will consider current
         connection dead and try to connect to another node. Without this "magic" it was taking
         up to 2/3 from session timeout (ttl) to figure out that connection was dead and we had
         only small time for reconnect and retry.
 
         This method is needed to return different value of read_timeout, which is not calculated
         from negotiated session timeout but from value of `loop_wait`. And it is 2 sec smaller
         than loop_wait, because we can spend up to 2 seconds when calling `touch_member()` and
         `write_leader_optime()` methods, which also may hang..."""
 
         ret = self._orig_kazoo_connect(*args)
-        return max(self.loop_wait - 2, 2)*1000, ret[1]
+        return max(self.loop_wait - 2, 2) * 1000, ret[1]
 
-    def session_listener(self, state):
+    def session_listener(self, state: str) -> None:
         if state in [KazooState.SUSPENDED, KazooState.LOST]:
             self.cluster_watcher(None)
 
-    def status_watcher(self, event):
+    def status_watcher(self, event: Optional[WatchedEvent]) -> None:
         self._fetch_status = True
         self.event.set()
 
-    def cluster_watcher(self, event):
+    def cluster_watcher(self, event: Optional[WatchedEvent]) -> None:
         self._fetch_cluster = True
         if not event or event.state != KazooState.CONNECTED or event.path.startswith(self.client_path('')):
             self.status_watcher(event)
 
-    def members_watcher(self, event):
-        self._fetch_cluster = True
-
-    def reload_config(self, config):
+    def reload_config(self, config: Union['Config', Dict[str, Any]]) -> None:
         self.set_retry_timeout(config['retry_timeout'])
 
         loop_wait = config['loop_wait']
 
         loop_wait_changed = self._loop_wait != loop_wait
         self._loop_wait = loop_wait
-        self._client.handler.set_connect_timeout(loop_wait)
+        if isinstance(self._client.handler, PatroniSequentialThreadingHandler):
+            self._client.handler.set_connect_timeout(loop_wait)
 
         # We need to reestablish connection to zookeeper if we want to change
         # read_timeout (and Ping interval respectively), because read_timeout
         # is calculated in `_kazoo_connect` method. If we are changing ttl at
         # the same time, set_ttl method will reestablish connection and return
         # `!True`, otherwise we will close existing connection and let kazoo
         # open the new one.
         if not self.set_ttl(config['ttl']) and loop_wait_changed:
             self._client._connection._socket.close()
 
-    def set_ttl(self, ttl):
+    def set_ttl(self, ttl: int) -> Optional[bool]:
         """It is not possible to change ttl (session_timeout) in zookeeper without
         destroying old session and creating the new one. This method returns `!True`
         if session_timeout has been changed (`restart()` has been called)."""
         ttl = int(ttl * 1000)
         if self._client._session_timeout != ttl:
             self._client._session_timeout = ttl
             self._client.restart()
             return True
 
     @property
-    def ttl(self):
-        return self._client._session_timeout / 1000.0
+    def ttl(self) -> int:
+        return int(self._client._session_timeout / 1000.0)
 
-    def set_retry_timeout(self, retry_timeout):
+    def set_retry_timeout(self, retry_timeout: int) -> None:
         retry = self._client.retry if isinstance(self._client.retry, KazooRetry) else self._client._retry
         retry.deadline = retry_timeout
 
-    def get_node(self, key, watch=None):
+    def get_node(
+            self, key: str, watch: Optional[Callable[[WatchedEvent], None]] = None
+    ) -> Optional[Tuple[str, ZnodeStat]]:
         try:
             ret = self._client.get(key, watch)
             return (ret[0].decode('utf-8'), ret[1])
         except NoNodeError:
             return None
 
-    def get_status(self, path, leader):
+    def get_status(self, path: str, leader: Optional[Leader]) -> Tuple[int, Optional[Dict[str, int]]]:
         watch = self.status_watcher if not leader or leader.name != self._name else None
 
         status = self.get_node(path + self._STATUS, watch)
         if status:
             try:
                 status = json.loads(status[0])
                 last_lsn = status.get(self._OPTIME)
@@ -208,40 +213,40 @@
                 slots = last_lsn = None
         else:
             last_lsn = self.get_node(path + self._LEADER_OPTIME, watch)
             last_lsn = last_lsn and last_lsn[0]
             slots = None
 
         try:
-            last_lsn = int(last_lsn)
+            last_lsn = int(last_lsn or '')
         except Exception:
             last_lsn = 0
 
         self._fetch_status = False
         return last_lsn, slots
 
     @staticmethod
-    def member(name, value, znode):
+    def member(name: str, value: str, znode: ZnodeStat) -> Member:
         return Member.from_node(znode.version, name, znode.ephemeralOwner, value)
 
-    def get_children(self, key, watch=None):
+    def get_children(self, key: str, watch: Optional[Callable[[WatchedEvent], None]] = None) -> List[str]:
         try:
             return self._client.get_children(key, watch)
         except NoNodeError:
             return []
 
-    def load_members(self, path):
-        members = []
+    def load_members(self, path: str) -> List[Member]:
+        members: List[Member] = []
         for member in self.get_children(path + self._MEMBERS, self.cluster_watcher):
             data = self.get_node(path + self._MEMBERS + member)
             if data is not None:
                 members.append(self.member(member, *data))
         return members
 
-    def _cluster_loader(self, path):
+    def _cluster_loader(self, path: str) -> Cluster:
         self._fetch_cluster = False
         self.event.clear()
         nodes = set(self.get_children(path, self.cluster_watcher))
         if not nodes:
             self._fetch_cluster = True
 
         # get initialize flag
@@ -264,15 +269,15 @@
 
         # get leader
         leader = self.get_node(path + self._LEADER) if self._LEADER in nodes else None
         if leader:
             member = Member(-1, leader[0], None, {})
             member = ([m for m in members if m.name == leader[0]] or [member])[0]
             leader = Leader(leader[1].version, leader[1].ephemeralOwner, member)
-            self._fetch_cluster = member.index == -1
+            self._fetch_cluster = member.version == -1
 
         # get last known leader lsn and slots
         last_lsn, slots = self.get_status(path, leader)
 
         # failover key
         failover = self.get_node(path + self._FAILOVER, watch=self.cluster_watcher) if self._FAILOVER in nodes else None
         failover = failover and Failover.from_node(failover[1].version, failover[0])
@@ -282,25 +287,27 @@
         try:
             failsafe = json.loads(failsafe[0]) if failsafe else None
         except Exception:
             failsafe = None
 
         return Cluster(initialize, config, leader, last_lsn, members, failover, sync, history, slots, failsafe)
 
-    def _citus_cluster_loader(self, path):
+    def _citus_cluster_loader(self, path: str) -> Dict[int, Cluster]:
         fetch_cluster = False
-        ret = {}
+        ret: Dict[int, Cluster] = {}
         for node in self.get_children(path, self.cluster_watcher):
             if citus_group_re.match(node):
                 ret[int(node)] = self._cluster_loader(path + node + '/')
                 fetch_cluster = fetch_cluster or self._fetch_cluster
         self._fetch_cluster = fetch_cluster
         return ret
 
-    def _load_cluster(self, path, loader):
+    def _load_cluster(
+            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]
+    ) -> Union[Cluster, Dict[int, Cluster]]:
         cluster = self.cluster if path == self._base_path + '/' else None
         if self._fetch_cluster or cluster is None:
             try:
                 cluster = self._client.retry(loader, path)
             except Exception:
                 logger.exception('get_cluster')
                 self.cluster_watcher(None)
@@ -311,100 +318,103 @@
             # If current node is the leader just clear the event without fetching anything (we are updating the /status)
             if cluster.leader and cluster.leader.name == self._name:
                 self.event.clear()
             else:
                 try:
                     last_lsn, slots = self.get_status(self.client_path(''), cluster.leader)
                     self.event.clear()
-                    cluster = list(cluster)
-                    cluster[3] = last_lsn
-                    cluster[8] = slots
-                    cluster = Cluster(*cluster)
+                    new_cluster: List[Any] = list(cluster)
+                    new_cluster[3] = last_lsn
+                    new_cluster[8] = slots
+                    cluster = Cluster(*new_cluster)
                 except Exception:
                     pass
         return cluster
 
-    def _bypass_caches(self):
+    def _bypass_caches(self) -> None:
         self._fetch_cluster = True
 
-    def _create(self, path, value, retry=False, ephemeral=False):
+    def _create(self, path: str, value: bytes, retry: bool = False, ephemeral: bool = False) -> bool:
         try:
             if retry:
                 self._client.retry(self._client.create, path, value, makepath=True, ephemeral=ephemeral)
             else:
                 self._client.create_async(path, value, makepath=True, ephemeral=ephemeral).get(timeout=1)
             return True
         except Exception:
             logger.exception('Failed to create %s', path)
         return False
 
-    def attempt_to_acquire_leader(self):
+    def attempt_to_acquire_leader(self) -> bool:
         try:
             self._client.retry(self._client.create, self.leader_path, self._name.encode('utf-8'),
                                makepath=True, ephemeral=True)
             return True
         except (ConnectionClosedError, RetryFailedError) as e:
             raise ZooKeeperError(e)
         except Exception as e:
             if not isinstance(e, NodeExistsError):
                 logger.error('Failed to create %s: %r', self.leader_path, e)
         logger.info('Could not take out TTL lock')
         return False
 
-    def _set_or_create(self, key, value, index=None, retry=False, do_not_create_empty=False):
-        value = value.encode('utf-8')
+    def _set_or_create(self, key: str, value: str, version: Optional[int] = None,
+                       retry: bool = False, do_not_create_empty: bool = False) -> Union[int, bool]:
+        value_bytes = value.encode('utf-8')
         try:
             if retry:
-                self._client.retry(self._client.set, key, value, version=index or -1)
+                ret = self._client.retry(self._client.set, key, value_bytes, version=version or -1)
             else:
-                self._client.set_async(key, value, version=index or -1).get(timeout=1)
-            return True
+                ret = self._client.set_async(key, value_bytes, version=version or -1).get(timeout=1)
+            return ret.version
         except NoNodeError:
-            if do_not_create_empty and not value:
+            if do_not_create_empty and not value_bytes:
                 return True
-            elif index is None:
-                return self._create(key, value, retry)
+            elif version is None:
+                if self._create(key, value_bytes, retry):
+                    return 0
             else:
                 return False
         except Exception:
             logger.exception('Failed to update %s', key)
         return False
 
-    def set_failover_value(self, value, index=None):
-        return self._set_or_create(self.failover_path, value, index)
+    def set_failover_value(self, value: str, version: Optional[int] = None) -> bool:
+        return self._set_or_create(self.failover_path, value, version) is not False
 
-    def set_config_value(self, value, index=None):
-        return self._set_or_create(self.config_path, value, index, retry=True)
+    def set_config_value(self, value: str, version: Optional[int] = None) -> bool:
+        return self._set_or_create(self.config_path, value, version, retry=True) is not False
 
-    def initialize(self, create_new=True, sysid=""):
-        sysid = sysid.encode('utf-8')
-        return self._create(self.initialize_path, sysid, retry=True) if create_new \
-            else self._client.retry(self._client.set, self.initialize_path, sysid)
+    def initialize(self, create_new: bool = True, sysid: str = "") -> bool:
+        sysid_bytes = sysid.encode('utf-8')
+        return self._create(self.initialize_path, sysid_bytes, retry=True) if create_new \
+            else self._client.retry(self._client.set, self.initialize_path, sysid_bytes)
 
-    def touch_member(self, data):
+    def touch_member(self, data: Dict[str, Any]) -> bool:
         cluster = self.cluster
         member = cluster and cluster.get_member(self._name, fallback_to_leader=False)
         member_data = self.__last_member_data or member and member.data
         #  We want to notify leader if some important fields in the member key changed by removing ZNode
-        if member and (self._client.client_id is not None and member.session != self._client.client_id[0] or
-                       not (deep_compare(member_data.get('tags', {}), data.get('tags', {})) and
-                            (member_data.get('state') == data.get('state') or
-                                'running' not in (member_data.get('state'), data.get('state'))) and
-                            member_data.get('version') == data.get('version') and
-                            member_data.get('checkpoint_after_promote') == data.get('checkpoint_after_promote'))):
+        if member and (self._client.client_id is not None and member.session != self._client.client_id[0]
+                       or not (member_data and deep_compare(member_data.get('tags', {}), data.get('tags', {}))
+                               and (member_data.get('state') == data.get('state')
+                                    or 'running' not in (member_data.get('state'), data.get('state')))
+                               and member_data.get('version') == data.get('version')
+                               and member_data.get('checkpoint_after_promote')
+                               == data.get('checkpoint_after_promote'))):
             try:
                 self._client.delete_async(self.member_path).get(timeout=1)
             except NoNodeError:
                 pass
             except Exception:
                 return False
             member = None
 
         encoded_data = json.dumps(data, separators=(',', ':')).encode('utf-8')
-        if member:
+        if member and member_data:
             if deep_compare(data, member_data):
                 return True
         else:
             try:
                 self._client.create_async(self.member_path, encoded_data, makepath=True, ephemeral=True).get(timeout=1)
                 self.__last_member_data = data
                 return True
@@ -417,30 +427,28 @@
             self.__last_member_data = data
             return True
         except Exception:
             logger.exception('touch_member')
 
         return False
 
-    def take_leader(self):
+    def take_leader(self) -> bool:
         return self.attempt_to_acquire_leader()
 
-    def _write_leader_optime(self, last_lsn):
-        return self._set_or_create(self.leader_optime_path, last_lsn)
+    def _write_leader_optime(self, last_lsn: str) -> bool:
+        return self._set_or_create(self.leader_optime_path, last_lsn) is not False
 
-    def _write_status(self, value):
-        return self._set_or_create(self.status_path, value)
+    def _write_status(self, value: str) -> bool:
+        return self._set_or_create(self.status_path, value) is not False
 
-    def _write_failsafe(self, value):
-        return self._set_or_create(self.failsafe_path, value)
+    def _write_failsafe(self, value: str) -> bool:
+        return self._set_or_create(self.failsafe_path, value) is not False
 
-    def _update_leader(self):
-        cluster = self.cluster
-        session = cluster and isinstance(cluster.leader, Leader) and cluster.leader.session
-        if self._client.client_id and self._client.client_id[0] != session:
+    def _update_leader(self, leader: Leader) -> bool:
+        if self._client.client_id and self._client.client_id[0] != leader.session:
             logger.warning('Recreating the leader ZNode due to ownership mismatch')
             try:
                 self._client.retry(self._client.delete, self.leader_path)
             except NoNodeError:
                 pass
             except (ConnectionClosedError, RetryFailedError) as e:
                 raise ZooKeeperError(e)
@@ -454,42 +462,44 @@
             except (ConnectionClosedError, RetryFailedError) as e:
                 raise ZooKeeperError(e)
             except Exception as e:
                 logger.error('Failed to create %s: %r', self.leader_path, e)
                 return False
         return True
 
-    def _delete_leader(self):
+    def _delete_leader(self) -> bool:
         self._client.restart()
         return True
 
-    def _cancel_initialization(self):
+    def _cancel_initialization(self) -> None:
         node = self.get_node(self.initialize_path)
         if node:
             self._client.delete(self.initialize_path, version=node[1].version)
 
-    def cancel_initialization(self):
+    def cancel_initialization(self) -> bool:
         try:
             self._client.retry(self._cancel_initialization)
+            return True
         except Exception:
             logger.exception("Unable to delete initialize key")
+        return False
 
-    def delete_cluster(self):
+    def delete_cluster(self) -> bool:
         try:
             return self._client.retry(self._client.delete, self.client_path(''), recursive=True)
         except NoNodeError:
             return True
 
-    def set_history_value(self, value):
-        return self._set_or_create(self.history_path, value)
+    def set_history_value(self, value: str) -> bool:
+        return self._set_or_create(self.history_path, value) is not False
 
-    def set_sync_state_value(self, value, index=None):
-        return self._set_or_create(self.sync_path, value, index, retry=True, do_not_create_empty=True)
+    def set_sync_state_value(self, value: str, version: Optional[int] = None) -> Union[int, bool]:
+        return self._set_or_create(self.sync_path, value, version, retry=True, do_not_create_empty=True)
 
-    def delete_sync_state(self, index=None):
-        return self.set_sync_state_value("{}", index)
+    def delete_sync_state(self, version: Optional[int] = None) -> bool:
+        return self.set_sync_state_value("{}", version) is not False
 
-    def watch(self, leader_index, timeout):
-        ret = super(ZooKeeper, self).watch(leader_index, timeout + 0.5)
+    def watch(self, leader_version: Optional[int], timeout: float) -> bool:
+        ret = super(ZooKeeper, self).watch(leader_version, timeout + 0.5)
         if ret and not self._fetch_status:
             self._fetch_cluster = True
         return ret or self._fetch_cluster
```

### Comparing `patroni-3.0.2/patroni/ha.py` & `patroni-3.0.3/patroni/ha.py`

 * *Files 6% similar despite different names*

```diff
@@ -2,153 +2,159 @@
 import functools
 import json
 import logging
 import sys
 import time
 import uuid
 
-from collections import namedtuple
 from multiprocessing.pool import ThreadPool
 from threading import RLock
+from typing import Any, Callable, Collection, Dict, List, NamedTuple, Optional, Union, Tuple, TYPE_CHECKING
 
 from . import psycopg
+from .__main__ import Patroni
 from .async_executor import AsyncExecutor, CriticalTask
+from .collections import CaseInsensitiveSet
+from .dcs import AbstractDCS, Cluster, Leader, Member, RemoteMember
 from .exceptions import DCSError, PostgresConnectionException, PatroniFatalException
 from .postgresql.callback_executor import CallbackAction
 from .postgresql.misc import postgres_version_to_int
+from .postgresql.postmaster import PostmasterProcess
 from .postgresql.rewind import Rewind
-from .utils import polling_loop, tzutc, is_standby_cluster as _is_standby_cluster, parse_int
-from .dcs import Cluster, Leader, RemoteMember
+from .utils import polling_loop, tzutc
 
 logger = logging.getLogger(__name__)
 
 
-class _MemberStatus(namedtuple('_MemberStatus', ['member', 'reachable', 'in_recovery',
-                                                 'dcs_last_seen', 'timeline', 'wal_position',
-                                                 'tags', 'watchdog_failed'])):
+class _MemberStatus(NamedTuple):
     """Node status distilled from API response:
 
         member - dcs.Member object of the node
         reachable - `!False` if the node is not reachable or is not responding with correct JSON
         in_recovery - `!True` if pg_is_in_recovery() == true
         dcs_last_seen - timestamp from JSON of last succesful communication with DCS
         timeline - timeline value from JSON
         wal_position - maximum value of `replayed_location` or `received_location` from JSON
         tags - dictionary with values of different tags (i.e. nofailover)
         watchdog_failed - indicates that watchdog is required by configuration but not available or failed
     """
+    member: Member
+    reachable: bool
+    in_recovery: Optional[bool]
+    dcs_last_seen: int
+    timeline: int
+    wal_position: int
+    tags: Dict[str, Any]
+    watchdog_failed: bool
+
     @classmethod
-    def from_api_response(cls, member, json):
+    def from_api_response(cls, member: Member, json: Dict[str, Any]) -> '_MemberStatus':
         """
         :param member: dcs.Member object
         :param json: RestApiHandler.get_postgresql_status() result
         :returns: _MemberStatus object
         """
         # If one of those is not in a response we want to count the node as not healthy/reachable
-        assert 'wal' in json or 'xlog' in json
-
-        wal = json.get('wal', json.get('xlog'))
-        in_recovery = not bool(wal.get('location'))  # abuse difference in primary/replica response format
+        wal: Dict[str, Any] = json.get('wal') or json['xlog']
+        # abuse difference in primary/replica response format
+        in_recovery = not bool(wal.get('location')) or json.get('role') in ('master', 'primary')
         timeline = json.get('timeline', 0)
         dcs_last_seen = json.get('dcs_last_seen', 0)
-        wal = in_recovery and max(wal.get('received_location', 0), wal.get('replayed_location', 0))
-        return cls(member, True, in_recovery, dcs_last_seen, timeline, wal,
+        lsn = int(in_recovery and max(wal.get('received_location', 0), wal.get('replayed_location', 0)))
+        return cls(member, True, in_recovery, dcs_last_seen, timeline, lsn,
                    json.get('tags', {}), json.get('watchdog_failed', False))
 
     @classmethod
-    def unknown(cls, member):
+    def unknown(cls, member: Member) -> '_MemberStatus':
         return cls(member, False, None, 0, 0, 0, {}, False)
 
-    def failover_limitation(self):
+    def failover_limitation(self) -> Optional[str]:
         """Returns reason why this node can't promote or None if everything is ok."""
         if not self.reachable:
             return 'not reachable'
         if self.tags.get('nofailover', False):
             return 'not allowed to promote'
         if self.watchdog_failed:
             return 'not watchdog capable'
         return None
 
 
 class Failsafe(object):
 
-    def __init__(self, dcs):
+    def __init__(self, dcs: AbstractDCS) -> None:
         self._lock = RLock()
         self._dcs = dcs
         self._last_update = 0
         self._name = None
         self._conn_url = None
         self._api_url = None
         self._slots = None
 
-    def update(self, data):
+    def update(self, data: Dict[str, Any]) -> None:
         with self._lock:
             self._last_update = time.time()
             self._name = data['name']
             self._conn_url = data['conn_url']
             self._api_url = data['api_url']
             self._slots = data.get('slots')
 
     @property
-    def leader(self):
+    def leader(self) -> Optional[Leader]:
         with self._lock:
-            if self._last_update + self._dcs.ttl > time.time():
-                return Leader(None, None,
-                              RemoteMember(self._name, {'api_url': self._api_url,
-                                                        'conn_url': self._conn_url,
-                                                        'slots': self._slots}))
+            if self._last_update + self._dcs.ttl > time.time() and self._name:
+                return Leader('', '', RemoteMember.from_name_and_data(self._name, {'api_url': self._api_url,
+                                                                                   'conn_url': self._conn_url,
+                                                                                   'slots': self._slots}))
 
-    def update_cluster(self, cluster):
+    def update_cluster(self, cluster: Cluster) -> Cluster:
         # Enreach cluster with the real leader if there was a ping from it
         leader = self.leader
         if leader:
-            cluster = list(cluster)
             # We rely on the strict order of fields in the namedtuple
-            cluster[2] = leader
-            cluster[8] = leader.member.data['slots']
-            cluster = Cluster(*cluster)
+            cluster = Cluster(*cluster[0:2], leader, *cluster[3:8], leader.member.data['slots'], *cluster[9:])
         return cluster
 
-    def is_active(self):
+    def is_active(self) -> bool:
         """Is used to report in REST API whether the failsafe mode was activated.
 
            On primary the self._last_update is set from the
            set_is_active() method and always returns the correct value.
 
            On replicas the self._last_update is set at the moment when
            the primary performs POST /failsafe REST API calls.
            The side-effect - it is possible that replicas will show
            failsafe_is_active values different from the primary."""
 
         with self._lock:
             return self._last_update + self._dcs.ttl > time.time()
 
-    def set_is_active(self, value):
+    def set_is_active(self, value: float) -> None:
         with self._lock:
             self._last_update = value
 
 
 class Ha(object):
 
-    def __init__(self, patroni):
+    def __init__(self, patroni: Patroni):
         self.patroni = patroni
         self.state_handler = patroni.postgresql
         self._rewind = Rewind(self.state_handler)
         self.dcs = patroni.dcs
-        self.cluster = None
-        self.old_cluster = None
+        self.cluster = Cluster.empty()
+        self.global_config = self.patroni.config.get_global_config(None)
+        self.old_cluster = Cluster.empty()
         self._is_leader = False
         self._is_leader_lock = RLock()
         self._failsafe = Failsafe(patroni.dcs)
         self._was_paused = False
         self._leader_timeline = None
         self.recovering = False
         self._async_response = CriticalTask()
         self._crash_recovery_executed = False
-        self._crash_recovery_started = None
+        self._crash_recovery_started = 0
         self._start_timeout = None
         self._async_executor = AsyncExecutor(self.state_handler.cancellable, self.wakeup)
         self.watchdog = patroni.watchdog
 
         # Each member publishes various pieces of information to the DCS using touch_member. This lock protects
         # the state and publishing procedure to have consistent ordering and avoid publishing stale values.
         self._member_state_lock = RLock()
@@ -161,138 +167,129 @@
         # We need following property to avoid shutdown of postgres when join of Patroni to the postgres
         # already running as replica was aborted due to cluster not being initialized in DCS.
         self._join_aborted = False
 
         # used only in backoff after failing a pre_promote script
         self._released_leader_key_timestamp = 0
 
-    def check_mode(self, mode):
-        # Try to protect from the case when DCS was wiped out during pause
-        if self.cluster and self.cluster.config and self.cluster.config.modify_index:
-            return self.cluster.check_mode(mode)
-        else:
-            return self.patroni.config.check_mode(mode)
-
-    def primary_stop_timeout(self):
-        """ Primary stop timeout """
-        ret = parse_int(self.patroni.config['primary_stop_timeout'])
-        return ret if ret and ret > 0 and self.is_synchronous_mode() else None
-
-    def is_paused(self):
-        return self.check_mode('pause')
-
-    def check_timeline(self):
-        return self.check_mode('check_timeline')
-
-    def get_standby_cluster_config(self):
-        if self.cluster and self.cluster.config and self.cluster.config.modify_index:
-            config = self.cluster.config.data
-        else:
-            config = self.patroni.config.dynamic_configuration
-        return config.get('standby_cluster')
-
-    def is_standby_cluster(self):
-        return _is_standby_cluster(self.get_standby_cluster_config())
+    def primary_stop_timeout(self) -> Union[int, None]:
+        """:returns: "primary_stop_timeout" from the global configuration or `None` when not in synchronous mode."""
+        ret = self.global_config.primary_stop_timeout
+        return ret if ret > 0 and self.is_synchronous_mode() else None
+
+    def is_paused(self) -> bool:
+        """:returns: `True` if in maintenance mode."""
+        return self.global_config.is_paused
+
+    def check_timeline(self) -> bool:
+        """:returns: `True` if should check whether the timeline is latest during the leader race."""
+        return self.global_config.check_mode('check_timeline')
+
+    def is_standby_cluster(self) -> bool:
+        """:returns: `True` if global configuration has a valid "standby_cluster" section."""
+        return self.global_config.is_standby_cluster
 
-    def is_leader(self):
+    def is_leader(self) -> bool:
         with self._is_leader_lock:
             return self._is_leader > time.time()
 
-    def set_is_leader(self, value):
+    def set_is_leader(self, value: bool) -> None:
         with self._is_leader_lock:
             self._is_leader = time.time() + self.dcs.ttl if value else 0
 
-    def load_cluster_from_dcs(self):
+    def load_cluster_from_dcs(self) -> None:
         cluster = self.dcs.get_cluster()
 
         # We want to keep the state of cluster when it was healthy
         if not cluster.is_unlocked() or not self.old_cluster:
             self.old_cluster = cluster
         self.cluster = cluster
 
         if self.cluster.is_unlocked() and self.is_failsafe_mode():
             # If failsafe mode is enabled we want to inject the "real" leader to the cluster
             self.cluster = cluster = self._failsafe.update_cluster(cluster)
 
         if not self.has_lock(False):
             self.set_is_leader(False)
 
-        self._leader_timeline = None if cluster.is_unlocked() else cluster.leader.timeline
+        self._leader_timeline = cluster.leader.timeline if cluster.leader else None
 
-    def acquire_lock(self):
+    def acquire_lock(self) -> bool:
         try:
             ret = self.dcs.attempt_to_acquire_leader()
         except DCSError:
             raise
         except Exception:
             logger.exception('Unexpected exception raised from attempt_to_acquire_leader, please report it as a BUG')
             ret = False
         self.set_is_leader(ret)
         return ret
 
-    def _failsafe_config(self):
+    def _failsafe_config(self) -> Optional[Dict[str, str]]:
         if self.is_failsafe_mode():
-            ret = {m.name: m.api_url for m in self.cluster.members}
+            ret = {m.name: m.api_url for m in self.cluster.members if m.api_url}
             if self.state_handler.name not in ret:
                 ret[self.state_handler.name] = self.patroni.api.connection_string
             return ret
 
-    def update_lock(self, write_leader_optime=False):
+    def update_lock(self, write_leader_optime: bool = False) -> bool:
         last_lsn = slots = None
         if write_leader_optime:
             try:
                 last_lsn = self.state_handler.last_operation()
                 slots = self.state_handler.slots()
             except Exception:
                 logger.exception('Exception when called state_handler.last_operation()')
+        if TYPE_CHECKING:  # pragma: no cover
+            assert self.cluster.leader is not None
         try:
-            ret = self.dcs.update_leader(last_lsn, slots, self._failsafe_config())
+            ret = self.dcs.update_leader(self.cluster.leader, last_lsn, slots, self._failsafe_config())
         except DCSError:
             raise
         except Exception:
             logger.exception('Unexpected exception raised from update_leader, please report it as a BUG')
             ret = False
         self.set_is_leader(ret)
         if ret:
             self.watchdog.keepalive()
         return ret
 
-    def has_lock(self, info=True):
+    def has_lock(self, info: bool = True) -> bool:
         lock_owner = self.cluster.leader and self.cluster.leader.name
         if info:
             logger.info('Lock owner: %s; I am %s', lock_owner, self.state_handler.name)
         return lock_owner == self.state_handler.name
 
-    def get_effective_tags(self):
+    def get_effective_tags(self) -> Dict[str, Any]:
         """Return configuration tags merged with dynamically applied tags."""
         tags = self.patroni.tags.copy()
         # _disable_sync could be modified concurrently, but we don't care as attribute get and set are atomic.
         if self._disable_sync > 0:
             tags['nosync'] = True
         return tags
 
-    def notify_citus_coordinator(self, event):
+    def notify_citus_coordinator(self, event: str) -> None:
         if self.state_handler.citus_handler.is_worker():
             coordinator = self.dcs.get_citus_coordinator()
-            if coordinator and coordinator.leader and coordinator.leader.conn_kwargs:
+            if coordinator and coordinator.leader and coordinator.leader.conn_url:
                 try:
                     data = {'type': event,
                             'group': self.state_handler.citus_handler.group(),
                             'leader': self.state_handler.name,
                             'timeout': self.dcs.ttl,
                             'cooldown': self.patroni.config['retry_timeout']}
                     timeout = self.dcs.ttl if event == 'before_demote' else 2
                     self.patroni.request(coordinator.leader.member, 'post', 'citus', data, timeout=timeout, retries=0)
                 except Exception as e:
                     logger.warning('Request to Citus coordinator leader %s %s failed: %r',
                                    coordinator.leader.name, coordinator.leader.member.api_url, e)
 
-    def touch_member(self):
+    def touch_member(self) -> bool:
         with self._member_state_lock:
-            data = {
+            data: Dict[str, Any] = {
                 'conn_url': self.state_handler.connection_string,
                 'api_url': self.patroni.api.connection_string,
                 'state': self.state_handler.state,
                 'role': self.state_handler.role,
                 'version': self.patroni.version
             }
 
@@ -306,14 +303,15 @@
             if tags:
                 data['tags'] = tags
             if self.state_handler.pending_restart:
                 data['pending_restart'] = True
             if self._async_executor.scheduled_action in (None, 'promote') \
                     and data['state'] in ['running', 'restarting', 'starting']:
                 try:
+                    timeline: Optional[int]
                     timeline, wal_position, pg_control_timeline = self.state_handler.timeline_wal_position()
                     data['xlog_location'] = wal_position
                     if not timeline:  # try pg_stat_wal_receiver to get the timeline
                         timeline = self.state_handler.received_timeline()
                     if not timeline:
                         # So far the only way to get the current timeline on the standby is from
                         # the replication connection. In order to avoid opening the replication
@@ -321,15 +319,15 @@
                         # that the timeline on the primary has changed.
                         # Unfortunately such optimization isn't possible on the standby_leader,
                         # therefore we will get the timeline from pg_control, either by calling
                         # pg_control_checkpoint() on 9.6+ or by parsing the output of pg_controldata.
                         if self.state_handler.role == 'standby_leader':
                             timeline = pg_control_timeline or self.state_handler.pg_control_timeline()
                         else:
-                            timeline = self.state_handler.replica_cached_timeline(self._leader_timeline)
+                            timeline = self.state_handler.replica_cached_timeline(self._leader_timeline) or 0
                     if timeline:
                         data['timeline'] = timeline
                 except Exception:
                     pass
             if self.patroni.scheduled_restart:
                 scheduled_restart_data = self.patroni.scheduled_restart.copy()
                 scheduled_restart_data['schedule'] = scheduled_restart_data['schedule'].isoformat()
@@ -342,87 +340,90 @@
             if ret:
                 new_state = (data['state'], {'master': 'primary'}.get(data['role'], data['role']))
                 if self._last_state != new_state and new_state == ('running', 'primary'):
                     self.notify_citus_coordinator('after_promote')
                 self._last_state = new_state
             return ret
 
-    def clone(self, clone_member=None, msg='(without leader)'):
+    def clone(self, clone_member: Union[Leader, Member, None] = None, msg: str = '(without leader)') -> Optional[bool]:
         if self.is_standby_cluster() and not isinstance(clone_member, RemoteMember):
             clone_member = self.get_remote_member(clone_member)
 
         self._rewind.reset_state()
         if self.state_handler.bootstrap.clone(clone_member):
             logger.info('bootstrapped %s', msg)
             cluster = self.dcs.get_cluster()
             node_to_follow = self._get_node_to_follow(cluster)
             return self.state_handler.follow(node_to_follow) is not False
         else:
             logger.error('failed to bootstrap %s', msg)
             self.state_handler.remove_data_directory()
 
-    def bootstrap(self):
-        if not self.cluster.is_unlocked():  # cluster already has leader
-            clone_member = self.cluster.get_clone_member(self.state_handler.name)
-            member_role = 'leader' if clone_member == self.cluster.leader else 'replica'
-            msg = "from {0} '{1}'".format(member_role, clone_member.name)
-            ret = self._async_executor.try_run_async('bootstrap {0}'.format(msg), self.clone, args=(clone_member, msg))
-            return ret or 'trying to bootstrap {0}'.format(msg)
-
+    def bootstrap(self) -> str:
         # no initialize key and node is allowed to be primary and has 'bootstrap' section in a configuration file
-        elif self.cluster.initialize is None and not self.patroni.nofailover and 'bootstrap' in self.patroni.config:
+        if self.cluster.is_unlocked() and self.cluster.initialize is None\
+                and not self.patroni.nofailover and 'bootstrap' in self.patroni.config:
             if self.dcs.initialize(create_new=True):  # race for initialization
                 self.state_handler.bootstrapping = True
                 with self._async_response:
                     self._async_response.reset()
 
                 if self.is_standby_cluster():
                     ret = self._async_executor.try_run_async('bootstrap_standby_leader', self.bootstrap_standby_leader)
                     return ret or 'trying to bootstrap a new standby leader'
                 else:
                     ret = self._async_executor.try_run_async('bootstrap', self.state_handler.bootstrap.bootstrap,
                                                              args=(self.patroni.config['bootstrap'],))
                     return ret or 'trying to bootstrap a new cluster'
             else:
                 return 'failed to acquire initialize lock'
-        else:
-            create_replica_methods = self.get_standby_cluster_config().get('create_replica_methods', []) \
-                                     if self.is_standby_cluster() else None
-            can_bootstrap = self.state_handler.can_create_replica_without_replication_connection(create_replica_methods)
-            concurrent_bootstrap = self.cluster.initialize == ""
-            if can_bootstrap and not concurrent_bootstrap:
-                msg = 'bootstrap (without leader)'
-                return self._async_executor.try_run_async(msg, self.clone) or 'trying to ' + msg
-            return 'waiting for {0}leader to bootstrap'.format('standby_' if self.is_standby_cluster() else '')
 
-    def bootstrap_standby_leader(self):
+        clone_member = self.cluster.get_clone_member(self.state_handler.name)
+        # cluster already has a leader, we can bootstrap from it or from one of replicas (if they allow)
+        if not self.cluster.is_unlocked() and clone_member:
+            member_role = 'leader' if clone_member == self.cluster.leader else 'replica'
+            msg = "from {0} '{1}'".format(member_role, clone_member.name)
+            ret = self._async_executor.try_run_async('bootstrap {0}'.format(msg), self.clone, args=(clone_member, msg))
+            return ret or 'trying to bootstrap {0}'.format(msg)
+
+        # no leader, but configuration may allowed replica creation using backup tools
+        create_replica_methods = self.global_config.get_standby_cluster_config().get('create_replica_methods', []) \
+            if self.is_standby_cluster() else None
+        can_bootstrap = self.state_handler.can_create_replica_without_replication_connection(create_replica_methods)
+        concurrent_bootstrap = self.cluster.initialize == ""
+        if can_bootstrap and not concurrent_bootstrap:
+            msg = 'bootstrap (without leader)'
+            return self._async_executor.try_run_async(msg, self.clone) or 'trying to ' + msg
+        return 'waiting for {0}leader to bootstrap'.format('standby_' if self.is_standby_cluster() else '')
+
+    def bootstrap_standby_leader(self) -> Optional[bool]:
         """ If we found 'standby' key in the configuration, we need to bootstrap
             not a real primary, but a 'standby leader', that will take base backup
             from a remote member and start follow it.
         """
         clone_source = self.get_remote_member()
         msg = 'clone from remote member {0}'.format(clone_source.conn_url)
         result = self.clone(clone_source, msg)
         with self._async_response:  # pretend that post_bootstrap was already executed
             self._async_response.complete(result)
         if result:
             self.state_handler.set_role('standby_leader')
 
         return result
 
-    def _handle_crash_recovery(self):
+    def _handle_crash_recovery(self) -> Optional[str]:
         if not self._crash_recovery_executed and (self.cluster.is_unlocked() or self._rewind.can_rewind):
             self._crash_recovery_executed = True
             self._crash_recovery_started = time.time()
             msg = 'doing crash recovery in a single user mode'
             return self._async_executor.try_run_async(msg, self._rewind.ensure_clean_shutdown) or msg
 
-    def _handle_rewind_or_reinitialize(self):
+    def _handle_rewind_or_reinitialize(self) -> Optional[str]:
         leader = self.get_remote_member() if self.is_standby_cluster() else self.cluster.leader
-        if not self._rewind.rewind_or_reinitialize_needed_and_possible(leader):
+        if not self._rewind.rewind_or_reinitialize_needed_and_possible(leader) or not leader:
             return None
 
         if self._rewind.can_rewind:
             # rewind is required, but postgres wasn't shut down cleanly.
             if not self.state_handler.is_running() and \
                     self.state_handler.controldata().get('Database cluster state') == 'in archive recovery':
                 msg = self._handle_crash_recovery()
@@ -432,20 +433,20 @@
             msg = 'running pg_rewind from ' + leader.name
             return self._async_executor.try_run_async(msg, self._rewind.execute, args=(leader,)) or msg
 
         if self._rewind.should_remove_data_directory_on_diverged_timelines and not self.is_standby_cluster():
             msg = 'reinitializing due to diverged timelines'
             return self._async_executor.try_run_async(msg, self._do_reinitialize, args=(self.cluster,)) or msg
 
-    def recover(self):
+    def recover(self) -> str:
         # Postgres is not running and we will restart in standby mode. Watchdog is not needed until we promote.
         self.watchdog.disable()
 
         if self.has_lock() and self.update_lock():
-            timeout = self.patroni.config['primary_start_timeout']
+            timeout = self.global_config.primary_start_timeout
             if timeout == 0:
                 # We are requested to prefer failing over to restarting primary. But see first if there
                 # is anyone to fail over to.
                 if self.is_failover_possible(self.cluster.members):
                     logger.info("Primary crashed. Failing over.")
                     self.demote('immediate')
                     return 'stopped PostgreSQL to fail over after a crash'
@@ -458,15 +459,18 @@
             msg = self._handle_crash_recovery()
             if msg:
                 return msg
 
         self.load_cluster_from_dcs()
 
         role = 'replica'
-        if self.is_standby_cluster() or not self.has_lock():
+        if self.has_lock() and not self.is_standby_cluster():
+            msg = "starting as readonly because i had the session lock"
+            node_to_follow = None
+        else:
             if not self._rewind.executed:
                 self._rewind.trigger_check_diverged_lsn()
             msg = self._handle_rewind_or_reinitialize()
             if msg:
                 return msg
 
             if self.has_lock():  # in standby cluster
@@ -477,49 +481,45 @@
                 msg = "trying to follow a remote member because standby cluster is unhealthy"
                 node_to_follow = self.get_remote_member()
             else:
                 msg = "starting as a secondary"
                 node_to_follow = self._get_node_to_follow(self.cluster)
 
             if self.is_synchronous_mode():
-                self.state_handler.sync_handler.set_synchronous_standby_names([])
-        elif self.has_lock():
-            msg = "starting as readonly because i had the session lock"
-            node_to_follow = None
+                self.state_handler.sync_handler.set_synchronous_standby_names(CaseInsensitiveSet())
 
         if self._async_executor.try_run_async('restarting after failure', self.state_handler.follow,
                                               args=(node_to_follow, role, timeout)) is None:
             self.recovering = True
         return msg
 
-    def _get_node_to_follow(self, cluster):
+    def _get_node_to_follow(self, cluster: Cluster) -> Union[Leader, Member, None]:
         # determine the node to follow. If replicatefrom tag is set,
         # try to follow the node mentioned there, otherwise, follow the leader.
-        standby_config = self.get_standby_cluster_config()
-        is_standby_cluster = _is_standby_cluster(standby_config)
-        if is_standby_cluster and (self.cluster.is_unlocked() or self.has_lock(False)):
+        if self.is_standby_cluster() and (self.cluster.is_unlocked() or self.has_lock(False)):
             node_to_follow = self.get_remote_member()
         elif self.patroni.replicatefrom and self.patroni.replicatefrom != self.state_handler.name:
             node_to_follow = cluster.get_member(self.patroni.replicatefrom)
         else:
             node_to_follow = cluster.leader if cluster.leader and cluster.leader.name else None
 
         node_to_follow = node_to_follow if node_to_follow and node_to_follow.name != self.state_handler.name else None
 
         if node_to_follow and not isinstance(node_to_follow, RemoteMember):
             # we are going to abuse Member.data to pass following parameters
             params = ('restore_command', 'archive_cleanup_command')
             for param in params:  # It is highly unlikely to happen, but we want to protect from the case
                 node_to_follow.data.pop(param, None)  # when above-mentioned params came from outside.
-            if is_standby_cluster:
+            if self.is_standby_cluster():
+                standby_config = self.global_config.get_standby_cluster_config()
                 node_to_follow.data.update({p: standby_config[p] for p in params if standby_config.get(p)})
 
         return node_to_follow
 
-    def follow(self, demote_reason, follow_reason, refresh=True):
+    def follow(self, demote_reason: str, follow_reason: str, refresh: bool = True) -> str:
         if refresh:
             self.load_cluster_from_dcs()
 
         is_leader = self.state_handler.is_leader()
 
         node_to_follow = self._get_node_to_follow(self.cluster)
 
@@ -566,86 +566,74 @@
                 self._rewind.trigger_check_diverged_lsn()
             elif role == 'standby_leader' and self.state_handler.role != role:
                 self.state_handler.set_role(role)
                 self.state_handler.call_nowait(CallbackAction.ON_ROLE_CHANGE)
 
         return follow_reason
 
-    def is_synchronous_mode(self):
-        return self.check_mode('synchronous_mode')
-
-    def is_synchronous_mode_strict(self):
-        return self.check_mode('synchronous_mode_strict')
+    def is_synchronous_mode(self) -> bool:
+        """:returns: `True` if synchronous replication is requested."""
+        return self.global_config.is_synchronous_mode
+
+    def is_failsafe_mode(self) -> bool:
+        """:returns: `True` if failsafe_mode is enabled in global configuration."""
+        return self.global_config.check_mode('failsafe_mode')
 
-    def is_failsafe_mode(self):
-        return self.check_mode('failsafe_mode')
-
-    def process_sync_replication(self):
+    def process_sync_replication(self) -> None:
         """Process synchronous standby beahvior.
 
         Synchronous standbys are registered in two places postgresql.conf and DCS. The order of updating them must
         be right. The invariant that should be kept is that if a node is primary and sync_standby is set in DCS,
         then that node must have synchronous_standby set to that value. Or more simple, first set in postgresql.conf
         and then in DCS. When removing, first remove in DCS, then in postgresql.conf. This is so we only consider
         promoting standbys that were guaranteed to be replicating synchronously.
         """
         if self.is_synchronous_mode():
-            sync_node_count = self.patroni.config['synchronous_node_count']
-            current = [] if self.cluster.sync.is_empty else self.cluster.sync.members
-            picked, allow_promote = self.state_handler.sync_handler.current_state(self.cluster, sync_node_count,
-                                                                                  self.patroni.config[
-                                                                                      'maximum_lag_on_syncnode'])
-            if set(picked) != set(current):
+            current = CaseInsensitiveSet(self.cluster.sync.members)
+            picked, allow_promote = self.state_handler.sync_handler.current_state(self.cluster)
+
+            if picked != current:
+                sync = self.cluster.sync
                 # update synchronous standby list in dcs temporarily to point to common nodes in current and picked
-                sync_common = list(set(current).intersection(set(allow_promote)))
-                if set(sync_common) != set(current):
-                    logger.info("Updating synchronous privilege temporarily from %s to %s", current, sync_common)
-                    if not self.dcs.write_sync_state(self.state_handler.name,
-                                                     sync_common or None,
-                                                     index=self.cluster.sync.index):
-                        logger.info('Synchronous replication key updated by someone else.')
-                        return
-
-                # Update  db param and wait for x secs
-                if self.is_synchronous_mode_strict() and not picked:
-                    picked = ['*']
+                sync_common = current & allow_promote
+                if sync_common != current:
+                    logger.info("Updating synchronous privilege temporarily from %s to %s",
+                                list(current), list(sync_common))
+                    sync = self.dcs.write_sync_state(self.state_handler.name, sync_common, version=sync.version)
+                    if not sync:
+                        return logger.info('Synchronous replication key updated by someone else.')
+
+                # When strict mode and no suitable replication connections put "*" to synchronous_standby_names
+                if self.global_config.is_synchronous_mode_strict and not picked:
+                    picked = CaseInsensitiveSet('*')
                     logger.warning("No standbys available!")
 
-                logger.info("Assigning synchronous standby status to %s", picked)
+                # Update postgresql.conf and wait 2 secs for changes to become active
+                logger.info("Assigning synchronous standby status to %s", list(picked))
                 self.state_handler.sync_handler.set_synchronous_standby_names(picked)
 
-                if picked and picked[0] != '*' and set(allow_promote) != set(picked) and not allow_promote:
+                if picked and picked != CaseInsensitiveSet('*') and allow_promote != picked:
                     # Wait for PostgreSQL to enable synchronous mode and see if we can immediately set sync_standby
                     time.sleep(2)
-                    _, allow_promote = self.state_handler.sync_handler.current_state(self.cluster,
-                                                                                     sync_node_count,
-                                                                                     self.patroni.config[
-                                                                                         'maximum_lag_on_syncnode'])
-                if allow_promote and set(allow_promote) != set(sync_common):
-                    try:
-                        cluster = self.dcs.get_cluster()
-                    except DCSError:
-                        return logger.warning("Could not get cluster state from DCS during process_sync_replication()")
-                    if not cluster.sync.is_empty and cluster.sync.leader != self.state_handler.name:
-                        logger.info("Synchronous replication key updated by someone else")
-                        return
-                    if not self.dcs.write_sync_state(self.state_handler.name, allow_promote, index=cluster.sync.index):
-                        logger.info("Synchronous replication key updated by someone else")
-                        return
-                    logger.info("Synchronous standby status assigned to %s", allow_promote)
+                    _, allow_promote = self.state_handler.sync_handler.current_state(self.cluster)
+                if allow_promote and allow_promote != sync_common:
+                    if not self.dcs.write_sync_state(self.state_handler.name, allow_promote, version=sync.version):
+                        return logger.info("Synchronous replication key updated by someone else")
+                    logger.info("Synchronous standby status assigned to %s", list(allow_promote))
         else:
-            if not self.cluster.sync.is_empty and self.dcs.delete_sync_state(index=self.cluster.sync.index):
+            if not self.cluster.sync.is_empty and self.dcs.delete_sync_state(version=self.cluster.sync.version):
                 logger.info("Disabled synchronous replication")
-            self.state_handler.sync_handler.set_synchronous_standby_names([])
+            self.state_handler.sync_handler.set_synchronous_standby_names(CaseInsensitiveSet())
 
-    def is_sync_standby(self, cluster):
-        return cluster.leader and cluster.sync.leader == cluster.leader.name \
-            and self.state_handler.name in cluster.sync.members
+    def is_sync_standby(self, cluster: Cluster) -> bool:
+        """:returns: `True` if the current node is a synchronous standby."""
+        return bool(cluster.leader) and cluster.sync.leader_matches(cluster.leader.name) \
+            and cluster.sync.matches(self.state_handler.name)
 
-    def while_not_sync_standby(self, func):
+    def while_not_sync_standby(self, func: Callable[..., Any]) -> Any:
         """Runs specified action while trying to make sure that the node is not assigned synchronous standby status.
 
         Tags us as not allowed to be a sync standby as we are going to go away, if we currently are wait for
         leader to notice and pick an alternative one or if the leader changes or goes away we are also free.
 
         If the connection to DCS fails we run the action anyway, as this is only a hint.
 
@@ -658,15 +646,15 @@
 
         with self._member_state_lock:
             self._disable_sync += 1
         try:
             if self.touch_member():
                 # Primary should notice the updated value during the next cycle. We will wait double that, if primary
                 # hasn't noticed the value by then not disabling sync replication is not likely to matter.
-                for _ in polling_loop(timeout=self.dcs.loop_wait*2, interval=2):
+                for _ in polling_loop(timeout=self.dcs.loop_wait * 2, interval=2):
                     try:
                         if not self.is_sync_standby(self.dcs.get_cluster()):
                             break
                     except DCSError:
                         logger.warning("Could not get cluster state, skipping synchronous standby disable")
                         break
                     logger.info("Waiting for primary to release us from synchronous standby")
@@ -674,40 +662,40 @@
                 logger.warning("Updating member state failed, skipping synchronous standby disable")
 
             return func()
         finally:
             with self._member_state_lock:
                 self._disable_sync -= 1
 
-    def update_cluster_history(self):
+    def update_cluster_history(self) -> None:
         primary_timeline = self.state_handler.get_primary_timeline()
-        cluster_history = self.cluster.history and self.cluster.history.lines
+        cluster_history = self.cluster.history.lines if self.cluster.history else []
         if primary_timeline == 1:
             if cluster_history:
                 self.dcs.set_history_value('[]')
         elif not cluster_history or cluster_history[-1][0] != primary_timeline - 1 or len(cluster_history[-1]) != 5:
-            cluster_history = {line[0]: line for line in cluster_history or []}
-            history = self.state_handler.get_history(primary_timeline)
-            if history and self.cluster.config:
+            cluster_history = {line[0]: line for line in cluster_history}
+            history: List[List[Any]] = list(map(list, self.state_handler.get_history(primary_timeline)))
+            if self.cluster.config:
                 history = history[-self.cluster.config.max_timelines_history:]
-                for line in history:
-                    # enrich current history with promotion timestamps stored in DCS
-                    if len(line) == 3 and line[0] in cluster_history \
-                            and len(cluster_history[line[0]]) >= 4 \
-                            and cluster_history[line[0]][1] == line[1]:
-                        line.append(cluster_history[line[0]][3])
-                        if len(cluster_history[line[0]]) == 5:
-                            line.append(cluster_history[line[0]][4])
+            for line in history:
+                # enrich current history with promotion timestamps stored in DCS
+                cluster_history_line = list(cluster_history.get(line[0], []))
+                if len(line) == 3 and len(cluster_history_line) >= 4 and cluster_history_line[1] == line[1]:
+                    line.append(cluster_history_line[3])
+                    if len(cluster_history_line) == 5:
+                        line.append(cluster_history_line[4])
+            if history:
                 self.dcs.set_history_value(json.dumps(history, separators=(',', ':')))
 
-    def enforce_follow_remote_member(self, message):
+    def enforce_follow_remote_member(self, message: str) -> str:
         demote_reason = 'cannot be a real primary in standby cluster'
         return self.follow(demote_reason, message)
 
-    def enforce_primary_role(self, message, promote_message):
+    def enforce_primary_role(self, message: str, promote_message: str) -> str:
         """
         Ensure the node that has won the race for the leader key meets criteria
         for promoting its PG server to the 'primary' role.
         """
         if not self.is_paused():
             if not self.watchdog.is_running and not self.watchdog.activate():
                 if self.state_handler.is_leader():
@@ -737,20 +725,20 @@
         elif self.state_handler.role in ('master', 'promoted', 'primary'):
             self.process_sync_replication()
             return message
         else:
             if self.is_synchronous_mode():
                 # Just set ourselves as the authoritative source of truth for now. We don't want to wait for standbys
                 # to connect. We will try finding a synchronous standby in the next cycle.
-                if not self.dcs.write_sync_state(self.state_handler.name, None, index=self.cluster.sync.index):
+                if not self.dcs.write_sync_state(self.state_handler.name, None, version=self.cluster.sync.version):
                     # Somebody else updated sync state, it may be due to us losing the lock. To be safe, postpone
                     # promotion until next cycle. TODO: trigger immediate retry of run_cycle
                     return 'Postponing promotion because synchronous replication state was updated by somebody else'
                 self.state_handler.sync_handler.set_synchronous_standby_names(
-                    ['*'] if self.is_synchronous_mode_strict() else [])
+                    CaseInsensitiveSet('*') if self.global_config.is_synchronous_mode_strict else CaseInsensitiveSet())
             if self.state_handler.role not in ('master', 'promoted', 'primary'):
                 def on_success():
                     self._rewind.reset_state()
                     logger.info("cleared rewind state after becoming the leader")
 
                 def before_promote():
                     self.notify_citus_coordinator('before_promote')
@@ -758,168 +746,184 @@
                 with self._async_response:
                     self._async_response.reset()
                 self._async_executor.try_run_async('promote', self.state_handler.promote,
                                                    args=(self.dcs.loop_wait, self._async_response,
                                                          before_promote, on_success))
             return promote_message
 
-    def fetch_node_status(self, member):
+    def fetch_node_status(self, member: Member) -> _MemberStatus:
         """This function perform http get request on member.api_url and fetches its status
         :returns: `_MemberStatus` object
         """
 
         try:
             response = self.patroni.request(member, timeout=2, retries=0)
             data = response.data.decode('utf-8')
             logger.info('Got response from %s %s: %s', member.name, member.api_url, data)
             return _MemberStatus.from_api_response(member, json.loads(data))
         except Exception as e:
             logger.warning("Request failed to %s: GET %s (%s)", member.name, member.api_url, e)
         return _MemberStatus.unknown(member)
 
-    def fetch_nodes_statuses(self, members):
+    def fetch_nodes_statuses(self, members: List[Member]) -> List[_MemberStatus]:
         pool = ThreadPool(len(members))
         results = pool.map(self.fetch_node_status, members)  # Run API calls on members in parallel
         pool.close()
         pool.join()
         return results
 
-    def update_failsafe(self, data):
+    def update_failsafe(self, data: Dict[str, Any]) -> Optional[str]:
         if self.state_handler.state == 'running' and self.state_handler.role in ('master', 'primary'):
             return 'Running as a leader'
         self._failsafe.update(data)
 
-    def failsafe_is_active(self):
+    def failsafe_is_active(self) -> bool:
         return self._failsafe.is_active()
 
-    def call_failsafe_member(self, data, member):
+    def call_failsafe_member(self, data: Dict[str, Any], member: Member) -> bool:
         try:
             response = self.patroni.request(member, 'post', 'failsafe', data, timeout=2, retries=1)
-            data = response.data.decode('utf-8')
-            logger.info('Got response from %s %s: %s', member.name, member.api_url, data)
-            return response.status == 200 and data == 'Accepted'
+            response_data = response.data.decode('utf-8')
+            logger.info('Got response from %s %s: %s', member.name, member.api_url, response_data)
+            return response.status == 200 and response_data == 'Accepted'
         except Exception as e:
             logger.warning("Request failed to %s: POST %s (%s)", member.name, member.api_url, e)
         return False
 
-    def check_failsafe_topology(self):
+    def check_failsafe_topology(self) -> bool:
         failsafe = self.dcs.failsafe
         if not isinstance(failsafe, dict) or self.state_handler.name not in failsafe:
             return False
-        data = {
+        data: Dict[str, Any] = {
             'name': self.state_handler.name,
             'conn_url': self.state_handler.connection_string,
             'api_url': self.patroni.api.connection_string,
         }
         try:
             data['slots'] = self.state_handler.slots()
         except Exception:
             logger.exception('Exception when called state_handler.slots()')
-        members = [RemoteMember(name, {'api_url': url})
-                   for name, url in failsafe.items()
-                   if name != self.state_handler.name]
+        members = [RemoteMember.from_name_and_data(name, {'api_url': url})
+                   for name, url in failsafe.items() if name != self.state_handler.name]
         if not members:  # A sinlge node cluster
             return True
         pool = ThreadPool(len(members))
         call_failsafe_member = functools.partial(self.call_failsafe_member, data)
         results = pool.map(call_failsafe_member, members)
         pool.close()
         pool.join()
         return all(results)
 
-    def is_lagging(self, wal_position):
+    def is_lagging(self, wal_position: int) -> bool:
         """Returns if instance with an wal should consider itself unhealthy to be promoted due to replication lag.
 
         :param wal_position: Current wal position.
         :returns True when node is lagging
         """
         lag = (self.cluster.last_lsn or 0) - wal_position
-        return lag > self.patroni.config.get('maximum_lag_on_failover', 0)
+        return lag > self.global_config.maximum_lag_on_failover
 
-    def _is_healthiest_node(self, members, check_replication_lag=True):
+    def _is_healthiest_node(self, members: Collection[Member], check_replication_lag: bool = True) -> bool:
         """This method tries to determine whether I am healthy enough to became a new leader candidate or not."""
 
         my_wal_position = self.state_handler.last_operation()
         if check_replication_lag and self.is_lagging(my_wal_position):
             logger.info('My wal position exceeds maximum replication lag')
             return False  # Too far behind last reported wal position on primary
 
         if not self.is_standby_cluster() and self.check_timeline():
             cluster_timeline = self.cluster.timeline
             my_timeline = self.state_handler.replica_cached_timeline(cluster_timeline)
+            if my_timeline is None:
+                logger.info('Can not figure out my timeline')
+                return False
             if my_timeline < cluster_timeline:
                 logger.info('My timeline %s is behind last known cluster timeline %s', my_timeline, cluster_timeline)
                 return False
 
         # Prepare list of nodes to run check against
         members = [m for m in members if m.name != self.state_handler.name and not m.nofailover and m.api_url]
 
         if members:
             for st in self.fetch_nodes_statuses(members):
                 if st.failover_limitation() is None:
-                    if not st.in_recovery:
+                    if st.in_recovery is False:
                         logger.warning('Primary (%s) is still alive', st.member.name)
                         return False
                     if my_wal_position < st.wal_position:
                         logger.info('Wal position of %s is ahead of my wal position', st.member.name)
                         # In synchronous mode the former leader might be still accessible and even be ahead of us.
                         # We should not disqualify himself from the leader race in such a situation.
-                        if not self.is_synchronous_mode() or st.member.name != self.cluster.sync.leader:
+                        if not self.is_synchronous_mode() or self.cluster.sync.is_empty\
+                                or not self.cluster.sync.leader_matches(st.member.name):
                             return False
                         logger.info('Ignoring the former leader being ahead of us')
         return True
 
-    def is_failover_possible(self, members, check_synchronous=True, cluster_lsn=None):
+    def is_failover_possible(self, members: List[Member], check_synchronous: Optional[bool] = True,
+                             cluster_lsn: Optional[int] = 0) -> bool:
+        """Checks whether one of the members from the list can possibly win the leader race.
+
+        :param members: list of members to check
+        :param check_synchronous: consider only members that are known to be listed in /sync key when sync replication.
+        :param cluster_lsn: to calculate replication lag and exclude member if it is laggin
+        :returns: `True` if there are members eligible to be the new leader
+        """
         ret = False
         cluster_timeline = self.cluster.timeline
         members = [m for m in members if m.name != self.state_handler.name and not m.nofailover and m.api_url]
-        if check_synchronous and self.is_synchronous_mode():
+        if check_synchronous and self.is_synchronous_mode() and not self.cluster.sync.is_empty:
             members = [m for m in members if self.cluster.sync.matches(m.name)]
         if members:
             for st in self.fetch_nodes_statuses(members):
                 not_allowed_reason = st.failover_limitation()
                 if not_allowed_reason:
                     logger.info('Member %s is %s', st.member.name, not_allowed_reason)
-                elif not isinstance(st.wal_position, int):
-                    logger.info('Member %s does not report wal_position', st.member.name)
                 elif cluster_lsn and st.wal_position < cluster_lsn or\
                         not cluster_lsn and self.is_lagging(st.wal_position):
                     logger.info('Member %s exceeds maximum replication lag', st.member.name)
                 elif self.check_timeline() and (not st.timeline or st.timeline < cluster_timeline):
                     logger.info('Timeline %s of member %s is behind the cluster timeline %s',
                                 st.timeline, st.member.name, cluster_timeline)
                 else:
                     ret = True
         else:
             logger.warning('manual failover: members list is empty')
         return ret
 
-    def manual_failover_process_no_leader(self):
+    def manual_failover_process_no_leader(self) -> Optional[bool]:
+        """Handles manual failover/switchover when the old leader already stepped down.
+
+        :returns: - `True` if the current node is the best candidate to become the new leader
+                  - `None` if the current node is running as a primary and requested candidate doesn't exist
+                  """
         failover = self.cluster.failover
+        if TYPE_CHECKING:  # pragma: no cover
+            assert failover is not None
         if failover.candidate:  # manual failover to specific member
             if failover.candidate == self.state_handler.name:  # manual failover to me
                 return True
             elif self.is_paused():
                 # Remove failover key if the node to failover has terminated to avoid waiting for it indefinitely
                 # In order to avoid attempts to delete this key from all nodes only the primary is allowed to do it.
-                if (not self.cluster.get_member(failover.candidate, fallback_to_leader=False) and
-                   self.state_handler.is_leader()):
+                if not self.cluster.get_member(failover.candidate, fallback_to_leader=False)\
+                        and self.state_handler.is_leader():
                     logger.warning("manual failover: removing failover key because failover candidate is not running")
-                    self.dcs.manual_failover('', '', index=self.cluster.failover.index)
+                    self.dcs.manual_failover('', '', version=failover.version)
                     return None
                 return False
 
             # in synchronous mode when our name is not in the /sync key
             # we shouldn't take any action even if the candidate is unhealthy
-            if self.is_synchronous_mode() and not self.cluster.sync.matches(self.state_handler.name):
+            if self.is_synchronous_mode() and not self.cluster.sync.matches(self.state_handler.name, True):
                 return False
 
             # find specific node and check that it is healthy
             member = self.cluster.get_member(failover.candidate, fallback_to_leader=False)
-            if member:
+            if isinstance(member, Member):
                 st = self.fetch_node_status(member)
                 not_allowed_reason = st.failover_limitation()
                 if not_allowed_reason is None:  # node is healthy
                     logger.info('manual failover: to %s, i am %s', st.member.name, self.state_handler.name)
                     return False
                 # we wanted to failover to specific member but it is not healthy
                 logger.warning('manual failover: member %s is %s', st.member.name, not_allowed_reason)
@@ -941,15 +945,20 @@
 
             # at this point we assume that our node is a candidate for a failover among all nodes except former leader
 
         # exclude former leader from the list (failover.leader can be None)
         members = [m for m in self.cluster.members if m.name != failover.leader]
         return self._is_healthiest_node(members, check_replication_lag=False)
 
-    def is_healthiest_node(self):
+    def is_healthiest_node(self) -> bool:
+        """Performs a series of checks to determine that the current node is the best candidate.
+
+        In case if manual failover/switchover is requested it calls :func:`manual_failover_process_no_leader` method.
+        :returns: `True` if the current node is among the best candidates to become the new leader.
+        """
         if time.time() - self._released_leader_key_timestamp < self.dcs.ttl:
             logger.info('backoff: skip leader race after pre_promote script failure and releasing the lock voluntarily')
             return False
 
         if self.is_paused() and not self.patroni.nofailover and \
                 self.cluster.failover and not self.cluster.failover.scheduled_at:
             ret = self.manual_failover_process_no_leader()
@@ -958,109 +967,111 @@
 
         if self.state_handler.is_starting():  # postgresql still starting up is unhealthy
             return False
 
         if self.state_handler.is_leader():
             # in pause leader is the healthiest only when no initialize or sysid matches with initialize!
             return not self.is_paused() or not self.cluster.initialize\
-                    or self.state_handler.sysid == self.cluster.initialize
+                or self.state_handler.sysid == self.cluster.initialize
 
         if self.is_paused():
             return False
 
         if self.patroni.nofailover:  # nofailover tag makes node always unhealthy
             return False
 
         if self.cluster.failover:
             # When doing a switchover in synchronous mode only synchronous nodes and former leader are allowed to race
             if self.is_synchronous_mode() and self.cluster.failover.leader and \
-                    not self.cluster.sync.matches(self.state_handler.name):
+                    not self.cluster.sync.is_empty and not self.cluster.sync.matches(self.state_handler.name, True):
                 return False
-            return self.manual_failover_process_no_leader()
+            return self.manual_failover_process_no_leader() or False
 
         if not self.watchdog.is_healthy:
             logger.warning('Watchdog device is not usable')
             return False
 
         all_known_members = self.old_cluster.members
         if self.is_failsafe_mode():
             failsafe_members = self.dcs.failsafe
             # We want to discard failsafe_mode if the /failsafe key contains garbage or empty.
             if isinstance(failsafe_members, dict):
                 # If current node is missing in the /failsafe key we immediately disqualify it from the race.
                 if failsafe_members and self.state_handler.name not in failsafe_members:
                     return False
                 # Race among not only existing cluster members, but also all known members from the failsafe config
-                all_known_members += [RemoteMember(name, {'api_url': url}) for name, url in failsafe_members.items()]
+                all_known_members += [RemoteMember.from_name_and_data(name, {'api_url': url})
+                                      for name, url in failsafe_members.items()]
         all_known_members += self.cluster.members
 
         # When in sync mode, only last known primary and sync standby are allowed to promote automatically.
         if self.is_synchronous_mode() and not self.cluster.sync.is_empty:
-            if not self.cluster.sync.matches(self.state_handler.name):
+            if not self.cluster.sync.matches(self.state_handler.name, True):
                 return False
             # pick between synchronous candidates so we minimize unnecessary failovers/demotions
-            members = {m.name: m for m in all_known_members if self.cluster.sync.matches(m.name)}
+            members = {m.name: m for m in all_known_members if self.cluster.sync.matches(m.name, True)}
         else:
             # run usual health check
             members = {m.name: m for m in all_known_members}
 
         return self._is_healthiest_node(members.values())
 
-    def _delete_leader(self, last_lsn=None):
+    def _delete_leader(self, last_lsn: Optional[int] = None) -> None:
         self.set_is_leader(False)
         self.dcs.delete_leader(last_lsn)
         self.dcs.reset_cluster()
 
-    def release_leader_key_voluntarily(self, last_lsn=None):
+    def release_leader_key_voluntarily(self, last_lsn: Optional[int] = None) -> None:
         self._delete_leader(last_lsn)
         self.touch_member()
         logger.info("Leader key released")
 
-    def demote(self, mode):
+    def demote(self, mode: str) -> Optional[bool]:
         """Demote PostgreSQL running as primary.
 
         :param mode: One of offline, graceful or immediate.
             offline is used when connection to DCS is not available.
             graceful is used when failing over to another node due to user request. May only be called running async.
             immediate is used when we determine that we are not suitable for primary and want to failover quickly
                 without regard for data durability. May only be called synchronously.
             immediate-nolock is used when find out that we have lost the lock to be primary. Need to bring down
                 PostgreSQL as quickly as possible without regard for data durability. May only be called synchronously.
         """
         mode_control = {
-            'offline':          dict(stop='fast', checkpoint=False, release=False, offline=True, async_req=False),
-            'graceful':         dict(stop='fast', checkpoint=True, release=True, offline=False, async_req=False),
-            'immediate':        dict(stop='immediate', checkpoint=False, release=True, offline=False, async_req=True),
-            'immediate-nolock': dict(stop='immediate', checkpoint=False, release=False, offline=False, async_req=True),
+            'offline':          dict(stop='fast',      checkpoint=False, release=False, offline=True,  async_req=False),  # noqa: E241,E501
+            'graceful':         dict(stop='fast',      checkpoint=True,  release=True,  offline=False, async_req=False),  # noqa: E241,E501
+            'immediate':        dict(stop='immediate', checkpoint=False, release=True,  offline=False, async_req=True),  # noqa: E241,E501
+            'immediate-nolock': dict(stop='immediate', checkpoint=False, release=False, offline=False, async_req=True),  # noqa: E241,E501
+
         }[mode]
 
         logger.info('Demoting self (%s)', mode)
 
         self._rewind.trigger_check_diverged_lsn()
 
         status = {'released': False}
 
-        def on_shutdown(checkpoint_location):
+        def on_shutdown(checkpoint_location: int) -> None:
             # Postmaster is still running, but pg_control already reports clean "shut down".
             # It could happen if Postgres is still archiving the backlog of WAL files.
             # If we know that there are replicas that received the shutdown checkpoint
             # location, we can remove the leader key and allow them to start leader race.
             if self.is_failover_possible(self.cluster.members, cluster_lsn=checkpoint_location):
                 self.state_handler.set_role('demoted')
                 with self._async_executor:
                     self.release_leader_key_voluntarily(checkpoint_location)
                     status['released'] = True
 
-        def before_shutdown():
+        def before_shutdown() -> None:
             if self.state_handler.citus_handler.is_coordinator():
                 self.state_handler.citus_handler.on_demote()
             else:
                 self.notify_citus_coordinator('before_demote')
 
-        self.state_handler.stop(mode_control['stop'], checkpoint=mode_control['checkpoint'],
+        self.state_handler.stop(str(mode_control['stop']), checkpoint=bool(mode_control['checkpoint']),
                                 on_safepoint=self.watchdog.disable if self.watchdog.is_running else None,
                                 on_shutdown=on_shutdown if mode_control['release'] else None,
                                 before_shutdown=before_shutdown if mode == 'graceful' else None,
                                 stop_timeout=self.primary_stop_timeout())
         self.state_handler.set_role('demoted')
         self.set_is_leader(False)
 
@@ -1076,27 +1087,28 @@
             try:
                 cluster = self.dcs.get_cluster()
                 node_to_follow, leader = self._get_node_to_follow(cluster), cluster.leader
             except Exception:
                 node_to_follow, leader = None, None
 
         if self.is_synchronous_mode():
-            self.state_handler.sync_handler.set_synchronous_standby_names([])
+            self.state_handler.sync_handler.set_synchronous_standby_names(CaseInsensitiveSet())
 
         # FIXME: with mode offline called from DCS exception handler and handle_long_action_in_progress
         # there could be an async action already running, calling follow from here will lead
         # to racy state handler state updates.
         if mode_control['async_req']:
             self._async_executor.try_run_async('starting after demotion', self.state_handler.follow, (node_to_follow,))
         else:
             if self._rewind.rewind_or_reinitialize_needed_and_possible(leader):
                 return False  # do not start postgres, but run pg_rewind on the next iteration
             self.state_handler.follow(node_to_follow)
 
-    def should_run_scheduled_action(self, action_name, scheduled_at, cleanup_fn):
+    def should_run_scheduled_action(self, action_name: str, scheduled_at: Optional[datetime.datetime],
+                                    cleanup_fn: Callable[..., Any]) -> bool:
         if scheduled_at and not self.is_paused():
             # If the scheduled action is in the far future, we shouldn't do anything and just return.
             # If the scheduled action is in the past, we consider the value to be stale and we remove
             # the value.
             # If the value is close to now, we initiate the scheduled action
             # Additionally, if the scheduled action cannot be executed altogether, i.e. there is an error
             # or the action is in the past - we take care of cleaning it up.
@@ -1122,27 +1134,27 @@
                 logger.info('Manual scheduled {0} at %s'.format(action_name), scheduled_at.isoformat())
                 return True
             except TypeError:
                 logger.warning('Incorrect value of scheduled_at: %s', scheduled_at)
                 cleanup_fn()
         return False
 
-    def process_manual_failover_from_leader(self):
+    def process_manual_failover_from_leader(self) -> Optional[str]:
         """Checks if manual failover is requested and takes action if appropriate.
 
         Cleans up failover key if failover conditions are not matched.
 
         :returns: action message if demote was initiated, None if no action was taken"""
         failover = self.cluster.failover
         if not failover or (self.is_paused() and not self.state_handler.is_leader()):
             return
 
         if (failover.scheduled_at and not
             self.should_run_scheduled_action("failover", failover.scheduled_at, lambda:
-                                             self.dcs.manual_failover('', '', index=failover.index))):
+                                             self.dcs.manual_failover('', '', version=failover.version))):
             return
 
         if not failover.leader or failover.leader == self.state_handler.name:
             if not failover.candidate or failover.candidate != self.state_handler.name:
                 if not failover.candidate and self.is_paused():
                     logger.warning('Failover is possible only to a specific candidate in a paused state')
                 else:
@@ -1164,26 +1176,26 @@
             else:
                 logger.warning('manual failover: I am already the leader, no need to failover')
         else:
             logger.warning('manual failover: leader name does not match: %s != %s',
                            failover.leader, self.state_handler.name)
 
         logger.info('Cleaning up failover key')
-        self.dcs.manual_failover('', '', index=failover.index)
+        self.dcs.manual_failover('', '', version=failover.version)
 
-    def process_unhealthy_cluster(self):
+    def process_unhealthy_cluster(self) -> str:
         """Cluster has no leader key"""
 
         if self.is_healthiest_node():
             if self.acquire_lock():
                 failover = self.cluster.failover
                 if failover:
                     if self.is_paused() and failover.leader and failover.candidate:
                         logger.info('Updating failover key after acquiring leader lock...')
-                        self.dcs.manual_failover('', failover.candidate, failover.scheduled_at, failover.index)
+                        self.dcs.manual_failover('', failover.candidate, failover.scheduled_at, failover.version)
                     else:
                         logger.info('Cleaning up failover key after acquiring leader lock...')
                         self.dcs.manual_failover('', '')
                 self.load_cluster_from_dcs()
 
                 if self.is_standby_cluster():
                     # standby leader disappeared, and this is the healthiest
@@ -1208,15 +1220,15 @@
 
             if self.patroni.nofailover:
                 return self.follow('demoting self because I am not allowed to become primary',
                                    'following a different leader because I am not allowed to promote')
             return self.follow('demoting self because i am not the healthiest node',
                                'following a different leader because i am not the healthiest node')
 
-    def process_healthy_cluster(self):
+    def process_healthy_cluster(self) -> str:
         if self.has_lock():
             if self.is_paused() and not self.state_handler.is_leader():
                 if self.cluster.failover and self.cluster.failover.candidate == self.state_handler.name:
                     return 'waiting to become primary after promote...'
 
                 if not self.is_standby_cluster():
                     self._delete_leader()
@@ -1255,46 +1267,46 @@
                     return 'not promoting because failed to update leader lock in DCS'
         else:
             logger.debug('does not have lock')
         lock_owner = self.cluster.leader and self.cluster.leader.name
         if self.is_standby_cluster():
             return self.follow('cannot be a real primary in a standby cluster',
                                'no action. I am ({0}), a secondary, and following a standby leader ({1})'.format(
-                                    self.state_handler.name, lock_owner), refresh=False)
+                                   self.state_handler.name, lock_owner), refresh=False)
         return self.follow('demoting self because I do not have the lock and I was a leader',
                            'no action. I am ({0}), a secondary, and following a leader ({1})'.format(
-                                self.state_handler.name, lock_owner), refresh=False)
+                               self.state_handler.name, lock_owner), refresh=False)
 
-    def evaluate_scheduled_restart(self):
+    def evaluate_scheduled_restart(self) -> Optional[str]:
         if self._async_executor.busy:  # Restart already in progress
             return None
 
         # restart if we need to
         restart_data = self.future_restart_scheduled()
         if restart_data:
             recent_time = self.state_handler.postmaster_start_time()
             request_time = restart_data['postmaster_start_time']
             # check if postmaster start time has changed since the last restart
             if recent_time and request_time and recent_time != request_time:
                 logger.info("Cancelling scheduled restart: postgres restart has already happened at %s", recent_time)
                 self.delete_future_restart()
                 return None
 
-        if (restart_data and
-           self.should_run_scheduled_action('restart', restart_data['schedule'], self.delete_future_restart)):
+        if restart_data\
+                and self.should_run_scheduled_action('restart', restart_data['schedule'], self.delete_future_restart):
             try:
                 ret, message = self.restart(restart_data, run_async=True)
                 if not ret:
                     logger.warning("Scheduled restart: %s", message)
                     return None
                 return message
             finally:
                 self.delete_future_restart()
 
-    def restart_matches(self, role, postgres_version, pending_restart):
+    def restart_matches(self, role: Optional[str], postgres_version: Optional[str], pending_restart: bool) -> bool:
         reason_to_cancel = ""
         # checking the restart filters here seem to be less ugly than moving them into the
         # run_scheduled_action.
         if role and role != self.state_handler.role:
             reason_to_cancel = "host role mismatch"
 
         if postgres_version and postgres_version_to_int(postgres_version) <= int(self.state_handler.server_version):
@@ -1305,40 +1317,39 @@
 
         if not reason_to_cancel:
             return True
         else:
             logger.info("not proceeding with the restart: %s", reason_to_cancel)
         return False
 
-    def schedule_future_restart(self, restart_data):
+    def schedule_future_restart(self, restart_data: Dict[str, Any]) -> bool:
         with self._async_executor:
             restart_data['postmaster_start_time'] = self.state_handler.postmaster_start_time()
             if not self.patroni.scheduled_restart:
                 self.patroni.scheduled_restart = restart_data
                 self.touch_member()
                 return True
         return False
 
-    def delete_future_restart(self):
+    def delete_future_restart(self) -> bool:
         ret = False
         with self._async_executor:
             if self.patroni.scheduled_restart:
                 self.patroni.scheduled_restart = {}
                 self.touch_member()
                 ret = True
         return ret
 
-    def future_restart_scheduled(self):
-        return self.patroni.scheduled_restart.copy() if (self.patroni.scheduled_restart and
-                                                         isinstance(self.patroni.scheduled_restart, dict)) else None
+    def future_restart_scheduled(self) -> Dict[str, Any]:
+        return self.patroni.scheduled_restart.copy()
 
-    def restart_scheduled(self):
+    def restart_scheduled(self) -> bool:
         return self._async_executor.scheduled_action == 'restart'
 
-    def restart(self, restart_data, run_async=False):
+    def restart(self, restart_data: Dict[str, Any], run_async: bool = False) -> Tuple[bool, str]:
         """ conditional and unconditional restart """
         assert isinstance(restart_data, dict)
 
         if (not self.restart_matches(restart_data.get('role'),
                                      restart_data.get('postgres_version'),
                                      ('restart_pending' in restart_data))):
             return (False, "restart conditions are not satisfied")
@@ -1351,21 +1362,21 @@
             # Make the main loop to think that we were recovering dead postgres. If we fail
             # to start postgres after a specified timeout (see below), we need to remove
             # leader key (if it belong to us) rather than trying to start postgres once again.
             self.recovering = True
 
         # Now that restart is scheduled we can set timeout for startup, it will get reset
         # once async executor runs and main loop notices PostgreSQL as up.
-        timeout = restart_data.get('timeout', self.patroni.config['primary_start_timeout'])
+        timeout = restart_data.get('timeout', self.global_config.primary_start_timeout)
         self.set_start_timeout(timeout)
 
-        def before_shutdown():
+        def before_shutdown() -> None:
             self.notify_citus_coordinator('before_demote')
 
-        def after_start():
+        def after_start() -> None:
             self.notify_citus_coordinator('after_promote')
 
         # For non async cases we want to wait for restart to complete or timeout before returning.
         do_restart = functools.partial(self.state_handler.restart, timeout, self._async_executor.critical_task,
                                        before_shutdown=before_shutdown if self.has_lock() else None,
                                        after_start=after_start if self.has_lock() else None)
         if self.is_synchronous_mode() and not self.has_lock():
@@ -1379,107 +1390,110 @@
             if res:
                 return (True, 'restarted successfully')
             elif res is None:
                 return (False, 'postgres is still starting')
             else:
                 return (False, 'restart failed')
 
-    def _do_reinitialize(self, cluster):
+    def _do_reinitialize(self, cluster: Cluster) -> Optional[bool]:
         self.state_handler.stop('immediate', stop_timeout=self.patroni.config['retry_timeout'])
         # Commented redundant data directory cleanup here
         # self.state_handler.remove_data_directory()
 
-        clone_member = self.cluster.get_clone_member(self.state_handler.name)
-        member_role = 'leader' if clone_member == self.cluster.leader else 'replica'
-        return self.clone(clone_member, "from {0} '{1}'".format(member_role, clone_member.name))
+        clone_member = cluster.get_clone_member(self.state_handler.name)
+        if clone_member:
+            member_role = 'leader' if clone_member == cluster.leader else 'replica'
+            return self.clone(clone_member, "from {0} '{1}'".format(member_role, clone_member.name))
 
-    def reinitialize(self, force=False):
+    def reinitialize(self, force: bool = False) -> Optional[str]:
         with self._async_executor:
             self.load_cluster_from_dcs()
 
             if self.cluster.is_unlocked():
                 return 'Cluster has no leader, can not reinitialize'
 
             if self.has_lock(False):
                 return 'I am the leader, can not reinitialize'
 
+            cluster = self.cluster
+
         if force:
             self._async_executor.cancel()
 
         with self._async_executor:
             action = self._async_executor.schedule('reinitialize')
             if action is not None:
                 return '{0} already in progress'.format(action)
 
-        self._async_executor.run_async(self._do_reinitialize, args=(self.cluster, ))
+        self._async_executor.run_async(self._do_reinitialize, args=(cluster, ))
 
-    def handle_long_action_in_progress(self):
+    def handle_long_action_in_progress(self) -> str:
         """
         Figure out what to do with the task AsyncExecutor is performing.
         """
         if self.has_lock() and self.update_lock():
             if self._async_executor.scheduled_action == 'doing crash recovery in a single user mode':
-                time_left = self.patroni.config['primary_start_timeout'] - (time.time() - self._crash_recovery_started)
+                time_left = self.global_config.primary_start_timeout - (time.time() - self._crash_recovery_started)
                 if time_left <= 0 and self.is_failover_possible(self.cluster.members):
                     logger.info("Demoting self because crash recovery is taking too long")
                     self.state_handler.cancellable.cancel(True)
                     self.demote('immediate')
                     return 'terminated crash recovery because of startup timeout'
 
-            return 'updated leader lock during ' + self._async_executor.scheduled_action
+            return 'updated leader lock during {0}'.format(self._async_executor.scheduled_action)
         elif not self.state_handler.bootstrapping and not self.is_paused():
             # Don't have lock, make sure we are not promoting or starting up a primary in the background
             if self._async_executor.scheduled_action == 'promote':
                 with self._async_response:
                     cancel = self._async_response.cancel()
                 if cancel:
                     self.state_handler.cancellable.cancel()
                     return 'lost leader before promote'
 
             if self.state_handler.role in ('master', 'primary'):
-                logger.info("Demoting primary during " + self._async_executor.scheduled_action)
+                logger.info('Demoting primary during %s', self._async_executor.scheduled_action)
                 if self._async_executor.scheduled_action == 'restart':
                     # Restart needs a special interlocking cancel because postmaster may be just started in a
                     # background thread and has not even written a pid file yet.
                     with self._async_executor.critical_task as task:
-                        if not task.cancel():
+                        if not task.cancel() and isinstance(task.result, PostmasterProcess):
                             self.state_handler.terminate_starting_postmaster(postmaster=task.result)
                 self.demote('immediate-nolock')
-                return 'lost leader lock during ' + self._async_executor.scheduled_action
+                return 'lost leader lock during {0}'.format(self._async_executor.scheduled_action)
         if self.cluster.is_unlocked():
             logger.info('not healthy enough for leader race')
 
-        return self._async_executor.scheduled_action + ' in progress'
+        return '{0} in progress'.format(self._async_executor.scheduled_action)
 
     @staticmethod
-    def sysid_valid(sysid):
+    def sysid_valid(sysid: Optional[str]) -> bool:
         # sysid does tv_sec << 32, where tv_sec is the number of seconds sine 1970,
         # so even 1 << 32 would have 10 digits.
         sysid = str(sysid)
         return len(sysid) >= 10 and sysid.isdigit()
 
-    def post_recover(self):
+    def post_recover(self) -> Optional[str]:
         if not self.state_handler.is_running():
             self.watchdog.disable()
             if self.has_lock():
                 if self.state_handler.role in ('master', 'primary', 'standby_leader'):
                     self.state_handler.set_role('demoted')
                 self._delete_leader()
                 return 'removed leader key after trying and failing to start postgres'
             return 'failed to start postgres'
         return None
 
-    def cancel_initialization(self):
+    def cancel_initialization(self) -> None:
         logger.info('removing initialize key after failed attempt to bootstrap the cluster')
         self.dcs.cancel_initialization()
         self.state_handler.stop('immediate', stop_timeout=self.patroni.config['retry_timeout'])
         self.state_handler.move_data_directory()
         raise PatroniFatalException('Failed to bootstrap cluster')
 
-    def post_bootstrap(self):
+    def post_bootstrap(self) -> str:
         with self._async_response:
             result = self._async_response.result
         # bootstrap has failed if postgres is not running
         if not self.state_handler.is_running() or result is False:
             self.cancel_initialization()
 
         if result is None:
@@ -1502,15 +1516,15 @@
         self.dcs.take_leader()
         self.set_is_leader(True)
         self.state_handler.call_nowait(CallbackAction.ON_START)
         self.load_cluster_from_dcs()
 
         return 'initialized a new cluster'
 
-    def handle_starting_instance(self):
+    def handle_starting_instance(self) -> Optional[str]:
         """Starting up PostgreSQL may take a long time. In case we are the leader we may want to
         fail over to."""
 
         # Check if we are in startup, when paused defer to main loop for manual failovers.
         if not self.state_handler.check_for_startup() or self.is_paused():
             self.set_start_timeout(None)
             if self.is_paused():
@@ -1520,15 +1534,15 @@
         # state_handler.state == 'starting' here
         if self.has_lock():
             if not self.update_lock():
                 logger.info("Lost lock while starting up. Demoting self.")
                 self.demote('immediate-nolock')
                 return 'stopped PostgreSQL while starting up because leader key was lost'
 
-            timeout = self._start_timeout or self.patroni.config['primary_start_timeout']
+            timeout = self._start_timeout or self.global_config.primary_start_timeout
             time_left = timeout - self.state_handler.time_in_state()
 
             if time_left <= 0:
                 if self.is_failover_possible(self.cluster.members):
                     logger.info("Demoting self because primary startup is taking too long")
                     self.demote('immediate')
                     return 'stopped PostgreSQL because of startup timeout'
@@ -1541,28 +1555,29 @@
 
                 return 'PostgreSQL is still starting up, {0:.0f} seconds until timeout'.format(time_left)
         else:
             # Use normal processing for standbys
             logger.info("Still starting up as a standby.")
             return None
 
-    def set_start_timeout(self, value):
+    def set_start_timeout(self, value: Optional[int]) -> None:
         """Sets timeout for starting as primary before eligible for failover.
 
         Must be called when async_executor is busy or in the main thread."""
         self._start_timeout = value
 
-    def _run_cycle(self):
+    def _run_cycle(self) -> str:
         dcs_failed = False
         try:
             try:
                 self.load_cluster_from_dcs()
-                self.state_handler.reset_cluster_info_state(self.cluster, self.patroni.nofailover)
+                self.global_config = self.patroni.config.get_global_config(self.cluster)
+                self.state_handler.reset_cluster_info_state(self.cluster, self.patroni.nofailover, self.global_config)
             except Exception:
-                self.state_handler.reset_cluster_info_state(None, self.patroni.nofailover)
+                self.state_handler.reset_cluster_info_state(None)
                 raise
 
             if self.is_paused():
                 self.watchdog.disable()
                 self._was_paused = True
             else:
                 if self._was_paused:
@@ -1607,14 +1622,16 @@
 
                 # The Raft cluster without a quorum takes a bit of time to stabilize.
                 # Therefore we want to postpone the leader race if we just started up.
                 if self.cluster.is_unlocked() and self.dcs.__class__.__name__ == 'Raft':
                     return 'started as a secondary'
 
             # is data directory empty?
+            data_directory_error = ''
+            data_directory_is_empty = None
             try:
                 data_directory_is_empty = self.state_handler.data_directory_empty()
                 data_directory_is_accessible = True
             except OSError as e:
                 data_directory_is_accessible = False
                 data_directory_error = e
 
@@ -1624,15 +1641,15 @@
                 # In case datadir went away while we were primary
                 self.watchdog.disable()
 
                 # is this instance the leader?
                 if self.has_lock():
                     self.release_leader_key_voluntarily()
                     return 'released leader key voluntarily as data dir {0} and currently leader'.format(
-                                'empty' if data_directory_is_accessible else 'not accessible')
+                        'empty' if data_directory_is_accessible else 'not accessible')
 
                 if not data_directory_is_accessible:
                     return 'data directory is not accessible: {0}'.format(data_directory_error)
                 if self.is_paused():
                     return 'running with empty data directory'
                 return self.bootstrap()  # new node
             else:
@@ -1715,15 +1732,15 @@
             return 'Error communicating with PostgreSQL. Will try again later'
         finally:
             if not dcs_failed:
                 if self.is_leader():
                     self._failsafe.set_is_active(0)
                 self.touch_member()
 
-    def _handle_dcs_error(self):
+    def _handle_dcs_error(self) -> str:
         if not self.is_paused() and self.state_handler.is_running():
             if self.state_handler.is_leader():
                 if self.is_failsafe_mode() and self.check_failsafe_topology():
                     self.set_is_leader(True)
                     self._failsafe.set_is_active(time.time())
                     self.watchdog.keepalive()
                     return 'continue to run as a leader because failsafe mode is enabled and all members are accessible'
@@ -1734,21 +1751,21 @@
                 logger.warning('AsyncExecutor is busy, demoting from the main thread')
                 self.demote('offline')
                 return 'demoted self because DCS is not accessible and I was a leader'
             else:
                 self._sync_replication_slots(True)
         return 'DCS is not accessible'
 
-    def _sync_replication_slots(self, dcs_failed):
+    def _sync_replication_slots(self, dcs_failed: bool) -> List[str]:
         """Handles replication slots.
 
         :param dcs_failed: bool, indicates that communication with DCS failed (get_cluster() or update_leader())
         :returns: list[str], replication slots names that should be copied from the primary"""
 
-        slots = []
+        slots: List[str] = []
 
         # If dcs_failed we don't want to touch replication slots on a leader or replicas if failsafe_mode isn't enabled.
         if not self.cluster or dcs_failed and (self.is_leader() or not self.is_failsafe_mode()):
             return slots
 
         # It could be that DCS is read-only, or only the leader can't access it.
         # Only the second one could be handled by `load_cluster_from_dcs()`.
@@ -1760,51 +1777,51 @@
             slots = self.state_handler.slots_handler.sync_replication_slots(cluster,
                                                                             self.patroni.nofailover,
                                                                             self.patroni.replicatefrom,
                                                                             self.is_paused())
         # Don't copy replication slots if failsafe_mode is active
         return [] if self.failsafe_is_active() else slots
 
-    def run_cycle(self):
+    def run_cycle(self) -> str:
         with self._async_executor:
             try:
                 info = self._run_cycle()
                 return (self.is_paused() and 'PAUSE: ' or '') + info
             except PatroniFatalException:
                 raise
             except Exception:
                 logger.exception('Unexpected exception')
                 return 'Unexpected exception raised, please report it as a BUG'
 
-    def shutdown(self):
+    def shutdown(self) -> None:
         if self.is_paused():
             logger.info('Leader key is not deleted and Postgresql is not stopped due paused state')
             self.watchdog.disable()
         elif not self._join_aborted:
             # FIXME: If stop doesn't reach safepoint quickly enough keepalive is triggered. If shutdown checkpoint
             # takes longer than ttl, then leader key is lost and replication might not have sent out all WAL.
             # This might not be the desired behavior of users, as a graceful shutdown of the host can mean lost data.
             # We probably need to something smarter here.
             disable_wd = self.watchdog.disable if self.watchdog.is_running else None
 
             status = {'deleted': False}
 
-            def _on_shutdown(checkpoint_location):
+            def _on_shutdown(checkpoint_location: int) -> None:
                 if self.is_leader():
                     # Postmaster is still running, but pg_control already reports clean "shut down".
                     # It could happen if Postgres is still archiving the backlog of WAL files.
                     # If we know that there are replicas that received the shutdown checkpoint
                     # location, we can remove the leader key and allow them to start leader race.
                     if self.is_failover_possible(self.cluster.members, cluster_lsn=checkpoint_location):
                         self.dcs.delete_leader(checkpoint_location)
                         status['deleted'] = True
                     else:
                         self.dcs.write_leader_optime(checkpoint_location)
 
-            def _before_shutdown():
+            def _before_shutdown() -> None:
                 self.notify_citus_coordinator('before_demote')
 
             on_shutdown = _on_shutdown if self.is_leader() else None
             before_shutdown = _before_shutdown if self.is_leader() else None
             self.while_not_sync_standby(lambda: self.state_handler.stop(checkpoint=False, on_safepoint=disable_wd,
                                                                         on_shutdown=on_shutdown,
                                                                         before_shutdown=before_shutdown,
@@ -1814,43 +1831,43 @@
                     checkpoint_location = self.state_handler.latest_checkpoint_location()
                     self.dcs.delete_leader(checkpoint_location)
                 self.touch_member()
             else:
                 # XXX: what about when Patroni is started as the wrong user that has access to the watchdog device
                 # but cannot shut down PostgreSQL. Root would be the obvious example. Would be nice to not kill the
                 # system due to a bad config.
-                logger.error("PostgreSQL shutdown failed, leader key not removed." +
+                logger.error("PostgreSQL shutdown failed, leader key not removed.%s",
                              (" Leaving watchdog running." if self.watchdog.is_running else ""))
 
-    def watch(self, timeout):
+    def watch(self, timeout: float) -> bool:
         # watch on leader key changes if the postgres is running and leader is known and current node is not lock owner
         if self._async_executor.busy or not self.cluster or self.cluster.is_unlocked() or self.has_lock(False):
-            leader_index = None
+            leader_version = None
         else:
-            leader_index = self.cluster.leader.index
+            leader_version = self.cluster.leader.version if self.cluster.leader else None
 
-        return self.dcs.watch(leader_index, timeout)
+        return self.dcs.watch(leader_version, timeout)
 
-    def wakeup(self):
+    def wakeup(self) -> None:
         """Call of this method will trigger the next run of HA loop if there is
         no "active" leader watch request in progress.
         This usually happens on the leader or if the node is running async action"""
         self.dcs.event.set()
 
-    def get_remote_member(self, member=None):
+    def get_remote_member(self, member: Union[Leader, Member, None] = None) -> RemoteMember:
         """ In case of standby cluster this will tel us from which remote
             member to stream. Config can be both patroni config or
             cluster.config.data
         """
-        cluster_params = self.get_standby_cluster_config()
+        data: Dict[str, Any] = {}
+        cluster_params = self.global_config.get_standby_cluster_config()
 
         if cluster_params:
-            name = member.name if member else 'remote_member:{}'.format(uuid.uuid1())
-
-            data = {k: v for k, v in cluster_params.items() if k in RemoteMember.allowed_keys()}
+            data.update({k: v for k, v in cluster_params.items() if k in RemoteMember.allowed_keys()})
             data['no_replication_slot'] = 'primary_slot_name' not in cluster_params
             conn_kwargs = member.conn_kwargs() if member else \
                 {k: cluster_params[k] for k in ('host', 'port') if k in cluster_params}
             if conn_kwargs:
                 data['conn_kwargs'] = conn_kwargs
 
-            return RemoteMember(name, data)
+        name = member.name if member else 'remote_member:{}'.format(uuid.uuid1())
+        return RemoteMember.from_name_and_data(name, data)
```

### Comparing `patroni-3.0.2/patroni/postgresql/__init__.py` & `patroni-3.0.3/patroni/postgresql/config.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,1180 +1,1179 @@
 import logging
 import os
 import re
-import shlex
 import shutil
-import subprocess
+import socket
+import stat
 import time
 
-from contextlib import contextmanager
-from copy import deepcopy
-from datetime import datetime
-from dateutil import tz
-from psutil import TimeoutExpired
-from threading import current_thread, Lock
-from typing import Optional
-
-from .bootstrap import Bootstrap
-from .callback_executor import CallbackAction, CallbackExecutor
-from .cancellable import CancellableSubprocess
-from .config import ConfigHandler, mtime
-from .connection import Connection, get_connection_cursor
-from .citus import CitusHandler
-from .misc import parse_history, parse_lsn, postgres_major_version_to_int
-from .postmaster import PostmasterProcess
-from .slots import SlotsHandler
-from .sync import SyncHandler
-from .. import psycopg
-from ..dcs import Member
-from ..exceptions import PostgresConnectionException
-from ..utils import Retry, RetryFailedError, polling_loop, data_directory_is_empty, parse_int
+from urllib.parse import urlparse, parse_qsl, unquote
+from types import TracebackType
+from typing import Any, Collection, Dict, List, Optional, Union, Tuple, Type, TYPE_CHECKING
+
+from .validator import recovery_parameters, transform_postgresql_parameter_value, transform_recovery_parameter_value
+from ..collections import CaseInsensitiveDict, CaseInsensitiveSet
+from ..dcs import Leader, Member, RemoteMember, slot_name_from_member_name
+from ..exceptions import PatroniFatalException
+from ..utils import compare_values, parse_bool, parse_int, split_host_port, uri, validate_directory, is_subpath
+from ..validator import IntValidator
 
+if TYPE_CHECKING:  # pragma: no cover
+    from . import Postgresql
 
 logger = logging.getLogger(__name__)
 
-STATE_RUNNING = 'running'
-STATE_REJECT = 'rejecting connections'
-STATE_NO_RESPONSE = 'not responding'
-STATE_UNKNOWN = 'unknown'
-
-STOP_POLLING_INTERVAL = 1
-
-
-@contextmanager
-def null_context():
-    yield
-
-
-class Postgresql(object):
-
-    POSTMASTER_START_TIME = "pg_catalog.pg_postmaster_start_time()"
-    TL_LSN = ("CASE WHEN pg_catalog.pg_is_in_recovery() THEN 0 "
-              "ELSE ('x' || pg_catalog.substr(pg_catalog.pg_{0}file_name("
-              "pg_catalog.pg_current_{0}_{1}()), 1, 8))::bit(32)::int END, "  # primary timeline
-              "CASE WHEN pg_catalog.pg_is_in_recovery() THEN 0 "
-              "ELSE pg_catalog.pg_{0}_{1}_diff(pg_catalog.pg_current_{0}_{1}(), '0/0')::bigint END, "  # write_lsn
-              "pg_catalog.pg_{0}_{1}_diff(pg_catalog.pg_last_{0}_replay_{1}(), '0/0')::bigint, "
-              "pg_catalog.pg_{0}_{1}_diff(COALESCE(pg_catalog.pg_last_{0}_receive_{1}(), '0/0'), '0/0')::bigint, "
-              "pg_catalog.pg_is_in_recovery() AND pg_catalog.pg_is_{0}_replay_paused()")
-
-    def __init__(self, config):
-        self.name = config['name']
-        self.scope = config['scope']
-        self._data_dir = config['data_dir']
-        self._database = config.get('database', 'postgres')
-        self._version_file = os.path.join(self._data_dir, 'PG_VERSION')
-        self._pg_control = os.path.join(self._data_dir, 'global', 'pg_control')
-        self._major_version = self.get_major_version()
-
-        self._state_lock = Lock()
-        self.set_state('stopped')
-
-        self._pending_restart = False
-        self._connection = Connection()
-        self.citus_handler = CitusHandler(self, config.get('citus'))
-        self.config = ConfigHandler(self, config)
-        self.config.check_directories()
-
-        self._bin_dir = config.get('bin_dir') or ''
-        self.bootstrap = Bootstrap(self)
-        self.bootstrapping = False
-        self.__thread_ident = current_thread().ident
-
-        self.slots_handler = SlotsHandler(self)
-        self.sync_handler = SyncHandler(self)
-
-        self._callback_executor = CallbackExecutor()
-        self.__cb_called = False
-        self.__cb_pending = None
-
-        self.cancellable = CancellableSubprocess()
-
-        self._sysid = None
-        self.retry = Retry(max_tries=-1, deadline=config['retry_timeout']/2.0, max_delay=1,
-                           retry_exceptions=PostgresConnectionException)
-
-        # Retry 'pg_is_in_recovery()' only once
-        self._is_leader_retry = Retry(max_tries=1, deadline=config['retry_timeout']/2.0, max_delay=1,
-                                      retry_exceptions=PostgresConnectionException)
-
-        self._role_lock = Lock()
-        self.set_role(self.get_postgres_role_from_data_directory())
-        self._state_entry_timestamp = None
-
-        self._cluster_info_state = {}
-        self._has_permanent_logical_slots = True
-        self._enforce_hot_standby_feedback = False
-        self._is_synchronous_mode = True
-        self._cached_replica_timeline = None
-
-        # Last known running process
-        self._postmaster_proc = None
-
-        if self.is_running():  # we are "joining" already running postgres
-            self.set_state('running')
-            self.set_role('master' if self.is_leader() else 'replica')
-            # postpone writing postgresql.conf for 12+ because recovery parameters are not yet known
-            if self.major_version < 120000 or self.is_leader():
-                self.config.write_postgresql_conf()
-            hba_saved = self.config.replace_pg_hba()
-            ident_saved = self.config.replace_pg_ident()
-            if hba_saved or ident_saved:
-                self.reload()
-        elif self.role in ('master', 'primary'):
-            self.set_role('demoted')
+PARAMETER_RE = re.compile(r'([a-z_]+)\s*=\s*')
 
-    @property
-    def create_replica_methods(self):
-        return self.config.get('create_replica_methods', []) or self.config.get('create_replica_method', [])
-
-    @property
-    def major_version(self):
-        return self._major_version
 
-    @property
-    def database(self):
-        return self._database
-
-    @property
-    def data_dir(self):
-        return self._data_dir
+def conninfo_uri_parse(dsn: str) -> Dict[str, str]:
+    ret: Dict[str, str] = {}
+    r = urlparse(dsn)
+    if r.username:
+        ret['user'] = r.username
+    if r.password:
+        ret['password'] = r.password
+    if r.path[1:]:
+        ret['dbname'] = r.path[1:]
+    hosts: List[str] = []
+    ports: List[str] = []
+    for netloc in r.netloc.split('@')[-1].split(','):
+        host = None
+        if '[' in netloc and ']' in netloc:
+            tmp = netloc.split(']') + ['']
+            host = tmp[0][1:]
+            netloc = ':'.join(tmp[:2])
+        tmp = netloc.rsplit(':', 1)
+        if host is None:
+            host = tmp[0]
+        hosts.append(host)
+        ports.append(tmp[1] if len(tmp) == 2 else '')
+    if hosts:
+        ret['host'] = ','.join(hosts)
+    if ports:
+        ret['port'] = ','.join(ports)
+    ret = {name: unquote(value) for name, value in ret.items()}
+    ret.update({name: value for name, value in parse_qsl(r.query)})
+    if ret.get('ssl') == 'true':
+        del ret['ssl']
+        ret['sslmode'] = 'require'
+    return ret
+
+
+def read_param_value(value: str) -> Union[Tuple[None, None], Tuple[str, int]]:
+    length = len(value)
+    ret = ''
+    is_quoted = value[0] == "'"
+    i = int(is_quoted)
+    while i < length:
+        if is_quoted:
+            if value[i] == "'":
+                return ret, i + 1
+        elif value[i].isspace():
+            break
+        if value[i] == '\\':
+            i += 1
+            if i >= length:
+                break
+        ret += value[i]
+        i += 1
+    return (None, None) if is_quoted else (ret, i)
+
+
+def conninfo_parse(dsn: str) -> Optional[Dict[str, str]]:
+    ret: Dict[str, str] = {}
+    length = len(dsn)
+    i = 0
+    while i < length:
+        if dsn[i].isspace():
+            i += 1
+            continue
 
-    @property
-    def callback(self):
-        return self.config.get('callbacks') or {}
-
-    @property
-    def wal_dir(self):
-        return os.path.join(self._data_dir, 'pg_' + self.wal_name)
-
-    @property
-    def wal_name(self):
-        return 'wal' if self._major_version >= 100000 else 'xlog'
+        param_match = PARAMETER_RE.match(dsn[i:])
+        if not param_match:
+            return
 
-    @property
-    def lsn_name(self):
-        return 'lsn' if self._major_version >= 100000 else 'location'
+        param = param_match.group(1)
+        i += param_match.end()
 
-    @property
-    def cluster_info_query(self):
-        """Returns the monitoring query with a fixed number of fields.
+        if i >= length:
+            return
 
-        The query text is constructed based on current state in DCS and PostgreSQL version:
-        1. function names depend on version. wal/lsn for v10+ and xlog/location for pre v10.
-        2. for primary we query timeline_id (extracted from pg_walfile_name()) and pg_current_wal_lsn()
-        3. for replicas we query pg_last_wal_receive_lsn(), pg_last_wal_replay_lsn(), and  pg_is_wal_replay_paused()
-        4. for v9.6+ we query primary_slot_name and primary_conninfo from pg_stat_get_wal_receiver()
-        5. for v11+ with permanent logical slots we query from pg_replication_slots and aggregate the result
-        6. for standby_leader node running v9.6+ we also query pg_control_checkpoint to fetch timeline_id
-        7. if sync replication is enabled we query pg_stat_replication and aggregate the result.
-           In addition to that we get current values of synchronous_commit and synchronous_standby_names GUCs.
-
-        If some conditions are not satisfied we simply put static values instead. E.g., NULL, 0, '', and so on."""
-
-        extra = ", " + (("pg_catalog.current_setting('synchronous_commit'), " +
-                         "pg_catalog.current_setting('synchronous_standby_names'), "
-                         "(SELECT pg_catalog.json_agg(r.*) FROM (SELECT w.pid as pid, application_name, sync_state," +
-                         " pg_catalog.pg_{0}_{1}_diff(write_{1}, '0/0')::bigint AS write_lsn," +
-                         " pg_catalog.pg_{0}_{1}_diff(flush_{1}, '0/0')::bigint AS flush_lsn," +
-                         " pg_catalog.pg_{0}_{1}_diff(replay_{1}, '0/0')::bigint AS replay_lsn " +
-                         "FROM pg_catalog.pg_stat_get_wal_senders() w," +
-                         " pg_catalog.pg_stat_get_activity(w.pid)" +
-                         " WHERE w.state = 'streaming') r)").format(self.wal_name, self.lsn_name)
-                        if self._is_synchronous_mode and self.role in ('master', 'primary') else "'on', '', NULL")
-
-        if self._major_version >= 90600:
-            extra = ("(SELECT pg_catalog.json_agg(s.*) FROM (SELECT slot_name, slot_type as type, datoid::bigint, " +
-                     "plugin, catalog_xmin, pg_catalog.pg_wal_lsn_diff(confirmed_flush_lsn, '0/0')::bigint" +
-                     " AS confirmed_flush_lsn FROM pg_catalog.pg_get_replication_slots()) AS s)"
-                     if self._has_permanent_logical_slots and self._major_version >= 110000 else "NULL") + extra
-            extra = (", CASE WHEN latest_end_lsn IS NULL THEN NULL ELSE received_tli END,"
-                     " slot_name, conninfo, {0} FROM pg_catalog.pg_stat_get_wal_receiver()").format(extra)
-            if self.role == 'standby_leader':
-                extra = "timeline_id" + extra + ", pg_catalog.pg_control_checkpoint()"
-            else:
-                extra = "0" + extra
+        value, end = read_param_value(dsn[i:])
+        if value is None or end is None:
+            return
+        i += end
+        ret[param] = value
+    return ret
+
+
+def parse_dsn(value: str) -> Optional[Dict[str, str]]:
+    """
+    Very simple equivalent of `psycopg2.extensions.parse_dsn` introduced in 2.7.0.
+    We are not using psycopg2 function in order to remain compatible with 2.5.4+.
+    There is one minor difference though, this function removes `dbname` from the result
+    and sets the `sslmode`, 'gssencmode', and `channel_binding` to `prefer` if it is not present in
+    the connection string. This is necessary to simplify comparison of the old and the new values.
+
+    >>> r = parse_dsn('postgresql://u%2Fse:pass@:%2f123,[::1]/db%2Fsdf?application_name=mya%2Fpp&ssl=true')
+    >>> r == {'application_name': 'mya/pp', 'host': ',::1', 'sslmode': 'require',\
+              'password': 'pass', 'port': '/123,', 'user': 'u/se', 'gssencmode': 'prefer', 'channel_binding': 'prefer'}
+    True
+    >>> r = parse_dsn(" host = 'host' dbname = db\\\\ name requiressl=1 ")
+    >>> r == {'host': 'host', 'sslmode': 'require', 'gssencmode': 'prefer', 'channel_binding': 'prefer'}
+    True
+    >>> parse_dsn('requiressl = 0\\\\') == {'sslmode': 'prefer', 'gssencmode': 'prefer', 'channel_binding': 'prefer'}
+    True
+    >>> parse_dsn("host=a foo = '") is None
+    True
+    >>> parse_dsn("host=a foo = ") is None
+    True
+    >>> parse_dsn("1") is None
+    True
+    """
+    if value.startswith('postgres://') or value.startswith('postgresql://'):
+        ret = conninfo_uri_parse(value)
+    else:
+        ret = conninfo_parse(value)
+
+    if ret:
+        if 'sslmode' not in ret:  # allow sslmode to take precedence over requiressl
+            requiressl = ret.pop('requiressl', None)
+            if requiressl == '1':
+                ret['sslmode'] = 'require'
+            elif requiressl is not None:
+                ret['sslmode'] = 'prefer'
+            ret.setdefault('sslmode', 'prefer')
+        if 'dbname' in ret:
+            del ret['dbname']
+        ret.setdefault('gssencmode', 'prefer')
+        ret.setdefault('channel_binding', 'prefer')
+    return ret
+
+
+def strip_comment(value: str) -> str:
+    i = value.find('#')
+    if i > -1:
+        value = value[:i].strip()
+    return value
+
+
+def read_recovery_param_value(value: str) -> Optional[str]:
+    """
+    >>> read_recovery_param_value('') is None
+    True
+    >>> read_recovery_param_value("'") is None
+    True
+    >>> read_recovery_param_value("''a") is None
+    True
+    >>> read_recovery_param_value('a b') is None
+    True
+    >>> read_recovery_param_value("'''") is None
+    True
+    >>> read_recovery_param_value("'\\\\") is None
+    True
+    >>> read_recovery_param_value("'a' s#") is None
+    True
+    >>> read_recovery_param_value("'\\\\'''' #a")
+    "''"
+    >>> read_recovery_param_value('asd')
+    'asd'
+    """
+    value = value.strip()
+    length = len(value)
+    if length == 0:
+        return None
+    elif value[0] == "'":
+        if length == 1:
+            return None
+        ret = ''
+        i = 1
+        while i < length:
+            if value[i] == '\\':
+                i += 1
+                if i >= length:
+                    return None
+            elif value[i] == "'":
+                i += 1
+                if i >= length:
+                    break
+                if value[i] in ('#', ' '):
+                    if strip_comment(value[i:]):
+                        return None
+                    break
+                if value[i] != "'":
+                    return None
+            ret += value[i]
+            i += 1
         else:
-            extra = "0, NULL, NULL, NULL, NULL" + extra
-
-        return ("SELECT " + self.TL_LSN + ", {2}").format(self.wal_name, self.lsn_name, extra)
-
-    def _version_file_exists(self):
-        return not self.data_directory_empty() and os.path.isfile(self._version_file)
-
-    def get_major_version(self) -> int:
-        """Reads major version from PG_VERSION file
-
-        :returns: major PostgreSQL version in integer format or 0 in case of missing file or errors"""
-        if self._version_file_exists():
-            try:
-                with open(self._version_file) as f:
-                    return postgres_major_version_to_int(f.read().strip())
-            except Exception:
-                logger.exception('Failed to read PG_VERSION from %s', self._data_dir)
-        return 0
+            return None
+        return ret
+    else:
+        value = strip_comment(value)
+        if not value or ' ' in value or '\\' in value:
+            return None
+    return value
 
-    def pgcommand(self, cmd):
-        """Returns path to the specified PostgreSQL command"""
-        return os.path.join(self._bin_dir, cmd)
 
-    def pg_ctl(self, cmd, *args, **kwargs):
-        """Builds and executes pg_ctl command
+def mtime(filename: str) -> Optional[float]:
+    try:
+        return os.stat(filename).st_mtime
+    except OSError:
+        return None
 
-        :returns: `!True` when return_code == 0, otherwise `!False`"""
 
-        pg_ctl = [self.pgcommand('pg_ctl'), cmd]
-        return subprocess.call(pg_ctl + ['-D', self._data_dir] + list(args), **kwargs) == 0
+class ConfigWriter(object):
 
-    def pg_isready(self):
-        """Runs pg_isready to see if PostgreSQL is accepting connections.
+    def __init__(self, filename: str) -> None:
+        self._filename = filename
+        self._fd = None
 
-        :returns: 'ok' if PostgreSQL is up, 'reject' if starting up, 'no_resopnse' if not up."""
+    def __enter__(self) -> 'ConfigWriter':
+        self._fd = open(self._filename, 'w')
+        self.writeline('# Do not edit this file manually!\n# It will be overwritten by Patroni!')
+        return self
 
-        r = self.config.local_connect_kwargs
-        cmd = [self.pgcommand('pg_isready'), '-p', r['port'], '-d', self._database]
+    def __exit__(self, exc_type: Optional[Type[BaseException]],
+                 exc_val: Optional[BaseException], exc_tb: Optional[TracebackType]) -> None:
+        if self._fd:
+            self._fd.close()
 
-        # Host is not set if we are connecting via default unix socket
-        if 'host' in r:
-            cmd.extend(['-h', r['host']])
+    def writeline(self, line: str) -> None:
+        if self._fd:
+            self._fd.write(line)
+            self._fd.write('\n')
 
-        # We only need the username because pg_isready does not try to authenticate
-        if 'user' in r:
-            cmd.extend(['-U', r['user']])
+    def writelines(self, lines: List[str]) -> None:
+        for line in lines:
+            self.writeline(line)
 
-        ret = subprocess.call(cmd)
-        return_codes = {0: STATE_RUNNING,
-                        1: STATE_REJECT,
-                        2: STATE_NO_RESPONSE,
-                        3: STATE_UNKNOWN}
-        return return_codes.get(ret, STATE_UNKNOWN)
+    @staticmethod
+    def escape(value: Any) -> str:  # Escape (by doubling) any single quotes or backslashes in given string
+        return re.sub(r'([\'\\])', r'\1\1', str(value))
 
-    def reload_config(self, config, sighup=False):
-        self.config.reload_config(config, sighup)
-        self._is_leader_retry.deadline = self.retry.deadline = config['retry_timeout']/2.0
+    def write_param(self, param: str, value: Any) -> None:
+        self.writeline("{0} = '{1}'".format(param, self.escape(value)))
 
-    @property
-    def pending_restart(self):
-        return self._pending_restart
 
-    def set_pending_restart(self, value):
-        self._pending_restart = value
+def _false_validator(value: Any) -> bool:
+    return False
+
+
+def _wal_level_validator(value: Any) -> bool:
+    return str(value).lower() in ('hot_standby', 'replica', 'logical')
+
+
+def _bool_validator(value: Any) -> bool:
+    return parse_bool(value) is not None
+
+
+class ConfigHandler(object):
+
+    # List of parameters which must be always passed to postmaster as command line options
+    # to make it not possible to change them with 'ALTER SYSTEM'.
+    # Some of these parameters have sane default value assigned and Patroni doesn't allow
+    # to decrease this value. E.g. 'wal_level' can't be lower then 'hot_standby' and so on.
+    # These parameters could be changed only globally, i.e. via DCS.
+    # P.S. 'listen_addresses' and 'port' are added here just for convenience, to mark them
+    # as a parameters which should always be passed through command line.
+    #
+    # Format:
+    #  key - parameter name
+    #  value - tuple(default_value, check_function, min_version)
+    #    default_value -- some sane default value
+    #    check_function -- if the new value is not correct must return `!False`
+    #    min_version -- major version of PostgreSQL when parameter was introduced
+    CMDLINE_OPTIONS = CaseInsensitiveDict({
+        'listen_addresses': (None, _false_validator, 90100),
+        'port': (None, _false_validator, 90100),
+        'cluster_name': (None, _false_validator, 90500),
+        'wal_level': ('hot_standby', _wal_level_validator, 90100),
+        'hot_standby': ('on', _false_validator, 90100),
+        'max_connections': (100, IntValidator(min=25), 90100),
+        'max_wal_senders': (10, IntValidator(min=3), 90100),
+        'wal_keep_segments': (8, IntValidator(min=1), 90100),
+        'wal_keep_size': ('128MB', IntValidator(min=16, base_unit='MB'), 130000),
+        'max_prepared_transactions': (0, IntValidator(min=0), 90100),
+        'max_locks_per_transaction': (64, IntValidator(min=32), 90100),
+        'track_commit_timestamp': ('off', _bool_validator, 90500),
+        'max_replication_slots': (10, IntValidator(min=4), 90400),
+        'max_worker_processes': (8, IntValidator(min=2), 90400),
+        'wal_log_hints': ('on', _false_validator, 90400)
+    })
+
+    _RECOVERY_PARAMETERS = CaseInsensitiveSet(recovery_parameters.keys())
+
+    def __init__(self, postgresql: 'Postgresql', config: Dict[str, Any]) -> None:
+        self._postgresql = postgresql
+        self._config_dir = os.path.abspath(config.get('config_dir', '') or postgresql.data_dir)
+        config_base_name = config.get('config_base_name', 'postgresql')
+        self._postgresql_conf = os.path.join(self._config_dir, config_base_name + '.conf')
+        self._postgresql_conf_mtime = None
+        self._postgresql_base_conf_name = config_base_name + '.base.conf'
+        self._postgresql_base_conf = os.path.join(self._config_dir, self._postgresql_base_conf_name)
+        self._pg_hba_conf = os.path.join(self._config_dir, 'pg_hba.conf')
+        self._pg_ident_conf = os.path.join(self._config_dir, 'pg_ident.conf')
+        self._recovery_conf = os.path.join(postgresql.data_dir, 'recovery.conf')
+        self._recovery_conf_mtime = None
+        self._recovery_signal = os.path.join(postgresql.data_dir, 'recovery.signal')
+        self._standby_signal = os.path.join(postgresql.data_dir, 'standby.signal')
+        self._auto_conf = os.path.join(postgresql.data_dir, 'postgresql.auto.conf')
+        self._auto_conf_mtime = None
+        self._pgpass = os.path.abspath(config.get('pgpass') or os.path.join(os.path.expanduser('~'), 'pgpass'))
+        if os.path.exists(self._pgpass) and not os.path.isfile(self._pgpass):
+            raise PatroniFatalException("'{0}' exists and it's not a file, check your `postgresql.pgpass` configuration"
+                                        .format(self._pgpass))
+        self._passfile = None
+        self._passfile_mtime = None
+        self._synchronous_standby_names = None
+        self._postmaster_ctime = None
+        self._current_recovery_params: Optional[CaseInsensitiveDict] = None
+        self._config = {}
+        self._recovery_params = CaseInsensitiveDict()
+        self._server_parameters: CaseInsensitiveDict
+        self.reload_config(config)
+
+    def setup_server_parameters(self) -> None:
+        self._server_parameters = self.get_server_parameters(self._config)
+        self._adjust_recovery_parameters()
+
+    def try_to_create_dir(self, d: str, msg: str) -> None:
+        d = os.path.join(self._postgresql.data_dir, d)
+        if (not is_subpath(self._postgresql.data_dir, d) or not self._postgresql.data_directory_empty()):
+            validate_directory(d, msg)
+
+    def check_directories(self) -> None:
+        if "unix_socket_directories" in self._server_parameters:
+            for d in self._server_parameters["unix_socket_directories"].split(","):
+                self.try_to_create_dir(d.strip(), "'{}' is defined in unix_socket_directories, {}")
+        if "stats_temp_directory" in self._server_parameters:
+            self.try_to_create_dir(self._server_parameters["stats_temp_directory"],
+                                   "'{}' is defined in stats_temp_directory, {}")
+        if not self._krbsrvname:
+            self.try_to_create_dir(os.path.dirname(self._pgpass),
+                                   "'{}' is defined in `postgresql.pgpass`, {}")
 
     @property
-    def sysid(self):
-        if not self._sysid and not self.bootstrapping:
-            data = self.controldata()
-            self._sysid = data.get('Database system identifier', "")
-        return self._sysid
-
-    def get_postgres_role_from_data_directory(self):
-        if self.data_directory_empty() or not self.controldata():
-            return 'uninitialized'
-        elif self.config.recovery_conf_exists():
-            return 'replica'
-        else:
-            return 'master'
+    def config_dir(self) -> str:
+        return self._config_dir
 
     @property
-    def server_version(self):
-        return self._connection.server_version
-
-    def connection(self):
-        return self._connection.get()
+    def _configuration_to_save(self) -> List[str]:
+        configuration = [os.path.basename(self._postgresql_conf)]
+        if 'custom_conf' not in self._config:
+            configuration.append(os.path.basename(self._postgresql_base_conf_name))
+        if not self.hba_file:
+            configuration.append('pg_hba.conf')
+        if not self.ident_file:
+            configuration.append('pg_ident.conf')
+        return configuration
 
-    def set_connection_kwargs(self, kwargs):
-        self._connection.set_conn_kwargs(kwargs.copy())
-        self.citus_handler.set_conn_kwargs(kwargs.copy())
-
-    def _query(self, sql, *params):
-        """We are always using the same cursor, therefore this method is not thread-safe!!!
-        You can call it from different threads only if you are holding explicit `AsyncExecutor` lock,
-        because the main thread is always holding this lock when running HA cycle."""
-        cursor = None
-        try:
-            cursor = self._connection.cursor()
-            cursor.execute(sql, params or None)
-            return cursor
-        except psycopg.Error as e:
-            if cursor and cursor.connection.closed == 0:
-                # When connected via unix socket, psycopg2 can't recoginze 'connection lost'
-                # and leaves `_cursor_holder.connection.closed == 0`, but psycopg2.OperationalError
-                # is still raised (what is correct). It doesn't make sense to continiue with existing
-                # connection and we will close it, to avoid its reuse by the `cursor` method.
-                if isinstance(e, psycopg.OperationalError):
-                    self._connection.close()
-                else:
-                    raise e
-            if self.state == 'restarting':
-                raise RetryFailedError('cluster is being restarted')
-            raise PostgresConnectionException('connection problems')
-
-    def query(self, sql, *args, **kwargs):
-        if not kwargs.get('retry', True):
-            return self._query(sql, *args)
-        try:
-            return self.retry(self._query, sql, *args)
-        except RetryFailedError as e:
-            raise PostgresConnectionException(str(e))
-
-    def pg_control_exists(self):
-        return os.path.isfile(self._pg_control)
-
-    def data_directory_empty(self):
-        if self.pg_control_exists():
-            return False
-        return data_directory_is_empty(self._data_dir)
-
-    def replica_method_options(self, method):
-        return deepcopy(self.config.get(method, {}))
-
-    def replica_method_can_work_without_replication_connection(self, method):
-        return method != 'basebackup' and (self.replica_method_options(method).get('no_master') or
-                                           self.replica_method_options(method).get('no_leader'))
-
-    def can_create_replica_without_replication_connection(self, replica_methods=None):
-        """ go through the replication methods to see if there are ones
-            that does not require a working replication connection.
+    def save_configuration_files(self, check_custom_bootstrap: bool = False) -> bool:
         """
-        if replica_methods is None:
-            replica_methods = self.create_replica_methods
-        return any(self.replica_method_can_work_without_replication_connection(m) for m in replica_methods)
-
-    @property
-    def enforce_hot_standby_feedback(self):
-        return self._enforce_hot_standby_feedback
-
-    def set_enforce_hot_standby_feedback(self, value):
-        # If we enable or disable the hot_standby_feedback we need to update postgresql.conf and reload
-        if self._enforce_hot_standby_feedback != value:
-            self._enforce_hot_standby_feedback = value
-            if self.is_running():
-                self.config.write_postgresql_conf()
-                self.reload()
-
-    def reset_cluster_info_state(self, cluster, nofailover=None):
-        self._cluster_info_state = {}
-        if cluster and cluster.config and cluster.config.modify_index:
-            self._has_permanent_logical_slots =\
-                cluster.has_permanent_logical_slots(self.name, nofailover, self.major_version)
-
-            # We want to enable hot_standby_feedback if the replica is supposed
-            # to have a logical slot or in case if it is the cascading replica.
-            self.set_enforce_hot_standby_feedback(
-                self._has_permanent_logical_slots or
-                cluster.should_enforce_hot_standby_feedback(self.name, nofailover, self.major_version))
-
-            self._is_synchronous_mode = cluster.is_synchronous_mode()
-
-    def _cluster_info_state_get(self, name):
-        if not self._cluster_info_state:
+            copy postgresql.conf to postgresql.conf.backup to be able to retrieve configuration files
+            - originally stored as symlinks, those are normally skipped by pg_basebackup
+            - in case of WAL-E basebackup (see http://comments.gmane.org/gmane.comp.db.postgresql.wal-e/239)
+        """
+        if not (check_custom_bootstrap and self._postgresql.bootstrap.running_custom_bootstrap):
             try:
-                result = self._is_leader_retry(self._query, self.cluster_info_query).fetchone()
-                cluster_info_state = dict(zip(['timeline', 'wal_position', 'replayed_location',
-                                               'received_location', 'replay_paused', 'pg_control_timeline',
-                                               'received_tli', 'slot_name', 'conninfo', 'slots', 'synchronous_commit',
-                                               'synchronous_standby_names', 'pg_stat_replication'], result))
-                if self._has_permanent_logical_slots:
-                    cluster_info_state['slots'] =\
-                        self.slots_handler.process_permanent_slots(cluster_info_state['slots'])
-                self._cluster_info_state = cluster_info_state
-            except RetryFailedError as e:  # SELECT failed two times
-                self._cluster_info_state = {'error': str(e)}
-                if not self.is_starting() and self.pg_isready() == STATE_REJECT:
-                    self.set_state('starting')
-
-        if 'error' in self._cluster_info_state:
-            raise PostgresConnectionException(self._cluster_info_state['error'])
-
-        return self._cluster_info_state.get(name)
-
-    def replayed_location(self):
-        return self._cluster_info_state_get('replayed_location')
-
-    def received_location(self):
-        return self._cluster_info_state_get('received_location')
+                for f in self._configuration_to_save:
+                    config_file = os.path.join(self._config_dir, f)
+                    backup_file = os.path.join(self._postgresql.data_dir, f + '.backup')
+                    if os.path.isfile(config_file):
+                        shutil.copy(config_file, backup_file)
+            except IOError:
+                logger.exception('unable to create backup copies of configuration files')
+        return True
 
-    def slots(self):
-        return self._cluster_info_state_get('slots')
+    def restore_configuration_files(self) -> None:
+        """ restore a previously saved postgresql.conf """
+        try:
+            for f in self._configuration_to_save:
+                config_file = os.path.join(self._config_dir, f)
+                backup_file = os.path.join(self._postgresql.data_dir, f + '.backup')
+                if not os.path.isfile(config_file):
+                    if os.path.isfile(backup_file):
+                        shutil.copy(backup_file, config_file)
+                    # Previously we didn't backup pg_ident.conf, if file is missing just create empty
+                    elif f == 'pg_ident.conf':
+                        open(config_file, 'w').close()
+        except IOError:
+            logger.exception('unable to restore configuration files from backup')
+
+    def write_postgresql_conf(self, configuration: Optional[CaseInsensitiveDict] = None) -> None:
+        # rename the original configuration if it is necessary
+        if 'custom_conf' not in self._config and not os.path.exists(self._postgresql_base_conf):
+            os.rename(self._postgresql_conf, self._postgresql_base_conf)
+
+        configuration = configuration or self._server_parameters.copy()
+        # Due to the permanent logical replication slots configured we have to enable hot_standby_feedback
+        if self._postgresql.enforce_hot_standby_feedback:
+            configuration['hot_standby_feedback'] = 'on'
+
+        with ConfigWriter(self._postgresql_conf) as f:
+            include = self._config.get('custom_conf') or self._postgresql_base_conf_name
+            f.writeline("include '{0}'\n".format(ConfigWriter.escape(include)))
+            for name, value in sorted((configuration).items()):
+                value = transform_postgresql_parameter_value(self._postgresql.major_version, name, value,
+                                                             self._postgresql.available_gucs)
+                if value is not None and\
+                        (name != 'hba_file' or not self._postgresql.bootstrap.running_custom_bootstrap):
+                    f.write_param(name, value)
+            # when we are doing custom bootstrap we assume that we don't know superuser password
+            # and in order to be able to change it, we are opening trust access from a certain address
+            # therefore we need to make sure that hba_file is not overridden
+            # after changing superuser password we will "revert" all these "changes"
+            if self._postgresql.bootstrap.running_custom_bootstrap or 'hba_file' not in self._server_parameters:
+                f.write_param('hba_file', self._pg_hba_conf)
+            if 'ident_file' not in self._server_parameters:
+                f.write_param('ident_file', self._pg_ident_conf)
+
+            if self._postgresql.major_version >= 120000:
+                if self._recovery_params:
+                    f.writeline('\n# recovery.conf')
+                    self._write_recovery_params(f, self._recovery_params)
+
+                if not self._postgresql.bootstrap.keep_existing_recovery_conf:
+                    self._sanitize_auto_conf()
+
+    def append_pg_hba(self, config: List[str]) -> bool:
+        if not self.hba_file and not self._config.get('pg_hba'):
+            with open(self._pg_hba_conf, 'a') as f:
+                f.write('\n{}\n'.format('\n'.join(config)))
+        return True
 
-    def primary_slot_name(self):
-        return self._cluster_info_state_get('slot_name')
+    def replace_pg_hba(self) -> Optional[bool]:
+        """
+        Replace pg_hba.conf content in the PGDATA if hba_file is not defined in the
+        `postgresql.parameters` and pg_hba is defined in `postgresql` configuration section.
 
-    def primary_conninfo(self):
-        return self._cluster_info_state_get('conninfo')
+        :returns: True if pg_hba.conf was rewritten.
+        """
 
-    def received_timeline(self):
-        return self._cluster_info_state_get('received_tli')
+        # when we are doing custom bootstrap we assume that we don't know superuser password
+        # and in order to be able to change it, we are opening trust access from a certain address
+        if self._postgresql.bootstrap.running_custom_bootstrap:
+            addresses = {} if os.name == 'nt' else {'': 'local'}  # windows doesn't yet support unix-domain sockets
+            if 'host' in self.local_replication_address and not self.local_replication_address['host'].startswith('/'):
+                addresses.update({sa[0] + '/32': 'host' for _, _, _, _, sa in socket.getaddrinfo(
+                                  self.local_replication_address['host'], self.local_replication_address['port'],
+                                  0, socket.SOCK_STREAM, socket.IPPROTO_TCP)})
+
+            with ConfigWriter(self._pg_hba_conf) as f:
+                for address, t in addresses.items():
+                    f.writeline((
+                        '{0}\treplication\t{1}\t{3}\ttrust\n'
+                        '{0}\tall\t{2}\t{3}\ttrust'
+                    ).format(t, self.replication['username'], self._superuser.get('username') or 'all', address))
+        elif not self.hba_file and self._config.get('pg_hba'):
+            with ConfigWriter(self._pg_hba_conf) as f:
+                f.writelines(self._config['pg_hba'])
+            return True
 
-    def synchronous_commit(self):
-        return self._cluster_info_state_get('synchronous_commit')
+    def replace_pg_ident(self) -> Optional[bool]:
+        """
+        Replace pg_ident.conf content in the PGDATA if ident_file is not defined in the
+        `postgresql.parameters` and pg_ident is defined in the `postgresql` section.
 
-    def synchronous_standby_names(self):
-        return self._cluster_info_state_get('synchronous_standby_names')
+        :returns: True if pg_ident.conf was rewritten.
+        """
 
-    def pg_stat_replication(self):
-        return self._cluster_info_state_get('pg_stat_replication') or []
+        if not self.ident_file and self._config.get('pg_ident'):
+            with ConfigWriter(self._pg_ident_conf) as f:
+                f.writelines(self._config['pg_ident'])
+            return True
 
-    def is_leader(self):
-        try:
-            return bool(self._cluster_info_state_get('timeline'))
-        except PostgresConnectionException:
-            logger.warning('Failed to determine PostgreSQL state from the connection, falling back to cached role')
-            return bool(self.is_running() and self.role in ('master', 'primary'))
-
-    def replay_paused(self):
-        return self._cluster_info_state_get('replay_paused')
-
-    def resume_wal_replay(self):
-        self._query('SELECT pg_catalog.pg_{0}_replay_resume()'.format(self.wal_name))
-
-    def handle_parameter_change(self):
-        if self.major_version >= 140000 and not self.is_starting() and self.replay_paused():
-            logger.info('Resuming paused WAL replay for PostgreSQL 14+')
-            self.resume_wal_replay()
+    def primary_conninfo_params(self, member: Union[Leader, Member, None]) -> Optional[Dict[str, Any]]:
+        if not member or not member.conn_url or member.name == self._postgresql.name:
+            return None
+        ret = member.conn_kwargs(self.replication)
+        ret['application_name'] = self._postgresql.name
+        ret.setdefault('sslmode', 'prefer')
+        if self._postgresql.major_version >= 120000:
+            ret.setdefault('gssencmode', 'prefer')
+        if self._postgresql.major_version >= 130000:
+            ret.setdefault('channel_binding', 'prefer')
+        if self._krbsrvname:
+            ret['krbsrvname'] = self._krbsrvname
+        if 'dbname' in ret:
+            del ret['dbname']
+        return ret
 
-    def pg_control_timeline(self):
-        try:
+    def format_dsn(self, params: Dict[str, Any], include_dbname: bool = False) -> str:
+        # A list of keywords that can be found in a conninfo string. Follows what is acceptable by libpq
+        keywords = ('dbname', 'user', 'passfile' if params.get('passfile') else 'password', 'host', 'port',
+                    'sslmode', 'sslcompression', 'sslcert', 'sslkey', 'sslpassword', 'sslrootcert', 'sslcrl',
+                    'sslcrldir', 'application_name', 'krbsrvname', 'gssencmode', 'channel_binding',
+                    'target_session_attrs')
+        if include_dbname:
+            params = params.copy()
+            if 'dbname' not in params:
+                params['dbname'] = self._postgresql.database
+            # we are abusing information about the necessity of dbname
+            # dsn should contain passfile or password only if there is no dbname in it (it is used in recovery.conf)
+            skip = {'passfile', 'password'}
+        else:
+            skip = {'dbname'}
 
-            return int(self.controldata().get("Latest checkpoint's TimeLineID"))
-        except (TypeError, ValueError):
-            logger.exception('Failed to parse timeline from pg_controldata output')
-
-    def parse_wal_record(self, timeline, lsn):
-        out, err = self.waldump(timeline, lsn, 1)
-        if out and not err:
-            match = re.match(r'^rmgr:\s+(.+?)\s+len \(rec/tot\):\s+\d+/\s+\d+, tx:\s+\d+, '
-                             r'lsn: ([0-9A-Fa-f]+/[0-9A-Fa-f]+), prev ([0-9A-Fa-f]+/[0-9A-Fa-f]+), '
-                             r'.*?desc: (.+)', out.decode('utf-8'))
-            if match:
-                return match.groups()
-        return None, None, None, None
-
-    def latest_checkpoint_location(self):
-        """Returns checkpoint location for the cleanly shut down primary.
-           But, if we know that the checkpoint was written to the new WAL
-           due to the archive_mode=on, we will return the LSN of prev wal record (SWITCH)."""
-
-        data = self.controldata()
-        timeline = data.get("Latest checkpoint's TimeLineID")
-        lsn = checkpoint_lsn = data.get('Latest checkpoint location')
-        if data.get('Database cluster state') == 'shut down' and lsn and timeline:
-            try:
-                checkpoint_lsn = parse_lsn(checkpoint_lsn)
-                rm_name, lsn, prev, desc = self.parse_wal_record(timeline, lsn)
-                desc = desc.strip().lower()
-                if rm_name == 'XLOG' and parse_lsn(lsn) == checkpoint_lsn and prev and\
-                        desc.startswith('checkpoint') and desc.endswith('shutdown'):
-                    _, lsn, _, desc = self.parse_wal_record(timeline, prev)
-                    prev = parse_lsn(prev)
-                    # If the cluster is shutdown with archive_mode=on, WAL is switched before writing the checkpoint.
-                    # In this case we want to take the LSN of previous record (switch) as the last known WAL location.
-                    if parse_lsn(lsn) == prev and desc.strip() in ('xlog switch', 'SWITCH'):
-                        return prev
-            except Exception as e:
-                logger.error('Exception when parsing WAL pg_%sdump output: %r', self.wal_name, e)
-            if isinstance(checkpoint_lsn, int):
-                return checkpoint_lsn
-
-    def is_running(self):
-        """Returns PostmasterProcess if one is running on the data directory or None. If most recently seen process
-        is running updates the cached process based on pid file."""
-        if self._postmaster_proc:
-            if self._postmaster_proc.is_running():
-                return self._postmaster_proc
-            self._postmaster_proc = None
+        def escape(value: Any) -> str:
+            return re.sub(r'([\'\\ ])', r'\\\1', str(value))
 
-        # we noticed that postgres was restarted, force syncing of replication slots and check of logical slots
-        self.slots_handler.schedule()
+        return ' '.join('{0}={1}'.format(kw, escape(params[kw])) for kw in keywords
+                        if kw not in skip and params.get(kw) is not None)
 
-        self._postmaster_proc = PostmasterProcess.from_pidfile(self._data_dir)
-        return self._postmaster_proc
+    def _write_recovery_params(self, fd: ConfigWriter, recovery_params: CaseInsensitiveDict) -> None:
+        if self._postgresql.major_version >= 90500:
+            pause_at_recovery_target = parse_bool(recovery_params.pop('pause_at_recovery_target', None))
+            if pause_at_recovery_target is not None:
+                recovery_params.setdefault('recovery_target_action', 'pause' if pause_at_recovery_target else 'promote')
+        else:
+            if str(recovery_params.pop('recovery_target_action', None)).lower() == 'promote':
+                recovery_params.setdefault('pause_at_recovery_target', 'false')
+        for name, value in sorted(recovery_params.items()):
+            if name == 'primary_conninfo':
+                if 'password' in value and self._postgresql.major_version >= 100000:
+                    self.write_pgpass(value)
+                    value['passfile'] = self._passfile = self._pgpass
+                    self._passfile_mtime = mtime(self._pgpass)
+                value = self.format_dsn(value)
+            else:
+                value = transform_recovery_parameter_value(self._postgresql.major_version, name, value,
+                                                           self._postgresql.available_gucs)
+                if value is None:
+                    continue
+            fd.write_param(name, value)
+
+    def build_recovery_params(self, member: Union[Leader, Member, None]) -> CaseInsensitiveDict:
+        recovery_params = CaseInsensitiveDict({p: v for p, v in (self.get('recovery_conf') or {}).items()
+                                               if not p.lower().startswith('recovery_target')
+                                               and p.lower() not in ('primary_conninfo', 'primary_slot_name')})
+        recovery_params.update({'standby_mode': 'on', 'recovery_target_timeline': 'latest'})
+        if self._postgresql.major_version >= 120000:
+            # on pg12 we want to protect from following params being set in one of included files
+            # not doing so might result in a standby being paused, promoted or shutted down.
+            recovery_params.update({'recovery_target': '', 'recovery_target_name': '', 'recovery_target_time': '',
+                                    'recovery_target_xid': '', 'recovery_target_lsn': ''})
+
+        is_remote_member = isinstance(member, RemoteMember)
+        primary_conninfo = self.primary_conninfo_params(member)
+        if primary_conninfo:
+            use_slots = self.get('use_slots', True) and self._postgresql.major_version >= 90400
+            if use_slots and not (is_remote_member and member.no_replication_slot):
+                primary_slot_name = member.primary_slot_name if is_remote_member else self._postgresql.name
+                recovery_params['primary_slot_name'] = slot_name_from_member_name(primary_slot_name)
+                # We are a standby leader and are using a replication slot. Make sure we connect to
+                # the leader of the main cluster (in case more than one host is specified in the
+                # connstr) by adding 'target_session_attrs=read-write' to primary_conninfo.
+                if is_remote_member and 'target_sesions_attrs' not in primary_conninfo and\
+                        self._postgresql.major_version >= 100000:
+                    primary_conninfo['target_session_attrs'] = 'read-write'
+            recovery_params['primary_conninfo'] = primary_conninfo
+
+        # standby_cluster config might have different parameters, we want to override them
+        standby_cluster_params = ['restore_command', 'archive_cleanup_command']\
+            + (['recovery_min_apply_delay'] if is_remote_member else [])
+        recovery_params.update({p: member.data.get(p) for p in standby_cluster_params if member and member.data.get(p)})
+        return recovery_params
+
+    def recovery_conf_exists(self) -> bool:
+        if self._postgresql.major_version >= 120000:
+            return os.path.exists(self._standby_signal) or os.path.exists(self._recovery_signal)
+        return os.path.exists(self._recovery_conf)
 
     @property
-    def cb_called(self):
-        return self.__cb_called
-
-    def call_nowait(self, cb_type: CallbackAction) -> None:
-        """pick a callback command and call it without waiting for it to finish """
-        if self.bootstrapping:
-            return
-        if cb_type in (CallbackAction.ON_START, CallbackAction.ON_STOP,
-                       CallbackAction.ON_RESTART, CallbackAction.ON_ROLE_CHANGE):
-            self.__cb_called = True
-
-        if self.callback and cb_type in self.callback:
-            cmd = self.callback[cb_type]
-            role = 'master' if self.role == 'promoted' else self.role
-            try:
-                cmd = shlex.split(self.callback[cb_type]) + [cb_type, role, self.scope]
-                self._callback_executor.call(cmd)
-            except Exception:
-                logger.exception('callback %s %r %s %s failed', cmd, cb_type, role, self.scope)
+    def triggerfile_good_name(self) -> str:
+        return 'trigger_file' if self._postgresql.major_version < 120000 else 'promote_trigger_file'
 
     @property
-    def role(self):
-        with self._role_lock:
-            return self._role
-
-    def set_role(self, value):
-        with self._role_lock:
-            self._role = value
+    def _triggerfile_wrong_name(self) -> str:
+        return 'trigger_file' if self._postgresql.major_version >= 120000 else 'promote_trigger_file'
 
     @property
-    def state(self):
-        with self._state_lock:
-            return self._state
-
-    def set_state(self, value):
-        with self._state_lock:
-            self._state = value
-            self._state_entry_timestamp = time.time()
-
-    def time_in_state(self):
-        return time.time() - self._state_entry_timestamp
-
-    def is_starting(self):
-        return self.state == 'starting'
-
-    def wait_for_port_open(self, postmaster, timeout):
-        """Waits until PostgreSQL opens ports."""
-        for _ in polling_loop(timeout):
-            if self.cancellable.is_cancelled:
-                return False
-
-            if not postmaster.is_running():
-                logger.error('postmaster is not running')
-                self.set_state('start failed')
-                return False
-
-            isready = self.pg_isready()
-            if isready != STATE_NO_RESPONSE:
-                if isready not in [STATE_REJECT, STATE_RUNNING]:
-                    logger.warning("Can't determine PostgreSQL startup status, assuming running")
-                return True
-
-        logger.warning("Timed out waiting for PostgreSQL to start")
-        return False
-
-    def start(self, timeout=None, task=None, block_callbacks=False, role=None, after_start=None):
-        """Start PostgreSQL
-
-        Waits for postmaster to open ports or terminate so pg_isready can be used to check startup completion
-        or failure.
-
-        :returns: True if start was initiated and postmaster ports are open,
-                  False if start failed, and None if postgres is still starting up"""
-        # make sure we close all connections established against
-        # the former node, otherwise, we might get a stalled one
-        # after kill -9, which would report incorrect data to
-        # patroni.
-        self._connection.close()
-
-        if self.is_running():
-            logger.error('Cannot start PostgreSQL because one is already running.')
-            self.set_state('starting')
-            return True
-
-        if not block_callbacks:
-            self.__cb_pending = CallbackAction.ON_START
-
-        self.set_role(role or self.get_postgres_role_from_data_directory())
-
-        self.set_state('starting')
-        self._pending_restart = False
+    def _recovery_parameters_to_compare(self) -> CaseInsensitiveSet:
+        skip_params = CaseInsensitiveSet({'pause_at_recovery_target', 'recovery_target_inclusive',
+                                          'recovery_target_action', 'standby_mode', self._triggerfile_wrong_name})
+        return CaseInsensitiveSet(self._RECOVERY_PARAMETERS - skip_params)
+
+    def _read_recovery_params(self) -> Tuple[Optional[CaseInsensitiveDict], Optional[bool]]:
+        if self._postgresql.is_starting():
+            return None, False
+
+        pg_conf_mtime = mtime(self._postgresql_conf)
+        auto_conf_mtime = mtime(self._auto_conf)
+        passfile_mtime = mtime(self._passfile) if self._passfile else False
+        postmaster_ctime = self._postgresql.is_running()
+        if postmaster_ctime:
+            postmaster_ctime = postmaster_ctime.create_time()
+
+        if self._postgresql_conf_mtime == pg_conf_mtime and self._auto_conf_mtime == auto_conf_mtime \
+                and self._passfile_mtime == passfile_mtime and self._postmaster_ctime == postmaster_ctime:
+            return None, False
 
         try:
-            if not self.ensure_major_version_is_known():
-                return None
-            configuration = self.config.effective_configuration
+            values = self._get_pg_settings(self._recovery_parameters_to_compare).values()
+            values = CaseInsensitiveDict({p[0]: [p[1], p[4] == 'postmaster', p[5]] for p in values})
+            self._postgresql_conf_mtime = pg_conf_mtime
+            self._auto_conf_mtime = auto_conf_mtime
+            self._postmaster_ctime = postmaster_ctime
         except Exception:
-            return None
+            values = None
+        return values, True
 
-        self.config.check_directories()
-        self.config.write_postgresql_conf(configuration)
-        self.config.resolve_connection_addresses()
-        self.config.replace_pg_hba()
-        self.config.replace_pg_ident()
-
-        options = ['--{0}={1}'.format(p, configuration[p]) for p in self.config.CMDLINE_OPTIONS
-                   if p in configuration and p not in ('wal_keep_segments', 'wal_keep_size')]
+    def _read_recovery_params_pre_v12(self) -> Tuple[Optional[CaseInsensitiveDict], Optional[bool]]:
+        recovery_conf_mtime = mtime(self._recovery_conf)
+        passfile_mtime = mtime(self._passfile) if self._passfile else False
+        if recovery_conf_mtime == self._recovery_conf_mtime and passfile_mtime == self._passfile_mtime:
+            return None, False
+
+        values = CaseInsensitiveDict()
+        with open(self._recovery_conf, 'r') as f:
+            for line in f:
+                line = line.strip()
+                if not line or line.startswith('#'):
+                    continue
+                value = None
+                match = PARAMETER_RE.match(line)
+                if match:
+                    value = read_recovery_param_value(line[match.end():])
+                if match is None or value is None:
+                    return None, True
+                values[match.group(1)] = [value, True]
+            self._recovery_conf_mtime = recovery_conf_mtime
+        values.setdefault('recovery_min_apply_delay', ['0', True])
+        values['recovery_min_apply_delay'][0] = parse_int(values['recovery_min_apply_delay'][0], 'ms')
+        values.update({param: ['', True] for param in self._recovery_parameters_to_compare if param not in values})
+        return values, True
+
+    def _check_passfile(self, passfile: str, wanted_primary_conninfo: Dict[str, Any]) -> bool:
+        # If there is a passfile in the primary_conninfo try to figure out that
+        # the passfile contains the line(s) allowing connection to the given node.
+        # We assume that the passfile was created by Patroni and therefore doing
+        # the full match and not covering cases when host, port or user are set to '*'
+        passfile_mtime = mtime(passfile)
+        if passfile_mtime:
+            try:
+                with open(passfile) as f:
+                    wanted_lines = (self._pgpass_line(wanted_primary_conninfo) or '').splitlines()
+                    file_lines = f.read().splitlines()
+                    if set(wanted_lines) == set(file_lines):
+                        self._passfile = passfile
+                        self._passfile_mtime = passfile_mtime
+                        return True
+            except Exception:
+                logger.info('Failed to read %s', passfile)
+        return False
 
-        if self.cancellable.is_cancelled:
+    def _check_primary_conninfo(self, primary_conninfo: Dict[str, Any],
+                                wanted_primary_conninfo: Dict[str, Any]) -> bool:
+        # first we will cover corner cases, when we are replicating from somewhere while shouldn't
+        # or there is no primary_conninfo but we should replicate from some specific node.
+        if not wanted_primary_conninfo:
+            return not primary_conninfo
+        elif not primary_conninfo:
             return False
 
-        with task or null_context():
-            if task and task.is_cancelled:
-                logger.info("PostgreSQL start cancelled.")
+        if not self._postgresql.is_starting():
+            wal_receiver_primary_conninfo = self._postgresql.primary_conninfo()
+            if wal_receiver_primary_conninfo:
+                wal_receiver_primary_conninfo = parse_dsn(wal_receiver_primary_conninfo)
+                # when wal receiver is alive use primary_conninfo from pg_stat_wal_receiver for comparison
+                if wal_receiver_primary_conninfo:
+                    primary_conninfo = wal_receiver_primary_conninfo
+                    # There could be no password in the primary_conninfo or it is masked.
+                    # Just copy the "desired" value in order to make comparison succeed.
+                    if 'password' in wanted_primary_conninfo:
+                        primary_conninfo['password'] = wanted_primary_conninfo['password']
+
+        if 'passfile' in primary_conninfo and 'password' not in primary_conninfo \
+                and 'password' in wanted_primary_conninfo:
+            if self._check_passfile(primary_conninfo['passfile'], wanted_primary_conninfo):
+                primary_conninfo['password'] = wanted_primary_conninfo['password']
+            else:
                 return False
 
-            self._postmaster_proc = PostmasterProcess.start(self.pgcommand('postgres'),
-                                                            self._data_dir,
-                                                            self.config.postgresql_conf,
-                                                            options)
-
-            if task:
-                task.complete(self._postmaster_proc)
-
-        start_timeout = timeout
-        if not start_timeout:
-            try:
-                start_timeout = float(self.config.get('pg_ctl_timeout', 60))
-            except ValueError:
-                start_timeout = 60
-
-        # We want postmaster to open ports before we continue
-        if not self._postmaster_proc or not self.wait_for_port_open(self._postmaster_proc, start_timeout):
-            return False
-
-        ret = self.wait_for_startup(start_timeout)
-        if ret is not None:
-            if ret and after_start:
-                after_start()
-            return ret
-        elif timeout is not None:
-            return False
-        else:
-            return None
+        return all(str(primary_conninfo.get(p)) == str(v) for p, v in wanted_primary_conninfo.items() if v is not None)
 
-    def checkpoint(self, connect_kwargs=None, timeout=None):
-        check_not_is_in_recovery = connect_kwargs is not None
-        connect_kwargs = connect_kwargs or self.config.local_connect_kwargs
-        for p in ['connect_timeout', 'options']:
-            connect_kwargs.pop(p, None)
-        if timeout:
-            connect_kwargs['connect_timeout'] = timeout
-        try:
-            with get_connection_cursor(**connect_kwargs) as cur:
-                cur.execute("SET statement_timeout = 0")
-                if check_not_is_in_recovery:
-                    cur.execute('SELECT pg_catalog.pg_is_in_recovery()')
-                    if cur.fetchone()[0]:
-                        return 'is_in_recovery=true'
-                cur.execute('CHECKPOINT')
-        except psycopg.Error:
-            logger.exception('Exception during CHECKPOINT')
-            return 'not accessible or not healty'
-
-    def stop(self, mode='fast', block_callbacks=False, checkpoint=None,
-             on_safepoint=None, on_shutdown=None, before_shutdown=None, stop_timeout=None):
-        """Stop PostgreSQL
-
-        Supports a callback when a safepoint is reached. A safepoint is when no user backend can return a successful
-        commit to users. Currently this means we wait for user backends to close. But in the future alternate mechanisms
-        could be added.
-
-        :param on_safepoint: This callback is called when no user backends are running.
-        :param on_shutdown: is called when pg_controldata starts reporting `Database cluster state: shut down`
-        :param before_shutdown: is called after running optional CHECKPOINT and before running pg_ctl stop
-        """
-        if checkpoint is None:
-            checkpoint = False if mode == 'immediate' else True
+    def check_recovery_conf(self, member: Union[Leader, Member, None]) -> Tuple[bool, bool]:
+        """Returns a tuple. The first boolean element indicates that recovery params don't match
+           and the second is set to `True` if the restart is required in order to apply new values"""
+
+        # TODO: recovery.conf could be stale, would be nice to detect that.
+        if self._postgresql.major_version >= 120000:
+            if not os.path.exists(self._standby_signal):
+                return True, True
 
-        success, pg_signaled = self._do_stop(mode, block_callbacks, checkpoint, on_safepoint,
-                                             on_shutdown, before_shutdown, stop_timeout)
-        if success:
-            # block_callbacks is used during restart to avoid
-            # running start/stop callbacks in addition to restart ones
-            if not block_callbacks:
-                self.set_state('stopped')
-                if pg_signaled:
-                    self.call_nowait(CallbackAction.ON_STOP)
+            _read_recovery_params = self._read_recovery_params
         else:
-            logger.warning('pg_ctl stop failed')
-            self.set_state('stop failed')
-        return success
-
-    def _do_stop(self, mode, block_callbacks, checkpoint, on_safepoint, on_shutdown, before_shutdown, stop_timeout):
-        postmaster = self.is_running()
-        if not postmaster:
-            if on_safepoint:
-                on_safepoint()
-            return True, False
-
-        if checkpoint and not self.is_starting():
-            self.checkpoint(timeout=stop_timeout)
-
-        if not block_callbacks:
-            self.set_state('stopping')
-
-        if before_shutdown:
-            before_shutdown()
-
-        # Send signal to postmaster to stop
-        success = postmaster.signal_stop(mode, self.pgcommand('pg_ctl'))
-        if success is not None:
-            if success and on_safepoint:
-                on_safepoint()
-            return success, True
-
-        # We can skip safepoint detection if we don't have a callback
-        if on_safepoint:
-            # Wait for our connection to terminate so we can be sure that no new connections are being initiated
-            self._wait_for_connection_close(postmaster)
-            postmaster.wait_for_user_backends_to_close(stop_timeout)
-            on_safepoint()
-
-        if on_shutdown and mode in ('fast', 'smart'):
-            i = 0
-            # Wait for pg_controldata `Database cluster state:` to change to "shut down"
-            while postmaster.is_running():
-                data = self.controldata()
-                if data.get('Database cluster state', '') == 'shut down':
-                    on_shutdown(self.latest_checkpoint_location())
-                    break
-                elif data.get('Database cluster state', '').startswith('shut down'):  # shut down in recovery
-                    break
-                elif stop_timeout and i >= stop_timeout:
-                    stop_timeout = 0
-                    break
-                time.sleep(STOP_POLLING_INTERVAL)
-                i += STOP_POLLING_INTERVAL
-
-        try:
-            postmaster.wait(timeout=stop_timeout)
-        except TimeoutExpired:
-            logger.warning("Timeout during postmaster stop, aborting Postgres.")
-            if not self.terminate_postmaster(postmaster, mode, stop_timeout):
-                postmaster.wait()
-
-        return True, True
-
-    def terminate_postmaster(self, postmaster, mode, stop_timeout):
-        if mode in ['fast', 'smart']:
-            try:
-                success = postmaster.signal_stop('immediate', self.pgcommand('pg_ctl'))
-                if success:
-                    return True
-                postmaster.wait(timeout=stop_timeout)
-                return True
-            except TimeoutExpired:
-                pass
-        logger.warning("Sending SIGKILL to Postmaster and its children")
-        return postmaster.signal_kill()
-
-    def terminate_starting_postmaster(self, postmaster):
-        """Terminates a postmaster that has not yet opened ports or possibly even written a pid file. Blocks
-        until the process goes away."""
-        postmaster.signal_stop('immediate', self.pgcommand('pg_ctl'))
-        postmaster.wait()
+            if not self.recovery_conf_exists():
+                return True, True
 
-    def _wait_for_connection_close(self, postmaster):
-        try:
-            with self.connection().cursor() as cur:
-                while postmaster.is_running():  # Need a timeout here?
-                    cur.execute("SELECT 1")
-                    time.sleep(STOP_POLLING_INTERVAL)
-        except psycopg.Error:
-            pass
-
-    def reload(self, block_callbacks=False):
-        ret = self.pg_ctl('reload')
-        if ret and not block_callbacks:
-            self.call_nowait(CallbackAction.ON_RELOAD)
-        return ret
+            _read_recovery_params = self._read_recovery_params_pre_v12
 
-    def check_for_startup(self):
-        """Checks PostgreSQL status and returns if PostgreSQL is in the middle of startup."""
-        return self.is_starting() and not self.check_startup_state_changed()
+        params, updated = _read_recovery_params()
+        # updated indicates that mtime of postgresql.conf, postgresql.auto.conf, or recovery.conf
+        # was changed and params were read either from the config or from the database connection.
+        if updated:
+            if params is None:  # exception or unparsable config
+                return True, True
+
+            # We will cache parsed value until the next config change.
+            self._current_recovery_params = params
+            primary_conninfo = params['primary_conninfo']
+            if primary_conninfo[0]:
+                primary_conninfo[0] = parse_dsn(params['primary_conninfo'][0])
+                # If we failed to parse non-empty connection string this indicates that config if broken.
+                if not primary_conninfo[0]:
+                    return True, True
+            else:  # empty string, primary_conninfo is not in the config
+                primary_conninfo[0] = {}
+
+        if not self._postgresql.is_starting() and self._current_recovery_params:
+            # when wal receiver is alive take primary_slot_name from pg_stat_wal_receiver
+            wal_receiver_primary_slot_name = self._postgresql.primary_slot_name()
+            if not wal_receiver_primary_slot_name and self._postgresql.primary_conninfo():
+                wal_receiver_primary_slot_name = ''
+            if wal_receiver_primary_slot_name is not None:
+                self._current_recovery_params['primary_slot_name'][0] = wal_receiver_primary_slot_name
+
+        # Increment the 'reload' to enforce write of postgresql.conf when joining the running postgres
+        required = {'restart': 0,
+                    'reload': int(self._postgresql.major_version >= 120000
+                                  and not self._postgresql.cb_called
+                                  and not self._postgresql.is_starting())}
+
+        def record_missmatch(mtype: bool) -> None:
+            required['restart' if mtype else 'reload'] += 1
+
+        wanted_recovery_params = self.build_recovery_params(member)
+        for param, value in (self._current_recovery_params or {}).items():
+            # Skip certain parameters defined in the included postgres config files
+            # if we know that they are not specified in the patroni configuration.
+            if len(value) > 2 and value[2] not in (self._postgresql_conf, self._auto_conf) and \
+                    param in ('archive_cleanup_command', 'promote_trigger_file', 'recovery_end_command',
+                              'recovery_min_apply_delay', 'restore_command') and param not in wanted_recovery_params:
+                continue
+            if param == 'recovery_min_apply_delay':
+                if not compare_values('integer', 'ms', value[0], wanted_recovery_params.get(param, 0)):
+                    record_missmatch(value[1])
+            elif param == 'standby_mode':
+                if not compare_values('bool', None, value[0], wanted_recovery_params.get(param, 'on')):
+                    record_missmatch(value[1])
+            elif param == 'primary_conninfo':
+                if not self._check_primary_conninfo(value[0], wanted_recovery_params.get('primary_conninfo', {})):
+                    record_missmatch(value[1])
+            elif (param != 'primary_slot_name' or wanted_recovery_params.get('primary_conninfo')) \
+                    and str(value[0]) != str(wanted_recovery_params.get(param, '')):
+                record_missmatch(value[1])
+        return required['restart'] + required['reload'] > 0, required['restart'] > 0
 
-    def check_startup_state_changed(self):
-        """Checks if PostgreSQL has completed starting up or failed or still starting.
+    @staticmethod
+    def _remove_file_if_exists(name: str) -> None:
+        if os.path.isfile(name) or os.path.islink(name):
+            os.unlink(name)
 
-        Should only be called when state == 'starting'
+    @staticmethod
+    def _pgpass_line(record: Dict[str, Any]) -> Optional[str]:
+        if 'password' in record:
+            def escape(value: Any) -> str:
+                return re.sub(r'([:\\])', r'\\\1', str(value))
+
+            record = {n: escape(record.get(n) or '*') for n in ('host', 'port', 'user', 'password')}
+            # 'host' could be several comma-separated hostnames, in this case
+            # we need to write on pgpass line per host
+            line = ''
+            for hostname in record['host'].split(','):
+                line += hostname + ':{port}:*:{user}:{password}'.format(**record) + '\n'
+            return line.rstrip()
+
+    def write_pgpass(self, record: Dict[str, Any]) -> Dict[str, str]:
+        line = self._pgpass_line(record)
+        if not line:
+            return os.environ.copy()
+
+        with open(self._pgpass, 'w') as f:
+            os.chmod(self._pgpass, stat.S_IWRITE | stat.S_IREAD)
+            f.write(line)
+
+        return {**os.environ, 'PGPASSFILE': self._pgpass}
+
+    def write_recovery_conf(self, recovery_params: CaseInsensitiveDict) -> None:
+        self._recovery_params = recovery_params
+        if self._postgresql.major_version >= 120000:
+            if parse_bool(recovery_params.pop('standby_mode', None)):
+                open(self._standby_signal, 'w').close()
+            else:
+                self._remove_file_if_exists(self._standby_signal)
+                open(self._recovery_signal, 'w').close()
 
-        :returns: True if state was changed from 'starting'
-        """
-        ready = self.pg_isready()
+            def restart_required(name: str) -> bool:
+                if self._postgresql.major_version >= 140000:
+                    return False
+                return name == 'restore_command' or (self._postgresql.major_version < 130000
+                                                     and name in ('primary_conninfo', 'primary_slot_name'))
 
-        if ready == STATE_REJECT:
-            return False
-        elif ready == STATE_NO_RESPONSE:
-            ret = not self.is_running()
-            if ret:
-                self.set_state('start failed')
-                self.slots_handler.schedule(False)  # TODO: can remove this?
-                self.config.save_configuration_files(True)  # TODO: maybe remove this?
-            return ret
+            self._current_recovery_params = CaseInsensitiveDict({n: [v, restart_required(n), self._postgresql_conf]
+                                                                 for n, v in recovery_params.items()})
         else:
-            if ready != STATE_RUNNING:
-                # Bad configuration or unexpected OS error. No idea of PostgreSQL status.
-                # Let the main loop of run cycle clean up the mess.
-                logger.warning("%s status returned from pg_isready",
-                               "Unknown" if ready == STATE_UNKNOWN else "Invalid")
-            self.set_state('running')
-            self.slots_handler.schedule()
-            self.config.save_configuration_files(True)
-            # TODO: __cb_pending can be None here after PostgreSQL restarts on its own. Do we want to call the callback?
-            # Previously we didn't even notice.
-            action = self.__cb_pending or CallbackAction.ON_START
-            self.call_nowait(action)
-            self.__cb_pending = None
-
-            return True
-
-    def wait_for_startup(self, timeout=None):
-        """Waits for PostgreSQL startup to complete or fail.
-
-        :returns: True if start was successful, False otherwise"""
-        if not self.is_starting():
-            # Should not happen
-            logger.warning("wait_for_startup() called when not in starting state")
+            with ConfigWriter(self._recovery_conf) as f:
+                os.chmod(self._recovery_conf, stat.S_IWRITE | stat.S_IREAD)
+                self._write_recovery_params(f, recovery_params)
+
+    def remove_recovery_conf(self) -> None:
+        for name in (self._recovery_conf, self._standby_signal, self._recovery_signal):
+            self._remove_file_if_exists(name)
+        self._recovery_params = CaseInsensitiveDict()
+        self._current_recovery_params = None
+
+    def _sanitize_auto_conf(self) -> None:
+        overwrite = False
+        lines: List[str] = []
 
-        while not self.check_startup_state_changed():
-            if self.cancellable.is_cancelled or timeout and self.time_in_state() > timeout:
-                return None
-            time.sleep(1)
-
-        return self.state == 'running'
-
-    def restart(self, timeout=None, task=None, block_callbacks=False,
-                role=None, before_shutdown=None, after_start=None):
-        """Restarts PostgreSQL.
-
-        When timeout parameter is set the call will block either until PostgreSQL has started, failed to start or
-        timeout arrives.
-
-        :returns: True when restart was successful and timeout did not expire when waiting.
-        """
-        self.set_state('restarting')
-        if not block_callbacks:
-            self.__cb_pending = CallbackAction.ON_RESTART
-        ret = self.stop(block_callbacks=True, before_shutdown=before_shutdown)\
-            and self.start(timeout, task, True, role, after_start)
-        if not ret and not self.is_starting():
-            self.set_state('restart failed ({0})'.format(self.state))
-        return ret
-
-    def is_healthy(self):
-        if not self.is_running():
-            logger.warning('Postgresql is not running.')
-            return False
-        return True
-
-    def get_guc_value(self, name):
-        cmd = [self.pgcommand('postgres'), '-D', self._data_dir, '-C', name,
-               '--config-file={}'.format(self.config.postgresql_conf)]
-        try:
-            data = subprocess.check_output(cmd)
-            if data:
-                return data.decode('utf-8').strip()
-        except Exception as e:
-            logger.error('Failed to execute %s: %r', cmd, e)
-
-    def controldata(self):
-        """ return the contents of pg_controldata, or non-True value if pg_controldata call failed """
-        # Don't try to call pg_controldata during backup restore
-        if self._version_file_exists() and self.state != 'creating replica':
+        if os.path.exists(self._auto_conf):
             try:
-                env = {**os.environ, 'LANG': 'C', 'LC_ALL': 'C'}
-                data = subprocess.check_output([self.pgcommand('pg_controldata'), self._data_dir], env=env)
-                if data:
-                    data = filter(lambda e: ':' in e, data.decode('utf-8').splitlines())
-                    # pg_controldata output depends on major version. Some of parameters are prefixed by 'Current '
-                    return {k.replace('Current ', '', 1): v.strip() for k, v in map(lambda e: e.split(':', 1), data)}
-            except subprocess.CalledProcessError:
-                logger.exception("Error when calling pg_controldata")
-        return {}
-
-    def waldump(self, timeline, lsn, limit):
-        cmd = self.pgcommand('pg_{0}dump'.format(self.wal_name))
-        env = {**os.environ, 'LANG': 'C', 'LC_ALL': 'C', 'PGDATA': self._data_dir}
-        try:
-            waldump = subprocess.Popen([cmd, '-t', str(timeline), '-s', str(lsn), '-n', str(limit)],
-                                       stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)
-            out, err = waldump.communicate()
-            waldump.wait()
-            return out, err
-        except Exception as e:
-            logger.error('Failed to execute `%s -t %s -s %s -n %s`: %r', cmd, timeline, lsn, limit, e)
-            return None, None
-
-    @contextmanager
-    def get_replication_connection_cursor(self, host=None, port=5432, **kwargs):
-        conn_kwargs = self.config.replication.copy()
-        conn_kwargs.update(host=host, port=int(port) if port else None, user=conn_kwargs.pop('username'),
-                           connect_timeout=3, replication=1, options='-c statement_timeout=2000')
-        with get_connection_cursor(**conn_kwargs) as cur:
-            yield cur
-
-    def get_replica_timeline(self):
-        try:
-            with self.get_replication_connection_cursor(**self.config.local_replication_address) as cur:
-                cur.execute('IDENTIFY_SYSTEM')
-                return cur.fetchone()[1]
-        except Exception:
-            logger.exception('Can not fetch local timeline and lsn from replication connection')
+                with open(self._auto_conf) as f:
+                    for raw_line in f:
+                        line = raw_line.strip()
+                        match = PARAMETER_RE.match(line)
+                        if match and match.group(1).lower() in self._RECOVERY_PARAMETERS:
+                            overwrite = True
+                        else:
+                            lines.append(raw_line)
+            except Exception:
+                logger.info('Failed to read %s', self._auto_conf)
 
-    def replica_cached_timeline(self, primary_timeline):
-        if not self._cached_replica_timeline or not primary_timeline\
-                or self._cached_replica_timeline != primary_timeline:
-            self._cached_replica_timeline = self.get_replica_timeline()
-        return self._cached_replica_timeline
-
-    def get_primary_timeline(self):
-        return self._cluster_info_state_get('timeline')
-
-    def get_history(self, timeline):
-        history_path = os.path.join(self.wal_dir, '{0:08X}.history'.format(timeline))
-        history_mtime = mtime(history_path)
-        if history_mtime:
+        if overwrite:
             try:
-                with open(history_path, 'r') as f:
-                    history = f.read()
-                history = list(parse_history(history))
-                if history[-1][0] == timeline - 1:
-                    history_mtime = datetime.fromtimestamp(history_mtime).replace(tzinfo=tz.tzlocal())
-                    history[-1].append(history_mtime.isoformat())
-                    history[-1].append(self.name)
-                return history
+                with open(self._auto_conf, 'w') as f:
+                    for raw_line in lines:
+                        f.write(raw_line)
             except Exception:
-                logger.exception('Failed to read and parse %s', (history_path,))
-
-    def follow(self,
-               member: Member,
-               role: Optional[str] = 'replica',
-               timeout: Optional[float] = None,
-               do_reload: Optional[bool] = False) -> Optional[bool]:
-        """Reconfigure postgres to follow a new member or use different recovery parameters.
-
-        Method may call `on_role_change` callback if role is changing.
-
-        :param member: The member to follow
-        :param role: The desired role, normally 'replica', but could also be a 'standby_leader'
-        :param timeout: start timeout, how long should the `start()` method wait for postgres accepting connections
-        :param do_reload: indicates that after updating postgresql.conf we just need to do a reload instead of restart
-
-        :returns: True - if restart/reload were successfully performed,
-                  False - if restart/reload failed
-                  None - if nothing was done or if Postgres is still in starting state after `timeout` seconds."""
+                logger.exception('Failed to remove some unwanted parameters from %s', self._auto_conf)
 
-        if not self.ensure_major_version_is_known():
-            return None
-
-        recovery_params = self.config.build_recovery_params(member)
-        self.config.write_recovery_conf(recovery_params)
-
-        # When we demoting the primary or standby_leader to replica or promoting replica to a standby_leader
-        # and we know for sure that postgres was already running before, we will only execute on_role_change
-        # callback and prevent execution of on_restart/on_start callback.
-        # If the role remains the same (replica or standby_leader), we will execute on_start or on_restart
-        change_role = self.cb_called and (self.role in ('master', 'primary', 'demoted') or
-                                          not {'standby_leader', 'replica'} - {self.role, role})
-        if change_role:
-            self.__cb_pending = CallbackAction.NOOP
-
-        ret = True
-        if self.is_running():
-            if do_reload:
-                self.config.write_postgresql_conf()
-                ret = self.reload(block_callbacks=change_role)
-                if ret and change_role:
-                    self.set_role(role)
+    def _adjust_recovery_parameters(self) -> None:
+        # It is not strictly necessary, but we can make patroni configs crossi-compatible with all postgres versions.
+        recovery_conf = {n: v for n, v in self._server_parameters.items() if n.lower() in self._RECOVERY_PARAMETERS}
+        if recovery_conf:
+            self._config['recovery_conf'] = recovery_conf
+
+        if self.get('recovery_conf'):
+            value = self._config['recovery_conf'].pop(self._triggerfile_wrong_name, None)
+            if self.triggerfile_good_name not in self._config['recovery_conf'] and value:
+                self._config['recovery_conf'][self.triggerfile_good_name] = value
+
+    def get_server_parameters(self, config: Dict[str, Any]) -> CaseInsensitiveDict:
+        parameters = config['parameters'].copy()
+        listen_addresses, port = split_host_port(config['listen'], 5432)
+        parameters.update(cluster_name=self._postgresql.scope, listen_addresses=listen_addresses, port=str(port))
+        if not self._postgresql.global_config or self._postgresql.global_config.is_synchronous_mode:
+            if self._synchronous_standby_names is None:
+                if self._postgresql.global_config and self._postgresql.global_config.is_synchronous_mode_strict\
+                        and self._postgresql.role in ('master', 'primary', 'promoted'):
+                    parameters['synchronous_standby_names'] = '*'
+                else:
+                    parameters.pop('synchronous_standby_names', None)
             else:
-                ret = self.restart(block_callbacks=change_role, role=role)
-        else:
-            ret = self.start(timeout=timeout, block_callbacks=change_role, role=role) or None
+                parameters['synchronous_standby_names'] = self._synchronous_standby_names
 
-        if change_role:
-            # TODO: postpone this until start completes, or maybe do even earlier
-            self.call_nowait(CallbackAction.ON_ROLE_CHANGE)
+        # Handle hot_standby <-> replica rename
+        if parameters.get('wal_level') == ('hot_standby' if self._postgresql.major_version >= 90600 else 'replica'):
+            parameters['wal_level'] = 'replica' if self._postgresql.major_version >= 90600 else 'hot_standby'
+
+        # Try to recalcualte wal_keep_segments <-> wal_keep_size assuming that typical wal_segment_size is 16MB.
+        # The real segment size could be estimated from pg_control, but we don't really care, because the only goal of
+        # this exercise is improving cross version compatibility and user must set the correct parameter in the config.
+        if self._postgresql.major_version >= 130000:
+            wal_keep_segments = parameters.pop('wal_keep_segments', self.CMDLINE_OPTIONS['wal_keep_segments'][0])
+            parameters.setdefault('wal_keep_size', str(int(wal_keep_segments) * 16) + 'MB')
+        elif self._postgresql.major_version:
+            wal_keep_size = parse_int(parameters.pop('wal_keep_size', self.CMDLINE_OPTIONS['wal_keep_size'][0]), 'MB')
+            parameters.setdefault('wal_keep_segments', int(((wal_keep_size or 0) + 8) / 16))
+
+        self._postgresql.citus_handler.adjust_postgres_gucs(parameters)
+
+        ret = CaseInsensitiveDict({k: v for k, v in parameters.items() if not self._postgresql.major_version
+                                   or self._postgresql.major_version >= self.CMDLINE_OPTIONS.get(k, (0, 1, 90100))[2]})
+        ret.update({k: os.path.join(self._config_dir, ret[k]) for k in ('hba_file', 'ident_file') if k in ret})
         return ret
 
-    def _wait_promote(self, wait_seconds):
-        for _ in polling_loop(wait_seconds):
-            data = self.controldata()
-            if data.get('Database cluster state') == 'in production':
-                self.set_role('master')
-                return True
+    @staticmethod
+    def _get_unix_local_address(unix_socket_directories: str) -> str:
+        for d in unix_socket_directories.split(','):
+            d = d.strip()
+            if d.startswith('/'):  # Only absolute path can be used to connect via unix-socket
+                return d
+        return ''
+
+    def _get_tcp_local_address(self) -> str:
+        listen_addresses = self._server_parameters['listen_addresses'].split(',')
+
+        for la in listen_addresses:
+            if la.strip().lower() in ('*', '0.0.0.0', '127.0.0.1', 'localhost'):  # we are listening on '*' or localhost
+                return 'localhost'  # connection via localhost is preferred
+        return listen_addresses[0].strip()  # can't use localhost, take first address from listen_addresses
 
-    def _pre_promote(self):
-        """
-        Runs a fencing script after the leader lock is acquired but before the replica is promoted.
-        If the script exits with a non-zero code, promotion does not happen and the leader key is removed from DCS.
-        """
+    @property
+    def local_connect_kwargs(self) -> Dict[str, Any]:
+        ret = self._local_address.copy()
+        # add all of the other connection settings that are available
+        ret.update(self._superuser)
+        # if the "username" parameter is present, it actually needs to be "user"
+        # for connecting to PostgreSQL
+        if 'username' in self._superuser:
+            ret['user'] = self._superuser['username']
+            del ret['username']
+        # ensure certain Patroni configurations are available
+        ret.update({'dbname': self._postgresql.database,
+                    'fallback_application_name': 'Patroni',
+                    'connect_timeout': 3,
+                    'options': '-c statement_timeout=2000'})
+        return ret
 
-        cmd = self.config.get('pre_promote')
-        if not cmd:
-            return True
+    def resolve_connection_addresses(self) -> None:
+        port = self._server_parameters['port']
+        tcp_local_address = self._get_tcp_local_address()
+        netloc = self._config.get('connect_address') or tcp_local_address + ':' + port
+
+        unix_local_address = {'port': port}
+        unix_socket_directories = self._server_parameters.get('unix_socket_directories')
+        if unix_socket_directories is not None:
+            # fallback to tcp if unix_socket_directories is set, but there are no suitable values
+            unix_local_address['host'] = self._get_unix_local_address(unix_socket_directories) or tcp_local_address
+
+        tcp_local_address = {'host': tcp_local_address, 'port': port}
+
+        self._local_address = unix_local_address if self._config.get('use_unix_socket') else tcp_local_address
+        self.local_replication_address = unix_local_address\
+            if self._config.get('use_unix_socket_repl') else tcp_local_address
+
+        self._postgresql.connection_string = uri('postgres', netloc, self._postgresql.database)
+        self._postgresql.set_connection_kwargs(self.local_connect_kwargs)
+
+    def _get_pg_settings(
+            self, names: Collection[str]
+    ) -> Dict[str, Tuple[str, str, Optional[str], str, str, Optional[str]]]:
+        return {r[0]: r for r in self._postgresql.query(('SELECT name, setting, unit, vartype, context, sourcefile'
+                                                         + ' FROM pg_catalog.pg_settings '
+                                                         + ' WHERE pg_catalog.lower(name) = ANY(%s)'),
+                                                        [n.lower() for n in names])}
 
-        ret = self.cancellable.call(shlex.split(cmd))
-        if ret is not None:
-            logger.info('pre_promote script `%s` exited with %s', cmd, ret)
-        return ret == 0
+    @staticmethod
+    def _handle_wal_buffers(old_values: Dict[str, Tuple[str, str, Optional[str], str, str, Optional[str]]],
+                            changes: CaseInsensitiveDict) -> None:
+        wal_block_size = parse_int(old_values['wal_block_size'][1]) or 8192
+        wal_segment_size = old_values['wal_segment_size']
+        wal_segment_unit = parse_int(wal_segment_size[2], 'B') or 8192 \
+            if wal_segment_size[2] is not None and wal_segment_size[2][0].isdigit() else 1
+        wal_segment_size = parse_int(wal_segment_size[1]) or (16777216 if wal_segment_size[2] is None else 2048)
+        wal_segment_size *= wal_segment_unit / wal_block_size
+        default_wal_buffers = min(max((parse_int(old_values['shared_buffers'][1]) or 16384) / 32, 8), wal_segment_size)
+
+        wal_buffers = old_values['wal_buffers']
+        new_value = str(changes['wal_buffers'] or -1)
+
+        new_value = default_wal_buffers if new_value == '-1' else parse_int(new_value, wal_buffers[2])
+        old_value = default_wal_buffers if wal_buffers[1] == '-1' else parse_int(*wal_buffers[1:3])
+
+        if new_value == old_value:
+            del changes['wal_buffers']
+
+    def reload_config(self, config: Dict[str, Any], sighup: bool = False) -> None:
+        self._superuser = config['authentication'].get('superuser', {})
+        server_parameters = self.get_server_parameters(config)
+
+        conf_changed = hba_changed = ident_changed = local_connection_address_changed = pending_restart = False
+        if self._postgresql.state == 'running':
+            changes = CaseInsensitiveDict({p: v for p, v in server_parameters.items()
+                                           if p.lower() not in self._RECOVERY_PARAMETERS})
+            changes.update({p: None for p in self._server_parameters.keys()
+                            if not (p in changes or p.lower() in self._RECOVERY_PARAMETERS)})
+            if changes:
+                undef = []
+                if 'wal_buffers' in changes:  # we need to calculate the default value of wal_buffers
+                    undef = [p for p in ('shared_buffers', 'wal_segment_size', 'wal_block_size') if p not in changes]
+                    changes.update({p: None for p in undef})
+                # XXX: query can raise an exception
+                old_values = self._get_pg_settings(changes.keys())
+                if 'wal_buffers' in changes:
+                    self._handle_wal_buffers(old_values, changes)
+                    for p in undef:
+                        del changes[p]
+
+                for r in old_values.values():
+                    if r[4] != 'internal' and r[0] in changes:
+                        new_value = changes.pop(r[0])
+                        if new_value is None or not compare_values(r[3], r[2], r[1], new_value):
+                            conf_changed = True
+                            if r[4] == 'postmaster':
+                                pending_restart = True
+                                logger.info('Changed %s from %s to %s (restart might be required)',
+                                            r[0], r[1], new_value)
+                                if config.get('use_unix_socket') and r[0] == 'unix_socket_directories'\
+                                        or r[0] in ('listen_addresses', 'port'):
+                                    local_connection_address_changed = True
+                            else:
+                                logger.info('Changed %s from %s to %s', r[0], r[1], new_value)
+                for param, value in changes.items():
+                    if '.' in param:
+                        # Check that user-defined-paramters have changed (parameters with period in name)
+                        if value is None or param not in self._server_parameters \
+                                or str(value) != str(self._server_parameters[param]):
+                            logger.info('Changed %s from %s to %s', param, self._server_parameters.get(param), value)
+                            conf_changed = True
+                    elif param in server_parameters:
+                        logger.warning('Removing invalid parameter `%s` from postgresql.parameters', param)
+                        server_parameters.pop(param)
+
+            if (not server_parameters.get('hba_file') or server_parameters['hba_file'] == self._pg_hba_conf) \
+                    and config.get('pg_hba'):
+                hba_changed = self._config.get('pg_hba', []) != config['pg_hba']
+
+            if (not server_parameters.get('ident_file') or server_parameters['ident_file'] == self._pg_hba_conf) \
+                    and config.get('pg_ident'):
+                ident_changed = self._config.get('pg_ident', []) != config['pg_ident']
+
+        self._config = config
+        self._postgresql.set_pending_restart(pending_restart)
+        self._server_parameters = server_parameters
+        self._adjust_recovery_parameters()
+        self._krbsrvname = config.get('krbsrvname')
+
+        # for not so obvious connection attempts that may happen outside of pyscopg2
+        if self._krbsrvname:
+            os.environ['PGKRBSRVNAME'] = self._krbsrvname
+
+        if not local_connection_address_changed:
+            self.resolve_connection_addresses()
+
+        proxy_addr = config.get('proxy_address')
+        self._postgresql.proxy_url = uri('postgres', proxy_addr, self._postgresql.database) if proxy_addr else None
+
+        if conf_changed:
+            self.write_postgresql_conf()
+
+        if hba_changed:
+            self.replace_pg_hba()
+
+        if ident_changed:
+            self.replace_pg_ident()
+
+        if sighup or conf_changed or hba_changed or ident_changed:
+            logger.info('Reloading PostgreSQL configuration.')
+            self._postgresql.reload()
+            if self._postgresql.major_version >= 90500:
+                time.sleep(1)
+                try:
+                    pending_restart = (self._postgresql.query(
+                        'SELECT COUNT(*) FROM pg_catalog.pg_settings'
+                        ' WHERE pg_catalog.lower(name) != ALL(%s) AND pending_restart',
+                        [n.lower() for n in self._RECOVERY_PARAMETERS]).fetchone() or (0,))[0] > 0
+                    self._postgresql.set_pending_restart(pending_restart)
+                except Exception as e:
+                    logger.warning('Exception %r when running query', e)
+        else:
+            logger.info('No PostgreSQL configuration items changed, nothing to reload.')
 
-    def promote(self, wait_seconds, task, before_promote=None, on_success=None):
-        if self.role in ('promoted', 'master', 'primary'):
+    def set_synchronous_standby_names(self, value: Optional[str]) -> Optional[bool]:
+        """Updates synchronous_standby_names and reloads if necessary.
+        :returns: True if value was updated."""
+        if value != self._synchronous_standby_names:
+            if value is None:
+                self._server_parameters.pop('synchronous_standby_names', None)
+            else:
+                self._server_parameters['synchronous_standby_names'] = value
+            self._synchronous_standby_names = value
+            if self._postgresql.state == 'running':
+                self.write_postgresql_conf()
+                self._postgresql.reload()
             return True
 
-        ret = self._pre_promote()
-        with task:
-            if task.is_cancelled:
-                return False
-            task.complete(ret)
-
-        if ret is False:
-            return False
-
-        if self.cancellable.is_cancelled:
-            logger.info("PostgreSQL promote cancelled.")
-            return False
-
-        if before_promote is not None:
-            before_promote()
+    @property
+    def effective_configuration(self) -> CaseInsensitiveDict:
+        """It might happen that the current value of one (or more) below parameters stored in
+        the controldata is higher than the value stored in the global cluster configuration.
+
+        Example: max_connections in global configuration is 100, but in controldata
+        `Current max_connections setting: 200`. If we try to start postgres with
+        max_connections=100, it will immediately exit.
+        As a workaround we will start it with the values from controldata and set `pending_restart`
+        to true as an indicator that current values of parameters are not matching expectations."""
+
+        if self._postgresql.role in ('master', 'primary'):
+            return self._server_parameters
+
+        options_mapping = {
+            'max_connections': 'max_connections setting',
+            'max_prepared_transactions': 'max_prepared_xacts setting',
+            'max_locks_per_transaction': 'max_locks_per_xact setting'
+        }
+
+        if self._postgresql.major_version >= 90400:
+            options_mapping['max_worker_processes'] = 'max_worker_processes setting'
+
+        if self._postgresql.major_version >= 120000:
+            options_mapping['max_wal_senders'] = 'max_wal_senders setting'
+
+        data = self._postgresql.controldata()
+        effective_configuration = self._server_parameters.copy()
+
+        for name, cname in options_mapping.items():
+            value = parse_int(effective_configuration[name])
+            if cname not in data:
+                logger.warning('%s is missing from pg_controldata output', cname)
+                continue
+
+            cvalue = parse_int(data[cname])
+            if cvalue is not None and value is not None and cvalue > value:
+                effective_configuration[name] = cvalue
+                self._postgresql.set_pending_restart(True)
+
+        # If we are using custom bootstrap with PITR it could fail when values like max_connections
+        # are increased, therefore we disable hot_standby if recovery_target_action == 'promote'.
+        if self._postgresql.bootstrap.running_custom_bootstrap:
+            disable_hot_standby = False
+            if self._postgresql.bootstrap.keep_existing_recovery_conf:
+                disable_hot_standby = True  # trust that pgBackRest does the right thing
+            # `pause_at_recovery_target` has no effect if hot_standby is not enabled, therefore we consider only 9.5+
+            elif self._postgresql.major_version >= 90500 and self._recovery_params:
+                pause_at_recovery_target = parse_bool(self._recovery_params.get('pause_at_recovery_target'))
+                recovery_target_action = self._recovery_params.get(
+                    'recovery_target_action', 'promote' if pause_at_recovery_target is False else 'pause')
+                disable_hot_standby = recovery_target_action == 'promote'
+
+            if disable_hot_standby:
+                effective_configuration['hot_standby'] = 'off'
+                self._postgresql.set_pending_restart(True)
 
-        self.slots_handler.on_promote()
-        self.citus_handler.schedule_cache_rebuild()
+        return effective_configuration
 
-        ret = self.pg_ctl('promote', '-W')
-        if ret:
-            self.set_role('promoted')
-            if on_success is not None:
-                on_success()
-            self.call_nowait(CallbackAction.ON_ROLE_CHANGE)
-            ret = self._wait_promote(wait_seconds)
-        return ret
-
-    @staticmethod
-    def _wal_position(is_leader, wal_position, received_location, replayed_location):
-        return wal_position if is_leader else max(received_location or 0, replayed_location or 0)
+    @property
+    def replication(self) -> Dict[str, Any]:
+        return self._config['authentication']['replication']
 
-    def timeline_wal_position(self):
-        # This method could be called from different threads (simultaneously with some other `_query` calls).
-        # If it is called not from main thread we will create a new cursor to execute statement.
-        if current_thread().ident == self.__thread_ident:
-            timeline = self._cluster_info_state_get('timeline')
-            wal_position = self._cluster_info_state_get('wal_position')
-            replayed_location = self.replayed_location()
-            received_location = self.received_location()
-            pg_control_timeline = self._cluster_info_state_get('pg_control_timeline')
-        else:
-            with self.connection().cursor() as cursor:
-                cursor.execute(self.cluster_info_query)
-                (timeline, wal_position, replayed_location,
-                 received_location, _, pg_control_timeline) = cursor.fetchone()[:6]
+    @property
+    def superuser(self) -> Dict[str, Any]:
+        return self._superuser
 
-        wal_position = self._wal_position(timeline, wal_position, received_location, replayed_location)
-        return (timeline, wal_position, pg_control_timeline)
+    @property
+    def rewind_credentials(self) -> Dict[str, Any]:
+        return self._config['authentication'].get('rewind', self._superuser) \
+            if self._postgresql.major_version >= 110000 else self._superuser
 
-    def postmaster_start_time(self):
-        try:
-            query = "SELECT " + self.POSTMASTER_START_TIME
-            if current_thread().ident == self.__thread_ident:
-                return self.query(query).fetchone()[0].isoformat(sep=' ')
-            with self.connection().cursor() as cursor:
-                cursor.execute(query)
-                return cursor.fetchone()[0].isoformat(sep=' ')
-        except psycopg.Error:
-            return None
+    @property
+    def ident_file(self) -> Optional[str]:
+        ident_file = self._server_parameters.get('ident_file')
+        return None if ident_file == self._pg_ident_conf else ident_file
 
-    def last_operation(self):
-        return self._wal_position(self.is_leader(), self._cluster_info_state_get('wal_position'),
-                                  self.received_location(), self.replayed_location())
-
-    def configure_server_parameters(self):
-        self._major_version = self.get_major_version()
-        self.config.setup_server_parameters()
-
-    def ensure_major_version_is_known(self) -> bool:
-        """Calls configure_server_parameters() if `_major_version` is not known
-
-        :returns: `True` if `_major_version` is set, otherwise `False`"""
-
-        if not self._major_version:
-            self.configure_server_parameters()
-        return self._major_version > 0
-
-    def pg_wal_realpath(self):
-        """Returns a dict containing the symlink (key) and target (value) for the wal directory"""
-        links = {}
-        for pg_wal_dir in ('pg_xlog', 'pg_wal'):
-            pg_wal_path = os.path.join(self._data_dir, pg_wal_dir)
-            if os.path.exists(pg_wal_path) and os.path.islink(pg_wal_path):
-                pg_wal_realpath = os.path.realpath(pg_wal_path)
-                links[pg_wal_path] = pg_wal_realpath
-        return links
-
-    def pg_tblspc_realpaths(self):
-        """Returns a dict containing the symlink (key) and target (values) for the tablespaces"""
-        links = {}
-        pg_tblsp_dir = os.path.join(self._data_dir, 'pg_tblspc')
-        if os.path.exists(pg_tblsp_dir):
-            for tsdn in os.listdir(pg_tblsp_dir):
-                pg_tsp_path = os.path.join(pg_tblsp_dir, tsdn)
-                if parse_int(tsdn) and os.path.islink(pg_tsp_path):
-                    pg_tsp_rpath = os.path.realpath(pg_tsp_path)
-                    links[pg_tsp_path] = pg_tsp_rpath
-        return links
+    @property
+    def hba_file(self) -> Optional[str]:
+        hba_file = self._server_parameters.get('hba_file')
+        return None if hba_file == self._pg_hba_conf else hba_file
 
-    def move_data_directory(self):
-        if os.path.isdir(self._data_dir) and not self.is_running():
-            try:
-                postfix = 'failed'
+    @property
+    def pg_hba_conf(self) -> str:
+        return self._pg_hba_conf
 
-                # let's see if the wal directory is a symlink, in this case we
-                # should move the target
-                for (source, pg_wal_realpath) in self.pg_wal_realpath().items():
-                    logger.info('renaming WAL directory and updating symlink: %s', pg_wal_realpath)
-                    new_name = '{0}.{1}'.format(pg_wal_realpath, postfix)
-                    if os.path.exists(new_name):
-                        shutil.rmtree(new_name)
-                    os.rename(pg_wal_realpath, new_name)
-                    os.unlink(source)
-                    os.symlink(new_name, source)
-
-                # Move user defined tablespace directory
-                for (source, pg_tsp_rpath) in self.pg_tblspc_realpaths().items():
-                    logger.info('renaming user defined tablespace directory and updating symlink: %s', pg_tsp_rpath)
-                    new_name = '{0}.{1}'.format(pg_tsp_rpath, postfix)
-                    if os.path.exists(new_name):
-                        shutil.rmtree(new_name)
-                    os.rename(pg_tsp_rpath, new_name)
-                    os.unlink(source)
-                    os.symlink(new_name, source)
-
-                new_name = '{0}.{1}'.format(self._data_dir, postfix)
-                logger.info('renaming data directory to %s', new_name)
-                if os.path.exists(new_name):
-                    shutil.rmtree(new_name)
-                os.rename(self._data_dir, new_name)
-            except OSError:
-                logger.exception("Could not rename data directory %s", self._data_dir)
-
-    def remove_data_directory(self):
-        self.set_role('uninitialized')
-        logger.info('Removing data directory: %s', self._data_dir)
-        try:
-            if os.path.islink(self._data_dir):
-                os.unlink(self._data_dir)
-            elif not os.path.exists(self._data_dir):
-                return
-            elif os.path.isfile(self._data_dir):
-                os.remove(self._data_dir)
-            elif os.path.isdir(self._data_dir):
-
-                # let's see if wal directory is a symlink, in this case we
-                # should clean the target
-                for pg_wal_realpath in self.pg_wal_realpath().values():
-                    logger.info('Removing WAL directory: %s', pg_wal_realpath)
-                    shutil.rmtree(pg_wal_realpath)
-
-                # Remove user defined tablespace directories
-                for pg_tsp_rpath in self.pg_tblspc_realpaths().values():
-                    logger.info('Removing user defined tablespace directory: %s', pg_tsp_rpath)
-                    shutil.rmtree(pg_tsp_rpath, ignore_errors=True)
-
-                shutil.rmtree(self._data_dir)
-        except (IOError, OSError):
-            logger.exception('Could not remove data directory %s', self._data_dir)
-            self.move_data_directory()
+    @property
+    def postgresql_conf(self) -> str:
+        return self._postgresql_conf
 
-    def schedule_sanity_checks_after_pause(self):
-        """
-            After coming out of pause we have to:
-            1. configure server parameters if necessary
-            2. sync replication slots, because it might happen that slots were removed
-            3. get new 'Database system identifier' to make sure that it wasn't changed
-        """
-        self.ensure_major_version_is_known()
-        self.slots_handler.schedule()
-        self.citus_handler.schedule_cache_rebuild()
-        self._sysid = None
+    def get(self, key: str, default: Optional[Any] = None) -> Optional[Any]:
+        return self._config.get(key, default)
```

### Comparing `patroni-3.0.2/patroni/postgresql/bootstrap.py` & `patroni-3.0.3/patroni/postgresql/bootstrap.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,106 +1,160 @@
 import logging
 import os
 import shlex
 import tempfile
 import time
 
-from ..dcs import RemoteMember
+from typing import Any, Callable, Dict, List, Optional, Union, Tuple, TYPE_CHECKING
+
+from ..async_executor import CriticalTask
+from ..dcs import Leader, Member, RemoteMember
 from ..psycopg import quote_ident, quote_literal
-from ..utils import deep_compare
+from ..utils import deep_compare, unquote
+
+if TYPE_CHECKING:  # pragma: no cover
+    from . import Postgresql
 
 logger = logging.getLogger(__name__)
 
 
 class Bootstrap(object):
 
-    def __init__(self, postgresql):
+    def __init__(self, postgresql: 'Postgresql') -> None:
         self._postgresql = postgresql
         self._running_custom_bootstrap = False
 
     @property
-    def running_custom_bootstrap(self):
+    def running_custom_bootstrap(self) -> bool:
         return self._running_custom_bootstrap
 
     @property
-    def keep_existing_recovery_conf(self):
+    def keep_existing_recovery_conf(self) -> bool:
         return self._running_custom_bootstrap and self._keep_existing_recovery_conf
 
     @staticmethod
-    def process_user_options(tool, options, not_allowed_options, error_handler):
-        user_options = []
+    def process_user_options(tool: str,
+                             options: Union[Any, Dict[str, str], List[Union[str, Dict[str, Any]]]],
+                             not_allowed_options: Tuple[str, ...],
+                             error_handler: Callable[[str], None]) -> List[str]:
+        """Format *options* in a list or dictionary format into command line long form arguments.
+
+        .. note::
+            The format of the output of this method is to prepare arguments for use in the ``initdb``
+            method of `self._postgres`.
+
+        :Example:
+
+            The *options* can be defined as a dictionary of key, values to be converted into arguments:
+            >>> Bootstrap.process_user_options('foo', {'foo': 'bar'}, (), print)
+            ['--foo=bar']
+
+            Or as a list of single string arguments
+            >>> Bootstrap.process_user_options('foo', ['yes'], (), print)
+            ['--yes']
+
+            Or as a list of key, value options
+            >>> Bootstrap.process_user_options('foo', [{'foo': 'bar'}], (), print)
+            ['--foo=bar']
+
+            Or a combination of single and key, values
+            >>> Bootstrap.process_user_options('foo', ['yes', {'foo': 'bar'}], (), print)
+            ['--yes', '--foo=bar']
+
+            Options that contain spaces are passed as is to ``subprocess.call``
+            >>> Bootstrap.process_user_options('foo', [{'foo': 'bar baz'}], (), print)
+            ['--foo=bar baz']
+
+            Options that are quoted will be unquoted, so the quotes aren't interpreted
+            literally by the postgres command
+            >>> Bootstrap.process_user_options('foo', [{'foo': '"bar baz"'}], (), print)
+            ['--foo=bar baz']
+
+        .. note::
+            The *error_handler* is called when any of these conditions are met:
+
+            * Key, value dictionaries in the list form contains multiple keys.
+            * If a key is listed in *not_allowed_options*.
+            * If the options list is not in the required structure.
+
+        :param tool: The name of the tool used in error reports to *error_handler*
+        :param options: Options to parse as a list of key, values or single values, or a dictionary
+        :param not_allowed_options: List of keys that cannot be used in the list of key, value formatted options
+        :param error_handler: A function which will be called when an error condition is encountered
+        :returns: List of long form arguments to pass to the named tool
+        """
+        user_options: List[str] = []
 
-        def option_is_allowed(name):
+        def option_is_allowed(name: str) -> bool:
             ret = name not in not_allowed_options
             if not ret:
                 error_handler('{0} option for {1} is not allowed'.format(name, tool))
             return ret
 
         if isinstance(options, dict):
-            for k, v in options.items():
-                if k and v:
-                    user_options.append('--{0}={1}'.format(k, v))
+            for key, val in options.items():
+                if key and val:
+                    user_options.append('--{0}={1}'.format(key, unquote(val)))
         elif isinstance(options, list):
             for opt in options:
                 if isinstance(opt, str) and option_is_allowed(opt):
                     user_options.append('--{0}'.format(opt))
                 elif isinstance(opt, dict):
                     keys = list(opt.keys())
                     if len(keys) != 1 or not isinstance(opt[keys[0]], str) or not option_is_allowed(keys[0]):
                         error_handler('Error when parsing {0} key-value option {1}: only one key-value is allowed'
                                       ' and value should be a string'.format(tool, opt[keys[0]]))
-                    user_options.append('--{0}={1}'.format(keys[0], opt[keys[0]]))
+                    user_options.append('--{0}={1}'.format(keys[0], unquote(opt[keys[0]])))
                 else:
                     error_handler('Error when parsing {0} option {1}: value should be string value'
                                   ' or a single key-value pair'.format(tool, opt))
         else:
             error_handler('{0} options must be list or dict'.format(tool))
         return user_options
 
-    def _initdb(self, config):
+    def _initdb(self, config: Any) -> bool:
         self._postgresql.set_state('initializing new cluster')
         not_allowed_options = ('pgdata', 'nosync', 'pwfile', 'sync-only', 'version')
 
-        def error_handler(e):
+        def error_handler(e: str) -> None:
             raise Exception(e)
 
         options = self.process_user_options('initdb', config or [], not_allowed_options, error_handler)
         pwfile = None
 
         if self._postgresql.config.superuser:
             if 'username' in self._postgresql.config.superuser:
                 options.append('--username={0}'.format(self._postgresql.config.superuser['username']))
             if 'password' in self._postgresql.config.superuser:
                 (fd, pwfile) = tempfile.mkstemp()
                 os.write(fd, self._postgresql.config.superuser['password'].encode('utf-8'))
                 os.close(fd)
                 options.append('--pwfile={0}'.format(pwfile))
-        options = ['-o', ' '.join(options)] if options else []
 
-        ret = self._postgresql.pg_ctl('initdb', *options)
+        ret = self._postgresql.initdb(*options)
         if pwfile:
             os.remove(pwfile)
         if ret:
             self._postgresql.configure_server_parameters()
         else:
             self._postgresql.set_state('initdb failed')
         return ret
 
-    def _post_restore(self):
+    def _post_restore(self) -> None:
         self._postgresql.config.restore_configuration_files()
         self._postgresql.configure_server_parameters()
 
         # make sure there is no trigger file or postgres will be automatically promoted
         trigger_file = self._postgresql.config.triggerfile_good_name
         trigger_file = (self._postgresql.config.get('recovery_conf') or {}).get(trigger_file) or 'promote'
         trigger_file = os.path.abspath(os.path.join(self._postgresql.data_dir, trigger_file))
         if os.path.exists(trigger_file):
             os.unlink(trigger_file)
 
-    def _custom_bootstrap(self, config):
+    def _custom_bootstrap(self, config: Any) -> bool:
         self._postgresql.set_state('running custom bootstrap script')
         params = [] if config.get('no_params') else ['--scope=' + self._postgresql.scope,
                                                      '--datadir=' + self._postgresql.data_dir]
         try:
             logger.info('Running custom bootstrap script: %s', config['command'])
             if self._postgresql.cancellable.call(shlex.split(config['command']) + params) != 0:
                 self._postgresql.set_state('custom bootstrap failed')
@@ -112,15 +166,15 @@
 
         if 'recovery_conf' in config:
             self._postgresql.config.write_recovery_conf(config['recovery_conf'])
         elif not self.keep_existing_recovery_conf:
             self._postgresql.config.remove_recovery_conf()
         return True
 
-    def call_post_bootstrap(self, config):
+    def call_post_bootstrap(self, config: Dict[str, Any]) -> bool:
         """
         runs a script after initdb or custom bootstrap script is called and waits until completion.
         """
         cmd = config.get('post_bootstrap') or config.get('post_init')
         if cmd:
             r = self._postgresql.config.local_connect_kwargs
             connstring = self._postgresql.config.format_dsn(r, True)
@@ -139,15 +193,15 @@
                 logger.error('post_init script %s failed', cmd)
                 return False
             if ret != 0:
                 logger.error('post_init script %s returned non-zero code %d', cmd, ret)
                 return False
         return True
 
-    def create_replica(self, clone_member):
+    def create_replica(self, clone_member: Union[Leader, Member, None]) -> Optional[int]:
         """
             create the replica according to the replica_method
             defined by the user.  this is a list, so we need to
             loop through all methods the user supplies
         """
 
         self._postgresql.set_state('creating replica')
@@ -226,15 +280,15 @@
                 except Exception:
                     logger.exception('Error creating replica using method %s', replica_method)
                     ret = 1
 
         self._postgresql.set_state('stopped')
         return ret
 
-    def basebackup(self, conn_url, env, options):
+    def basebackup(self, conn_url: str, env: Dict[str, str], options: Dict[str, Any]) -> Optional[int]:
         # creates a replica data dir using pg_basebackup.
         # this is the default, built-in create_replica_methods
         # tries twice, then returns failure (as 1)
         # uses "stream" as the xlog-method to avoid sync issues
         # supports additional user-supplied options, those are not validated
         maxfailures = 2
         ret = 1
@@ -261,42 +315,42 @@
 
             if bbfailures < maxfailures - 1:
                 logger.warning('Trying again in 5 seconds')
                 time.sleep(5)
 
         return ret
 
-    def clone(self, clone_member):
+    def clone(self, clone_member: Union[Leader, Member, None]) -> bool:
         """
              - initialize the replica from an existing member (primary or replica)
              - initialize the replica using the replica creation method that
                works without the replication connection (i.e. restore from on-disk
                base backup)
         """
 
         ret = self.create_replica(clone_member) == 0
         if ret:
             self._post_restore()
         return ret
 
-    def bootstrap(self, config):
+    def bootstrap(self, config: Dict[str, Any]) -> bool:
         """ Initialize a new node from scratch and start it. """
         pg_hba = config.get('pg_hba', [])
         method = config.get('method') or 'initdb'
         if method != 'initdb' and method in config and 'command' in config[method]:
             self._keep_existing_recovery_conf = config[method].get('keep_existing_recovery_conf')
             self._running_custom_bootstrap = True
             do_initialize = self._custom_bootstrap
         else:
             method = 'initdb'
             do_initialize = self._initdb
         return do_initialize(config.get(method)) and self._postgresql.config.append_pg_hba(pg_hba) \
-            and self._postgresql.config.save_configuration_files() and self._postgresql.start()
+            and self._postgresql.config.save_configuration_files() and bool(self._postgresql.start())
 
-    def create_or_update_role(self, name, password, options):
+    def create_or_update_role(self, name: str, password: Optional[str], options: List[str]) -> None:
         options = list(map(str.upper, options))
         if 'NOLOGIN' not in options and 'LOGIN' not in options:
             options.append('LOGIN')
 
         if password:
             options.extend(['PASSWORD', quote_literal(password)])
 
@@ -318,15 +372,15 @@
             self._postgresql.query(sql)
         finally:
             self._postgresql.query('RESET log_min_error_statement')
             self._postgresql.query('RESET log_min_duration_statement')
             self._postgresql.query('RESET log_statement')
             self._postgresql.query('RESET pg_stat_statements.track_utility')
 
-    def post_bootstrap(self, config, task):
+    def post_bootstrap(self, config: Dict[str, Any], task: CriticalTask) -> Optional[bool]:
         try:
             postgresql = self._postgresql
             superuser = postgresql.config.superuser
             if 'username' in superuser and 'password' in superuser:
                 self.create_or_update_role(superuser['username'], superuser['password'], ['SUPERUSER'])
 
             task.complete(self.call_post_bootstrap(config))
```

### Comparing `patroni-3.0.2/patroni/postgresql/callback_executor.py` & `patroni-3.0.3/patroni/postgresql/callback_executor.py`

 * *Files 3% similar despite different names*

```diff
@@ -13,15 +13,15 @@
     NOOP = "noop"
     ON_START = "on_start"
     ON_STOP = "on_stop"
     ON_RESTART = "on_restart"
     ON_RELOAD = "on_reload"
     ON_ROLE_CHANGE = "on_role_change"
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return self.value
 
 
 class OnReloadExecutor(CancellableSubprocess):
 
     def call_nowait(self, cmd: List[str]) -> None:
         """Run one `on_reload` callback at most.
@@ -56,19 +56,21 @@
             return self._on_reload_executor.call_nowait(cmd)
 
         self._kill_process()
         with self._condition:
             self._cmd = cmd
             self._condition.notify()
 
-    def run(self):
+    def run(self) -> None:
         while True:
             with self._condition:
                 if self._cmd is None:
                     self._condition.wait()
                 cmd, self._cmd = self._cmd, None
 
-            with self._lock:
-                if not self._start_process(cmd, close_fds=True):
-                    continue
-            self._process.wait()
-            self._kill_children()
+            if cmd is not None:
+                with self._lock:
+                    if not self._start_process(cmd, close_fds=True):
+                        continue
+                if self._process:
+                    self._process.wait()
+                    self._kill_children()
```

### Comparing `patroni-3.0.2/patroni/postgresql/cancellable.py` & `patroni-3.0.3/patroni/postgresql/cancellable.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,42 +1,43 @@
 import logging
 import psutil
 import subprocess
 
 from patroni.exceptions import PostgresException
 from patroni.utils import polling_loop
 from threading import Lock
+from typing import Any, Dict, List, Optional, Union
 
 logger = logging.getLogger(__name__)
 
 
 class CancellableExecutor(object):
 
     """
     There must be only one such process so that AsyncExecutor can easily cancel it.
     """
 
-    def __init__(self):
+    def __init__(self) -> None:
         self._process = None
         self._process_cmd = None
-        self._process_children = []
+        self._process_children: List[psutil.Process] = []
         self._lock = Lock()
 
-    def _start_process(self, cmd, *args, **kwargs):
+    def _start_process(self, cmd: List[str], *args: Any, **kwargs: Any) -> Optional[bool]:
         """This method must be executed only when the `_lock` is acquired"""
 
         try:
             self._process_children = []
             self._process_cmd = cmd
             self._process = psutil.Popen(cmd, *args, **kwargs)
         except Exception:
-            return logger.exception('Failed to execute %s',  cmd)
+            return logger.exception('Failed to execute %s', cmd)
         return True
 
-    def _kill_process(self):
+    def _kill_process(self) -> None:
         with self._lock:
             if self._process is not None and self._process.is_running() and not self._process_children:
                 try:
                     self._process.suspend()  # Suspend the process before getting list of children
                 except psutil.Error as e:
                     logger.info('Failed to suspend the process: %s', e.msg)
 
@@ -49,39 +50,40 @@
                     self._process.kill()
                     logger.warning('Killed %s because it was still running', self._process_cmd)
                 except psutil.NoSuchProcess:
                     pass
                 except psutil.AccessDenied as e:
                     logger.warning('Failed to kill the process: %s', e.msg)
 
-    def _kill_children(self):
-        waitlist = []
+    def _kill_children(self) -> None:
+        waitlist: List[psutil.Process] = []
         with self._lock:
             for child in self._process_children:
                 try:
                     child.kill()
                 except psutil.NoSuchProcess:
                     continue
                 except psutil.AccessDenied as e:
                     logger.info('Failed to kill child process: %s', e.msg)
                 waitlist.append(child)
         psutil.wait_procs(waitlist)
 
 
 class CancellableSubprocess(CancellableExecutor):
 
-    def __init__(self):
+    def __init__(self) -> None:
         super(CancellableSubprocess, self).__init__()
         self._is_cancelled = False
 
-    def call(self, *args, **kwargs):
+    def call(self, *args: Any, **kwargs: Union[Any, Dict[str, str]]) -> Optional[int]:
         for s in ('stdin', 'stdout', 'stderr'):
             kwargs.pop(s, None)
 
-        communicate = kwargs.pop('communicate', None)
+        communicate: Optional[Dict[str, str]] = kwargs.pop('communicate', None)
+        input_data = None
         if isinstance(communicate, dict):
             input_data = communicate.get('input')
             if input_data:
                 if input_data[-1] != '\n':
                     input_data += '\n'
                 input_data = input_data.encode('utf-8')
             kwargs['stdin'] = subprocess.PIPE
@@ -92,33 +94,33 @@
             with self._lock:
                 if self._is_cancelled:
                     raise PostgresException('cancelled')
 
                 self._is_cancelled = False
                 started = self._start_process(*args, **kwargs)
 
-            if started:
+            if started and self._process is not None:
                 if isinstance(communicate, dict):
                     communicate['stdout'], communicate['stderr'] = self._process.communicate(input_data)
                 return self._process.wait()
         finally:
             with self._lock:
                 self._process = None
             self._kill_children()
 
-    def reset_is_cancelled(self):
+    def reset_is_cancelled(self) -> None:
         with self._lock:
             self._is_cancelled = False
 
     @property
-    def is_cancelled(self):
+    def is_cancelled(self) -> bool:
         with self._lock:
             return self._is_cancelled
 
-    def cancel(self, kill=False):
+    def cancel(self, kill: bool = False) -> None:
         with self._lock:
             self._is_cancelled = True
             if self._process is None or not self._process.is_running():
                 return
 
             logger.info('Terminating %s', self._process_cmd)
             self._process.terminate()
```

### Comparing `patroni-3.0.2/patroni/postgresql/citus.py` & `patroni-3.0.3/patroni/postgresql/citus.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,124 +1,133 @@
 import logging
 import re
 import time
 
 from threading import Condition, Event, Thread
 from urllib.parse import urlparse
+from typing import Any, Dict, List, Optional, Union, Tuple, TYPE_CHECKING
 
 from .connection import Connection
-from ..dcs import CITUS_COORDINATOR_GROUP_ID
+from ..dcs import CITUS_COORDINATOR_GROUP_ID, Cluster
 from ..psycopg import connect, quote_ident
 
+if TYPE_CHECKING:  # pragma: no cover
+    from psycopg import Cursor
+    from psycopg2 import cursor
+    from . import Postgresql
+
 CITUS_SLOT_NAME_RE = re.compile(r'^citus_shard_(move|split)_slot(_[1-9][0-9]*){2,3}$')
 logger = logging.getLogger(__name__)
 
 
 class PgDistNode(object):
     """Represents a single row in the `pg_dist_node` table"""
 
-    def __init__(self, group, host, port, event, nodeid=None, timeout=None, cooldown=None):
+    def __init__(self, group: int, host: str, port: int, event: str, nodeid: Optional[int] = None,
+                 timeout: Optional[float] = None, cooldown: Optional[float] = None) -> None:
         self.group = group
         # A weird way of pausing client connections by adding the `-demoted` suffix to the hostname
         self.host = host + ('-demoted' if event == 'before_demote' else '')
         self.port = port
         # Event that is trying to change or changed the given row.
         # Possible values: before_demote, before_promote, after_promote.
         self.event = event
         self.nodeid = nodeid
 
         # If transaction was started, we need to COMMIT/ROLLBACK before the deadline
         self.timeout = timeout
         self.cooldown = cooldown or 10000  # 10s by default
-        self.deadline = 0
+        self.deadline: float = 0
 
         # All changes in the pg_dist_node are serialized on the Patroni
         # side by performing them from a thread. The thread, that is
         # requested a change, sometimes needs to wait for a result.
         # For example, we want to pause client connections before demoting
         # the worker, and once it is done notify the calling thread.
         self._event = Event()
 
-    def wait(self):
+    def wait(self) -> None:
         self._event.wait()
 
-    def wakeup(self):
+    def wakeup(self) -> None:
         self._event.set()
 
-    def __eq__(self, other):
+    def __eq__(self, other: Any) -> bool:
         return isinstance(other, PgDistNode) and self.event == other.event\
             and self.host == other.host and self.port == other.port
 
-    def __ne__(self, other):
+    def __ne__(self, other: Any) -> bool:
         return not self == other
 
-    def __str__(self):
+    def __str__(self) -> str:
         return ('PgDistNode(nodeid={0},group={1},host={2},port={3},event={4})'
                 .format(self.nodeid, self.group, self.host, self.port, self.event))
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return str(self)
 
 
 class CitusHandler(Thread):
 
-    def __init__(self, postgresql, config):
+    def __init__(self, postgresql: 'Postgresql', config: Optional[Dict[str, Union[str, int]]]) -> None:
         super(CitusHandler, self).__init__()
         self.daemon = True
         self._postgresql = postgresql
         self._config = config
         self._connection = Connection()
-        self._pg_dist_node = {}  # Cache of pg_dist_node: {groupid: PgDistNode()}
-        self._tasks = []  # Requests to change pg_dist_node, every task is a `PgDistNode`
-        self._condition = Condition()  # protects _pg_dist_node, _tasks, and _schedule_load_pg_dist_node
-        self._in_flight = None  # Reference to the `PgDistNode` if there is a transaction in progress changing it
+        self._pg_dist_node: Dict[int, PgDistNode] = {}  # Cache of pg_dist_node: {groupid: PgDistNode()}
+        self._tasks: List[PgDistNode] = []  # Requests to change pg_dist_node, every task is a `PgDistNode`
+        self._in_flight: Optional[PgDistNode] = None  # Reference to the `PgDistNode` being changed in a transaction
+        self._schedule_load_pg_dist_node = True  # Flag that "pg_dist_node" should be queried from the database
+        self._condition = Condition()  # protects _pg_dist_node, _tasks, _in_flight, and _schedule_load_pg_dist_node
         self.schedule_cache_rebuild()
 
-    def is_enabled(self):
+    def is_enabled(self) -> bool:
         return isinstance(self._config, dict)
 
-    def group(self):
-        return self._config['group']
+    def group(self) -> Optional[int]:
+        return int(self._config['group']) if isinstance(self._config, dict) else None
 
-    def is_coordinator(self):
+    def is_coordinator(self) -> bool:
         return self.is_enabled() and self.group() == CITUS_COORDINATOR_GROUP_ID
 
-    def is_worker(self):
+    def is_worker(self) -> bool:
         return self.is_enabled() and not self.is_coordinator()
 
-    def set_conn_kwargs(self, kwargs):
-        if self.is_enabled():
+    def set_conn_kwargs(self, kwargs: Dict[str, Any]) -> None:
+        if isinstance(self._config, dict):  # self.is_enabled():
             kwargs.update({'dbname': self._config['database'],
                            'options': '-c statement_timeout=0 -c idle_in_transaction_session_timeout=0'})
             self._connection.set_conn_kwargs(kwargs)
 
-    def schedule_cache_rebuild(self):
+    def schedule_cache_rebuild(self) -> None:
         with self._condition:
             self._schedule_load_pg_dist_node = True
 
-    def on_demote(self):
+    def on_demote(self) -> None:
         with self._condition:
             self._pg_dist_node.clear()
             self._tasks[:] = []
             self._in_flight = None
 
-    def query(self, sql, *params):
+    def query(self, sql: str, *params: Any) -> Union['Cursor[Any]', 'cursor']:
         try:
             logger.debug('query(%s, %s)', sql, params)
             cursor = self._connection.cursor()
-            cursor.execute(sql, params or None)
+            cursor.execute(sql.encode('utf-8'), params or None)
             return cursor
         except Exception as e:
             logger.error('Exception when executing query "%s", (%s): %r', sql, params, e)
             self._connection.close()
-            self._in_flight = None
+            with self._condition:
+                self._in_flight = None
             self.schedule_cache_rebuild()
             raise e
 
-    def load_pg_dist_node(self):
+    def load_pg_dist_node(self) -> bool:
         """Read from the `pg_dist_node` table and put it into the local cache"""
 
         with self._condition:
             if not self._schedule_load_pg_dist_node:
                 return True
             self._schedule_load_pg_dist_node = False
 
@@ -128,15 +137,15 @@
         except Exception:
             return False
 
         with self._condition:
             self._pg_dist_node = {r[1]: PgDistNode(r[1], r[2], r[3], 'after_promote', r[0]) for r in cursor}
         return True
 
-    def sync_pg_dist_node(self, cluster):
+    def sync_pg_dist_node(self, cluster: Cluster) -> None:
         """Maintain the `pg_dist_node` from the coordinator leader every heartbeat loop.
 
         We can't always rely on REST API calls from worker nodes in order
         to maintain `pg_dist_node`, therefore at least once per heartbeat
         loop we make sure that workes registered in `self._pg_dist_node`
         cache are matching the cluster view from DCS by creating tasks
         the same way as it is done from the REST API."""
@@ -152,20 +161,20 @@
 
         for group, worker in cluster.workers.items():
             leader = worker.leader
             if leader and leader.conn_url\
                     and leader.data.get('role') in ('master', 'primary') and leader.data.get('state') == 'running':
                 self.add_task('after_promote', group, leader.conn_url)
 
-    def find_task_by_group(self, group):
+    def find_task_by_group(self, group: int) -> Optional[int]:
         for i, task in enumerate(self._tasks):
             if task.group == group:
                 return i
 
-    def pick_task(self):
+    def pick_task(self) -> Tuple[Optional[int], Optional[PgDistNode]]:
         """Returns the tuple(i, task), where `i` - is the task index in the self._tasks list
 
         Tasks are picked by following priorities:
         1. If there is already a transaction in progress, pick a task
            that that will change already affected worker primary.
         2. If the coordinator address should be changed - pick a task
            with group=0 (coordinators are always in group 0).
@@ -191,79 +200,89 @@
             # When tasks are added it could happen that self._pg_dist_node
             # wasn't ready (self._schedule_load_pg_dist_node is False)
             # and hence the nodeid wasn't filled.
             if task and task.group in self._pg_dist_node:
                 task.nodeid = self._pg_dist_node[task.group].nodeid
             return i, task
 
-    def update_node(self, task):
+    def update_node(self, task: PgDistNode) -> None:
         if task.nodeid is not None:
             self.query('SELECT pg_catalog.citus_update_node(%s, %s, %s, true, %s)',
                        task.nodeid, task.host, task.port, task.cooldown)
         elif task.event != 'before_demote':
-            task.nodeid = self.query("SELECT pg_catalog.citus_add_node(%s, %s, %s, 'primary', 'default')",
-                                     task.host, task.port, task.group).fetchone()[0]
+            row = self.query("SELECT pg_catalog.citus_add_node(%s, %s, %s, 'primary', 'default')",
+                             task.host, task.port, task.group).fetchone()
+            if row is not None:
+                task.nodeid = row[0]
 
-    def process_task(self, task):
+    def process_task(self, task: PgDistNode) -> bool:
         """Updates a single row in `pg_dist_node` table, optionally in a transaction.
 
-        The transaction is started if we do a demote of the worker node
-        or before promoting the other worker if there is not transaction
-        in progress. And, the transaction it is committed when the
-        switchover/failover completed.
-
-        This method returns `True` if node was updated (optionally,
-        transaction was committed) as an indicator that
-        the `self._pg_dist_node` cache should be updated.
+        The transaction is started if we do a demote of the worker node or before promoting the other worker if
+        there is no transaction in progress. And, the transaction is committed when the switchover/failover completed.
+
+        .. note:
+            The maximum lifetime of the transaction in progress is controlled outside of this method.
+
+        .. note:
+            Read access to `self._in_flight` isn't protected because we know it can't be changed outside of our thread.
 
-        The maximum lifetime of the transaction in progress
-        is controlled outside of this method."""
+        :param task: reference to a :class:`PgDistNode` object that represents a row to be updated/created.
+        :returns: `True` if the row was succesfully created/updated or transaction in progress
+            was committed as an indicator that the `self._pg_dist_node` cache should be updated,
+            or, if the new transaction was opened, this method returns `False`.
+        """
 
         if task.event == 'after_promote':
             # The after_promote may happen without previous before_demote and/or
             # before_promore.  In this case we just call self.update_node() method.
             # If there is a transaction in progress, it could be that it already did
             # required changes and we can simply COMMIT.
             if not self._in_flight or self._in_flight.host != task.host or self._in_flight.port != task.port:
                 self.update_node(task)
             if self._in_flight:
                 self.query('COMMIT')
-                self._in_flight = None
             return True
         else:  # before_demote, before_promote
             if task.timeout:
                 task.deadline = time.time() + task.timeout
             if not self._in_flight:
                 self.query('BEGIN')
             self.update_node(task)
-            self._in_flight = task
         return False
 
-    def process_tasks(self):
+    def process_tasks(self) -> None:
         while True:
+            # Read access to `_in_flight` isn't protected because we know it can't be changed outside of our thread.
             if not self._in_flight and not self.load_pg_dist_node():
                 break
 
             i, task = self.pick_task()
-            if not task:
+            if not task or i is None:
                 break
             try:
                 update_cache = self.process_task(task)
             except Exception as e:
                 logger.error('Exception when working with pg_dist_node: %r', e)
-                update_cache = False
+                update_cache = None
             with self._condition:
                 if self._tasks:
                     if update_cache:
                         self._pg_dist_node[task.group] = task
+
+                    if update_cache is False:  # an indicator that process_tasks has started a transaction
+                        self._in_flight = task
+                    else:
+                        self._in_flight = None
+
                     if id(self._tasks[i]) == id(task):
                         self._tasks.pop(i)
             task.wakeup()
 
-    def run(self):
+    def run(self) -> None:
         while True:
             try:
                 with self._condition:
                     if self._schedule_load_pg_dist_node:
                         timeout = -1
                     elif self._in_flight:
                         timeout = self._in_flight.deadline - time.time() if self._tasks else None
@@ -276,23 +295,31 @@
                         logger.warning('Rolling back transaction. Last known status: %s', self._in_flight)
                         self.query('ROLLBACK')
                         self._in_flight = None
                 self.process_tasks()
             except Exception:
                 logger.exception('run')
 
-    def _add_task(self, task):
+    def _add_task(self, task: PgDistNode) -> bool:
         with self._condition:
             i = self.find_task_by_group(task.group)
 
-            # task.timeout is None is an indicator that it was scheduled
-            # from the sync_pg_dist_node() and we don't want to override
-            # already existing task created from REST API.
-            if task.timeout is None and (i is not None or self._in_flight and self._in_flight.group == task.group):
-                return False
+            # The `PgDistNode.timeout` == None is an indicator that it was scheduled from the sync_pg_dist_node().
+            if task.timeout is None:
+                # We don't want to override the already existing task created from REST API.
+                if i is not None and self._tasks[i].timeout is not None:
+                    return False
+
+                # There is a little race condition with tasks created from REST API - the call made "before" the member
+                # key is updated in DCS. Therefore it is possible that :func:`sync_pg_dist_node` will try to create a
+                # task based on the outdated values of "state"/"role". To solve it we introduce an artificial timeout.
+                # Only when the timeout is reached new tasks could be scheduled from sync_pg_dist_node()
+                if self._in_flight and self._in_flight.group == task.group and self._in_flight.timeout is not None\
+                        and self._in_flight.deadline > time.time():
+                    return False
 
             # Override already existing task for the same worker group
             if i is not None:
                 if task != self._tasks[i]:
                     logger.debug('Overriding existing task: %s != %s', self._tasks[i], task)
                     self._tasks[i] = task
                     self._condition.notify()
@@ -302,49 +329,52 @@
                     or self._in_flight and task.group == self._in_flight.group:
                 logger.debug('Adding the new task: %s', task)
                 self._tasks.append(task)
                 self._condition.notify()
                 return True
         return False
 
-    def add_task(self, event, group, conn_url, timeout=None, cooldown=None):
+    def add_task(self, event: str, group: int, conn_url: str,
+                 timeout: Optional[float] = None, cooldown: Optional[float] = None) -> Optional[PgDistNode]:
         try:
             r = urlparse(conn_url)
         except Exception as e:
             return logger.error('Failed to parse connection url %s: %r', conn_url, e)
         host = r.hostname
-        port = r.port or 5432
-        task = PgDistNode(group, host, port, event, timeout=timeout, cooldown=cooldown)
-        return task if self._add_task(task) else None
+        if host:
+            port = r.port or 5432
+            task = PgDistNode(group, host, port, event, timeout=timeout, cooldown=cooldown)
+            return task if self._add_task(task) else None
 
-    def handle_event(self, cluster, event):
+    def handle_event(self, cluster: Cluster, event: Dict[str, Any]) -> None:
         if not self.is_alive():
             return
 
-        cluster = cluster.workers.get(event['group'])
-        if not (cluster and cluster.leader and cluster.leader.name == event['leader'] and cluster.leader.conn_url):
+        worker = cluster.workers.get(event['group'])
+        if not (worker and worker.leader and worker.leader.name == event['leader'] and worker.leader.conn_url):
             return
 
         task = self.add_task(event['type'], event['group'],
-                             cluster.leader.conn_url,
-                             event['timeout'], event['cooldown']*1000)
+                             worker.leader.conn_url,
+                             event['timeout'], event['cooldown'] * 1000)
         if task and event['type'] == 'before_demote':
             task.wait()
 
-    def bootstrap(self):
-        if not self.is_enabled():
+    def bootstrap(self) -> None:
+        if not isinstance(self._config, dict):  # self.is_enabled()
             return
 
         conn_kwargs = self._postgresql.config.local_connect_kwargs
         conn_kwargs['options'] = '-c synchronous_commit=local -c statement_timeout=0'
         if self._config['database'] != self._postgresql.database:
             conn = connect(**conn_kwargs)
             try:
                 with conn.cursor() as cur:
-                    cur.execute('CREATE DATABASE {0}'.format(quote_ident(self._config['database'], conn)))
+                    cur.execute('CREATE DATABASE {0}'.format(
+                        quote_ident(self._config['database'], conn)).encode('utf-8'))
             finally:
                 conn.close()
 
         conn_kwargs['dbname'] = self._config['database']
         conn = connect(**conn_kwargs)
         try:
             with conn.cursor() as cur:
@@ -360,15 +390,15 @@
                 if self.is_coordinator():
                     r = urlparse(self._postgresql.connection_string)
                     cur.execute("SELECT pg_catalog.citus_set_coordinator_host(%s, %s, 'primary', 'default')",
                                 (r.hostname, r.port or 5432))
         finally:
             conn.close()
 
-    def adjust_postgres_gucs(self, parameters):
+    def adjust_postgres_gucs(self, parameters: Dict[str, Any]) -> None:
         if not self.is_enabled():
             return
 
         # citus extension must be on the first place in shared_preload_libraries
         shared_preload_libraries = list(filter(
             lambda el: el and el != 'citus',
             [p.strip() for p in parameters.get('shared_preload_libraries', '').split(',')]))
@@ -377,13 +407,13 @@
         # if not explicitly set Citus overrides max_prepared_transactions to max_connections*2
         if parameters.get('max_prepared_transactions') == 0:
             parameters['max_prepared_transactions'] = parameters['max_connections'] * 2
 
         # Resharding in Citus implemented using logical replication
         parameters['wal_level'] = 'logical'
 
-    def ignore_replication_slot(self, slot):
-        if self.is_enabled() and self._postgresql.is_leader() and\
+    def ignore_replication_slot(self, slot: Dict[str, str]) -> bool:
+        if isinstance(self._config, dict) and self._postgresql.is_leader() and\
                 slot['type'] == 'logical' and slot['database'] == self._config['database']:
             m = CITUS_SLOT_NAME_RE.match(slot['name'])
-            return m and {'move': 'pgoutput', 'split': 'citus'}.get(m.group(1)) == slot['plugin']
+            return bool(m and {'move': 'pgoutput', 'split': 'citus'}.get(m.group(1)) == slot['plugin'])
         return False
```

### Comparing `patroni-3.0.2/patroni/postgresql/connection.py` & `patroni-3.0.3/patroni/postgresql/connection.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,46 +1,51 @@
 import logging
 
 from contextlib import contextmanager
 from threading import Lock
+from typing import Any, Dict, Generator, Union, TYPE_CHECKING
+if TYPE_CHECKING:  # pragma: no cover
+    from psycopg import Connection as Connection3, Cursor
+    from psycopg2 import connection, cursor
 
 from .. import psycopg
 
 logger = logging.getLogger(__name__)
 
 
 class Connection(object):
+    server_version: int
 
-    def __init__(self):
+    def __init__(self) -> None:
         self._lock = Lock()
         self._connection = None
         self._cursor_holder = None
 
-    def set_conn_kwargs(self, conn_kwargs):
+    def set_conn_kwargs(self, conn_kwargs: Dict[str, Any]) -> None:
         self._conn_kwargs = conn_kwargs
 
-    def get(self):
+    def get(self) -> Union['connection', 'Connection3[Any]']:
         with self._lock:
             if not self._connection or self._connection.closed != 0:
                 self._connection = psycopg.connect(**self._conn_kwargs)
-                self.server_version = self._connection.server_version
+                self.server_version = getattr(self._connection, 'server_version', 0)
         return self._connection
 
-    def cursor(self):
+    def cursor(self) -> Union['cursor', 'Cursor[Any]']:
         if not self._cursor_holder or self._cursor_holder.closed or self._cursor_holder.connection.closed != 0:
             logger.info("establishing a new patroni connection to the postgres cluster")
             self._cursor_holder = self.get().cursor()
         return self._cursor_holder
 
-    def close(self):
+    def close(self) -> None:
         if self._connection and self._connection.closed == 0:
             self._connection.close()
             logger.info("closed patroni connection to the postgresql cluster")
         self._cursor_holder = self._connection = None
 
 
 @contextmanager
-def get_connection_cursor(**kwargs):
+def get_connection_cursor(**kwargs: Any) -> Generator[Union['cursor', 'Cursor[Any]'], None, None]:
     conn = psycopg.connect(**kwargs)
     with conn.cursor() as cur:
         yield cur
     conn.close()
```

### Comparing `patroni-3.0.2/patroni/postgresql/misc.py` & `patroni-3.0.3/patroni/postgresql/misc.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 import errno
 import logging
 import os
 
-from patroni.exceptions import PostgresException
+from typing import Iterable, Tuple
+
+from ..exceptions import PostgresException
 
 logger = logging.getLogger(__name__)
 
 
 def postgres_version_to_int(pg_version: str) -> int:
     """Convert the server_version to integer
 
@@ -51,37 +53,35 @@
     100000
     >>> postgres_major_version_to_int('9.6')
     90600
     """
     return postgres_version_to_int(pg_version + '.0')
 
 
-def parse_lsn(lsn):
+def parse_lsn(lsn: str) -> int:
     t = lsn.split('/')
     return int(t[0], 16) * 0x100000000 + int(t[1], 16)
 
 
-def parse_history(data):
+def parse_history(data: str) -> Iterable[Tuple[int, int, str]]:
     for line in data.split('\n'):
         values = line.strip().split('\t')
         if len(values) == 3:
             try:
-                values[0] = int(values[0])
-                values[1] = parse_lsn(values[1])
-                yield values
+                yield int(values[0]), parse_lsn(values[1]), values[2]
             except (IndexError, ValueError):
                 logger.exception('Exception when parsing timeline history line "%s"', values)
 
 
-def format_lsn(lsn, full=False):
+def format_lsn(lsn: int, full: bool = False) -> str:
     template = '{0:X}/{1:08X}' if full else '{0:X}/{1:X}'
     return template.format(lsn >> 32, lsn & 0xFFFFFFFF)
 
 
-def fsync_dir(path):
+def fsync_dir(path: str) -> None:
     if os.name != 'nt':
         fd = os.open(path, os.O_DIRECTORY)
         try:
             os.fsync(fd)
         except OSError as e:
             # Some filesystems don't like fsyncing directories and raise EINVAL. Ignoring it is usually safe.
             if e.errno != errno.EINVAL:
```

### Comparing `patroni-3.0.2/patroni/postgresql/postmaster.py` & `patroni-3.0.3/patroni/postgresql/postmaster.py`

 * *Files 16% similar despite different names*

```diff
@@ -3,14 +3,17 @@
 import os
 import psutil
 import re
 import signal
 import subprocess
 import sys
 
+from multiprocessing.connection import Connection
+from typing import Dict, Optional, List
+
 from patroni import PATRONI_ENV_PREFIX, KUBERNETES_ENV_PREFIX
 
 # avoid spawning the resource tracker process
 if sys.version_info >= (3, 8):  # pragma: no cover
     import multiprocessing.resource_tracker
     multiprocessing.resource_tracker.getfd = lambda: 0
 elif sys.version_info >= (3, 4):  # pragma: no cover
@@ -22,49 +25,50 @@
 STOP_SIGNALS = {
     'smart': 'TERM',
     'fast': 'INT',
     'immediate': 'QUIT',
 }
 
 
-def pg_ctl_start(conn, cmdline, env):
+def pg_ctl_start(conn: Connection, cmdline: List[str], env: Dict[str, str]) -> None:
     if os.name != 'nt':
         os.setsid()
     try:
         postmaster = subprocess.Popen(cmdline, close_fds=True, env=env)
         conn.send(postmaster.pid)
     except Exception:
         logger.exception('Failed to execute %s', cmdline)
         conn.send(None)
     conn.close()
 
 
 class PostmasterProcess(psutil.Process):
 
-    def __init__(self, pid):
+    def __init__(self, pid: int) -> None:
+        self._postmaster_pid: Dict[str, str]
         self.is_single_user = False
         if pid < 0:
             pid = -pid
             self.is_single_user = True
         super(PostmasterProcess, self).__init__(pid)
 
     @staticmethod
-    def _read_postmaster_pidfile(data_dir):
+    def _read_postmaster_pidfile(data_dir: str) -> Dict[str, str]:
         """Reads and parses postmaster.pid from the data directory
 
         :returns dictionary of values if successful, empty dictionary otherwise
         """
         pid_line_names = ['pid', 'data_dir', 'start_time', 'port', 'socket_dir', 'listen_addr', 'shmem_key']
         try:
             with open(os.path.join(data_dir, 'postmaster.pid')) as f:
                 return {name: line.rstrip('\n') for name, line in zip(pid_line_names, f)}
         except IOError:
             return {}
 
-    def _is_postmaster_process(self):
+    def _is_postmaster_process(self) -> bool:
         try:
             start_time = int(self._postmaster_pid.get('start_time', 0))
             if start_time and abs(self.create_time() - start_time) > 3:
                 logger.info('Process %s is not postmaster, too much difference between PID file start time %s and '
                             'process start time %s', self.pid, self.create_time(), start_time)
                 return False
         except ValueError:
@@ -75,41 +79,41 @@
             logger.info('Patroni (pid=%s, ppid=%s), "fake postmaster" (pid=%s, ppid=%s)',
                         os.getpid(), os.getppid(), self.pid, self.ppid())
             return False
 
         return True
 
     @classmethod
-    def _from_pidfile(cls, data_dir):
+    def _from_pidfile(cls, data_dir: str) -> Optional['PostmasterProcess']:
         postmaster_pid = PostmasterProcess._read_postmaster_pidfile(data_dir)
         try:
             pid = int(postmaster_pid.get('pid', 0))
             if pid:
                 proc = cls(pid)
                 proc._postmaster_pid = postmaster_pid
                 return proc
         except ValueError:
-            pass
+            return None
 
     @staticmethod
-    def from_pidfile(data_dir):
+    def from_pidfile(data_dir: str) -> Optional['PostmasterProcess']:
         try:
             proc = PostmasterProcess._from_pidfile(data_dir)
             return proc if proc and proc._is_postmaster_process() else None
         except psutil.NoSuchProcess:
             return None
 
     @classmethod
-    def from_pid(cls, pid):
+    def from_pid(cls, pid: int) -> Optional['PostmasterProcess']:
         try:
             return cls(pid)
         except psutil.NoSuchProcess:
             return None
 
-    def signal_kill(self):
+    def signal_kill(self) -> bool:
         """to suspend and kill postmaster and all children
 
         :returns True if postmaster and children are killed, False if error
         """
         try:
             self.suspend()
         except psutil.NoSuchProcess:
@@ -137,15 +141,15 @@
             try:
                 child.kill()
             except psutil.Error:
                 pass
         psutil.wait_procs(children + [self])
         return True
 
-    def signal_stop(self, mode, pg_ctl='pg_ctl'):
+    def signal_stop(self, mode: str, pg_ctl: str = 'pg_ctl') -> Optional[bool]:
         """Signal postmaster process to stop
 
         :returns None if signaled, True if process is already gone, False if error
         """
         if self.is_single_user:
             logger.warning("Cannot stop server; single-user server is running (PID: {0})".format(self.pid))
             return False
@@ -157,57 +161,57 @@
             return True
         except psutil.AccessDenied as e:
             logger.warning("Could not send stop signal to PostgreSQL (error: {0})".format(e))
             return False
 
         return None
 
-    def pg_ctl_kill(self, mode, pg_ctl):
+    def pg_ctl_kill(self, mode: str, pg_ctl: str) -> Optional[bool]:
         try:
             status = subprocess.call([pg_ctl, "kill", STOP_SIGNALS[mode], str(self.pid)])
         except OSError:
             return False
         if status == 0:
             return None
         else:
             return not self.is_running()
 
-    def wait_for_user_backends_to_close(self, stop_timeout):
+    def wait_for_user_backends_to_close(self, stop_timeout: Optional[float]) -> None:
         # These regexps are cross checked against versions PostgreSQL 9.1 .. 15
         aux_proc_re = re.compile("(?:postgres:)( .*:)? (?:(?:archiver|startup|autovacuum launcher|autovacuum worker|"
                                  "checkpointer|logger|stats collector|wal receiver|wal writer|writer)(?: process  )?|"
                                  "walreceiver|wal sender process|walsender|walwriter|background writer|"
                                  "logical replication launcher|logical replication worker for|bgworker:) ")
 
         try:
             children = self.children()
         except psutil.Error:
             return logger.debug('Failed to get list of postmaster children')
 
-        user_backends = []
-        user_backends_cmdlines = {}
+        user_backends: List[psutil.Process] = []
+        user_backends_cmdlines: Dict[int, str] = {}
         for child in children:
             try:
                 cmdline = child.cmdline()
                 if cmdline and not aux_proc_re.match(cmdline[0]):
                     user_backends.append(child)
                     user_backends_cmdlines[child.pid] = cmdline[0]
             except psutil.NoSuchProcess:
                 pass
         if user_backends:
             logger.debug('Waiting for user backends %s to close', ', '.join(user_backends_cmdlines.values()))
-            gone, live = psutil.wait_procs(user_backends, stop_timeout)
+            _, live = psutil.wait_procs(user_backends, stop_timeout)
             if stop_timeout and live:
                 live = [user_backends_cmdlines[b.pid] for b in live]
                 logger.warning('Backends still alive after %s: %s', stop_timeout, ', '.join(live))
             else:
                 logger.debug("Backends closed")
 
     @staticmethod
-    def start(pgcommand, data_dir, conf, options):
+    def start(pgcommand: str, data_dir: str, conf: str, options: List[str]) -> Optional['PostmasterProcess']:
         # Unfortunately `pg_ctl start` does not return postmaster pid to us. Without this information
         # it is hard to know the current state of postgres startup, so we had to reimplement pg_ctl start
         # in python. It will start postgres, wait for port to be open and wait until postgres will start
         # accepting connections.
         # Important!!! We can't just start postgres using subprocess.Popen, because in this case it
         # will be our child for the rest of our live and we will have to take care of it (`waitpid`).
         # So we will use the same approach as pg_ctl uses: start a new process, which will start postgres.
@@ -230,15 +234,15 @@
                 # in the `PG_GRANDPARENT_PID` environment variable and postmaster will ignore it.
                 logger.info("Telling pg_ctl that it is safe to ignore postmaster.pid for process %s", proc.pid)
                 env['PG_GRANDPARENT_PID'] = str(proc.pid)
         except psutil.NoSuchProcess:
             pass
         cmdline = [pgcommand, '-D', data_dir, '--config-file={}'.format(conf)] + options
         logger.debug("Starting postgres: %s", " ".join(cmdline))
-        ctx = multiprocessing.get_context('spawn') if sys.version_info >= (3, 4) else multiprocessing
+        ctx = multiprocessing.get_context('spawn')
         parent_conn, child_conn = ctx.Pipe(False)
         proc = ctx.Process(target=pg_ctl_start, args=(child_conn, cmdline, env))
         proc.start()
         pid = parent_conn.recv()
         proc.join()
         if pid is None:
             return
```

### Comparing `patroni-3.0.2/patroni/postgresql/rewind.py` & `patroni-3.0.3/patroni/postgresql/rewind.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,44 +1,54 @@
 import logging
 import os
 import re
 import shlex
 import shutil
 import subprocess
 
+from enum import IntEnum
 from threading import Lock, Thread
+from typing import Any, Callable, Dict, List, Optional, Union, Tuple
 
+from . import Postgresql
 from .connection import get_connection_cursor
 from .misc import format_lsn, fsync_dir, parse_history, parse_lsn
 from ..async_executor import CriticalTask
-from ..dcs import Leader
+from ..dcs import Leader, RemoteMember
 
 logger = logging.getLogger(__name__)
 
-REWIND_STATUS = type('Enum', (), {'INITIAL': 0, 'CHECKPOINT': 1, 'CHECK': 2, 'NEED': 3,
-                                  'NOT_NEED': 4, 'SUCCESS': 5, 'FAILED': 6})
+
+class REWIND_STATUS(IntEnum):
+    INITIAL = 0
+    CHECKPOINT = 1
+    CHECK = 2
+    NEED = 3
+    NOT_NEED = 4
+    SUCCESS = 5
+    FAILED = 6
 
 
 class Rewind(object):
 
-    def __init__(self, postgresql):
+    def __init__(self, postgresql: Postgresql) -> None:
         self._postgresql = postgresql
         self._checkpoint_task_lock = Lock()
         self.reset_state()
 
     @staticmethod
-    def configuration_allows_rewind(data):
+    def configuration_allows_rewind(data: Dict[str, str]) -> bool:
         return data.get('wal_log_hints setting', 'off') == 'on' or data.get('Data page checksum version', '0') != '0'
 
     @property
-    def enabled(self):
-        return self._postgresql.config.get('use_pg_rewind')
+    def enabled(self) -> bool:
+        return bool(self._postgresql.config.get('use_pg_rewind'))
 
     @property
-    def can_rewind(self):
+    def can_rewind(self) -> bool:
         """ check if pg_rewind executable is there and that pg_controldata indicates
             we have either wal_log_hints or checksums turned on
         """
         # low-hanging fruit: check if pg_rewind configuration is there
         if not self.enabled:
             return False
 
@@ -48,64 +58,66 @@
             if ret != 0:  # pg_rewind is not there, close up the shop and go home
                 return False
         except OSError:
             return False
         return self.configuration_allows_rewind(self._postgresql.controldata())
 
     @property
-    def should_remove_data_directory_on_diverged_timelines(self):
-        return self._postgresql.config.get('remove_data_directory_on_diverged_timelines')
+    def should_remove_data_directory_on_diverged_timelines(self) -> bool:
+        return bool(self._postgresql.config.get('remove_data_directory_on_diverged_timelines'))
 
     @property
-    def can_rewind_or_reinitialize_allowed(self):
+    def can_rewind_or_reinitialize_allowed(self) -> bool:
         return self.should_remove_data_directory_on_diverged_timelines or self.can_rewind
 
-    def trigger_check_diverged_lsn(self):
+    def trigger_check_diverged_lsn(self) -> None:
         if self.can_rewind_or_reinitialize_allowed and self._state != REWIND_STATUS.NEED:
             self._state = REWIND_STATUS.CHECK
 
     @staticmethod
-    def check_leader_is_not_in_recovery(conn_kwargs):
+    def check_leader_is_not_in_recovery(conn_kwargs: Dict[str, Any]) -> Optional[bool]:
         try:
             with get_connection_cursor(connect_timeout=3, options='-c statement_timeout=2000', **conn_kwargs) as cur:
                 cur.execute('SELECT pg_catalog.pg_is_in_recovery()')
-                if not cur.fetchone()[0]:
+                row = cur.fetchone()
+                if not row or not row[0]:
                     return True
                 logger.info('Leader is still in_recovery and therefore can\'t be used for rewind')
         except Exception:
             return logger.exception('Exception when working with leader')
 
     @staticmethod
-    def check_leader_has_run_checkpoint(conn_kwargs):
+    def check_leader_has_run_checkpoint(conn_kwargs: Dict[str, Any]) -> Optional[str]:
         try:
             with get_connection_cursor(connect_timeout=3, options='-c statement_timeout=2000', **conn_kwargs) as cur:
-                cur.execute("SELECT NOT pg_catalog.pg_is_in_recovery()" +
-                            " AND ('x' || pg_catalog.substr(pg_catalog.pg_walfile_name(" +
-                            " pg_catalog.pg_current_wal_lsn()), 1, 8))::bit(32)::int = timeline_id" +
+                cur.execute("SELECT NOT pg_catalog.pg_is_in_recovery()"
+                            " AND ('x' || pg_catalog.substr(pg_catalog.pg_walfile_name("
+                            " pg_catalog.pg_current_wal_lsn()), 1, 8))::bit(32)::int = timeline_id"
                             " FROM pg_catalog.pg_control_checkpoint()")
-                if not cur.fetchone()[0]:
+                row = cur.fetchone()
+                if not row or not row[0]:
                     return 'leader has not run a checkpoint yet'
         except Exception:
             logger.exception('Exception when working with leader')
             return 'not accessible or not healty'
 
-    def _get_checkpoint_end(self, timeline, lsn):
+    def _get_checkpoint_end(self, timeline: int, lsn: int) -> int:
         """The checkpoint record size in WAL depends on postgres major version and platform (memory alignment).
         Hence, the only reliable way to figure out where it ends, read the record from file with the help of pg_waldump
         and parse the output. We are trying to read two records, and expect that it will fail to read the second one:
         `pg_waldump: fatal: error in WAL record at 0/182E220: invalid record length at 0/182E298: wanted 24, got 0`
         The error message contains information about LSN of the next record, which is exactly where checkpoint ends."""
 
         lsn8 = format_lsn(lsn, True)
-        lsn = format_lsn(lsn)
-        out, err = self._postgresql.waldump(timeline, lsn, 2)
+        lsn_str = format_lsn(lsn)
+        out, err = self._postgresql.waldump(timeline, lsn_str, 2)
         if out is not None and err is not None:
             out = out.decode('utf-8').rstrip().split('\n')
             err = err.decode('utf-8').rstrip().split('\n')
-            pattern = 'error in WAL record at {0}: invalid record length at '.format(lsn)
+            pattern = 'error in WAL record at {0}: invalid record length at '.format(lsn_str)
 
             if len(out) == 1 and len(err) == 1 and ', lsn: {0}, prev '.format(lsn8) in out[0] and pattern in err[0]:
                 i = err[0].find(pattern) + len(pattern)
                 j = err[0].find(": wanted ", i)
                 if j > -1:
                     try:
                         return parse_lsn(err[0][i:j])
@@ -113,81 +125,82 @@
                         logger.error('Failed to parse lsn %s: %r', err[0][i:j], e)
             logger.error('Failed to parse pg_%sdump output', self._postgresql.wal_name)
             logger.error(' stdout=%s', '\n'.join(out))
             logger.error(' stderr=%s', '\n'.join(err))
 
         return 0
 
-    def _get_local_timeline_lsn_from_controldata(self):
+    def _get_local_timeline_lsn_from_controldata(self) -> Tuple[Optional[bool], Optional[int], Optional[int]]:
         in_recovery = timeline = lsn = None
         data = self._postgresql.controldata()
         try:
             if data.get('Database cluster state') in ('shut down in recovery', 'in archive recovery'):
                 in_recovery = True
                 lsn = data.get('Minimum recovery ending location')
-                timeline = int(data.get("Min recovery ending loc's timeline"))
+                timeline = int(data.get("Min recovery ending loc's timeline", ""))
                 if lsn == '0/0' or timeline == 0:  # it was a primary when it crashed
                     data['Database cluster state'] = 'shut down'
             if data.get('Database cluster state') == 'shut down':
                 in_recovery = False
                 lsn = data.get('Latest checkpoint location')
-                timeline = int(data.get("Latest checkpoint's TimeLineID"))
+                timeline = int(data.get("Latest checkpoint's TimeLineID", ""))
         except (TypeError, ValueError):
             logger.exception('Failed to get local timeline and lsn from pg_controldata output')
 
         if lsn is not None:
             try:
                 lsn = parse_lsn(lsn)
             except (IndexError, ValueError) as e:
                 logger.error('Exception when parsing lsn %s: %r', lsn, e)
                 lsn = None
 
         return in_recovery, timeline, lsn
 
-    def _get_local_timeline_lsn(self):
+    def _get_local_timeline_lsn(self) -> Tuple[Optional[bool], Optional[int], Optional[int]]:
         if self._postgresql.is_running():  # if postgres is running - get timeline from replication connection
             in_recovery = True
             timeline = self._postgresql.received_timeline() or self._postgresql.get_replica_timeline()
             lsn = self._postgresql.replayed_location()
         else:  # otherwise analyze pg_controldata output
             in_recovery, timeline, lsn = self._get_local_timeline_lsn_from_controldata()
 
         log_lsn = format_lsn(lsn) if isinstance(lsn, int) else lsn
         logger.info('Local timeline=%s lsn=%s', timeline, log_lsn)
         return in_recovery, timeline, lsn
 
     @staticmethod
-    def _log_primary_history(history, i):
+    def _log_primary_history(history: List[Tuple[int, int, str]], i: int) -> None:
         start = max(0, i - 3)
         end = None if i + 4 >= len(history) else i + 2
-        history_show = []
+        history_show: List[str] = []
 
-        def format_history_line(line):
+        def format_history_line(line: Tuple[int, int, str]) -> str:
             return '{0}\t{1}\t{2}'.format(line[0], format_lsn(line[1]), line[2])
 
+        line = None
         for line in history[start:end]:
             history_show.append(format_history_line(line))
 
         if line != history[-1]:
             history_show.append('...')
             history_show.append(format_history_line(history[-1]))
 
         logger.info('primary: history=%s', '\n'.join(history_show))
 
-    def _conn_kwargs(self, member, auth):
+    def _conn_kwargs(self, member: Union[Leader, RemoteMember], auth: Dict[str, Any]) -> Dict[str, Any]:
         ret = member.conn_kwargs(auth)
         if not ret.get('dbname'):
             ret['dbname'] = self._postgresql.database
         # Add target_session_attrs in case more than one hostname is specified
         # (libpq client-side failover) making sure we hit the primary
         if 'target_session_attrs' not in ret and self._postgresql.major_version >= 100000:
             ret['target_session_attrs'] = 'read-write'
         return ret
 
-    def _check_timeline_and_lsn(self, leader):
+    def _check_timeline_and_lsn(self, leader: Union[Leader, RemoteMember]) -> None:
         in_recovery, local_timeline, local_lsn = self._get_local_timeline_lsn()
         if local_timeline is None or local_lsn is None:
             return
 
         if isinstance(leader, Leader) and leader.member.data.get('role') not in ('master', 'primary'):
             return
 
@@ -201,31 +214,36 @@
         if not self.check_leader_is_not_in_recovery(self._conn_kwargs(leader, check_credentials)):
             return
 
         history = need_rewind = None
         try:
             with self._postgresql.get_replication_connection_cursor(**leader.conn_kwargs()) as cur:
                 cur.execute('IDENTIFY_SYSTEM')
-                primary_timeline = cur.fetchone()[1]
-                logger.info('primary_timeline=%s', primary_timeline)
-                if local_timeline > primary_timeline:  # Not always supported by pg_rewind
-                    need_rewind = True
-                elif local_timeline == primary_timeline:
-                    need_rewind = False
-                elif primary_timeline > 1:
-                    cur.execute('TIMELINE_HISTORY {0}'.format(primary_timeline))
-                    history = cur.fetchone()[1]
-                    if not isinstance(history, str):
-                        history = bytes(history).decode('utf-8')
-                    logger.debug('primary: history=%s', history)
+                row = cur.fetchone()
+                if row:
+                    primary_timeline = row[1]
+                    logger.info('primary_timeline=%s', primary_timeline)
+                    if local_timeline > primary_timeline:  # Not always supported by pg_rewind
+                        need_rewind = True
+                    elif local_timeline == primary_timeline:
+                        need_rewind = False
+                    elif primary_timeline > 1:
+                        cur.execute('TIMELINE_HISTORY {0}'.format(primary_timeline).encode('utf-8'))
+                        row = cur.fetchone()
+                        if row:
+                            history = row[1]
+                            if not isinstance(history, str):
+                                history = bytes(history).decode('utf-8')
+                            logger.debug('primary: history=%s', history)
         except Exception:
             return logger.exception('Exception when working with primary via replication connection')
 
         if history is not None:
             history = list(parse_history(history))
+            i = len(history)
             for i, (parent_timeline, switchpoint, _) in enumerate(history):
                 if parent_timeline == local_timeline:
                     # We don't need to rewind when:
                     # 1. for replica: replayed location is not ahead of switchpoint
                     # 2. for the former primary: end of checkpoint record is the same as switchpoint
                     if in_recovery:
                         need_rewind = local_lsn > switchpoint
@@ -239,30 +257,30 @@
                     break
             else:
                 need_rewind = True
             self._log_primary_history(history, i)
 
         self._state = need_rewind and REWIND_STATUS.NEED or REWIND_STATUS.NOT_NEED
 
-    def rewind_or_reinitialize_needed_and_possible(self, leader):
+    def rewind_or_reinitialize_needed_and_possible(self, leader: Union[Leader, RemoteMember, None]) -> bool:
         if leader and leader.name != self._postgresql.name and leader.conn_url and self._state == REWIND_STATUS.CHECK:
             self._check_timeline_and_lsn(leader)
-        return leader and leader.conn_url and self._state == REWIND_STATUS.NEED
+        return bool(leader and leader.conn_url) and self._state == REWIND_STATUS.NEED
 
-    def __checkpoint(self, task, wakeup):
+    def __checkpoint(self, task: CriticalTask, wakeup: Callable[..., Any]) -> None:
         try:
             result = self._postgresql.checkpoint()
         except Exception as e:
             result = 'Exception: ' + str(e)
         with task:
             task.complete(not bool(result))
             if task.result:
                 wakeup()
 
-    def ensure_checkpoint_after_promote(self, wakeup):
+    def ensure_checkpoint_after_promote(self, wakeup: Callable[..., Any]) -> None:
         """After promote issue a CHECKPOINT from a new thread and asynchronously check the result.
         In case if CHECKPOINT failed, just check that timeline in pg_control was updated."""
 
         if self._state == REWIND_STATUS.INITIAL and self._postgresql.is_leader():
             with self._checkpoint_task_lock:
                 if self._checkpoint_task:
                     with self._checkpoint_task:
@@ -271,18 +289,18 @@
                             self._checkpoint_task = None
                 elif self._postgresql.get_primary_timeline() == self._postgresql.pg_control_timeline():
                     self._state = REWIND_STATUS.CHECKPOINT
                 else:
                     self._checkpoint_task = CriticalTask()
                     Thread(target=self.__checkpoint, args=(self._checkpoint_task, wakeup)).start()
 
-    def checkpoint_after_promote(self):
+    def checkpoint_after_promote(self) -> bool:
         return self._state == REWIND_STATUS.CHECKPOINT
 
-    def _buid_archiver_command(self, command, wal_filename):
+    def _buid_archiver_command(self, command: str, wal_filename: str) -> str:
         """Replace placeholders in the given archiver command's template.
         Applicable for archive_command and restore_command.
         Can also be used for archive_cleanup_command and recovery_end_command,
         however %r value is always set to 000000010000000000000001."""
         cmd = ''
         length = len(command)
         i = 0
@@ -302,34 +320,34 @@
                     i -= 1
             else:
                 cmd += command[i]
             i += 1
 
         return cmd
 
-    def _fetch_missing_wal(self, restore_command, wal_filename):
+    def _fetch_missing_wal(self, restore_command: str, wal_filename: str) -> bool:
         cmd = self._buid_archiver_command(restore_command, wal_filename)
 
         logger.info('Trying to fetch the missing wal: %s', cmd)
         return self._postgresql.cancellable.call(shlex.split(cmd)) == 0
 
-    def _find_missing_wal(self, data):
+    def _find_missing_wal(self, data: bytes) -> Optional[str]:
         # could not open file "$PGDATA/pg_wal/0000000A00006AA100000068": No such file or directory
         pattern = 'could not open file "'
         for line in data.decode('utf-8').split('\n'):
             b = line.find(pattern)
             if b > -1:
                 b += len(pattern)
                 e = line.find('": ', b)
                 if e > -1 and '/' in line[b:e]:
                     waldir, wal_filename = line[b:e].rsplit('/', 1)
                     if waldir.endswith('/pg_' + self._postgresql.wal_name) and len(wal_filename) == 24:
                         return wal_filename
 
-    def _archive_ready_wals(self):
+    def _archive_ready_wals(self) -> None:
         """Try to archive WALs that have .ready files just in case
         archive_mode was not set to 'always' before promote, while
         after it the WALs were recycled on the promoted replica.
         With this we prevent the entire loss of such WALs and the
         consequent old leader's start failure."""
         archive_mode = self._postgresql.get_guc_value('archive_mode')
         archive_cmd = self._postgresql.get_guc_value('archive_command')
@@ -357,52 +375,53 @@
                     try:
                         shutil.move(old_name, new_name)
                     except Exception as e:
                         logger.error('Unable to rename %s to %s: %r', old_name, new_name, e)
                 else:
                     logger.info('Failed to archive WAL segment %s', wal)
 
-    def _maybe_clean_pg_replslot(self):
+    def _maybe_clean_pg_replslot(self) -> None:
         """Clean pg_replslot directory if pg version is less then 11
         (pg_rewind deletes $PGDATA/pg_replslot content only since pg11)."""
         if self._postgresql.major_version < 110000:
             replslot_dir = self._postgresql.slots_handler.pg_replslot_dir
             try:
                 for f in os.listdir(replslot_dir):
                     shutil.rmtree(os.path.join(replslot_dir, f))
                 fsync_dir(replslot_dir)
             except Exception as e:
                 logger.warning('Unable to clean %s: %r', replslot_dir, e)
 
-    def pg_rewind(self, r):
+    def pg_rewind(self, r: Dict[str, Any]) -> bool:
         # prepare pg_rewind connection
         env = self._postgresql.config.write_pgpass(r)
         env.update(LANG='C', LC_ALL='C', PGOPTIONS='-c statement_timeout=0')
         dsn = self._postgresql.config.format_dsn(r, True)
         logger.info('running pg_rewind from %s', dsn)
 
-        restore_command = self._postgresql.config.get('recovery_conf', {}).get('restore_command') \
+        restore_command = (self._postgresql.config.get('recovery_conf') or {}).get('restore_command') \
             if self._postgresql.major_version < 120000 else self._postgresql.get_guc_value('restore_command')
 
         # Until v15 pg_rewind expected postgresql.conf to be inside $PGDATA, which is not the case on e.g. Debian
-        pg_rewind_can_restore = restore_command and (self._postgresql.major_version >= 150000 or
-                                                     (self._postgresql.major_version >= 130000 and
-                                                      self._postgresql.config._config_dir == self._postgresql.data_dir))
+        pg_rewind_can_restore = restore_command and (self._postgresql.major_version >= 150000
+                                                     or (self._postgresql.major_version >= 130000
+                                                         and self._postgresql.config.config_dir
+                                                         == self._postgresql.data_dir))
 
         cmd = [self._postgresql.pgcommand('pg_rewind')]
         if pg_rewind_can_restore:
             cmd.append('--restore-target-wal')
             if self._postgresql.major_version >= 150000 and\
-                    self._postgresql.config._config_dir != self._postgresql.data_dir:
+                    self._postgresql.config.config_dir != self._postgresql.data_dir:
                 cmd.append('--config-file={0}'.format(self._postgresql.config.postgresql_conf))
 
         cmd.extend(['-D', self._postgresql.data_dir, '--source-server', dsn])
 
         while True:
-            results = {}
+            results: Dict[str, bytes] = {}
             ret = self._postgresql.cancellable.call(cmd, env=env, communicate=results)
 
             logger.info('pg_rewind exit code=%s', ret)
             if ret is None:
                 return False
 
             logger.info(' stdout=%s', results['stdout'].decode('utf-8'))
@@ -417,15 +436,15 @@
             if not missing_wal:
                 return False
 
             if not self._fetch_missing_wal(restore_command, missing_wal):
                 logger.info('Failed to fetch WAL segment %s required for pg_rewind', missing_wal)
                 return False
 
-    def execute(self, leader):
+    def execute(self, leader: Union[Leader, RemoteMember]) -> Optional[bool]:
         if self._postgresql.is_running() and not self._postgresql.stop(checkpoint=False):
             return logger.warning('Can not run pg_rewind because postgres is still running')
 
         self._archive_ready_wals()
 
         # prepare pg_rewind connection
         r = self._conn_kwargs(leader, self._postgresql.config.rewind_credentials)
@@ -435,15 +454,15 @@
         #   running a checkpoint or
         #   waiting until Patroni on the primary will expose checkpoint_after_promote=True
         checkpoint_status = leader.checkpoint_after_promote if isinstance(leader, Leader) else None
         if checkpoint_status is None:  # we are the standby-cluster leader or primary still runs the old Patroni
             # superuser credentials match rewind_credentials if the latter are not provided or we run 10 or older
             if self._postgresql.config.superuser == self._postgresql.config.rewind_credentials:
                 leader_status = self._postgresql.checkpoint(
-                        self._conn_kwargs(leader, self._postgresql.config.superuser))
+                    self._conn_kwargs(leader, self._postgresql.config.superuser))
             else:  # we run 11+ and have a dedicated pg_rewind user
                 leader_status = self.check_leader_has_run_checkpoint(r)
             if leader_status:  # we tried to run/check for a checkpoint on the remote leader, but it failed
                 return logger.warning('Can not use %s for rewind: %s', leader.name, leader_status)
         elif not checkpoint_status:
             return logger.info('Waiting for checkpoint on %s before rewind', leader.name)
         elif not self.check_leader_is_not_in_recovery(r):
@@ -466,77 +485,78 @@
                     if self._postgresql.config.get(name):
                         logger.warning('%s is set. removing...', name)
                         self._postgresql.remove_data_directory()
                         self._state = REWIND_STATUS.INITIAL
                         break
         return False
 
-    def reset_state(self):
+    def reset_state(self) -> None:
         self._state = REWIND_STATUS.INITIAL
         with self._checkpoint_task_lock:
             self._checkpoint_task = None
 
     @property
-    def is_needed(self):
+    def is_needed(self) -> bool:
         return self._state in (REWIND_STATUS.CHECK, REWIND_STATUS.NEED)
 
     @property
-    def executed(self):
+    def executed(self) -> bool:
         return self._state > REWIND_STATUS.NOT_NEED
 
     @property
-    def failed(self):
+    def failed(self) -> bool:
         return self._state == REWIND_STATUS.FAILED
 
-    def read_postmaster_opts(self):
+    def read_postmaster_opts(self) -> Dict[str, str]:
         """returns the list of option names/values from postgres.opts, Empty dict if read failed or no file"""
-        result = {}
+        result: Dict[str, str] = {}
         try:
             with open(os.path.join(self._postgresql.data_dir, 'postmaster.opts')) as f:
                 data = f.read()
                 for opt in data.split('" "'):
                     if '=' in opt and opt.startswith('--'):
                         name, val = opt.split('=', 1)
                         result[name.strip('-')] = val.rstrip('"\n')
         except IOError:
             logger.exception('Error when reading postmaster.opts')
         return result
 
-    def single_user_mode(self, communicate=None, options=None):
+    def single_user_mode(self, communicate: Optional[Dict[str, Any]] = None,
+                         options: Optional[Dict[str, str]] = None) -> Optional[int]:
         """run a given command in a single-user mode. If the command is empty - then just start and stop"""
         cmd = [self._postgresql.pgcommand('postgres'), '--single', '-D', self._postgresql.data_dir]
         for opt, val in sorted((options or {}).items()):
             cmd.extend(['-c', '{0}={1}'.format(opt, val)])
         # need a database name to connect
         cmd.append('template1')
         return self._postgresql.cancellable.call(cmd, communicate=communicate)
 
-    def cleanup_archive_status(self):
+    def cleanup_archive_status(self) -> None:
         status_dir = os.path.join(self._postgresql.wal_dir, 'archive_status')
         try:
             for f in os.listdir(status_dir):
                 path = os.path.join(status_dir, f)
                 try:
                     if os.path.islink(path):
                         os.unlink(path)
                     elif os.path.isfile(path):
                         os.remove(path)
                 except OSError:
                     logger.exception('Unable to remove %s', path)
         except OSError:
             logger.exception('Unable to list %s', status_dir)
 
-    def ensure_clean_shutdown(self):
+    def ensure_clean_shutdown(self) -> Optional[bool]:
         self._archive_ready_wals()
         self.cleanup_archive_status()
 
         # Start in a single user mode and stop to produce a clean shutdown
         opts = self.read_postmaster_opts()
         opts.update({'archive_mode': 'on', 'archive_command': 'false'})
         self._postgresql.config.remove_recovery_conf()
-        output = {}
+        output: Dict[str, bytes] = {}
         ret = self.single_user_mode(communicate=output, options=opts)
         if ret != 0:
             logger.error('Crash recovery finished with code=%s', ret)
             logger.info(' stdout=%s', output['stdout'].decode('utf-8'))
             logger.info(' stderr=%s', output['stderr'].decode('utf-8'))
         return ret == 0 or None
```

### Comparing `patroni-3.0.2/patroni/postgresql/slots.py` & `patroni-3.0.3/patroni/postgresql/slots.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,44 +1,51 @@
 import logging
 import os
 import shutil
 
 from collections import defaultdict
 from contextlib import contextmanager
 from threading import Condition, Thread
+from typing import Any, Dict, Generator, List, Optional, Union, Tuple, TYPE_CHECKING
 
 from .connection import get_connection_cursor
 from .misc import format_lsn, fsync_dir
+from ..dcs import Cluster, Leader
 from ..psycopg import OperationalError
 
+if TYPE_CHECKING:  # pragma: no cover
+    from psycopg import Cursor
+    from psycopg2 import cursor
+    from . import Postgresql
+
 logger = logging.getLogger(__name__)
 
 
-def compare_slots(s1, s2, dbid='database'):
-    return s1['type'] == s2['type'] and (s1['type'] == 'physical' or
-                                         s1.get(dbid) == s2.get(dbid) and s1['plugin'] == s2['plugin'])
+def compare_slots(s1: Dict[str, Any], s2: Dict[str, Any], dbid: str = 'database') -> bool:
+    return s1['type'] == s2['type'] and (s1['type'] == 'physical'
+                                         or s1.get(dbid) == s2.get(dbid) and s1['plugin'] == s2['plugin'])
 
 
 class SlotsAdvanceThread(Thread):
 
-    def __init__(self, slots_handler):
+    def __init__(self, slots_handler: 'SlotsHandler') -> None:
         super(SlotsAdvanceThread, self).__init__()
         self.daemon = True
         self._slots_handler = slots_handler
 
         # _copy_slots and _failed are used to asynchronously give some feedback to the main thread
-        self._copy_slots = []
+        self._copy_slots: List[str] = []
         self._failed = False
-
-        self._scheduled = defaultdict(dict)  # {'dbname1': {'slot1': 100, 'slot2': 100}, 'dbname2': {'slot3': 100}}
+        # {'dbname1': {'slot1': 100, 'slot2': 100}, 'dbname2': {'slot3': 100}}
+        self._scheduled: Dict[str, Dict[str, int]] = defaultdict(dict)
         self._condition = Condition()  # protect self._scheduled from concurrent access and to wakeup the run() method
 
         self.start()
 
-    def sync_slot(self, cur, database, slot, lsn):
+    def sync_slot(self, cur: Union['cursor', 'Cursor[Any]'], database: str, slot: str, lsn: int) -> None:
         failed = copy = False
         try:
             cur.execute("SELECT pg_catalog.pg_replication_slot_advance(%s, %s)", (slot, format_lsn(lsn)))
         except Exception as e:
             logger.error("Failed to advance logical replication slot '%s': %r", slot, e)
             failed = True
             copy = isinstance(e, OperationalError) and e.diag.sqlstate == '58P01'  # WAL file is gone
@@ -51,111 +58,111 @@
             new_lsn = self._scheduled.get(database, {}).get(slot, 0)
             # remove slot from the self._scheduled structure only if it wasn't changed
             if new_lsn == lsn and database in self._scheduled:
                 self._scheduled[database].pop(slot)
                 if not self._scheduled[database]:
                     self._scheduled.pop(database)
 
-    def sync_slots_in_database(self, database, slots):
+    def sync_slots_in_database(self, database: str, slots: List[str]) -> None:
         with self._slots_handler.get_local_connection_cursor(dbname=database, options='-c statement_timeout=0') as cur:
             for slot in slots:
                 with self._condition:
                     lsn = self._scheduled.get(database, {}).get(slot, 0)
                 if lsn:
                     self.sync_slot(cur, database, slot, lsn)
 
-    def sync_slots(self):
+    def sync_slots(self) -> None:
         with self._condition:
             databases = list(self._scheduled.keys())
         for database in databases:
             with self._condition:
                 slots = list(self._scheduled.get(database, {}).keys())
             if slots:
                 try:
                     self.sync_slots_in_database(database, slots)
                 except Exception as e:
                     logger.error('Failed to advance replication slots in database %s: %r', database, e)
 
-    def run(self):
+    def run(self) -> None:
         while True:
             with self._condition:
                 if not self._scheduled:
                     self._condition.wait()
 
             self.sync_slots()
 
-    def schedule(self, advance_slots):
+    def schedule(self, advance_slots: Dict[str, Dict[str, int]]) -> Tuple[bool, List[str]]:
         with self._condition:
             for database, values in advance_slots.items():
                 self._scheduled[database].update(values)
             ret = (self._failed, self._copy_slots)
             self._copy_slots = []
             self._failed = False
             self._condition.notify()
 
         return ret
 
-    def on_promote(self):
+    def on_promote(self) -> None:
         with self._condition:
             self._scheduled.clear()
             self._failed = False
             self._copy_slots = []
 
 
 class SlotsHandler(object):
 
-    def __init__(self, postgresql):
+    def __init__(self, postgresql: 'Postgresql') -> None:
         self._postgresql = postgresql
         self._advance = None
-        self._replication_slots = {}  # already existing replication slots
-        self._unready_logical_slots = {}
+        self._replication_slots: Dict[str, Dict[str, Any]] = {}  # already existing replication slots
+        self._unready_logical_slots: Dict[str, Optional[int]] = {}
         self.pg_replslot_dir = os.path.join(self._postgresql.data_dir, 'pg_replslot')
         self.schedule()
 
-    def _query(self, sql, *params):
+    def _query(self, sql: str, *params: Any) -> Union['cursor', 'Cursor[Any]']:
         return self._postgresql.query(sql, *params, retry=False)
 
     @staticmethod
-    def _copy_items(src, dst, keys=None):
+    def _copy_items(src: Dict[str, Any], dst: Dict[str, Any], keys: Optional[List[str]] = None) -> None:
         dst.update({key: src[key] for key in keys or ('datoid', 'catalog_xmin', 'confirmed_flush_lsn')})
 
-    def process_permanent_slots(self, slots):
+    def process_permanent_slots(self, slots: List[Dict[str, Any]]) -> Dict[str, int]:
         """This methods solves three problems at once (I know, it is weird).
 
         The cluster_info_query from `Postgresql` is executed every HA loop and returns
         information about all replication slots that exists on the current host.
         Based on this information we perform the following actions:
         1. For the primary we want to expose to DCS permanent logical slots, therefore the method
            builds (and returns) a dict, that maps permanent logical slot names and confirmed_flush_lsns.
         2. This method also detects if one of the previously known permanent slots got missing and schedules resync.
         3. Updates the local cache with the fresh catalog_xmin and confirmed_flush_lsn for every known slot.
            This info is used when performing the check of logical slot readiness on standbys.
         """
-        ret = {}
+        ret: Dict[str, int] = {}
 
-        slots = {slot['slot_name']: slot for slot in slots or []}
-        if slots:
-            for name, value in slots.items():
+        slots_dict: Dict[str, Dict[str, Any]] = {slot['slot_name']: slot for slot in slots or []}
+        if slots_dict:
+            for name, value in slots_dict.items():
                 if name in self._replication_slots:
                     if compare_slots(value, self._replication_slots[name], 'datoid'):
                         if value['type'] == 'logical':
                             ret[name] = value['confirmed_flush_lsn']
                             self._copy_items(value, self._replication_slots[name])
                     else:
                         self._schedule_load_slots = True
 
-        # It could happen that the slots was deleted in the background, we want to detect this case
-        if any(name not in slots for name in self._replication_slots.keys()):
+        # It could happen that the slot was deleted in the background, we want to detect this case
+        if any(name not in slots_dict for name in self._replication_slots.keys()):
             self._schedule_load_slots = True
 
         return ret
 
-    def load_replication_slots(self):
+    def load_replication_slots(self) -> None:
         if self._postgresql.major_version >= 90400 and self._schedule_load_slots:
-            replication_slots = {}
+            replication_slots: Dict[str, Dict[str, Any]] = {}
             extra = ", catalog_xmin, pg_catalog.pg_wal_lsn_diff(confirmed_flush_lsn, '0/0')::bigint"\
                 if self._postgresql.major_version >= 100000 else ""
             skip_temp_slots = ' WHERE NOT temporary' if self._postgresql.major_version >= 100000 else ''
             cursor = self._query('SELECT slot_name, slot_type, plugin, database, datoid'
                                  '{0} FROM pg_catalog.pg_replication_slots{1}'.format(extra, skip_temp_slots))
             for r in cursor:
                 value = {'type': r[1]}
@@ -166,33 +173,37 @@
                 replication_slots[r[0]] = value
             self._replication_slots = replication_slots
             self._schedule_load_slots = False
             if self._force_readiness_check:
                 self._unready_logical_slots = {n: None for n, v in replication_slots.items() if v['type'] == 'logical'}
                 self._force_readiness_check = False
 
-    def ignore_replication_slot(self, cluster, name):
+    def ignore_replication_slot(self, cluster: Cluster, name: str) -> bool:
         slot = self._replication_slots[name]
-        for matcher in cluster.config.ignore_slots_matchers:
-            if ((matcher.get("name") is None or matcher["name"] == name)
-               and all(not matcher.get(a) or matcher[a] == slot.get(a) for a in ('database', 'plugin', 'type'))):
-                return True
+        if cluster.config:
+            for matcher in cluster.config.ignore_slots_matchers:
+                if ((matcher.get("name") is None or matcher["name"] == name)
+                   and all(not matcher.get(a) or matcher[a] == slot.get(a) for a in ('database', 'plugin', 'type'))):
+                    return True
         return self._postgresql.citus_handler.ignore_replication_slot(slot)
 
-    def drop_replication_slot(self, name):
+    def drop_replication_slot(self, name: str) -> Tuple[bool, bool]:
         """Returns a tuple(active, dropped)"""
-        cursor = self._query(('WITH slots AS (SELECT slot_name, active' +
-                              ' FROM pg_catalog.pg_replication_slots WHERE slot_name = %s),' +
-                              ' dropped AS (SELECT pg_catalog.pg_drop_replication_slot(slot_name),' +
-                              ' true AS dropped FROM slots WHERE not active) ' +
-                              'SELECT active, COALESCE(dropped, false) FROM slots' +
+        cursor = self._query(('WITH slots AS (SELECT slot_name, active'
+                              ' FROM pg_catalog.pg_replication_slots WHERE slot_name = %s),'
+                              ' dropped AS (SELECT pg_catalog.pg_drop_replication_slot(slot_name),'
+                              ' true AS dropped FROM slots WHERE not active) '
+                              'SELECT active, COALESCE(dropped, false) FROM slots'
                               ' FULL OUTER JOIN dropped ON true'), name)
-        return cursor.fetchone() if cursor.rowcount == 1 else (False, False)
+        row = cursor.fetchone()
+        if not row:
+            row = (False, False)
+        return row
 
-    def _drop_incorrect_slots(self, cluster, slots, paused):
+    def _drop_incorrect_slots(self, cluster: Cluster, slots: Dict[str, Any], paused: bool) -> None:
         # drop old replication slots which are not presented in desired slots
         for name in set(self._replication_slots) - set(slots):
             if not paused and not self.ignore_replication_slot(cluster, name):
                 active, dropped = self.drop_replication_slot(name)
                 if dropped:
                     logger.info("Dropped unknown replication slot '%s'", name)
                 else:
@@ -207,89 +218,91 @@
                             name, self._replication_slots[name], value)
                 if self.drop_replication_slot(name) == (False, True):
                     self._replication_slots.pop(name)
                 else:
                     logger.error("Failed to drop replication slot '%s'", name)
                     self._schedule_load_slots = True
 
-    def _ensure_physical_slots(self, slots):
+    def _ensure_physical_slots(self, slots: Dict[str, Any]) -> None:
         immediately_reserve = ', true' if self._postgresql.major_version >= 90600 else ''
         for name, value in slots.items():
             if name not in self._replication_slots and value['type'] == 'physical':
                 try:
-                    self._query(("SELECT pg_catalog.pg_create_physical_replication_slot(%s{0})" +
-                                 " WHERE NOT EXISTS (SELECT 1 FROM pg_catalog.pg_replication_slots" +
+                    self._query(("SELECT pg_catalog.pg_create_physical_replication_slot(%s{0})"
+                                 " WHERE NOT EXISTS (SELECT 1 FROM pg_catalog.pg_replication_slots"
                                  " WHERE slot_type = 'physical' AND slot_name = %s)").format(
                                      immediately_reserve), name, name)
                 except Exception:
                     logger.exception("Failed to create physical replication slot '%s'", name)
                 self._schedule_load_slots = True
 
     @contextmanager
-    def get_local_connection_cursor(self, **kwargs):
+    def get_local_connection_cursor(self, **kwargs: Any) -> Generator[Union['cursor', 'Cursor[Any]'], None, None]:
         conn_kwargs = self._postgresql.config.local_connect_kwargs
         conn_kwargs.update(kwargs)
         with get_connection_cursor(**conn_kwargs) as cur:
             yield cur
 
-    def _ensure_logical_slots_primary(self, slots):
+    def _ensure_logical_slots_primary(self, slots: Dict[str, Any]) -> None:
         # Group logical slots to be created by database name
-        logical_slots = defaultdict(dict)
+        logical_slots: Dict[str, Dict[str, Dict[str, Any]]] = defaultdict(dict)
         for name, value in slots.items():
             if value['type'] == 'logical':
                 # If the logical already exists, copy some information about it into the original structure
                 if self._replication_slots.get(name, {}).get('datoid'):
                     self._copy_items(self._replication_slots[name], value)
                 else:
                     logical_slots[value['database']][name] = value
 
         # Create new logical slots
         for database, values in logical_slots.items():
             with self.get_local_connection_cursor(dbname=database) as cur:
                 for name, value in values.items():
                     try:
-                        cur.execute("SELECT pg_catalog.pg_create_logical_replication_slot(%s, %s)" +
-                                    " WHERE NOT EXISTS (SELECT 1 FROM pg_catalog.pg_replication_slots" +
+                        cur.execute("SELECT pg_catalog.pg_create_logical_replication_slot(%s, %s)"
+                                    " WHERE NOT EXISTS (SELECT 1 FROM pg_catalog.pg_replication_slots"
                                     " WHERE slot_type = 'logical' AND slot_name = %s)",
                                     (name, value['plugin'], name))
                     except Exception as e:
                         logger.error("Failed to create logical replication slot '%s' plugin='%s': %r",
                                      name, value['plugin'], e)
                         slots.pop(name)
                     self._schedule_load_slots = True
 
-    def schedule_advance_slots(self, slots):
+    def schedule_advance_slots(self, slots: Dict[str, Dict[str, int]]) -> Tuple[bool, List[str]]:
         if not self._advance:
             self._advance = SlotsAdvanceThread(self)
         return self._advance.schedule(slots)
 
-    def _ensure_logical_slots_replica(self, cluster, slots):
-        advance_slots = defaultdict(dict)  # Group logical slots to be advanced by database name
-        create_slots = []  # And collect logical slots to be created on the replica
+    def _ensure_logical_slots_replica(self, cluster: Cluster, slots: Dict[str, Any]) -> List[str]:
+        # Group logical slots to be advanced by database name
+        advance_slots: Dict[str, Dict[str, int]] = defaultdict(dict)
+        create_slots: List[str] = []  # And collect logical slots to be created on the replica
         for name, value in slots.items():
             if value['type'] == 'logical':
                 # If the logical already exists, copy some information about it into the original structure
                 if self._replication_slots.get(name, {}).get('datoid'):
                     self._copy_items(self._replication_slots[name], value)
-                    if name in cluster.slots:
+                    if cluster.slots and name in cluster.slots:
                         try:  # Skip slots that doesn't need to be advanced
                             if value['confirmed_flush_lsn'] < int(cluster.slots[name]):
                                 advance_slots[value['database']][name] = int(cluster.slots[name])
                         except Exception as e:
                             logger.error('Failed to parse "%s": %r', cluster.slots[name], e)
-                elif name in cluster.slots:  # We want to copy only slots with feedback in a DCS
+                elif cluster.slots and name in cluster.slots:  # We want to copy only slots with feedback in a DCS
                     create_slots.append(name)
 
         error, copy_slots = self.schedule_advance_slots(advance_slots)
         if error:
             self._schedule_load_slots = True
         return create_slots + copy_slots
 
-    def sync_replication_slots(self, cluster, nofailover, replicatefrom=None, paused=False):
-        ret = None
+    def sync_replication_slots(self, cluster: Cluster, nofailover: bool,
+                               replicatefrom: Optional[str] = None, paused: bool = False) -> List[str]:
+        ret = []
         if self._postgresql.major_version >= 90400 and cluster.config:
             try:
                 self.load_replication_slots()
 
                 slots = cluster.get_replication_slots(self._postgresql.name, self._postgresql.role,
                                                       nofailover, self._postgresql.major_version, True)
 
@@ -308,22 +321,23 @@
                 self._replication_slots = slots
             except Exception:
                 logger.exception('Exception when changing replication slots')
                 self._schedule_load_slots = True
         return ret
 
     @contextmanager
-    def _get_leader_connection_cursor(self, leader):
+    def _get_leader_connection_cursor(self, leader: Leader) -> Generator[Union['cursor', 'Cursor[Any]'], None, None]:
         conn_kwargs = leader.conn_kwargs(self._postgresql.config.rewind_credentials)
         conn_kwargs['dbname'] = self._postgresql.database
         with get_connection_cursor(connect_timeout=3, options="-c statement_timeout=2000", **conn_kwargs) as cur:
             yield cur
 
-    def check_logical_slots_readiness(self, cluster, nofailover, replicatefrom):
-        if self._unready_logical_slots:
+    def check_logical_slots_readiness(self, cluster: Cluster, nofailover: bool, replicatefrom: Optional[str]) -> None:
+        catalog_xmin = None
+        if self._unready_logical_slots and cluster.leader:
             slot_name = cluster.get_my_slot_name_on_primary(self._postgresql.name, replicatefrom)
             try:
                 with self._get_leader_connection_cursor(cluster.leader) as cur:
                     cur.execute("SELECT slot_name, catalog_xmin FROM pg_catalog.pg_get_replication_slots()"
                                 " WHERE NOT pg_catalog.pg_is_in_recovery() AND slot_name = ANY(%s)",
                                 ([n for n, v in self._unready_logical_slots.items() if v is None] + [slot_name],))
                     slots = {row[0]: row[1] for row in cur}
@@ -331,64 +345,68 @@
                         return logger.warning('Physical slot %s does not exist on the primary', slot_name)
                     catalog_xmin = slots.pop(slot_name)
             except Exception as e:
                 return logger.error("Failed to check %s physical slot on the primary: %r", slot_name, e)
             # Remember catalog_xmin of logical slots on the primary when catalog_xmin of
             # the physical slot became valid. Logical slots on replica will be safe to use after
             # promote when catalog_xmin of the physical slot overtakes these values.
-            if catalog_xmin:
+            if catalog_xmin is not None:
                 for name, value in slots.items():
                     self._unready_logical_slots[name] = value
             else:  # Replica isn't streaming or the hot_standby_feedback isn't enabled
                 try:
                     cur = self._query("SELECT pg_catalog.current_setting('hot_standby_feedback')::boolean")
-                    if not cur.fetchone()[0]:
+                    row = cur.fetchone()
+                    if row and not row[0]:
                         logger.error('Logical slot failover requires "hot_standby_feedback".'
                                      ' Please check postgresql.auto.conf')
                 except Exception as e:
                     logger.error('Failed to check the hot_standby_feedback setting: %r', e)
                 return  # since `catalog_xmin` isn't valid further checks don't make any sense
 
         for name in list(self._unready_logical_slots):
             value = self._replication_slots.get(name)
             # The logical slot on a replica is safe to use when the physical replica slot on the primary:
             # 1. has a nonzero/non-null catalog_xmin
             # 2. has a catalog_xmin that is not newer (greater) than the catalog_xmin of any slot on the standby
             # 3. overtook the catalog_xmin of remembered values of logical slots on the primary.
-            if not value or self._unready_logical_slots[name] <= catalog_xmin <= value['catalog_xmin']:
+            if not value or catalog_xmin is not None and\
+                    self._unready_logical_slots[name] <= catalog_xmin <= value['catalog_xmin']:
                 del self._unready_logical_slots[name]
                 if value:
                     logger.info('Logical slot %s is safe to be used after a failover', name)
 
-    def copy_logical_slots(self, cluster, create_slots):
+    def copy_logical_slots(self, cluster: Cluster, create_slots: List[str]) -> None:
         leader = cluster.leader
+        if not leader:
+            return
         slots = cluster.get_replication_slots(self._postgresql.name, 'replica', False, self._postgresql.major_version)
+        copy_slots: Dict[str, Dict[str, Any]] = {}
         with self._get_leader_connection_cursor(leader) as cur:
             try:
                 cur.execute("SELECT slot_name, slot_type, datname, plugin, catalog_xmin, "
                             "pg_catalog.pg_wal_lsn_diff(confirmed_flush_lsn, '0/0')::bigint, "
                             "pg_catalog.pg_read_binary_file('pg_replslot/' || slot_name || '/state')"
                             " FROM pg_catalog.pg_get_replication_slots() JOIN pg_catalog.pg_database ON datoid = oid"
                             " WHERE NOT pg_catalog.pg_is_in_recovery() AND slot_name = ANY(%s)", (create_slots,))
 
-                create_slots = {}
                 for r in cur:
                     if r[0] in slots:  # slot_name is defined in the global configuration
                         slot = {'type': r[1], 'database': r[2], 'plugin': r[3],
                                 'catalog_xmin': r[4], 'confirmed_flush_lsn': r[5], 'data': r[6]}
                         if compare_slots(slot, slots[r[0]]):
-                            create_slots[r[0]] = slot
+                            copy_slots[r[0]] = slot
                         else:
-                            logger.warning('Will not copy the logical slot "%s" due to the configuration mismatch: ' +
+                            logger.warning('Will not copy the logical slot "%s" due to the configuration mismatch: '
                                            'configuration=%s, slot on the primary=%s', r[0], slots[r[0]], slot)
             except Exception as e:
                 logger.error("Failed to copy logical slots from the %s via postgresql connection: %r", leader.name, e)
 
-        if isinstance(create_slots, dict) and create_slots and self._postgresql.stop():
-            for name, value in create_slots.items():
+        if copy_slots and self._postgresql.stop():
+            for name, value in copy_slots.items():
                 slot_dir = os.path.join(self._postgresql.slots_handler.pg_replslot_dir, name)
                 slot_tmp_dir = slot_dir + '.tmp'
                 if os.path.exists(slot_tmp_dir):
                     shutil.rmtree(slot_tmp_dir)
                 os.makedirs(slot_tmp_dir)
                 fsync_dir(slot_tmp_dir)
                 with open(os.path.join(slot_tmp_dir, 'state'), 'wb') as f:
@@ -399,19 +417,19 @@
                     shutil.rmtree(slot_dir)
                 os.rename(slot_tmp_dir, slot_dir)
                 fsync_dir(slot_dir)
                 self._unready_logical_slots[name] = None
             fsync_dir(self._postgresql.slots_handler.pg_replslot_dir)
             self._postgresql.start()
 
-    def schedule(self, value=None):
+    def schedule(self, value: Optional[bool] = None) -> None:
         if value is None:
             value = self._postgresql.major_version >= 90400
         self._schedule_load_slots = self._force_readiness_check = value
 
-    def on_promote(self):
+    def on_promote(self) -> None:
         if self._advance:
             self._advance.on_promote()
 
         if self._unready_logical_slots:
             logger.warning('Logical replication slots that might be unsafe to use after promote: %s',
                            set(self._unready_logical_slots))
```

### Comparing `patroni-3.0.2/patroni/postgresql/sync.py` & `patroni-3.0.3/patroni/postgresql/sync.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,15 +1,19 @@
 import logging
 import re
 import time
 
 from copy import deepcopy
+from typing import Collection, List, NamedTuple, Tuple, TYPE_CHECKING
 
-from .validator import CaseInsensitiveDict
+from ..collections import CaseInsensitiveDict, CaseInsensitiveSet
+from ..dcs import Cluster
 from ..psycopg import quote_ident as _quote_ident
+if TYPE_CHECKING:  # pragma: no cover
+    from . import Postgresql
 
 logger = logging.getLogger(__name__)
 
 SYNC_STANDBY_NAME_RE = re.compile(r'^[A-Za-z_][A-Za-z_0-9\$]*$')
 SYNC_REP_PARSER_RE = re.compile(r"""
            (?P<first> [fF][iI][rR][sS][tT] )
          | (?P<any> [aA][nN][yY] )
@@ -19,53 +23,67 @@
          | (?P<star> [*] )
          | (?P<num> \d+ )
          | (?P<comma> , )
          | (?P<parenstart> \( )
          | (?P<parenend> \) )
          | (?P<JUNK> . )
         """, re.X)
-_EMPTY_SSN = {'type': 'off', 'num': 0, 'members': CaseInsensitiveDict({})}
 
 
-def quote_ident(value):
-    """Very simplified version of quote_ident"""
+def quote_ident(value: str) -> str:
+    """Very simplified version of `psycopg` :func:`quote_ident` function."""
     return value if SYNC_STANDBY_NAME_RE.match(value) else _quote_ident(value)
 
 
-def parse_sync_standby_names(value):
+class _SSN(NamedTuple):
+    """class representing "synchronous_standby_names" value after parsing.
+
+    :ivar sync_type: possible values: 'off', 'priority', 'quorum'
+    :ivar has_star: is set to `True` if "synchronous_standby_names" contains '*'
+    :ivar num: how many nodes are required to be synchronous
+    :ivar members: collection of standby names listed in "synchronous_standby_names"
+    """
+    sync_type: str
+    has_star: bool
+    num: int
+    members: CaseInsensitiveSet
+
+
+_EMPTY_SSN = _SSN('off', False, 0, CaseInsensitiveSet())
+
+
+def parse_sync_standby_names(value: str) -> _SSN:
     """Parse postgresql synchronous_standby_names to constituent parts.
-    Returns dict with the following keys:
-    * type: 'quorum'|'priority'
-    * num: int
-    * members: CaseInsensitiveDict, with names as keys
-    * has_star: bool - Present if true
-    If the configuration value can not be parsed, raises a ValueError.
 
-    >>> parse_sync_standby_names('')['type']
+    :param value: the value of `synchronous_standby_names`
+    :returns: :class:`_SSN` object
+    :raises `ValueError`: if the configuration value can not be parsed
+
+    >>> parse_sync_standby_names('').sync_type
     'off'
 
-    >>> parse_sync_standby_names('FiRsT')['type']
+    >>> parse_sync_standby_names('FiRsT').sync_type
     'priority'
 
-    >>> parse_sync_standby_names('FiRsT')['members']
-    {'FiRsT': True}
+    >>> 'first' in parse_sync_standby_names('FiRsT').members
+    True
 
-    >>> parse_sync_standby_names('"1"')['members']
-    {'1': True}
+    >>> set(parse_sync_standby_names('"1"').members)
+    {'1'}
 
-    >>> parse_sync_standby_names(' a , b ')['members']
-    {'a': True, 'b': True}
+    >>> parse_sync_standby_names(' a , b ').members == {'a', 'b'}
+    True
 
-    >>> parse_sync_standby_names(' a , b ')['num']
+    >>> parse_sync_standby_names(' a , b ').num
     1
 
-    >>> parse_sync_standby_names('ANY 4("a",*,b)')['has_star']
+    >>> parse_sync_standby_names('ANY 4("a",*,b)').has_star
     True
 
-    >>> parse_sync_standby_names('ANY 4("a",*,b)')['num']
+    >>> parse_sync_standby_names('ANY 4("a",*,b)').num
     4
 
     >>> parse_sync_standby_names('1')  # doctest: +IGNORE_EXCEPTION_DETAIL
     Traceback (most recent call last):
         ...
     ValueError: Unparseable synchronous_standby_names value
 
@@ -92,164 +110,182 @@
     tokens = [(m.lastgroup, m.group(0), m.start())
               for m in SYNC_REP_PARSER_RE.finditer(value)
               if m.lastgroup != 'space']
     if not tokens:
         return deepcopy(_EMPTY_SSN)
 
     if [t[0] for t in tokens[0:3]] == ['any', 'num', 'parenstart'] and tokens[-1][0] == 'parenend':
-        result = {'type': 'quorum', 'num': int(tokens[1][1])}
+        sync_type = 'quorum'
+        num = int(tokens[1][1])
         synclist = tokens[3:-1]
     elif [t[0] for t in tokens[0:3]] == ['first', 'num', 'parenstart'] and tokens[-1][0] == 'parenend':
-        result = {'type': 'priority', 'num': int(tokens[1][1])}
+        sync_type = 'priority'
+        num = int(tokens[1][1])
         synclist = tokens[3:-1]
     elif [t[0] for t in tokens[0:2]] == ['num', 'parenstart'] and tokens[-1][0] == 'parenend':
-        result = {'type': 'priority', 'num': int(tokens[0][1])}
+        sync_type = 'priority'
+        num = int(tokens[0][1])
         synclist = tokens[2:-1]
     else:
-        result = {'type': 'priority', 'num': 1}
+        sync_type = 'priority'
+        num = 1
         synclist = tokens
-    result['members'] = CaseInsensitiveDict({})
+
+    has_star = False
+    members = CaseInsensitiveSet()
     for i, (a_type, a_value, a_pos) in enumerate(synclist):
         if i % 2 == 1:  # odd elements are supposed to be commas
             if len(synclist) == i + 1:  # except the last token
                 raise ValueError("Unparseable synchronous_standby_names value %r: Unexpected token %s %r at %d" %
                                  (value, a_type, a_value, a_pos))
             elif a_type != 'comma':
                 raise ValueError("Unparseable synchronous_standby_names value %r: ""Got token %s %r while"
                                  " expecting comma at %d" % (value, a_type, a_value, a_pos))
         elif a_type in {'ident', 'first', 'any'}:
-            result['members'][a_value] = True
+            members.add(a_value)
         elif a_type == 'star':
-            result['members'][a_value] = True
-            result['has_star'] = True
+            members.add(a_value)
+            has_star = True
         elif a_type == 'dquot':
-            result['members'][a_value[1:-1].replace('""', '"')] = True
+            members.add(a_value[1:-1].replace('""', '"'))
         else:
             raise ValueError("Unparseable synchronous_standby_names value %r: Unexpected token %s %r at %d" %
                              (value, a_type, a_value, a_pos))
-    return result
+    return _SSN(sync_type, has_star, num, members)
 
 
 class SyncHandler(object):
     """Class responsible for working with the `synchronous_standby_names`.
 
     Sync standbys are chosen based on their state in `pg_stat_replication`.
     When `synchronous_standby_names` is changed we memorize the `_primary_flush_lsn`
     and the `current_state()` method will count newly added names as "sync" only when
     they reached memorized LSN and also reported as "sync" by `pg_stat_replication`"""
 
-    def __init__(self, postgresql):
+    def __init__(self, postgresql: 'Postgresql') -> None:
         self._postgresql = postgresql
         self._synchronous_standby_names = ''  # last known value of synchronous_standby_names
         self._ssn_data = deepcopy(_EMPTY_SSN)
         self._primary_flush_lsn = 0
         # "sync" replication connections, that were verified to reach self._primary_flush_lsn at some point
         self._ready_replicas = CaseInsensitiveDict({})  # keys: member names, values: connection pids
 
-    def _handle_synchronous_standby_names_change(self):
-        """If synchronous_standby_names has changed we need to check that newly added replicas
-        have reached self._primary_flush_lsn. Only after that they could be counted as sync."""
+    def _handle_synchronous_standby_names_change(self) -> None:
+        """Handles changes of "synchronous_standby_names" GUC.
+
+        If "synchronous_standby_names" was changed, we need to check that newly added replicas have
+        reached `self._primary_flush_lsn`. Only after that they could be counted as synchronous.
+        """
         synchronous_standby_names = self._postgresql.synchronous_standby_names()
         if synchronous_standby_names == self._synchronous_standby_names:
-            return False
+            return
 
         self._synchronous_standby_names = synchronous_standby_names
         try:
             self._ssn_data = parse_sync_standby_names(synchronous_standby_names)
         except ValueError as e:
             logger.warning('%s', e)
             self._ssn_data = deepcopy(_EMPTY_SSN)
 
         # Invalidate cache of "sync" connections
         for app_name in list(self._ready_replicas.keys()):
-            if app_name not in self._ssn_data['members']:
+            if app_name not in self._ssn_data.members:
                 del self._ready_replicas[app_name]
 
         # Newly connected replicas will be counted as sync only when reached self._primary_flush_lsn
         self._primary_flush_lsn = self._postgresql.last_operation()
         self._postgresql.query('SELECT pg_catalog.txid_current()')  # Ensure some WAL traffic to move replication
         self._postgresql.reset_cluster_info_state(None)  # Reset internal cache to query fresh values
 
-    def current_state(self, cluster, sync_node_count=1, sync_node_maxlag=-1):
+    def current_state(self, cluster: Cluster) -> Tuple[CaseInsensitiveSet, CaseInsensitiveSet]:
         """Finds best candidates to be the synchronous standbys.
 
         Current synchronous standby is always preferred, unless it has disconnected or does not want to be a
         synchronous standby any longer.
-        Parameter sync_node_maxlag(maximum_lag_on_syncnode) would help swapping unhealthy sync replica in case
-        if it stops responding (or hung). Please set the value high enough so it won't unncessarily swap sync
-        standbys during high loads. Any less or equal of 0 value keep the behavior backward compatible and
-        will not swap. Please note that it will not also swap sync standbys in case where all replicas are hung.
 
-        :returns: tuple of candidates list and synchronous standby list."""
+        Standbys are selected based on values from the global configuration:
+        - `maximum_lag_on_syncnode`: would help swapping unhealthy sync replica in case if it stops
+          responding (or hung). Please set the value high enough so it won't unncessarily swap sync
+          standbys during high loads. Any value less or equal of 0 keeps the behavior backward compatible.
+          Please note that it will not also swap sync standbys in case where all replicas are hung.
+        - `synchronous_node_count`: controlls how many nodes should be set as synchronous.
 
+        :returns: tuple of candidates :class:`CaseInsensitiveSet` and synchronous standbys :class:`CaseInsensitiveSet`.
+        """
         self._handle_synchronous_standby_names_change()
 
         # Pick candidates based on who has higher replay/remote_write/flush lsn.
         sort_col = {
             'remote_apply': 'replay',
             'remote_write': 'write'
         }.get(self._postgresql.synchronous_commit(), 'flush') + '_lsn'
 
         pg_stat_replication = [(r['pid'], r['application_name'], r['sync_state'], r[sort_col])
                                for r in self._postgresql.pg_stat_replication()
                                if r[sort_col] is not None]
 
         members = CaseInsensitiveDict({m.name: m for m in cluster.members})
-        replica_list = []
+        replica_list: List[Tuple[int, str, str, int, bool]] = []
         # pg_stat_replication.sync_state has 4 possible states - async, potential, quorum, sync.
         # That is, alphabetically they are in the reversed order of priority.
         # Since we are doing reversed sort on (sync_state, lsn) tuples, it helps to keep the result
         # consistent in case if a synchronous standby member is slowed down OR async node receiving
         # changes faster than the sync member (very rare but possible).
         # Such cases would trigger sync standby member swapping, but only if lag on a sync node exceeding a threshold.
         for pid, app_name, sync_state, replica_lsn in sorted(pg_stat_replication, key=lambda r: r[2:4], reverse=True):
             member = members.get(app_name)
             if member and member.is_running and not member.tags.get('nosync', False):
                 replica_list.append((pid, member.name, sync_state, replica_lsn, bool(member.nofailover)))
 
         max_lsn = max(replica_list, key=lambda x: x[3])[3]\
             if len(replica_list) > 1 else self._postgresql.last_operation()
 
-        if self._postgresql.major_version < 90600:
-            sync_node_count = 1
+        if TYPE_CHECKING:  # pragma: no cover
+            assert self._postgresql.global_config is not None
+        sync_node_count = self._postgresql.global_config.synchronous_node_count\
+            if self._postgresql.supports_multiple_sync else 1
+        sync_node_maxlag = self._postgresql.global_config.maximum_lag_on_syncnode
 
-        candidates = []
-        sync_nodes = []
+        candidates = CaseInsensitiveSet()
+        sync_nodes = CaseInsensitiveSet()
         # Prefer members without nofailover tag. We are relying on the fact that sorts are guaranteed to be stable.
         for pid, app_name, sync_state, replica_lsn, _ in sorted(replica_list, key=lambda x: x[4]):
             # if standby name is listed in the /sync key we can count it as synchronous, otherwice
-            # ig becomes really synchronous when sync_state = 'sync' and it is known that it managed to catch up
-            if app_name not in self._ready_replicas and app_name in self._ssn_data['members'] and\
-                    (cluster.sync and app_name in cluster.sync.members or
-                     sync_state == 'sync' and replica_lsn >= self._primary_flush_lsn):
+            # it becomes really synchronous when sync_state = 'sync' and it is known that it managed to catch up
+            if app_name not in self._ready_replicas and app_name in self._ssn_data.members and\
+                    (cluster.sync.matches(app_name) or sync_state == 'sync' and replica_lsn >= self._primary_flush_lsn):
                 self._ready_replicas[app_name] = pid
 
             if sync_node_maxlag <= 0 or max_lsn - replica_lsn <= sync_node_maxlag:
-                candidates.append(app_name)
+                candidates.add(app_name)
                 if sync_state == 'sync' and app_name in self._ready_replicas:
-                    sync_nodes.append(app_name)
+                    sync_nodes.add(app_name)
             if len(candidates) >= sync_node_count:
                 break
 
         return candidates, sync_nodes
 
-    def set_synchronous_standby_names(self, value):
-        """Constructs and sets `synchronous_standby_names` value.
+    def set_synchronous_standby_names(self, sync: Collection[str]) -> None:
+        """Constructs and sets "synchronous_standby_names" GUC value.
 
-        :param value: list[str] - the list of wanted sync members"""
-        if value and value != ['*']:
-            value = [quote_ident(x) for x in value]
+        :param sync: set of nodes to sync to
+        """
+        has_asterisk = '*' in sync
+        if has_asterisk:
+            sync = ['*']
+        else:
+            sync = [quote_ident(x) for x in sync]
 
-        if self._postgresql.major_version >= 90600 and len(value) > 1:
-            sync_param = '{0} ({1})'.format(len(value), ','.join(value))
+        if self._postgresql.supports_multiple_sync and len(sync) > 1:
+            sync_param = '{0} ({1})'.format(len(sync), ','.join(sync))
         else:
-            sync_param = next(iter(value), None)
+            sync_param = next(iter(sync), None)
 
-        if not (self._postgresql.config.set_synchronous_standby_names(sync_param) and
-                self._postgresql.state == 'running' and self._postgresql.is_leader()) or value == ['*']:
+        if not (self._postgresql.config.set_synchronous_standby_names(sync_param)
+                and self._postgresql.state == 'running' and self._postgresql.is_leader()) or has_asterisk:
             return
 
         time.sleep(0.1)  # Usualy it takes 1ms to reload postgresql.conf, but we will give it 100ms
 
         # Reset internal cache to query fresh values
         self._postgresql.reset_cluster_info_state(None)
```

### Comparing `patroni-3.0.2/patroni/raft_controller.py` & `patroni-3.0.3/patroni/raft_controller.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,29 +1,33 @@
 import logging
 
-from .daemon import AbstractPatroniDaemon, abstract_main
+from .config import Config
+from .daemon import AbstractPatroniDaemon, abstract_main, get_base_arg_parser
 from .dcs.raft import KVStoreTTL
 
 logger = logging.getLogger(__name__)
 
 
 class RaftController(AbstractPatroniDaemon):
 
-    def __init__(self, config):
+    def __init__(self, config: Config) -> None:
         super(RaftController, self).__init__(config)
 
-        config = self.config.get('raft')
-        assert 'self_addr' in config
-        self._raft = KVStoreTTL(None, None, None, **config)
+        kvstore_config = self.config.get('raft')
+        assert 'self_addr' in kvstore_config
+        self._raft = KVStoreTTL(None, None, None, **kvstore_config)
 
-    def _run_cycle(self):
+    def _run_cycle(self) -> None:
         try:
             self._raft.doTick(self._raft.conf.autoTickPeriod)
         except Exception:
             logger.exception('doTick')
 
-    def _shutdown(self):
+    def _shutdown(self) -> None:
         self._raft.destroy()
 
 
-def main():
-    abstract_main(RaftController)
+def main() -> None:
+    parser = get_base_arg_parser()
+    args = parser.parse_args()
+
+    abstract_main(RaftController, args.configfile)
```

### Comparing `patroni-3.0.2/patroni/scripts/aws.py` & `patroni-3.0.3/patroni/scripts/aws.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,28 +1,30 @@
 #!/usr/bin/env python
 
 import json
 import logging
 import sys
 import boto3
 
-from ..utils import Retry, RetryFailedError
-
 from botocore.exceptions import ClientError
 from botocore.utils import IMDSFetcher
 
+from typing import Any, Optional
+
+from ..utils import Retry, RetryFailedError
+
 logger = logging.getLogger(__name__)
 
 
 class AWSConnection(object):
 
-    def __init__(self, cluster_name):
+    def __init__(self, cluster_name: Optional[str]) -> None:
         self.available = False
         self.cluster_name = cluster_name if cluster_name is not None else 'unknown'
-        self._retry = Retry(deadline=300, max_delay=30, max_tries=-1, retry_exceptions=(ClientError,))
+        self._retry = Retry(deadline=300, max_delay=30, max_tries=-1, retry_exceptions=ClientError)
         try:
             # get the instance id
             fetcher = IMDSFetcher(timeout=2.1)
             token = fetcher._fetch_metadata_token()
             r = fetcher._get_request("/latest/dynamic/instance-identity/document", None, token)
         except Exception:
             logger.error('cannot query AWS meta-data')
@@ -34,38 +36,38 @@
                 self.instance_id = content['instanceId']
                 self.region = content['region']
             except Exception:
                 logger.exception('unable to fetch instance id and region from AWS meta-data')
                 return
             self.available = True
 
-    def retry(self, *args, **kwargs):
+    def retry(self, *args: Any, **kwargs: Any) -> Any:
         return self._retry.copy()(*args, **kwargs)
 
-    def aws_available(self):
+    def aws_available(self) -> bool:
         return self.available
 
-    def _tag_ebs(self, conn, role):
+    def _tag_ebs(self, conn: Any, role: str) -> None:
         """ set tags, carrying the cluster name, instance role and instance id for the EBS storage """
         tags = [{'Key': 'Name', 'Value': 'spilo_' + self.cluster_name},
                 {'Key': 'Role', 'Value': role},
                 {'Key': 'Instance', 'Value': self.instance_id}]
         volumes = conn.volumes.filter(Filters=[{'Name': 'attachment.instance-id', 'Values': [self.instance_id]}])
         conn.create_tags(Resources=[v.id for v in volumes], Tags=tags)
 
-    def _tag_ec2(self, conn, role):
+    def _tag_ec2(self, conn: Any, role: str) -> None:
         """ tag the current EC2 instance with a cluster role """
         tags = [{'Key': 'Role', 'Value': role}]
         conn.create_tags(Resources=[self.instance_id], Tags=tags)
 
-    def on_role_change(self, new_role):
+    def on_role_change(self, new_role: str) -> bool:
         if not self.available:
             return False
         try:
-            conn = boto3.resource('ec2', region_name=self.region)
+            conn = boto3.resource('ec2', region_name=self.region)  # type: ignore
             self.retry(self._tag_ec2, conn, new_role)
             self.retry(self._tag_ebs, conn, new_role)
         except RetryFailedError:
             logger.warning("Unable to communicate to AWS "
                            "when setting tags for the EC2 instance {0} "
                            "and attached EBS volumes".format(self.instance_id))
             return False
```

### Comparing `patroni-3.0.2/patroni/scripts/wale_restore.py` & `patroni-3.0.3/patroni/scripts/wale_restore.py`

 * *Files 5% similar despite different names*

```diff
@@ -27,45 +27,45 @@
 import csv
 import logging
 import os
 import subprocess
 import sys
 import time
 
-from collections import namedtuple
+from enum import IntEnum
+from typing import Any, List, NamedTuple, Optional, Tuple, TYPE_CHECKING
 
 from .. import psycopg
 
 logger = logging.getLogger(__name__)
 
 RETRY_SLEEP_INTERVAL = 1
 si_prefixes = ['K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y']
 
 
 # Meaningful names to the exit codes used by WALERestore
-ExitCode = type('Enum', (), {
-    'SUCCESS': 0,  #: Succeeded
-    'RETRY_LATER': 1,  #: External issue, retry later
-    'FAIL': 2  #: Don't try again unless configuration changes
-})
+class ExitCode(IntEnum):
+    SUCCESS = 0  #: Succeeded
+    RETRY_LATER = 1  #: External issue, retry later
+    FAIL = 2  #: Don't try again unless configuration changes
 
 
 # We need to know the current PG version in order to figure out the correct WAL directory name
-def get_major_version(data_dir):
+def get_major_version(data_dir: str) -> float:
     version_file = os.path.join(data_dir, 'PG_VERSION')
     if os.path.isfile(version_file):  # version file exists
         try:
             with open(version_file) as f:
                 return float(f.read())
         except Exception:
             logger.exception('Failed to read PG_VERSION from %s', data_dir)
     return 0.0
 
 
-def repr_size(n_bytes):
+def repr_size(n_bytes: float) -> str:
     """
     >>> repr_size(1000)
     '1000 Bytes'
     >>> repr_size(8257332324597)
     '7.5 TiB'
     """
     if n_bytes < 1024:
@@ -73,42 +73,38 @@
     i = -1
     while n_bytes > 1023:
         n_bytes /= 1024.0
         i += 1
     return '{0} {1}iB'.format(round(n_bytes, 1), si_prefixes[i])
 
 
-def size_as_bytes(size_, prefix):
+def size_as_bytes(size: float, prefix: str) -> int:
     """
     >>> size_as_bytes(7.5, 'T')
     8246337208320
     """
     prefix = prefix.upper()
 
     assert prefix in si_prefixes
 
     exponent = si_prefixes.index(prefix) + 1
 
-    return int(size_ * (1024.0 ** exponent))
+    return int(size * (1024.0 ** exponent))
 
 
-WALEConfig = namedtuple(
-    'WALEConfig',
-    [
-        'env_dir',
-        'threshold_mb',
-        'threshold_pct',
-        'cmd',
-    ]
-)
+class WALEConfig(NamedTuple):
+    env_dir: str
+    threshold_mb: int
+    threshold_pct: int
+    cmd: List[str]
 
 
 class WALERestore(object):
-    def __init__(self, scope, datadir, connstring, env_dir, threshold_mb,
-                 threshold_pct, use_iam, no_leader, retries):
+    def __init__(self, scope: str, datadir: str, connstring: str, env_dir: str, threshold_mb: int,
+                 threshold_pct: int, use_iam: int, no_leader: bool, retries: int) -> None:
         self.scope = scope
         self.leader_connection = connstring
         self.data_dir = datadir
         self.no_leader = no_leader
 
         wale_cmd = [
             'envdir',
@@ -125,15 +121,15 @@
             threshold_pct=threshold_pct,
             cmd=wale_cmd,
         )
 
         self.init_error = (not os.path.exists(self.wal_e.env_dir))
         self.retries = retries
 
-    def run(self):
+    def run(self) -> int:
         """
         Creates a new replica using WAL-E
 
         Returns
         -------
         ExitCode
             0 = Success
@@ -154,15 +150,15 @@
                 return self.create_replica_with_s3()
             elif not should_use_s3:
                 return ExitCode.FAIL
         except Exception:
             logger.exception("Unhandled exception when running WAL-E restore")
         return ExitCode.FAIL
 
-    def should_use_s3_to_create_replica(self):
+    def should_use_s3_to_create_replica(self) -> Optional[bool]:
         """ determine whether it makes sense to use S3 and not pg_basebackup """
 
         threshold_megabytes = self.wal_e.threshold_mb
         threshold_percent = self.wal_e.threshold_pct
 
         try:
             cmd = self.wal_e.cmd + ['backup-list', '--detail', 'LATEST']
@@ -214,30 +210,31 @@
         attempts_no = 0
         while True:
             if self.leader_connection:
                 con = None
                 try:
                     # get the difference in bytes between the current WAL location and the backup start offset
                     con = psycopg.connect(self.leader_connection)
-                    if con.server_version >= 100000:
+                    if getattr(con, 'server_version', 0) >= 100000:
                         wal_name = 'wal'
                         lsn_name = 'lsn'
                     else:
                         wal_name = 'xlog'
                         lsn_name = 'location'
                     with con.cursor() as cur:
                         cur.execute(("SELECT CASE WHEN pg_catalog.pg_is_in_recovery()"
                                      " THEN GREATEST(pg_catalog.pg_{0}_{1}_diff(COALESCE("
                                      "pg_last_{0}_receive_{1}(), '0/0'), %s)::bigint, "
                                      "pg_catalog.pg_{0}_{1}_diff(pg_catalog.pg_last_{0}_replay_{1}(), %s)::bigint)"
                                      " ELSE pg_catalog.pg_{0}_{1}_diff(pg_catalog.pg_current_{0}_{1}(), %s)::bigint"
                                      " END").format(wal_name, lsn_name),
                                     (backup_start_lsn, backup_start_lsn, backup_start_lsn))
-
-                        diff_in_bytes = int(cur.fetchone()[0])
+                        for row in cur:
+                            diff_in_bytes = int(row[0])
+                            break
                 except psycopg.Error:
                     logger.exception('could not determine difference with the leader location')
                     if attempts_no < self.retries:  # retry in case of a temporarily connection issue
                         attempts_no = attempts_no + 1
                         time.sleep(RETRY_SLEEP_INTERVAL)
                         continue
                     else:
@@ -258,30 +255,30 @@
         # or exceeds the pre-determined size - pg_basebackup is chosen instead.
         is_size_thresh_ok = diff_in_bytes < int(threshold_megabytes) * 1048576
         threshold_pct_bytes = backup_size * threshold_percent / 100.0
         is_percentage_thresh_ok = float(diff_in_bytes) < int(threshold_pct_bytes)
         are_thresholds_ok = is_size_thresh_ok and is_percentage_thresh_ok
 
         class Size(object):
-            def __init__(self, n_bytes, prefix=None):
+            def __init__(self, n_bytes: float, prefix: Optional[str] = None) -> None:
                 self.n_bytes = n_bytes
                 self.prefix = prefix
 
-            def __repr__(self):
+            def __repr__(self) -> str:
                 if self.prefix is not None:
                     n_bytes = size_as_bytes(self.n_bytes, self.prefix)
                 else:
                     n_bytes = self.n_bytes
                 return repr_size(n_bytes)
 
         class HumanContext(object):
-            def __init__(self, items):
+            def __init__(self, items: List[Tuple[str, Any]]) -> None:
                 self.items = items
 
-            def __repr__(self):
+            def __repr__(self) -> str:
                 return ', '.join('{}={!r}'.format(key, value)
                                  for key, value in self.items)
 
         human_context = repr(HumanContext([
             ('threshold_size', Size(threshold_megabytes, 'M')),
             ('threshold_percent', threshold_percent),
             ('threshold_percent_size', Size(threshold_pct_bytes)),
@@ -294,15 +291,15 @@
         if not are_thresholds_ok:
             logger.info('wal-e backup size diff is over threshold, falling back '
                         'to other means of restore: %s', human_context)
         else:
             logger.info('Thresholds are OK, using wal-e basebackup: %s', human_context)
         return are_thresholds_ok
 
-    def fix_subdirectory_path_if_broken(self, dirname):
+    def fix_subdirectory_path_if_broken(self, dirname: str) -> bool:
         # in case it is a symlink pointing to a non-existing location, remove it and create the actual directory
         path = os.path.join(self.data_dir, dirname)
         if not os.path.exists(path):
             if os.path.islink(path):  # broken xlog symlink, to remove
                 try:
                     os.remove(path)
                 except OSError:
@@ -312,15 +309,15 @@
             try:
                 os.mkdir(path)
             except OSError:
                 logger.exception("could not create missing %s directory path", dirname)
                 return False
         return True
 
-    def create_replica_with_s3(self):
+    def create_replica_with_s3(self) -> int:
         # if we're set up, restore the replica using fetch latest
         try:
             cmd = self.wal_e.cmd + ['backup-fetch',
                                     '{}'.format(self.data_dir),
                                     'LATEST']
             logger.debug('calling: %r', cmd)
             exit_code = subprocess.call(cmd)
@@ -330,15 +327,15 @@
 
         if (exit_code == 0 and not
            self.fix_subdirectory_path_if_broken('pg_xlog' if get_major_version(self.data_dir) < 10 else 'pg_wal')):
             return ExitCode.FAIL
         return exit_code
 
 
-def main():
+def main() -> int:
     logging.basicConfig(format='%(asctime)s %(levelname)s: %(message)s', level=logging.INFO)
     parser = argparse.ArgumentParser(description='Script to image replicas using WAL-E')
     parser.add_argument('--scope', required=True)
     parser.add_argument('--role', required=False)
     parser.add_argument('--datadir', required=True)
     parser.add_argument('--connstring', required=True)
     parser.add_argument('--retries', type=int, default=1)
@@ -359,17 +356,19 @@
     # whether to use WAL-E or not depending on the no_leader flag.
     for _ in range(0, args.retries + 1):
         restore = WALERestore(scope=args.scope, datadir=args.datadir, connstring=args.connstring,
                               env_dir=args.envdir, threshold_mb=args.threshold_megabytes,
                               threshold_pct=args.threshold_backup_size_percentage, use_iam=args.use_iam,
                               no_leader=args.no_leader, retries=args.retries)
         exit_code = restore.run()
-        if not exit_code == ExitCode.RETRY_LATER:  # only WAL-E failures lead to the retry
+        if exit_code != ExitCode.RETRY_LATER:  # only WAL-E failures lead to the retry
             logger.debug('exit_code is %r, not retrying', exit_code)
             break
         time.sleep(RETRY_SLEEP_INTERVAL)
 
+    if TYPE_CHECKING:  # pragma: no cover
+        assert exit_code is not None
     return exit_code
 
 
 if __name__ == '__main__':
     sys.exit(main())
```

### Comparing `patroni-3.0.2/patroni/watchdog/base.py` & `patroni-3.0.3/patroni/watchdog/base.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,106 +1,112 @@
 import abc
 import logging
 import platform
 import sys
 from threading import RLock
 
-from patroni.exceptions import WatchdogError
+from typing import Any, Callable, Dict, Optional, Union
+
+from ..config import Config
+from ..exceptions import WatchdogError
 
 __all__ = ['WatchdogError', 'Watchdog']
 
 logger = logging.getLogger(__name__)
 
 MODE_REQUIRED = 'required'    # Will not run if a watchdog is not available
 MODE_AUTOMATIC = 'automatic'  # Will use a watchdog if one is available
 MODE_OFF = 'off'              # Will not try to use a watchdog
 
 
-def parse_mode(mode):
+def parse_mode(mode: Union[bool, str]) -> str:
     if mode is False:
         return MODE_OFF
-    mode = mode.lower()
+    mode = str(mode).lower()
     if mode in ['require', 'required']:
         return MODE_REQUIRED
     elif mode in ['auto', 'automatic']:
         return MODE_AUTOMATIC
     else:
         if mode not in ['off', 'disable', 'disabled']:
             logger.warning("Watchdog mode {0} not recognized, disabling watchdog".format(mode))
         return MODE_OFF
 
 
-def synchronized(func):
-    def wrapped(self, *args, **kwargs):
-        with self._lock:
+def synchronized(func: Callable[..., Any]) -> Callable[..., Any]:
+    def wrapped(self: 'Watchdog', *args: Any, **kwargs: Any) -> Any:
+        with self.lock:
             return func(self, *args, **kwargs)
     return wrapped
 
 
 class WatchdogConfig(object):
     """Helper to contain a snapshot of configuration"""
-    def __init__(self, config):
-        self.mode = parse_mode(config['watchdog'].get('mode', 'automatic'))
+    def __init__(self, config: Config) -> None:
+        watchdog_config = config.get("watchdog") or {'mode': 'automatic'}
+
+        self.mode = parse_mode(watchdog_config.get('mode', 'automatic'))
         self.ttl = config['ttl']
         self.loop_wait = config['loop_wait']
-        self.safety_margin = config['watchdog'].get('safety_margin', 5)
-        self.driver = config['watchdog'].get('driver', 'default')
-        self.driver_config = dict((k, v) for k, v in config['watchdog'].items()
+        self.safety_margin = watchdog_config.get('safety_margin', 5)
+        self.driver = watchdog_config.get('driver', 'default')
+        self.driver_config = dict((k, v) for k, v in watchdog_config.items()
                                   if k not in ['mode', 'safety_margin', 'driver'])
 
-    def __eq__(self, other):
+    def __eq__(self, other: Any) -> bool:
         return isinstance(other, WatchdogConfig) and \
             all(getattr(self, attr) == getattr(other, attr) for attr in
                 ['mode', 'ttl', 'loop_wait', 'safety_margin', 'driver', 'driver_config'])
 
-    def __ne__(self, other):
+    def __ne__(self, other: Any) -> bool:
         return not self == other
 
-    def get_impl(self):
+    def get_impl(self) -> 'WatchdogBase':
         if self.driver == 'testing':  # pragma: no cover
             from patroni.watchdog.linux import TestingWatchdogDevice
             return TestingWatchdogDevice.from_config(self.driver_config)
         elif platform.system() == 'Linux' and self.driver == 'default':
             from patroni.watchdog.linux import LinuxWatchdogDevice
             return LinuxWatchdogDevice.from_config(self.driver_config)
         else:
             return NullWatchdog()
 
     @property
-    def timeout(self):
+    def timeout(self) -> int:
         if self.safety_margin == -1:
             return int(self.ttl // 2)
         else:
             return self.ttl - self.safety_margin
 
     @property
-    def timing_slack(self):
+    def timing_slack(self) -> int:
         return self.timeout - self.loop_wait
 
 
 class Watchdog(object):
     """Facade to dynamically manage watchdog implementations and handle config changes.
 
     When activation fails underlying implementation will be switched to a Null implementation. To avoid log spam
     activation will only be retried when watchdog configuration is changed."""
-    def __init__(self, config):
-        self.active_config = self.config = WatchdogConfig(config)
-        self._lock = RLock()
+    def __init__(self, config: Config) -> None:
+        self.config = WatchdogConfig(config)
+        self.active_config: WatchdogConfig = self.config
+        self.lock = RLock()
         self.active = False
 
         if self.config.mode == MODE_OFF:
             self.impl = NullWatchdog()
         else:
             self.impl = self.config.get_impl()
             if self.config.mode == MODE_REQUIRED and self.impl.is_null:
                 logger.error("Configuration requires a watchdog, but watchdog is not supported on this platform.")
                 sys.exit(1)
 
     @synchronized
-    def reload_config(self, config):
+    def reload_config(self, config: Config) -> None:
         self.config = WatchdogConfig(config)
         # Turning a watchdog off can always be done immediately
         if self.config.mode == MODE_OFF:
             if self.active:
                 self._disable()
             self.active_config = self.config
             self.impl = NullWatchdog()
@@ -109,43 +115,44 @@
         if not self.active:
             if self.config.driver != self.active_config.driver or \
                self.config.driver_config != self.active_config.driver_config:
                 self.impl = self.config.get_impl()
             self.active_config = self.config
 
     @synchronized
-    def activate(self):
+    def activate(self) -> bool:
         """Activates the watchdog device with suitable timeouts. While watchdog is active keepalive needs
         to be called every time loop_wait expires.
 
         :returns False if a safe watchdog could not be configured, but is required.
         """
         self.active = True
         return self._activate()
 
-    def _activate(self):
+    def _activate(self) -> bool:
         self.active_config = self.config
 
         if self.config.timing_slack < 0:
             logger.warning('Watchdog not supported because leader TTL {0} is less than 2x loop_wait {1}'
                            .format(self.config.ttl, self.config.loop_wait))
             self.impl = NullWatchdog()
 
         try:
             self.impl.open()
             actual_timeout = self._set_timeout()
         except WatchdogError as e:
             logger.warning("Could not activate %s: %s", self.impl.describe(), e)
             self.impl = NullWatchdog()
+            actual_timeout = self.impl.get_timeout()
 
         if self.impl.is_running and not self.impl.can_be_disabled:
             logger.warning("Watchdog implementation can't be disabled."
                            " Watchdog will trigger after Patroni loses leader key.")
 
-        if not self.impl.is_running or actual_timeout > self.config.timeout:
+        if not self.impl.is_running or actual_timeout and actual_timeout > self.config.timeout:
             if self.config.mode == MODE_REQUIRED:
                 if self.impl.is_null:
                     logger.error("Configuration requires watchdog, but watchdog could not be configured.")
                 else:
                     logger.error("Configuration requires watchdog, but a safe watchdog timeout {0} could"
                                  " not be configured. Watchdog timeout is {1}.".format(
                                      self.config.timeout, actual_timeout))
@@ -161,15 +168,15 @@
         else:
             if self.config.mode == MODE_REQUIRED:
                 logger.error("Configuration requires watchdog, but watchdog could not be activated")
                 return False
 
         return True
 
-    def _set_timeout(self):
+    def _set_timeout(self) -> Optional[int]:
         if self.impl.has_set_timeout():
             self.impl.set_timeout(self.config.timeout)
 
         # Safety checks for watchdog implementations that don't support configurable timeouts
         actual_timeout = self.impl.get_timeout()
         if self.impl.is_running and actual_timeout < self.config.loop_wait:
             logger.error('loop_wait of {0} seconds is too long for watchdog {1} second timeout'
@@ -178,31 +185,31 @@
                 logger.info('Disabling watchdog due to unsafe timeout.')
                 self.impl.close()
                 self.impl = NullWatchdog()
                 return None
         return actual_timeout
 
     @synchronized
-    def disable(self):
+    def disable(self) -> None:
         self._disable()
         self.active = False
 
-    def _disable(self):
+    def _disable(self) -> None:
         try:
             if self.impl.is_running and not self.impl.can_be_disabled:
                 # Give sysadmin some extra time to clean stuff up.
                 self.impl.keepalive()
                 logger.warning("Watchdog implementation can't be disabled. System will reboot after "
                                "{0} seconds when watchdog times out.".format(self.impl.get_timeout()))
             self.impl.close()
         except WatchdogError as e:
             logger.error("Error while disabling watchdog: %s", e)
 
     @synchronized
-    def keepalive(self):
+    def keepalive(self) -> None:
         try:
             if self.active:
                 self.impl.keepalive()
             # In case there are any pending configuration changes apply them now.
             if self.active and self.config != self.active_config:
                 if self.config.mode != MODE_OFF and self.active_config.mode == MODE_OFF:
                     self.impl = self.config.get_impl()
@@ -219,97 +226,96 @@
                                     .format(self.impl.describe(), self.impl.get_timeout(), self.config.timing_slack))
                 self.active_config = self.config
         except WatchdogError as e:
             logger.error("Error while sending keepalive: %s", e)
 
     @property
     @synchronized
-    def is_running(self):
+    def is_running(self) -> bool:
         return self.impl.is_running
 
     @property
     @synchronized
-    def is_healthy(self):
+    def is_healthy(self) -> bool:
         if self.config.mode != MODE_REQUIRED:
             return True
         return self.config.timing_slack >= 0 and self.impl.is_healthy
 
 
 class WatchdogBase(abc.ABC):
     """A watchdog object when opened requires periodic calls to keepalive.
     When keepalive is not called within a timeout the system will be terminated."""
     is_null = False
 
     @property
-    def is_running(self):
+    def is_running(self) -> bool:
         """Returns True when watchdog is activated and capable of performing it's task."""
         return False
 
     @property
-    def is_healthy(self):
+    def is_healthy(self) -> bool:
         """Returns False when calling open() is known to fail."""
         return False
 
     @property
-    def can_be_disabled(self):
+    def can_be_disabled(self) -> bool:
         """Returns True when watchdog will be disabled by calling close(). Some watchdog devices
         will keep running no matter what once activated. May raise WatchdogError if called without
         calling open() first."""
         return True
 
     @abc.abstractmethod
-    def open(self):
+    def open(self) -> None:
         """Open watchdog device.
 
         When watchdog is opened keepalive must be called. Returns nothing on success
         or raises WatchdogError if the device could not be opened."""
 
     @abc.abstractmethod
-    def close(self):
+    def close(self) -> None:
         """Gracefully close watchdog device."""
 
     @abc.abstractmethod
-    def keepalive(self):
+    def keepalive(self) -> None:
         """Resets the watchdog timer.
 
         Watchdog must be open when keepalive is called."""
 
     @abc.abstractmethod
-    def get_timeout(self):
+    def get_timeout(self) -> int:
         """Returns the current keepalive timeout in effect."""
 
-    @staticmethod
-    def has_set_timeout():
+    def has_set_timeout(self) -> bool:
         """Returns True if setting a timeout is supported."""
         return False
 
-    def set_timeout(self, timeout):
+    def set_timeout(self, timeout: int) -> None:
         """Set the watchdog timer timeout.
 
         :param timeout: watchdog timeout in seconds"""
         raise WatchdogError("Setting timeout is not supported on {0}".format(self.describe()))
 
-    def describe(self):
+    def describe(self) -> str:
         """Human readable name for this device"""
         return self.__class__.__name__
 
     @classmethod
-    def from_config(cls, config):
+    def from_config(cls, config: Dict[str, Any]) -> 'WatchdogBase':
         return cls()
 
 
 class NullWatchdog(WatchdogBase):
     """Null implementation when watchdog is not supported."""
     is_null = True
 
-    def open(self):
+    def open(self) -> None:
         return
 
-    def close(self):
+    def close(self) -> None:
         return
 
-    def keepalive(self):
+    def keepalive(self) -> None:
         return
 
-    def get_timeout(self):
+    def get_timeout(self) -> int:
         # A big enough number to not matter
         return 1000000000
```

### Comparing `patroni-3.0.2/patroni/watchdog/linux.py` & `patroni-3.0.3/patroni/watchdog/linux.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,12 +1,15 @@
-import collections
+# pyright: reportConstantRedefinition=false
 import ctypes
 import os
 import platform
-from patroni.watchdog.base import WatchdogBase, WatchdogError
+
+from typing import Any, Dict, NamedTuple
+
+from .base import WatchdogBase, WatchdogError
 
 # Pythonification of linux/ioctl.h
 IOC_NONE = 0
 IOC_WRITE = 1
 IOC_READ = 2
 
 IOC_NRBITS = 8
@@ -15,37 +18,37 @@
 IOC_DIRBITS = 2
 
 # Non-generic platform special cases
 machine = platform.machine()
 if machine in ['mips', 'sparc', 'powerpc', 'ppc64', 'ppc64le']:  # pragma: no cover
     IOC_SIZEBITS = 13
     IOC_DIRBITS = 3
-    IOC_NONE, IOC_WRITE, IOC_READ = 1, 4, 2
+    IOC_NONE, IOC_WRITE = 1, 4
 elif machine == 'parisc':  # pragma: no cover
     IOC_WRITE, IOC_READ = 2, 1
 
 IOC_NRSHIFT = 0
 IOC_TYPESHIFT = IOC_NRSHIFT + IOC_NRBITS
 IOC_SIZESHIFT = IOC_TYPESHIFT + IOC_TYPEBITS
 IOC_DIRSHIFT = IOC_SIZESHIFT + IOC_SIZEBITS
 
 
-def IOW(type_, nr, size):
+def IOW(type_: str, nr: int, size: int) -> int:
     return IOC(IOC_WRITE, type_, nr, size)
 
 
-def IOR(type_, nr, size):
+def IOR(type_: str, nr: int, size: int) -> int:
     return IOC(IOC_READ, type_, nr, size)
 
 
-def IOWR(type_, nr, size):
+def IOWR(type_: str, nr: int, size: int) -> int:
     return IOC(IOC_READ | IOC_WRITE, type_, nr, size)
 
 
-def IOC(dir_, type_, nr, size):
+def IOC(dir_: int, type_: str, nr: int, size: int) -> int:
     return (dir_ << IOC_DIRSHIFT) \
         | (ord(type_) << IOC_TYPESHIFT) \
         | (nr << IOC_NRSHIFT) \
         | (size << IOC_SIZESHIFT)
 
 
 # Pythonification of linux/watchdog.h
@@ -100,136 +103,144 @@
     "ENABLECARD": 0x0002,     # Turn on the watchdog timer
     "TEMPPANIC": 0x0004,      # Kernel panic on temperature trip
 }
 
 # Implementation
 
 
-class WatchdogInfo(collections.namedtuple('WatchdogInfo', 'options,version,identity')):
+class WatchdogInfo(NamedTuple):
     """Watchdog descriptor from the kernel"""
-    def __getattr__(self, name):
+    options: int
+    version: int
+    identity: str
+
+    def __getattr__(self, name: str) -> bool:
         """Convenience has_XYZ attributes for checking WDIOF bits in options"""
         if name.startswith('has_') and name[4:] in WDIOF:
             return bool(self.options & WDIOF[name[4:]])
 
         raise AttributeError("WatchdogInfo instance has no attribute '{0}'".format(name))
 
 
 class LinuxWatchdogDevice(WatchdogBase):
     DEFAULT_DEVICE = '/dev/watchdog'
 
-    def __init__(self, device):
+    def __init__(self, device: str) -> None:
         self.device = device
         self._support_cache = None
         self._fd = None
 
     @classmethod
-    def from_config(cls, config):
+    def from_config(cls, config: Dict[str, Any]) -> 'LinuxWatchdogDevice':
         device = config.get('device', cls.DEFAULT_DEVICE)
         return cls(device)
 
     @property
-    def is_running(self):
+    def is_running(self) -> bool:
         return self._fd is not None
 
     @property
-    def is_healthy(self):
+    def is_healthy(self) -> bool:
         return os.path.exists(self.device) and os.access(self.device, os.W_OK)
 
-    def open(self):
+    def open(self) -> None:
         try:
             self._fd = os.open(self.device, os.O_WRONLY)
         except OSError as e:
             raise WatchdogError("Can't open watchdog device: {0}".format(e))
 
-    def close(self):
-        if self.is_running:
+    def close(self) -> None:
+        if self._fd is not None:  # self.is_running
             try:
                 os.write(self._fd, b'V')
                 os.close(self._fd)
                 self._fd = None
             except OSError as e:
                 raise WatchdogError("Error while closing {0}: {1}".format(self.describe(), e))
 
     @property
-    def can_be_disabled(self):
+    def can_be_disabled(self) -> bool:
         return self.get_support().has_MAGICCLOSE
 
-    def _ioctl(self, func, arg):
+    def _ioctl(self, func: int, arg: Any) -> None:
         """Runs the specified ioctl on the underlying fd.
 
         Raises WatchdogError if the device is closed.
         Raises OSError or IOError (Python 2) when the ioctl fails."""
         if self._fd is None:
             raise WatchdogError("Watchdog device is closed")
         if os.name != 'nt':
             import fcntl
             fcntl.ioctl(self._fd, func, arg, True)
 
-    def get_support(self):
+    def get_support(self) -> WatchdogInfo:
         if self._support_cache is None:
             info = watchdog_info()
             try:
                 self._ioctl(WDIOC_GETSUPPORT, info)
             except (WatchdogError, OSError, IOError) as e:
                 raise WatchdogError("Could not get information about watchdog device: {}".format(e))
             self._support_cache = WatchdogInfo(info.options,
                                                info.firmware_version,
                                                bytearray(info.identity).decode(errors='ignore').rstrip('\x00'))
         return self._support_cache
 
-    def describe(self):
+    def describe(self) -> str:
         dev_str = " at {0}".format(self.device) if self.device != self.DEFAULT_DEVICE else ""
         ver_str = ""
         identity = "Linux watchdog device"
         if self._fd:
             try:
                 _, version, identity = self.get_support()
                 ver_str = " (firmware {0})".format(version) if version else ""
             except WatchdogError:
                 pass
 
         return identity + ver_str + dev_str
 
-    def keepalive(self):
+    def keepalive(self) -> None:
+        if self._fd is None:
+            raise WatchdogError("Watchdog device is closed")
         try:
             os.write(self._fd, b'1')
         except OSError as e:
             raise WatchdogError("Could not send watchdog keepalive: {0}".format(e))
 
-    def has_set_timeout(self):
+    def has_set_timeout(self) -> bool:
         """Returns True if setting a timeout is supported."""
         return self.get_support().has_SETTIMEOUT
 
-    def set_timeout(self, timeout):
+    def set_timeout(self, timeout: int) -> None:
         timeout = int(timeout)
         if not 0 < timeout < 0xFFFF:
             raise WatchdogError("Invalid timeout {0}. Supported values are between 1 and 65535".format(timeout))
         try:
             self._ioctl(WDIOC_SETTIMEOUT, ctypes.c_int(timeout))
         except (WatchdogError, OSError, IOError) as e:
             raise WatchdogError("Could not set timeout on watchdog device: {}".format(e))
 
-    def get_timeout(self):
+    def get_timeout(self) -> int:
         timeout = ctypes.c_int()
         try:
             self._ioctl(WDIOC_GETTIMEOUT, timeout)
         except (WatchdogError, OSError, IOError) as e:
             raise WatchdogError("Could not get timeout on watchdog device: {}".format(e))
         return timeout.value
 
 
 class TestingWatchdogDevice(LinuxWatchdogDevice):  # pragma: no cover
     """Converts timeout ioctls to regular writes that can be intercepted from a named pipe."""
     timeout = 60
 
-    def get_support(self):
+    def get_support(self) -> WatchdogInfo:
         return WatchdogInfo(WDIOF['MAGICCLOSE'] | WDIOF['SETTIMEOUT'], 0, "Watchdog test harness")
 
-    def set_timeout(self, timeout):
+    def set_timeout(self, timeout: int) -> None:
+        if self._fd is None:
+            raise WatchdogError("Watchdog device is closed")
         buf = "Ctimeout={0}\n".format(timeout).encode('utf8')
         while len(buf):
             buf = buf[os.write(self._fd, buf):]
         self.timeout = timeout
 
-    def get_timeout(self):
+    def get_timeout(self) -> int:
         return self.timeout
```

### Comparing `patroni-3.0.2/patroni.egg-info/PKG-INFO` & `patroni-3.0.3/patroni.egg-info/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: patroni
-Version: 3.0.2
+Version: 3.0.3
 Summary: PostgreSQL High-Available orchestrator and CLI
 Home-page: https://github.com/zalando/patroni
 Author: Alexander Kukushkin, Polina Bungina
 Author-email: akukushkin@microsoft.com, polina.bungina@zalando.de
 License: The MIT License
 Keywords: etcd governor patroni postgresql postgres ha haproxy confd zookeeper exhibitor consul streaming replication kubernetes k8s
 Classifier: Development Status :: 5 - Production/Stable
@@ -41,15 +41,15 @@
 --------------------------------------------------------------------
 
 You can find a version of this documentation that is searchable and also easier to navigate at `patroni.readthedocs.io <https://patroni.readthedocs.io>`__.
 
 
 There are many ways to run high availability with PostgreSQL; for a list, see the `PostgreSQL Documentation <https://wiki.postgresql.org/wiki/Replication,_Clustering,_and_Connection_Pooling>`__.
 
-Patroni is a template for you to create your own customized, high-availability solution using Python and - for maximum accessibility - a distributed configuration store like `ZooKeeper <https://zookeeper.apache.org/>`__, `etcd <https://github.com/coreos/etcd>`__, `Consul <https://github.com/hashicorp/consul>`__ or `Kubernetes <https://kubernetes.io>`__. Database engineers, DBAs, DevOps engineers, and SREs who are looking to quickly deploy HA PostgreSQL in the datacenter-or anywhere else-will hopefully find it useful.
+Patroni is a template for high availability (HA) PostgreSQL solutions using Python. For maximum accessibility, Patroni supports a variety of distributed configuration stores like `ZooKeeper <https://zookeeper.apache.org/>`__, `etcd <https://github.com/coreos/etcd>`__, `Consul <https://github.com/hashicorp/consul>`__ or `Kubernetes <https://kubernetes.io>`__. Database engineers, DBAs, DevOps engineers, and SREs who are looking to quickly deploy HA PostgreSQL in datacenters — or anywhere else — will hopefully find it useful.
 
 We call Patroni a "template" because it is far from being a one-size-fits-all or plug-and-play replication system. It will have its own caveats. Use wisely.
 
 Currently supported PostgreSQL versions: 9.3 to 15.
 
 **Note to Citus users**: Starting from 3.0 Patroni nicely integrates with the `Citus <https://github.com/citusdata/citus>`__ database extension to Postgres. Please check the `Citus support page <https://github.com/zalando/patroni/blob/master/docs/citus.rst>`__ in the Patroni documentation for more info about how to use Patroni high availability together with a Citus distributed cluster.
 
@@ -82,15 +82,15 @@
 
 We report new releases information `here <https://github.com/zalando/patroni/releases>`__.
 
 =========
 Community
 =========
 
-There are two places to connect with the Patroni community: `on github <https://github.com/zalando/patroni>`__, via Issues and PRs, and on channel `#patroni <https://postgresteam.slack.com/archives/C9XPYG92A>`__ in the `PostgreSQL Slack <https://join.slack.com/t/postgresteam/shared_invite/zt-1qj14i9sj-E9WqIFlvcOiHsEk2yFEMjA>`__.  If you're using Patroni, or just interested, please join us.
+There are two places to connect with the Patroni community: `on github <https://github.com/zalando/patroni>`__, via Issues and PRs, and on channel `#patroni <https://postgresteam.slack.com/archives/C9XPYG92A>`__ in the `PostgreSQL Slack <https://pgtreats.info/slack-invite>`__.  If you're using Patroni, or just interested, please join us.
 
 ===================================
 Technical Requirements/Installation
 ===================================
 
 **Pre-requirements for Mac OS**
 
@@ -152,15 +152,15 @@
 exhibitor
     `kazoo` module in order to use Exhibitor as DCS (same dependencies as for Zookeeper)
 kubernetes
     `kubernetes` module in order to use Kubernetes as DCS in Patroni
 raft
     `pysyncobj` module in order to use python Raft implementation as DCS
 aws
-    `boto` in order to use AWS callbacks
+    `boto3` in order to use AWS callbacks
 
 For example, the command in order to install Patroni together with dependencies for Etcd as a DCS and AWS callbacks is:
 
 ::
 
     pip install patroni[etcd,aws]
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `patroni-3.0.2/patroni.egg-info/SOURCES.txt` & `patroni-3.0.3/patroni.egg-info/SOURCES.txt`

 * *Files 5% similar despite different names*

```diff
@@ -4,14 +4,15 @@
 requirements.dev.txt
 requirements.txt
 setup.py
 patroni/__init__.py
 patroni/__main__.py
 patroni/api.py
 patroni/async_executor.py
+patroni/collections.py
 patroni/config.py
 patroni/ctl.py
 patroni/daemon.py
 patroni/exceptions.py
 patroni/ha.py
 patroni/log.py
 patroni/psycopg.py
@@ -43,14 +44,15 @@
 patroni/postgresql/connection.py
 patroni/postgresql/misc.py
 patroni/postgresql/postmaster.py
 patroni/postgresql/rewind.py
 patroni/postgresql/slots.py
 patroni/postgresql/sync.py
 patroni/postgresql/validator.py
+patroni/postgresql/available_parameters/0_postgres.yml
 patroni/scripts/__init__.py
 patroni/scripts/aws.py
 patroni/scripts/wale_restore.py
 patroni/watchdog/__init__.py
 patroni/watchdog/base.py
 patroni/watchdog/linux.py
 tests/test_api.py
```

### Comparing `patroni-3.0.2/setup.py` & `patroni-3.0.3/setup.py`

 * *Files 8% similar despite different names*

```diff
@@ -153,15 +153,18 @@
         author_email=AUTHOR_EMAIL,
         description=DESCRIPTION,
         license=LICENSE,
         keywords=KEYWORDS,
         long_description=read('README.rst'),
         classifiers=CLASSIFIERS,
         packages=find_packages(exclude=['tests', 'tests.*']),
-        package_data={MAIN_PACKAGE: ["*.json"]},
+        package_data={MAIN_PACKAGE: [
+            "postgresql/available_parameters/*.yml",
+            "postgresql/available_parameters/*.yaml",
+        ]},
         install_requires=install_requires,
         extras_require=EXTRAS_REQUIRE,
         cmdclass=cmdclass,
         entry_points={'console_scripts': CONSOLE_SCRIPTS},
     )
```

### Comparing `patroni-3.0.2/tests/test_api.py` & `patroni-3.0.3/tests/test_api.py`

 * *Files 3% similar despite different names*

```diff
@@ -7,14 +7,15 @@
 
 from http.server import HTTPServer
 from io import BytesIO as IO
 from mock import Mock, PropertyMock, patch
 from socketserver import ThreadingMixIn
 
 from patroni.api import RestApiHandler, RestApiServer
+from patroni.config import GlobalConfig
 from patroni.dcs import ClusterConfig, Member
 from patroni.ha import _MemberStatus
 from patroni.utils import tzutc
 
 from . import psycopg_connect, MockCursor
 from .test_ha import get_cluster_initialized_without_leader
 
@@ -112,30 +113,32 @@
     def wakeup():
         pass
 
     @staticmethod
     def is_paused():
         return True
 
-    @staticmethod
-    def is_standby_cluster():
-        return False
-
 
 class MockLogger(object):
 
     NORMAL_LOG_QUEUE_SIZE = 2
     queue_size = 3
     records_lost = 1
 
 
+class MockConfig(object):
+
+    def get_global_config(self, _):
+        return GlobalConfig({})
+
+
 class MockPatroni(object):
 
     ha = MockHa()
-    config = Mock()
+    config = MockConfig()
     postgresql = ha.state_handler
     dcs = Mock()
     logger = MockLogger()
     tags = {"key1": True, "key2": False, "key3": 1, "key4": 1.4, "key5": "RandomTag"}
     version = '0.00'
     noloadbalance = PropertyMock(return_value=False)
     scheduled_restart = {'schedule': future_restart_time,
@@ -173,23 +176,25 @@
                             'http_extra_headers': {'foo': 'bar'}, 'https_extra_headers': {'foo': 'sbar'}}
         super(MockRestApiServer, self).__init__(MockPatroni(), config)
         Handler(MockRequest(request), ('0.0.0.0', 8080), self)
 
 
 @patch('ssl.SSLContext.load_cert_chain', Mock())
 @patch('ssl.SSLContext.wrap_socket', Mock(return_value=0))
+@patch('ssl.SSLContext.load_verify_locations', Mock(return_value=[Mock()]))
 @patch.object(HTTPServer, '__init__', Mock())
 class TestRestApiHandler(unittest.TestCase):
 
     _authorization = '\nAuthorization: Basic dGVzdDp0ZXN0'
 
     def test_do_GET(self):
         MockPatroni.dcs.cluster.last_lsn = 20
         MockPatroni.dcs.cluster.sync.members = [MockPostgresql.name]
-        MockRestApiServer(RestApiHandler, 'GET /replica')
+        with patch.object(GlobalConfig, 'is_synchronous_mode', PropertyMock(return_value=True)):
+            MockRestApiServer(RestApiHandler, 'GET /replica')
         MockRestApiServer(RestApiHandler, 'GET /replica?lag=1M')
         MockRestApiServer(RestApiHandler, 'GET /replica?lag=10MB')
         MockRestApiServer(RestApiHandler, 'GET /replica?lag=10485760')
         MockRestApiServer(RestApiHandler, 'GET /read-only')
         with patch.object(RestApiHandler, 'get_postgresql_status', Mock(return_value={})):
             MockRestApiServer(RestApiHandler, 'GET /replica')
         with patch.object(RestApiHandler, 'get_postgresql_status', Mock(return_value={'role': 'primary'})):
@@ -203,25 +208,26 @@
             MockRestApiServer(RestApiHandler, 'GET /read-only-sync')
         with patch.object(RestApiHandler, 'get_postgresql_status', Mock(return_value={'role': 'replica'})):
             MockPatroni.dcs.cluster.sync.members = []
             MockRestApiServer(RestApiHandler, 'GET /asynchronous')
         with patch.object(MockHa, 'is_leader', Mock(return_value=True)):
             MockRestApiServer(RestApiHandler, 'GET /replica')
             MockRestApiServer(RestApiHandler, 'GET /read-only-sync')
-            with patch.object(MockHa, 'is_standby_cluster', Mock(return_value=True)):
+            with patch.object(GlobalConfig, 'is_standby_cluster', Mock(return_value=True)):
                 MockRestApiServer(RestApiHandler, 'GET /standby_leader')
         MockPatroni.dcs.cluster = None
         with patch.object(RestApiHandler, 'get_postgresql_status', Mock(return_value={'role': 'primary'})):
             MockRestApiServer(RestApiHandler, 'GET /primary')
         with patch.object(MockHa, 'restart_scheduled', Mock(return_value=True)):
             MockRestApiServer(RestApiHandler, 'GET /primary')
         self.assertIsNotNone(MockRestApiServer(RestApiHandler, 'GET /primary'))
         with patch.object(RestApiServer, 'query', Mock(return_value=[('', 1, '', '', '', '', False, '')])):
             self.assertIsNotNone(MockRestApiServer(RestApiHandler, 'GET /patroni'))
-        with patch.object(MockHa, 'is_standby_cluster', Mock(return_value=True)):
+        with patch.object(GlobalConfig, 'is_standby_cluster', Mock(return_value=True)),\
+                patch.object(GlobalConfig, 'is_paused', Mock(return_value=True)):
             MockRestApiServer(RestApiHandler, 'GET /standby_leader')
 
         # test tags
         #
         MockRestApiServer(RestApiHandler, 'GET /primary?lag=1M&'
                                           'tag_key1=true&tag_key2=false&'
                                           'tag_key3=1&tag_key4=1.4&tag_key5=RandomTag')
@@ -389,29 +395,27 @@
         type(mock_dcs).failsafe = PropertyMock(return_value=None)
         self.assertIsNotNone(MockRestApiServer(RestApiHandler, 'GET /failsafe'))
 
     def test_do_POST_failsafe(self):
         with patch.object(MockHa, 'is_failsafe_mode', Mock(return_value=False), create=True):
             self.assertIsNotNone(MockRestApiServer(RestApiHandler, 'POST /failsafe HTTP/1.0' + self._authorization))
         with patch.object(MockHa, 'is_failsafe_mode', Mock(return_value=True), create=True):
-            self.assertIsNotNone(MockRestApiServer(RestApiHandler, 'POST /failsafe HTTP/1.0' + self._authorization +
-                                                                   '\nContent-Length: 9\n\n{"a":"b"}'))
+            self.assertIsNotNone(MockRestApiServer(RestApiHandler, 'POST /failsafe HTTP/1.0' + self._authorization
+                                                   + '\nContent-Length: 9\n\n{"a":"b"}'))
 
     @patch.object(MockPatroni, 'sighup_handler', Mock())
     def test_do_POST_reload(self):
         self.assertIsNotNone(MockRestApiServer(RestApiHandler, 'POST /reload HTTP/1.0' + self._authorization))
 
     @patch('os.environ', {'BEHAVE_DEBUG': 'true'})
     @patch('os.name', 'nt')
     def test_do_POST_sigterm(self):
         self.assertIsNotNone(MockRestApiServer(RestApiHandler, 'POST /sigterm HTTP/1.0' + self._authorization))
 
-    @patch.object(MockPatroni, 'dcs')
-    def test_do_POST_restart(self, mock_dcs):
-        mock_dcs.get_cluster.return_value.is_paused.return_value = False
+    def test_do_POST_restart(self):
         request = 'POST /restart HTTP/1.0' + self._authorization
         self.assertIsNotNone(MockRestApiServer(RestApiHandler, request))
 
         with patch.object(MockHa, 'restart', Mock(side_effect=Exception)):
             MockRestApiServer(RestApiHandler, request)
 
         post = request + '\nContent-Length: '
@@ -445,20 +449,20 @@
             with patch.object(MockHa, 'schedule_future_restart', Mock(return_value=retval)):
                 request = make_request(schedule=future_restart_time.isoformat())
                 MockRestApiServer(RestApiHandler, request)
             with patch.object(MockHa, 'restart', Mock(return_value=(retval, "foo"))):
                 request = make_request(role='primary', postgres_version='9.5.2')
                 MockRestApiServer(RestApiHandler, request)
 
-        mock_dcs.get_cluster.return_value.is_paused.return_value = True
-        MockRestApiServer(RestApiHandler, make_request(schedule='2016-08-42 12:45TZ+1', role='primary'))
-        # Valid timeout
-        MockRestApiServer(RestApiHandler, make_request(timeout='60s'))
-        # Invalid timeout
-        MockRestApiServer(RestApiHandler, make_request(timeout='42towels'))
+        with patch.object(GlobalConfig, 'is_paused', PropertyMock(return_value=True)):
+            MockRestApiServer(RestApiHandler, make_request(schedule='2016-08-42 12:45TZ+1', role='primary'))
+            # Valid timeout
+            MockRestApiServer(RestApiHandler, make_request(timeout='60s'))
+            # Invalid timeout
+            MockRestApiServer(RestApiHandler, make_request(timeout='42towels'))
 
     def test_do_DELETE_restart(self):
         for retval in (True, False):
             with patch.object(MockHa, 'delete_future_restart', Mock(return_value=retval)):
                 request = 'DELETE /restart HTTP/1.0' + self._authorization
                 self.assertIsNotNone(MockRestApiServer(RestApiHandler, request))
 
@@ -467,18 +471,15 @@
         request = 'DELETE /switchover HTTP/1.0' + self._authorization
         self.assertIsNotNone(MockRestApiServer(RestApiHandler, request))
         mock_dcs.manual_failover.return_value = False
         self.assertIsNotNone(MockRestApiServer(RestApiHandler, request))
         mock_dcs.get_cluster.return_value.failover = None
         self.assertIsNotNone(MockRestApiServer(RestApiHandler, request))
 
-    @patch.object(MockPatroni, 'dcs')
-    def test_do_POST_reinitialize(self, mock_dcs):
-        cluster = mock_dcs.get_cluster.return_value
-        cluster.is_paused.return_value = False
+    def test_do_POST_reinitialize(self):
         request = 'POST /reinitialize HTTP/1.0' + self._authorization + '\nContent-Length: 15\n\n{"force": true}'
         MockRestApiServer(RestApiHandler, request)
         with patch.object(MockHa, 'reinitialize', Mock(return_value=None)):
             MockRestApiServer(RestApiHandler, request)
 
     @patch('time.sleep', Mock())
     def test_RestApiServer_query(self):
@@ -488,43 +489,43 @@
             self.assertIsNotNone(MockRestApiServer(RestApiHandler, 'GET /patroni'))
 
     @patch('time.sleep', Mock())
     @patch.object(MockPatroni, 'dcs')
     def test_do_POST_switchover(self, dcs):
         dcs.loop_wait = 10
         cluster = dcs.get_cluster.return_value
-        cluster.is_synchronous_mode.return_value = False
-        cluster.is_paused.return_value = False
 
         post = 'POST /switchover HTTP/1.0' + self._authorization + '\nContent-Length: '
 
         MockRestApiServer(RestApiHandler, post + '7\n\n{"1":2}')
 
         request = post + '0\n\n'
         MockRestApiServer(RestApiHandler, request)
 
         cluster.leader.name = 'postgresql1'
         MockRestApiServer(RestApiHandler, request)
 
         request = post + '25\n\n{"leader": "postgresql1"}'
 
-        cluster.is_paused.return_value = True
-        MockRestApiServer(RestApiHandler, request)
-
-        cluster.is_paused.return_value = False
-        for cluster.is_synchronous_mode.return_value in (True, False):
+        with patch.object(GlobalConfig, 'is_paused', PropertyMock(return_value=True)):
             MockRestApiServer(RestApiHandler, request)
 
+        for is_synchronous_mode in (True, False):
+            with patch.object(GlobalConfig, 'is_synchronous_mode', PropertyMock(return_value=is_synchronous_mode)):
+                MockRestApiServer(RestApiHandler, request)
+
         cluster.leader.name = 'postgresql2'
         request = post + '53\n\n{"leader": "postgresql1", "candidate": "postgresql2"}'
         MockRestApiServer(RestApiHandler, request)
 
         cluster.leader.name = 'postgresql1'
-        for cluster.is_synchronous_mode.return_value in (True, False):
-            MockRestApiServer(RestApiHandler, request)
+        cluster.sync.matches.return_value = False
+        for is_synchronous_mode in (True, False):
+            with patch.object(GlobalConfig, 'is_synchronous_mode', PropertyMock(return_value=is_synchronous_mode)):
+                MockRestApiServer(RestApiHandler, request)
 
         cluster.members = [Member(0, 'postgresql0', 30, {'api_url': 'http'}),
                            Member(0, 'postgresql2', 30, {'api_url': 'http'})]
         MockRestApiServer(RestApiHandler, request)
 
         cluster.failover = None
         MockRestApiServer(RestApiHandler, request)
@@ -550,15 +551,16 @@
         with patch.object(MockHa, 'fetch_nodes_statuses', Mock(return_value=[])):
             MockRestApiServer(RestApiHandler, request)
 
         # Valid future date
         request = post + '103\n\n{"leader": "postgresql1", "member": "postgresql2",' +\
                          ' "scheduled_at": "6016-02-15T18:13:30.568224+01:00"}'
         MockRestApiServer(RestApiHandler, request)
-        with patch.object(MockPatroni, 'dcs') as d:
+        with patch.object(GlobalConfig, 'is_paused', PropertyMock(return_value=True)),\
+                patch.object(MockPatroni, 'dcs') as d:
             d.manual_failover.return_value = False
             MockRestApiServer(RestApiHandler, request)
 
         # Exception: No timezone specified
         request = post + '97\n\n{"leader": "postgresql1", "member": "postgresql2",' +\
                          ' "scheduled_at": "6016-02-15T18:13:30.568224"}'
         MockRestApiServer(RestApiHandler, request)
@@ -566,33 +568,32 @@
         # Exception: Scheduled in the past
         request = post + '103\n\n{"leader": "postgresql1", "member": "postgresql2", "scheduled_at": "'
         MockRestApiServer(RestApiHandler, request + '1016-02-15T18:13:30.568224+01:00"}')
 
         # Invalid date
         self.assertIsNotNone(MockRestApiServer(RestApiHandler, request + '2010-02-29T18:13:30.568224+01:00"}'))
 
-    @patch.object(MockPatroni, 'dcs', Mock())
     def test_do_POST_failover(self):
         post = 'POST /failover HTTP/1.0' + self._authorization + '\nContent-Length: '
         MockRestApiServer(RestApiHandler, post + '14\n\n{"leader":"1"}')
         MockRestApiServer(RestApiHandler, post + '37\n\n{"candidate":"2","scheduled_at": "1"}')
 
-    @patch.object(MockPatroni, 'dcs', Mock())
     @patch.object(MockHa, 'is_leader', Mock(return_value=True))
     def test_do_POST_citus(self):
         post = 'POST /citus HTTP/1.0' + self._authorization + '\nContent-Length: '
         MockRestApiServer(RestApiHandler, post + '0\n\n')
         MockRestApiServer(RestApiHandler, post + '14\n\n{"leader":"1"}')
 
 
 class TestRestApiServer(unittest.TestCase):
 
     @patch('ssl.SSLContext.load_cert_chain', Mock())
     @patch('ssl.SSLContext.set_ciphers', Mock())
     @patch('ssl.SSLContext.wrap_socket', Mock(return_value=0))
+    @patch('ssl.SSLContext.load_verify_locations', Mock(return_value=[Mock()]))
     @patch.object(HTTPServer, '__init__', Mock())
     def setUp(self):
         self.srv = MockRestApiServer(Mock(), '', {'listen': '*:8008', 'certfile': 'a', 'verify_client': 'required',
                                                   'ciphers': '!SSLv1:!SSLv2:!SSLv3:!TLSv1:!TLSv1.1',
                                                   'allowlist': ['127.0.0.1', '::1/128', '::1/zxc'],
                                                   'allowlist_include_members': True})
 
@@ -618,28 +619,43 @@
         mock_rh.request.getpeercert.return_value = None
         self.assertIsNot(self.srv.check_access(mock_rh), True)
 
     def test_handle_error(self):
         try:
             raise Exception()
         except Exception:
-            self.assertIsNone(MockRestApiServer.handle_error(None, ('127.0.0.1', 55555)))
+            self.assertIsNone(self.srv.handle_error(None, ('127.0.0.1', 55555)))
 
     @patch.object(HTTPServer, '__init__', Mock(side_effect=socket.error))
     def test_socket_error(self):
         self.assertRaises(socket.error, MockRestApiServer, Mock(), '', {'listen': '*:8008'})
 
+    def __create_socket(self):
+        sock = socket.socket()
+        try:
+            import ssl
+            ctx = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)
+            ctx.check_hostname = False
+            sock = ctx.wrap_socket(sock=sock)
+            sock.do_handshake = Mock()
+            sock.unwrap = Mock(side_effect=Exception)
+        except Exception:
+            pass
+        return sock
+
     @patch.object(ThreadingMixIn, 'process_request_thread', Mock())
     def test_process_request_thread(self):
-        self.srv.process_request_thread(Mock(), '2')
+        self.srv.process_request_thread(self.__create_socket(), ('2', 54321))
 
     @patch.object(MockRestApiServer, 'process_request', Mock(side_effect=RuntimeError))
     @patch.object(MockRestApiServer, 'get_request')
     def test_process_request_error(self, mock_get_request):
-        mock_request = Mock()
-        mock_request.unwrap.side_effect = Exception
-        mock_get_request.return_value = (mock_request, ('127.0.0.1', 55555))
+        mock_get_request.return_value = (self.__create_socket(), ('127.0.0.1', 55555))
         self.srv._handle_request_noblock()
 
-    @patch('ssl._ssl._test_decode_cert', Mock())
+    @patch('ssl.SSLContext.load_verify_locations', Mock(return_value=[Mock()]))
     def test_reload_local_certificate(self):
         self.assertTrue(self.srv.reload_local_certificate())
+
+    @patch('ssl.SSLContext.load_verify_locations', Mock(side_effect=Exception))
+    def test_get_certificate_serial_number(self):
+        self.assertIsNone(self.srv.get_certificate_serial_number())
```

### Comparing `patroni-3.0.2/tests/test_async_executor.py` & `patroni-3.0.3/tests/test_async_executor.py`

 * *Files identical despite different names*

### Comparing `patroni-3.0.2/tests/test_aws.py` & `patroni-3.0.3/tests/test_aws.py`

 * *Files identical despite different names*

### Comparing `patroni-3.0.2/tests/test_bootstrap.py` & `patroni-3.0.3/tests/test_bootstrap.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,23 +1,25 @@
 import os
+import sys
 
 from mock import Mock, PropertyMock, patch
 
 from patroni.async_executor import CriticalTask
 from patroni.postgresql import Postgresql
 from patroni.postgresql.bootstrap import Bootstrap
 from patroni.postgresql.cancellable import CancellableSubprocess
 from patroni.postgresql.config import ConfigHandler
 
-from . import psycopg_connect, BaseTestPostgresql
+from . import psycopg_connect, BaseTestPostgresql, mock_available_gucs
 
 
 @patch('subprocess.call', Mock(return_value=0))
 @patch('patroni.psycopg.connect', psycopg_connect)
 @patch('os.rename', Mock())
+@patch.object(Postgresql, 'available_gucs', mock_available_gucs)
 class TestBootstrap(BaseTestPostgresql):
 
     @patch('patroni.postgresql.CallbackExecutor', Mock())
     def setUp(self):
         super(TestBootstrap, self).setUp()
         self.b = self.p.bootstrap
 
@@ -95,14 +97,56 @@
 
     def test__initdb(self):
         self.assertRaises(Exception, self.b.bootstrap, {'initdb': [{'pgdata': 'bar'}]})
         self.assertRaises(Exception, self.b.bootstrap, {'initdb': [{'foo': 'bar', 1: 2}]})
         self.assertRaises(Exception, self.b.bootstrap, {'initdb': [1]})
         self.assertRaises(Exception, self.b.bootstrap, {'initdb': 1})
 
+    def test__process_user_options(self):
+        def error_handler(msg):
+            raise Exception(msg)
+
+        self.assertEqual(self.b.process_user_options('initdb', ['string'], (), error_handler), ['--string'])
+        self.assertEqual(
+            self.b.process_user_options(
+                'initdb',
+                [{'key': 'value'}],
+                (), error_handler
+            ),
+            ['--key=value'])
+        if sys.platform != 'win32':
+            self.assertEqual(
+                self.b.process_user_options(
+                    'initdb',
+                    [{'key': 'value with spaces'}],
+                    (), error_handler
+                ),
+                ["--key=value with spaces"])
+            self.assertEqual(
+                self.b.process_user_options(
+                    'initdb',
+                    [{'key': "'value with spaces'"}],
+                    (), error_handler
+                ),
+                ["--key=value with spaces"])
+            self.assertEqual(
+                self.b.process_user_options(
+                    'initdb',
+                    {'key': 'value with spaces'},
+                    (), error_handler
+                ),
+                ["--key=value with spaces"])
+            self.assertEqual(
+                self.b.process_user_options(
+                    'initdb',
+                    {'key': "'value with spaces'"},
+                    (), error_handler
+                ),
+                ["--key=value with spaces"])
+
     @patch.object(CancellableSubprocess, 'call', Mock())
     @patch.object(Postgresql, 'is_running', Mock(return_value=True))
     @patch.object(Postgresql, 'data_directory_empty', Mock(return_value=False))
     @patch.object(Postgresql, 'controldata', Mock(return_value={'max_connections setting': 100,
                                                                 'max_prepared_xacts setting': 0,
                                                                 'max_locks_per_xact setting': 64}))
     def test_bootstrap(self):
@@ -128,15 +172,15 @@
         self.b.bootstrap(config)
         with open(os.path.join(self.p.data_dir, 'pg_hba.conf')) as f:
             lines = f.readlines()
             self.assertTrue('host replication replicator 127.0.0.1/32 md5\n' in lines)
 
     @patch.object(CancellableSubprocess, 'call')
     @patch.object(Postgresql, 'get_major_version', Mock(return_value=90600))
-    @patch.object(Postgresql, 'controldata',  Mock(return_value={'Database cluster state': 'in production'}))
+    @patch.object(Postgresql, 'controldata', Mock(return_value={'Database cluster state': 'in production'}))
     def test_custom_bootstrap(self, mock_cancellable_subprocess_call):
         self.p.config._config.pop('pg_hba')
         config = {'method': 'foo', 'foo': {'command': 'bar'}}
 
         mock_cancellable_subprocess_call.return_value = 1
         self.assertFalse(self.b.bootstrap(config))
```

### Comparing `patroni-3.0.2/tests/test_callback_executor.py` & `patroni-3.0.3/tests/test_callback_executor.py`

 * *Files identical despite different names*

### Comparing `patroni-3.0.2/tests/test_cancellable.py` & `patroni-3.0.3/tests/test_cancellable.py`

 * *Files identical despite different names*

### Comparing `patroni-3.0.2/tests/test_citus.py` & `patroni-3.0.3/tests/test_citus.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+import time
 from mock import Mock, patch
 from patroni.postgresql.citus import CitusHandler
 
 from . import BaseTestPostgresql, MockCursor, psycopg_connect, SleepException
 from .test_ha import get_cluster_initialized_with_leader
 
 
@@ -12,15 +13,15 @@
     def setUp(self):
         super(TestCitus, self).setUp()
         self.c = self.p.citus_handler
         self.c.set_conn_kwargs({'host': 'localhost', 'dbname': 'postgres'})
         self.cluster = get_cluster_initialized_with_leader()
         self.cluster.workers[1] = self.cluster
 
-    @patch('time.time', Mock(side_effect=[100, 130, 160, 190, 220, 250, 280]))
+    @patch('time.time', Mock(side_effect=[100, 130, 160, 190, 220, 250, 280, 310, 340, 370]))
     @patch('patroni.postgresql.citus.logger.exception', Mock(side_effect=SleepException))
     @patch('patroni.postgresql.citus.logger.warning')
     @patch('patroni.postgresql.citus.PgDistNode.wait', Mock())
     @patch.object(CitusHandler, 'is_alive', Mock(return_value=True))
     def test_run(self, mock_logger_warning):
         # `before_demote` or `before_promote` REST API calls starting a
         # transaction. We want to make sure that it finishes during
@@ -62,19 +63,22 @@
             self.assertTrue(mock_logger.call_args[0][0].startswith('Adding the new task:'))
 
         with patch('patroni.postgresql.citus.logger.debug') as mock_logger:
             self.c.add_task('before_promote', 1, 'postgres://host:5432/postgres', 30)
             mock_logger.assert_called_once()
             self.assertTrue(mock_logger.call_args[0][0].startswith('Overriding existing task:'))
 
-        # add_task called from sync_pg_dist_node should not override already scheduled or in flight task
+        # add_task called from sync_pg_dist_node should not override already scheduled or in flight task until deadline
         self.assertIsNotNone(self.c.add_task('after_promote', 1, 'postgres://host:5432/postgres', 30))
         self.assertIsNone(self.c.add_task('after_promote', 1, 'postgres://host:5432/postgres'))
         self.c._in_flight = self.c._tasks.pop()
+        self.c._in_flight.deadline = self.c._in_flight.timeout + time.time()
         self.assertIsNone(self.c.add_task('after_promote', 1, 'postgres://host:5432/postgres'))
+        self.c._in_flight.deadline = 0
+        self.assertIsNotNone(self.c.add_task('after_promote', 1, 'postgres://host:5432/postgres'))
 
         # If there is no transaction in progress and cached pg_dist_node matching desired state task should not be added
         self.c._schedule_load_pg_dist_node = False
         self.c._pg_dist_node[self.c._in_flight.group] = self.c._in_flight
         self.c._in_flight = None
         self.assertIsNone(self.c.add_task('after_promote', 1, 'postgres://host:5432/postgres'))
 
@@ -132,16 +136,16 @@
                       'max_prepared_transactions': 0,
                       'shared_preload_libraries': 'foo , citus, bar '}
         self.c.adjust_postgres_gucs(parameters)
         self.assertEqual(parameters['max_prepared_transactions'], 202)
         self.assertEqual(parameters['shared_preload_libraries'], 'citus,foo,bar')
         self.assertEqual(parameters['wal_level'], 'logical')
 
-    @patch.object(CitusHandler, 'is_enabled', Mock(return_value=False))
     def test_bootstrap(self):
+        self.c._config = None
         self.c.bootstrap()
 
     def test_ignore_replication_slot(self):
         self.assertFalse(self.c.ignore_replication_slot({'name': 'foo', 'type': 'physical',
                                                          'database': 'bar', 'plugin': 'wal2json'}))
         self.assertFalse(self.c.ignore_replication_slot({'name': 'foo', 'type': 'logical',
                                                          'database': 'bar', 'plugin': 'wal2json'}))
```

### Comparing `patroni-3.0.2/tests/test_config.py` & `patroni-3.0.3/tests/test_config.py`

 * *Files 3% similar despite different names*

```diff
@@ -15,18 +15,17 @@
     def setUp(self):
         sys.argv = ['patroni.py']
         os.environ[Config.PATRONI_CONFIG_VARIABLE] = 'restapi: {}\npostgresql: {data_dir: foo}'
         self.config = Config(None)
 
     def test_set_dynamic_configuration(self):
         with patch.object(Config, '_build_effective_configuration', Mock(side_effect=Exception)):
-            self.assertIsNone(self.config.set_dynamic_configuration({'foo': 'bar'}))
-        self.assertTrue(self.config.set_dynamic_configuration({'synchronous_mode': True,
-                                                               'standby_cluster': {}, 'master_start_timeout': 1}))
-        self.assertEqual(self.config.get('primary_start_timeout'), 1)
+            self.assertFalse(self.config.set_dynamic_configuration({'foo': 'bar'}))
+        self.assertTrue(self.config.set_dynamic_configuration({'standby_cluster': {}, 'postgresql': {
+            'parameters': {'cluster_name': 1, 'wal_keep_size': 1, 'track_commit_timestamp': 1, 'wal_level': 1}}}))
 
     def test_reload_local_configuration(self):
         os.environ.update({
             'PATRONI_NAME': 'postgres0',
             'PATRONI_NAMESPACE': '/patroni/',
             'PATRONI_SCOPE': 'batman2',
             'PATRONI_LOGLEVEL': 'ERROR',
@@ -67,15 +66,16 @@
             'PATRONI_RAFT_PARTNER_ADDRS': "'host1:1234','host2:1234'",
             'PATRONI_foo_HOSTS': '[host1,host2',  # Exception in parse_list
             'PATRONI_SUPERUSER_USERNAME': 'postgres',
             'PATRONI_SUPERUSER_PASSWORD': 'zalando',
             'PATRONI_REPLICATION_USERNAME': 'replicator',
             'PATRONI_REPLICATION_PASSWORD': 'rep-pass',
             'PATRONI_admin_PASSWORD': 'admin',
-            'PATRONI_admin_OPTIONS': 'createrole,createdb'
+            'PATRONI_admin_OPTIONS': 'createrole,createdb',
+            'PATRONI_POSTGRESQL_BIN_POSTGRES': 'sergtsop'
         })
         config = Config('postgres0.yml')
         with patch.object(Config, '_load_config_file', Mock(return_value={'restapi': {}})):
             with patch.object(Config, '_build_effective_configuration', Mock(side_effect=Exception)):
                 config.reload_local_configuration()
             self.assertTrue(config.reload_local_configuration())
             self.assertIsNone(config.reload_local_configuration())
```

### Comparing `patroni-3.0.2/tests/test_consul.py` & `patroni-3.0.3/tests/test_consul.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,41 +1,43 @@
 import consul
 import unittest
 
 from consul import ConsulException, NotFound
 from mock import Mock, PropertyMock, patch
 from patroni.dcs.consul import AbstractDCS, Cluster, Consul, ConsulInternalError, \
-        ConsulError, ConsulClient, HTTPClient, InvalidSessionTTL, InvalidSession, RetryFailedError
+    ConsulError, ConsulClient, HTTPClient, InvalidSessionTTL, InvalidSession, RetryFailedError
 from . import SleepException
 
 
 def kv_get(self, key, **kwargs):
     if key == 'service/test/members/postgresql1':
         return '1', {'Session': 'fd4f44fe-2cac-bba5-a60b-304b51ff39b7'}
     if key == 'service/test/':
         return None, None
     if key == 'service/good/leader':
         return '1', None
+    if key == 'service/good/sync':
+        return '1', {'ModifyIndex': 1, 'Value': b'{}'}
     good_cls = ('6429',
                 [{'CreateIndex': 1334, 'Flags': 0, 'Key': key + 'failover', 'LockIndex': 0,
                   'ModifyIndex': 1334, 'Value': b''},
                  {'CreateIndex': 1334, 'Flags': 0, 'Key': key + '1/initialize', 'LockIndex': 0,
                   'ModifyIndex': 1334, 'Value': b'postgresql0'},
                  {'CreateIndex': 1334, 'Flags': 0, 'Key': key + 'initialize', 'LockIndex': 0,
                   'ModifyIndex': 1334, 'Value': b'postgresql0'},
                  {'CreateIndex': 2621, 'Flags': 0, 'Key': key + 'leader', 'LockIndex': 1,
                   'ModifyIndex': 2621, 'Session': 'fd4f44fe-2cac-bba5-a60b-304b51ff39b7', 'Value': b'postgresql1'},
                  {'CreateIndex': 6156, 'Flags': 0, 'Key': key + 'members/postgresql0', 'LockIndex': 1,
                   'ModifyIndex': 6156, 'Session': '782e6da4-ed02-3aef-7963-99a90ed94b53',
-                  'Value': ('postgres://replicator:rep-pass@127.0.0.1:5432/postgres' +
-                            '?application_name=http://127.0.0.1:8008/patroni').encode('utf-8')},
+                  'Value': ('postgres://replicator:rep-pass@127.0.0.1:5432/postgres'
+                            + '?application_name=http://127.0.0.1:8008/patroni').encode('utf-8')},
                  {'CreateIndex': 2630, 'Flags': 0, 'Key': key + 'members/postgresql1', 'LockIndex': 1,
                   'ModifyIndex': 2630, 'Session': 'fd4f44fe-2cac-bba5-a60b-304b51ff39b7',
-                  'Value': ('postgres://replicator:rep-pass@127.0.0.1:5433/postgres' +
-                            '?application_name=http://127.0.0.1:8009/patroni').encode('utf-8')},
+                  'Value': ('postgres://replicator:rep-pass@127.0.0.1:5433/postgres'
+                            + '?application_name=http://127.0.0.1:8009/patroni').encode('utf-8')},
                  {'CreateIndex': 1085, 'Flags': 0, 'Key': key + 'optime/leader', 'LockIndex': 0,
                   'ModifyIndex': 6429, 'Value': b'4496294792'},
                  {'CreateIndex': 1085, 'Flags': 0, 'Key': key + 'sync', 'LockIndex': 0,
                   'ModifyIndex': 6429, 'Value': b'{"leader": "leader", "sync_standby": null}'},
                  {'CreateIndex': 1085, 'Flags': 0, 'Key': key + 'failsafe', 'LockIndex': 0,
                   'ModifyIndex': 6429, 'Value': b'{'},
                  {'CreateIndex': 1085, 'Flags': 0, 'Key': key + 'status', 'LockIndex': 0,
@@ -170,35 +172,38 @@
     def test_write_leader_optime(self):
         self.c.get_cluster()
         self.c.write_leader_optime('1')
 
     @patch.object(consul.Consul.Session, 'renew')
     @patch.object(consul.Consul.KV, 'put', Mock(side_effect=ConsulException))
     def test_update_leader(self, mock_renew):
+        leader = self.c.get_cluster().leader
         self.c._session = 'fd4f44fe-2cac-bba5-a60b-304b51ff39b8'
         with patch.object(consul.Consul.KV, 'delete', Mock(return_value=True)):
             with patch.object(consul.Consul.KV, 'put', Mock(return_value=True)):
-                self.assertTrue(self.c.update_leader(12345, failsafe={'foo': 'bar'}))
+                self.assertTrue(self.c.update_leader(leader, 12345, failsafe={'foo': 'bar'}))
             with patch.object(consul.Consul.KV, 'put', Mock(side_effect=ConsulException)):
-                self.assertFalse(self.c.update_leader(12345))
-            with patch('time.time', Mock(side_effect=[0, 0, 0, 0, 0, 100, 200, 300])):
-                self.assertRaises(ConsulError, self.c.update_leader, 12345)
+                self.assertFalse(self.c.update_leader(leader, 12345))
+            with patch('time.time', Mock(side_effect=[0, 0, 0, 0, 100, 200, 300])):
+                self.assertRaises(ConsulError, self.c.update_leader, leader, 12345)
         with patch('time.time', Mock(side_effect=[0, 100, 200, 300])):
-            self.assertRaises(ConsulError, self.c.update_leader, 12345)
+            self.assertRaises(ConsulError, self.c.update_leader, leader, 12345)
         with patch.object(consul.Consul.KV, 'delete', Mock(side_effect=ConsulException)):
-            self.assertFalse(self.c.update_leader(12347))
+            self.assertFalse(self.c.update_leader(leader, 12347))
         mock_renew.side_effect = RetryFailedError('')
         self.c._last_session_refresh = 0
-        self.assertRaises(ConsulError, self.c.update_leader, 12346)
+        self.assertRaises(ConsulError, self.c.update_leader, leader, 12346)
         mock_renew.side_effect = ConsulException
-        self.assertFalse(self.c.update_leader(12347))
+        self.assertFalse(self.c.update_leader(leader, 12347))
 
     @patch.object(consul.Consul.KV, 'delete', Mock(return_value=True))
     def test_delete_leader(self):
         self.c.delete_leader()
+        self.c._name = 'other'
+        self.c.delete_leader()
 
     @patch.object(consul.Consul.KV, 'put', Mock(return_value=True))
     def test_initialize(self):
         self.c.initialize()
 
     @patch.object(consul.Consul.KV, 'delete', Mock(return_value=True))
     def test_cancel_initialization(self):
@@ -218,15 +223,19 @@
 
     def test_set_retry_timeout(self):
         self.c.set_retry_timeout(10)
 
     @patch.object(consul.Consul.KV, 'delete', Mock(return_value=True))
     @patch.object(consul.Consul.KV, 'put', Mock(return_value=True))
     def test_sync_state(self):
-        self.assertTrue(self.c.set_sync_state_value('{}'))
+        self.assertEqual(self.c.set_sync_state_value('{}'), 1)
+        with patch('time.time', Mock(side_effect=[1, 100, 1000])):
+            self.assertFalse(self.c.set_sync_state_value('{}'))
+        with patch.object(consul.Consul.KV, 'put', Mock(return_value=False)):
+            self.assertFalse(self.c.set_sync_state_value('{}'))
         self.assertTrue(self.c.delete_sync_state())
 
     @patch.object(consul.Consul.KV, 'put', Mock(return_value=True))
     def test_set_history_value(self):
         self.assertTrue(self.c.set_history_value('{}'))
 
     @patch.object(consul.Consul.Agent.Service, 'register', Mock(side_effect=(False, True, True, True)))
```

### Comparing `patroni-3.0.2/tests/test_ctl.py` & `patroni-3.0.3/tests/test_ctl.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,18 +1,19 @@
 import etcd
+import mock
 import os
 import unittest
 
 from click.testing import CliRunner
 from datetime import datetime, timedelta
-from mock import patch, Mock, call
+from mock import patch, Mock, PropertyMock
 from patroni.ctl import ctl, load_config, output_members, get_dcs, parse_dcs, \
     get_all_members, get_any_member, get_cursor, query_member, PatroniCtlException, apply_config_changes, \
     format_config_for_editing, show_diff, invoke_editor, format_pg_version, CONFIG_FILE_PATH, PatronictlPrettyTable
-from patroni.dcs.etcd import AbstractEtcdClientWithFailover, Failover
+from patroni.dcs.etcd import AbstractEtcdClientWithFailover, Cluster, Failover
 from patroni.psycopg import OperationalError
 from patroni.utils import tzutc
 from prettytable import PrettyTable, ALL
 from urllib3 import PoolManager
 
 from . import MockConnect, MockCursor, MockResponse, psycopg_connect
 from .test_etcd import etcd_read, socket_getaddrinfo
@@ -71,14 +72,15 @@
         assert parse_dcs(None) is None
         assert parse_dcs('localhost') == {'etcd': {'host': 'localhost:2379'}}
         assert parse_dcs('') == {'etcd': {'host': 'localhost:2379'}}
         assert parse_dcs('localhost:8500') == {'consul': {'host': 'localhost:8500'}}
         assert parse_dcs('zookeeper://localhost') == {'zookeeper': {'hosts': ['localhost:2181']}}
         assert parse_dcs('exhibitor://dummy') == {'exhibitor': {'hosts': ['dummy'], 'port': 8181}}
         assert parse_dcs('consul://localhost') == {'consul': {'host': 'localhost:8500'}}
+        assert parse_dcs('etcd3://random.com:2399') == {'etcd3': {'host': 'random.com:2399'}}
         self.assertRaises(PatroniCtlException, parse_dcs, 'invalid://test')
 
     def test_output_members(self):
         scheduled_at = datetime.now(tzutc) + timedelta(seconds=600)
         cluster = get_cluster_initialized_with_leader(Failover(1, 'foo', 'bar', scheduled_at))
         del cluster.members[1].data['conn_url']
         for fmt in ('pretty', 'json', 'yaml', 'tsv', 'topology'):
@@ -93,15 +95,15 @@
         result = self.runner.invoke(ctl, ['switchover', 'dummy', '--group', '0'], input='leader\nother\n\ny')
         assert 'leader' in result.output
 
         result = self.runner.invoke(ctl, ['switchover', 'dummy', '--group', '0'],
                                     input='leader\nother\n2300-01-01T12:23:00\ny')
         assert result.exit_code == 0
 
-        with patch('patroni.dcs.Cluster.is_paused', Mock(return_value=True)):
+        with patch('patroni.config.GlobalConfig.is_paused', PropertyMock(return_value=True)):
             result = self.runner.invoke(ctl, ['switchover', 'dummy', '--group', '0',
                                               '--force', '--scheduled', '2015-01-01T12:00:00'])
             assert result.exit_code == 1
 
         # Aborting switchover, as we answer NO to the confirmation
         result = self.runner.invoke(ctl, ['switchover', 'dummy', '--group', '0'], input='leader\nother\n\nN')
         assert result.exit_code == 1
@@ -304,15 +306,15 @@
         result = self.runner.invoke(ctl, ['restart', 'alpha', '--pending', '--force', '--timeout', '10min'])
         assert result.exit_code == 0
 
         # normal restart, the schedule is actually parsed, but not validated in patronictl
         result = self.runner.invoke(ctl, ['restart', 'alpha', 'other', '--force', '--scheduled', '2300-10-01T14:30'])
         assert 'Failed: flush scheduled restart' in result.output
 
-        with patch('patroni.dcs.Cluster.is_paused', Mock(return_value=True)):
+        with patch('patroni.config.GlobalConfig.is_paused', PropertyMock(return_value=True)):
             result = self.runner.invoke(ctl,
                                         ['restart', 'alpha', 'other', '--force', '--scheduled', '2300-10-01T14:30'])
             assert result.exit_code == 1
 
         # force restart with restart already present
         result = self.runner.invoke(ctl, ['restart', 'alpha', 'other', '--force', '--scheduled', '2300-10-01T14:30'])
         assert result.exit_code == 0
@@ -465,15 +467,15 @@
 
         mock_get_dcs.return_value.get_cluster = get_cluster_initialized_with_leader
         result = self.runner.invoke(ctl, ['flush', 'dummy', 'switchover'])
         assert 'No pending scheduled switchover' in result.output
 
         scheduled_at = datetime.now(tzutc) + timedelta(seconds=600)
         mock_get_dcs.return_value.get_cluster = Mock(
-                return_value=get_cluster_initialized_with_leader(Failover(1, 'a', 'b', scheduled_at)))
+            return_value=get_cluster_initialized_with_leader(Failover(1, 'a', 'b', scheduled_at)))
         result = self.runner.invoke(ctl, ['flush', 'dummy', 'switchover'])
         assert result.output.startswith('Success: ')
 
         mock_get_dcs.return_value.manual_failover = Mock()
         with patch.object(PoolManager, 'request', side_effect=[MockResponse(409), Exception]):
             result = self.runner.invoke(ctl, ['flush', 'dummy', 'switchover'])
             assert 'Could not find any accessible member of cluster' in result.output
@@ -486,15 +488,15 @@
         mock_get_dcs.return_value.get_cluster = get_cluster_initialized_with_leader
 
         mock_post.return_value.status = 500
         result = self.runner.invoke(ctl, ['pause', 'dummy'])
         assert 'Failed' in result.output
 
         mock_post.return_value.status = 200
-        with patch('patroni.dcs.Cluster.is_paused', Mock(return_value=True)):
+        with patch('patroni.config.GlobalConfig.is_paused', PropertyMock(return_value=True)):
             result = self.runner.invoke(ctl, ['pause', 'dummy'])
             assert 'Cluster is already paused' in result.output
 
         result = self.runner.invoke(ctl, ['pause', 'dummy', '--wait'])
         assert "'pause' request sent" in result.output
         mock_get_dcs.return_value.get_cluster = Mock(side_effect=[get_cluster_initialized_with_leader(),
                                                                   get_cluster(None, None, [], None, None)])
@@ -507,19 +509,19 @@
     @patch.object(PoolManager, 'request')
     @patch('patroni.ctl.get_dcs')
     def test_resume_cluster(self, mock_get_dcs, mock_post):
         mock_get_dcs.return_value = self.e
         mock_get_dcs.return_value.get_cluster = get_cluster_initialized_with_leader
 
         mock_post.return_value.status = 200
-        with patch('patroni.dcs.Cluster.is_paused', Mock(return_value=False)):
+        with patch('patroni.config.GlobalConfig.is_paused', PropertyMock(return_value=False)):
             result = self.runner.invoke(ctl, ['resume', 'dummy'])
             assert 'Cluster is not paused' in result.output
 
-        with patch('patroni.dcs.Cluster.is_paused', Mock(return_value=True)):
+        with patch('patroni.config.GlobalConfig.is_paused', PropertyMock(return_value=True)):
             result = self.runner.invoke(ctl, ['resume', 'dummy'])
             assert 'Success' in result.output
 
             mock_post.return_value.status = 500
             result = self.runner.invoke(ctl, ['resume', 'dummy'])
             assert 'Failed' in result.output
 
@@ -572,16 +574,16 @@
         self.assertEqual(
             str(e.exception),
             'No pager could be found. Either set PAGER environment variable with '
             'your pager or install either "less" or "more" in the host.'
         )
         mock_env_get.assert_called_once_with('PAGER')
         mock_which.assert_has_calls([
-            call('less'),
-            call('more'),
+            mock.call('less'),
+            mock.call('more'),
         ])
         mock_markup_to_pager.assert_not_called()
 
         # TTY with PAGER set but invalid
         mock_env_get.reset_mock()
         mock_env_get.return_value = 'random'
         mock_which.reset_mock()
@@ -590,17 +592,17 @@
         self.assertEqual(
             str(e.exception),
             'No pager could be found. Either set PAGER environment variable with '
             'your pager or install either "less" or "more" in the host.'
         )
         mock_env_get.assert_called_once_with('PAGER')
         mock_which.assert_has_calls([
-            call('random'),
-            call('less'),
-            call('more'),
+            mock.call('random'),
+            mock.call('less'),
+            mock.call('more'),
         ])
         mock_markup_to_pager.assert_not_called()
 
         # TTY with valid executable
         mock_which.side_effect = [None, '/usr/bin/less', None]
         show_diff("foo:\n  bar: 1\n", "foo:\n  bar: 2\n")
         mock_markup_to_pager.assert_called_once()
@@ -633,14 +635,18 @@
         self.runner.invoke(ctl, ['edit-config', 'dummy'])
         self.runner.invoke(ctl, ['edit-config', 'dummy', '-s', 'foo=bar'])
         self.runner.invoke(ctl, ['edit-config', 'dummy', '--replace', 'postgres0.yml'])
         self.runner.invoke(ctl, ['edit-config', 'dummy', '--apply', '-'], input='foo: bar')
         self.runner.invoke(ctl, ['edit-config', 'dummy', '--force', '--apply', '-'], input='foo: bar')
         mock_get_dcs.return_value.set_config_value.return_value = True
         self.runner.invoke(ctl, ['edit-config', 'dummy', '--force', '--apply', '-'], input='foo: bar')
+        mock_get_dcs.return_value.get_cluster = Mock(return_value=Cluster.empty())
+        result = self.runner.invoke(ctl, ['edit-config', 'dummy'])
+        assert result.exit_code == 1
+        assert 'The config key does not exist in the cluster dummy' in result.output
 
     @patch('patroni.ctl.get_dcs')
     def test_version(self, mock_get_dcs):
         mock_get_dcs.return_value = self.e
         mock_get_dcs.return_value.get_cluster = get_cluster_initialized_with_leader
         with patch.object(PoolManager, 'request') as mocked:
             result = self.runner.invoke(ctl, ['version'])
```

### Comparing `patroni-3.0.2/tests/test_etcd.py` & `patroni-3.0.3/tests/test_etcd.py`

 * *Files 1% similar despite different names*

```diff
@@ -57,21 +57,21 @@
                     {"key": "/service/batman5/optime/leader", "value": "2164261704",
                      "modifiedIndex": 20729, "createdIndex": 20729}],
                  "modifiedIndex": 20437, "createdIndex": 20437},
                 {"key": "/service/batman5/sync", "value": '{"leader": "leader"}',
                  "modifiedIndex": 1582, "createdIndex": 1582},
                 {"key": "/service/batman5/members", "dir": True, "nodes": [
                     {"key": "/service/batman5/members/postgresql1",
-                     "value": "postgres://replicator:rep-pass@127.0.0.1:5434/postgres" +
-                        "?application_name=http://127.0.0.1:8009/patroni",
+                     "value": "postgres://replicator:rep-pass@127.0.0.1:5434/postgres"
+                     + "?application_name=http://127.0.0.1:8009/patroni",
                      "expiration": "2015-05-15T09:10:59.949384522Z", "ttl": 21,
                      "modifiedIndex": 20727, "createdIndex": 20727},
                     {"key": "/service/batman5/members/postgresql0",
-                     "value": "postgres://replicator:rep-pass@127.0.0.1:5433/postgres" +
-                        "?application_name=http://127.0.0.1:8008/patroni",
+                     "value": "postgres://replicator:rep-pass@127.0.0.1:5433/postgres"
+                     + "?application_name=http://127.0.0.1:8008/patroni",
                      "expiration": "2015-05-15T09:11:09.611860899Z", "ttl": 30,
                      "modifiedIndex": 20730, "createdIndex": 20730}],
                  "modifiedIndex": 1581, "createdIndex": 1581},
                 {"key": "/service/batman5/failsafe", "value": '{', "modifiedIndex": 1582, "createdIndex": 1582},
                 {"key": "/service/batman5/status", "value": '{"optime":2164261704,"slots":{"ls":12345}}',
                  "modifiedIndex": 1582, "createdIndex": 1582}], "modifiedIndex": 1581, "createdIndex": 1581}}
     if key == '/service/legacy/':
@@ -134,15 +134,17 @@
 
     @patch('dns.resolver.query', dns_query)
     @patch('socket.getaddrinfo', socket_getaddrinfo)
     @patch('patroni.dcs.etcd.requests_get', requests_get)
     @patch.object(EtcdClient, '_get_machines_list',
                   Mock(return_value=['http://localhost:2379', 'http://localhost:4001']))
     def setUp(self):
-        self.client = EtcdClient({'srv': 'test', 'retry_timeout': 3}, DnsCachingResolver())
+        self.etcd = Etcd({'namespace': '/patroni/', 'ttl': 30, 'retry_timeout': 3,
+                          'srv': 'test', 'scope': 'test', 'name': 'foo'})
+        self.client = self.etcd._client
         self.client.http.request = http_request
         self.client.http.request_encode_body = http_request
 
     def test_machines(self):
         self.client._base_uri = 'http://localhost:4002'
         self.client._machines_cache = ['http://localhost:4002', 'http://localhost:2379']
         self.assertIsNotNone(self.client.machines)
@@ -252,15 +254,14 @@
         with patch.object(EtcdClient, '_get_machines_list', Mock(return_value=[])):
             self.assertRaises(SleepException, self.etcd.get_etcd_client,
                               {'proxy': 'https://user:password@test:2379', 'retry_timeout': 10}, EtcdClient)
 
     def test_get_cluster(self):
         cluster = self.etcd.get_cluster()
         self.assertIsInstance(cluster, Cluster)
-        self.assertFalse(cluster.is_synchronous_mode())
         self.etcd._base_path = '/service/legacy'
         self.assertIsInstance(self.etcd.get_cluster(), Cluster)
         self.etcd._base_path = '/service/broken'
         self.assertIsInstance(self.etcd.get_cluster(), Cluster)
         self.etcd._base_path = '/service/nocluster'
         cluster = self.etcd.get_cluster()
         self.assertIsInstance(cluster, Cluster)
@@ -291,22 +292,23 @@
 
     @patch.object(Cluster, 'min_version', PropertyMock(return_value=(2, 0)))
     def test_write_leader_optime(self):
         self.etcd.get_cluster()
         self.etcd.write_leader_optime('0')
 
     def test_update_leader(self):
-        self.assertTrue(self.etcd.update_leader(None, failsafe={'foo': 'bar'}))
+        leader = self.etcd.get_cluster().leader
+        self.assertTrue(self.etcd.update_leader(leader, None, failsafe={'foo': 'bar'}))
         with patch.object(etcd.Client, 'write',
                           Mock(side_effect=[etcd.EtcdConnectionFailed, etcd.EtcdClusterIdChanged, Exception])):
-            self.assertRaises(EtcdError, self.etcd.update_leader, None)
-            self.assertFalse(self.etcd.update_leader(None))
-            self.assertRaises(EtcdError, self.etcd.update_leader, None)
+            self.assertRaises(EtcdError, self.etcd.update_leader, leader, None)
+            self.assertFalse(self.etcd.update_leader(leader, None))
+            self.assertRaises(EtcdError, self.etcd.update_leader, leader, None)
         with patch.object(etcd.Client, 'write', Mock(side_effect=etcd.EtcdKeyNotFound)):
-            self.assertFalse(self.etcd.update_leader(None))
+            self.assertFalse(self.etcd.update_leader(leader, None))
 
     def test_initialize(self):
         self.assertFalse(self.etcd.initialize())
 
     def test_cancel_initializion(self):
         self.assertFalse(self.etcd.cancel_initialization())
 
@@ -333,15 +335,15 @@
         self.assertRaises(EtcdError, self.etcd.cancel_initialization)
 
     def test_set_ttl(self):
         self.etcd.set_ttl(20)
         self.assertTrue(self.etcd.watch(None, 1))
 
     def test_sync_state(self):
-        self.assertFalse(self.etcd.write_sync_state('leader', None))
+        self.assertIsNone(self.etcd.write_sync_state('leader', None))
         self.assertFalse(self.etcd.delete_sync_state())
 
     def test_set_history_value(self):
         self.assertFalse(self.etcd.set_history_value('{}'))
 
     def test_last_seen(self):
         self.assertIsNotNone(self.etcd.last_seen)
```

### Comparing `patroni-3.0.2/tests/test_etcd3.py` & `patroni-3.0.3/tests/test_etcd3.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 import etcd
 import json
 import unittest
 import urllib3
 
-from mock import Mock, patch
+from mock import Mock, PropertyMock, patch
 from patroni.dcs.etcd import DnsCachingResolver
 from patroni.dcs.etcd3 import PatroniEtcd3Client, Cluster, Etcd3Client, Etcd3Error, Etcd3ClientError, RetryFailedError,\
-        InvalidAuthToken, Unavailable, Unknown, UnsupportedEtcdVersion, UserEmpty, AuthFailed, base64_encode, Etcd3
+    InvalidAuthToken, Unavailable, Unknown, UnsupportedEtcdVersion, UserEmpty, AuthFailed, base64_encode, Etcd3
 from threading import Thread
 
 from . import SleepException, MockResponse
 
 
 def mock_urlopen(self, method, url, **kwargs):
     ret = MockResponse()
@@ -50,16 +50,19 @@
                 {'kv': {'key': key, 'value': base64_encode('buzz'), 'mod_revision': '3'}},
                 {'type': 'DELETE', 'kv': {'key': key, 'mod_revision': '4'}},
                 {'kv': {'key': base64_encode('/patroni/test/optime/leader'),
                         'value': base64_encode('1234567'), 'mod_revision': '5'}},
             ]}
         })[:-1].encode('utf-8'), b'}{"error":{"grpc_code":14,"message":"","http_code":503}}'])
     elif url.endswith('/kv/put') or url.endswith('/kv/txn'):
-        ret.status_code = 400
-        ret.content = '{"code":5,"error":"etcdserver: requested lease not found"}'
+        if base64_encode('/patroni/test/sync') in kwargs['body']:
+            ret.content = '{"header":{"revision":"1"},"succeeded":true}'
+        else:
+            ret.status_code = 400
+            ret.content = '{"code":5,"error":"etcdserver: requested lease not found"}'
     elif not url.endswith('/kv/deleterange'):
         raise Exception('Unexpected url: {0} {1} {2}'.format(method, url, kwargs))
     return ret
 
 
 class TestEtcd3Client(unittest.TestCase):
 
@@ -81,31 +84,39 @@
                             'username': 'etcduser', 'password': 'etcdpassword'})
         self.client = self.etcd3._client
         self.kv_cache = self.client._kv_cache
 
 
 class TestKVCache(BaseTestEtcd3):
 
+    @patch.object(urllib3.PoolManager, 'urlopen', mock_urlopen)
+    @patch.object(Etcd3Client, 'watchprefix', Mock(return_value=urllib3.response.HTTPResponse()))
+    def test__build_cache(self):
+        self.kv_cache._build_cache()
+
     def test__do_watch(self):
         self.client.watchprefix = Mock(return_value=False)
         self.assertRaises(AttributeError, self.kv_cache._do_watch, '1')
 
     @patch('time.sleep', Mock(side_effect=SleepException))
     @patch('patroni.dcs.etcd3.KVCache._build_cache', Mock(side_effect=Exception))
     def test_run(self):
         self.assertRaises(SleepException, self.kv_cache.run)
 
-    @patch.object(urllib3.PoolManager, 'urlopen', mock_urlopen)
+    @patch.object(urllib3.response.HTTPResponse, 'read_chunked',
+                  Mock(return_value=[b'{"error":{"grpc_code":14,"message":"","http_code":503}}']))
+    @patch.object(Etcd3Client, 'watchprefix', Mock(return_value=urllib3.response.HTTPResponse()))
     def test_kill_stream(self):
         self.assertRaises(Unavailable, self.kv_cache._do_watch, '1')
-        self.kv_cache.kill_stream()
-        with patch.object(MockResponse, 'connection', create=True) as mock_conn:
+        with patch.object(urllib3.response.HTTPResponse, 'connection') as mock_conn:
             self.kv_cache.kill_stream()
             mock_conn.sock.close.side_effect = Exception
             self.kv_cache.kill_stream()
+            type(mock_conn).sock = PropertyMock(side_effect=Exception)
+            self.kv_cache.kill_stream()
 
 
 class TestPatroniEtcd3Client(BaseTestEtcd3):
 
     @patch('patroni.dcs.etcd3.Etcd3Client.authenticate', Mock(side_effect=AuthFailed))
     def test__init__(self):
         self.assertRaises(SystemExit, self.setUp)
@@ -176,15 +187,16 @@
 @patch.object(urllib3.PoolManager, 'urlopen', mock_urlopen)
 class TestEtcd3(BaseTestEtcd3):
 
     @patch.object(Thread, 'start', Mock())
     @patch.object(urllib3.PoolManager, 'urlopen', mock_urlopen)
     def setUp(self):
         super(TestEtcd3, self).setUp()
-        self.assertRaises(AttributeError, self.kv_cache._build_cache)
+#        self.assertRaises(AttributeError, self.kv_cache._build_cache)
+        self.kv_cache._build_cache()
         self.kv_cache._is_ready = True
         self.etcd3.get_cluster()
 
     def test_get_cluster(self):
         self.assertIsInstance(self.etcd3.get_cluster(), Cluster)
         self.client._kv_cache = None
         with patch.object(urllib3.PoolManager, 'urlopen') as mock_urlopen:
@@ -219,24 +231,28 @@
         self.etcd3.touch_member({})
         self.etcd3._lease = 'bla'
         self.etcd3.touch_member({})
         with patch.object(PatroniEtcd3Client, 'lease_grant', Mock(side_effect=Etcd3ClientError)):
             self.etcd3.touch_member({})
 
     def test__update_leader(self):
+        leader = self.etcd3.get_cluster().leader
         self.etcd3._lease = None
-        self.etcd3.update_leader('123', failsafe={'foo': 'bar'})
+        with patch.object(Etcd3Client, 'txn', Mock(return_value={'succeeded': True})):
+            self.etcd3.update_leader(leader, '123', failsafe={'foo': 'bar'})
         self.etcd3._last_lease_refresh = 0
-        self.etcd3.update_leader('124')
+        self.etcd3.update_leader(leader, '124')
         with patch.object(PatroniEtcd3Client, 'lease_keepalive', Mock(return_value=True)),\
                 patch('time.time', Mock(side_effect=[0, 100, 200, 300])):
-            self.assertRaises(Etcd3Error, self.etcd3.update_leader, '126')
+            self.assertRaises(Etcd3Error, self.etcd3.update_leader, leader, '126')
+        self.etcd3._lease = leader.session
+        self.etcd3.update_leader(leader, '124')
         self.etcd3._last_lease_refresh = 0
         with patch.object(PatroniEtcd3Client, 'lease_keepalive', Mock(side_effect=Unknown)):
-            self.assertFalse(self.etcd3.update_leader('125'))
+            self.assertFalse(self.etcd3.update_leader(leader, '125'))
 
     def test_take_leader(self):
         self.assertFalse(self.etcd3.take_leader())
 
     def test_attempt_to_acquire_leader(self):
         self.assertFalse(self.etcd3.attempt_to_acquire_leader())
         with patch('time.time', Mock(side_effect=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 200])):
@@ -272,28 +288,30 @@
         self.etcd3.initialize()
 
     def test_cancel_initialization(self):
         self.etcd3.cancel_initialization()
 
     def test_delete_leader(self):
         self.etcd3.delete_leader()
+        self.etcd3._name = 'other'
+        self.etcd3.delete_leader()
 
     def test_delete_cluster(self):
         self.etcd3.delete_cluster()
 
     def test_set_history_value(self):
         self.etcd3.set_history_value('')
 
     def test_set_sync_state_value(self):
-        self.etcd3.set_sync_state_value('')
+        self.etcd3.set_sync_state_value('', 1)
 
     def test_delete_sync_state(self):
         self.etcd3.delete_sync_state()
 
     def test_watch(self):
         self.etcd3.set_ttl(10)
         self.etcd3.watch(None, 0)
-        self.etcd3.watch(None, 0)
+        self.etcd3.watch('5', 0)
 
     def test_set_socket_options(self):
         with patch('socket.SIO_KEEPALIVE_VALS', 1, create=True):
             self.etcd3.set_socket_options(Mock(), None)
```

### Comparing `patroni-3.0.2/tests/test_exhibitor.py` & `patroni-3.0.3/tests/test_exhibitor.py`

 * *Files identical despite different names*

### Comparing `patroni-3.0.2/tests/test_ha.py` & `patroni-3.0.3/tests/test_ha.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 import datetime
 import etcd
 import os
 import sys
 
 from mock import Mock, MagicMock, PropertyMock, patch, mock_open
+from patroni.collections import CaseInsensitiveSet
 from patroni.config import Config
 from patroni.dcs import Cluster, ClusterConfig, Failover, Leader, Member, get_dcs, SyncState, TimelineHistory
 from patroni.dcs.etcd import AbstractEtcdClientWithFailover
 from patroni.exceptions import DCSError, PostgresConnectionException, PatroniFatalException
 from patroni.ha import Ha, _MemberStatus
 from patroni.postgresql import Postgresql
 from patroni.postgresql.bootstrap import Bootstrap
@@ -132,15 +133,14 @@
     port: 8181
 """
         # We rely on sys.argv in Config, so it's necessary to reset
         # all the extra values that are coming from py.test
         sys.argv = sys.argv[:1]
 
         self.config = Config(None)
-        self.config.set_dynamic_configuration({'maximum_lag_on_failover': 5})
         self.version = '1.5.7'
         self.postgresql = p
         self.dcs = d
         self.api = Mock()
         self.tags = {'foo': 'bar'}
         self.nofailover = None
         self.replicatefrom = None
@@ -240,15 +240,14 @@
         self.p.is_healthy = false
         self.assertEqual(self.ha.run_cycle(), 'starting as a secondary')
 
     @patch('patroni.dcs.etcd.Etcd.initialize', return_value=True)
     def test_bootstrap_as_standby_leader(self, initialize):
         self.p.data_directory_empty = true
         self.ha.cluster = get_cluster_not_initialized_without_leader(cluster_config=ClusterConfig(0, {}, 0))
-        self.ha.cluster.is_unlocked = true
         self.ha.patroni.config._dynamic_configuration = {"standby_cluster": {"port": 5432}}
         self.assertEqual(self.ha.run_cycle(), 'trying to bootstrap a new standby leader')
 
     def test_bootstrap_waiting_for_standby_leader(self):
         self.p.data_directory_empty = true
         self.ha.cluster = get_cluster_initialized_without_leader()
         self.ha.cluster.config.data.update({'standby_cluster': {'port': 5432}})
@@ -257,15 +256,14 @@
     @patch.object(Cluster, 'get_clone_member',
                   Mock(return_value=Member(0, 'test', 1, {'api_url': 'http://127.0.0.1:8011/patroni',
                                                           'conn_url': 'postgres://127.0.0.1:5432/postgres'})))
     @patch.object(Bootstrap, 'create_replica', Mock(return_value=0))
     def test_start_as_cascade_replica_in_standby_cluster(self):
         self.p.data_directory_empty = true
         self.ha.cluster = get_standby_cluster_initialized_with_only_leader()
-        self.ha.cluster.is_unlocked = false
         self.assertEqual(self.ha.run_cycle(), "trying to bootstrap from replica 'test'")
 
     def test_recover_replica_failed(self):
         self.p.controldata = lambda: {'Database cluster state': 'in recovery', 'Database system identifier': SYSID}
         self.p.is_running = false
         self.p.follow = false
         self.assertEqual(self.ha.run_cycle(), 'starting as a secondary')
@@ -296,15 +294,16 @@
         self.p.controldata = lambda: {'Database cluster state': 'in production', 'Database system identifier': SYSID}
         self.assertEqual(self.ha.run_cycle(), 'doing crash recovery in a single user mode')
         with patch('patroni.async_executor.AsyncExecutor.busy', PropertyMock(return_value=True)),\
                 patch.object(Ha, 'check_timeline', Mock(return_value=False)):
             self.ha._async_executor.schedule('doing crash recovery in a single user mode')
             self.ha.state_handler.cancellable._process = Mock()
             self.ha._crash_recovery_started -= 600
-            self.ha.patroni.config.set_dynamic_configuration({'maximum_lag_on_failover': 10})
+            self.ha.cluster.config.data.update({'maximum_lag_on_failover': 10})
+            self.ha.global_config = self.ha.patroni.config.get_global_config(self.ha.cluster)
             self.assertEqual(self.ha.run_cycle(), 'terminated crash recovery because of startup timeout')
 
     @patch.object(Rewind, 'ensure_clean_shutdown', Mock())
     @patch.object(Rewind, 'rewind_or_reinitialize_needed_and_possible', Mock(return_value=True))
     @patch.object(Rewind, 'can_rewind', PropertyMock(return_value=True))
     def test_crash_recovery_before_rewind(self):
         self.p.is_leader = false
@@ -376,16 +375,16 @@
         self.assertEqual(self.ha.run_cycle(), 'following a different leader because i am not the healthiest node')
 
     def test_lost_leader_lock_during_promote(self):
         with patch('patroni.async_executor.AsyncExecutor.busy', PropertyMock(return_value=True)):
             self.ha._async_executor.schedule('promote')
             self.assertEqual(self.ha.run_cycle(), 'lost leader before promote')
 
+    @patch.object(Cluster, 'is_unlocked', Mock(return_value=False))
     def test_long_promote(self):
-        self.ha.cluster.is_unlocked = false
         self.ha.has_lock = true
         self.p.is_leader = false
         self.p.set_role('primary')
         self.assertEqual(self.ha.run_cycle(), 'no action. I am (postgresql0), the leader with the lock')
 
     def test_demote_after_failing_to_obtain_lock(self):
         self.ha.acquire_lock = false
@@ -402,74 +401,71 @@
         self.assertEqual(self.ha.run_cycle(), 'demoting self because i am not the healthiest node')
 
     def test_follow_new_leader_because_not_healthiest(self):
         self.ha.is_healthiest_node = false
         self.p.is_leader = false
         self.assertEqual(self.ha.run_cycle(), 'following a different leader because i am not the healthiest node')
 
+    @patch.object(Cluster, 'is_unlocked', Mock(return_value=False))
     def test_promote_because_have_lock(self):
-        self.ha.cluster.is_unlocked = false
         self.ha.has_lock = true
         self.p.is_leader = false
         self.assertEqual(self.ha.run_cycle(), 'promoted self to leader because I had the session lock')
 
     def test_promote_without_watchdog(self):
-        self.ha.cluster.is_unlocked = false
         self.ha.has_lock = true
         self.p.is_leader = true
         with patch.object(Watchdog, 'activate', Mock(return_value=False)):
             self.assertEqual(self.ha.run_cycle(), 'Demoting self because watchdog could not be activated')
             self.p.is_leader = false
             self.assertEqual(self.ha.run_cycle(), 'Not promoting self because watchdog could not be activated')
 
     def test_leader_with_lock(self):
         self.ha.cluster = get_cluster_initialized_with_leader()
-        self.ha.cluster.is_unlocked = false
         self.ha.has_lock = true
         self.assertEqual(self.ha.run_cycle(), 'no action. I am (postgresql0), the leader with the lock')
 
     def test_coordinator_leader_with_lock(self):
         self.ha.cluster = get_cluster_initialized_with_leader()
-        self.ha.cluster.is_unlocked = false
         self.ha.has_lock = true
         self.assertEqual(self.ha.run_cycle(), 'no action. I am (postgresql0), the leader with the lock')
 
     @patch.object(Postgresql, '_wait_for_connection_close', Mock())
+    @patch.object(Cluster, 'is_unlocked', Mock(return_value=False))
     def test_demote_because_not_having_lock(self):
-        self.ha.cluster.is_unlocked = false
         with patch.object(Watchdog, 'is_running', PropertyMock(return_value=True)):
             self.assertEqual(self.ha.run_cycle(), 'demoting self because I do not have the lock and I was a leader')
 
+    @patch.object(Cluster, 'is_unlocked', Mock(return_value=False))
     def test_demote_because_update_lock_failed(self):
-        self.ha.cluster.is_unlocked = false
         self.ha.has_lock = true
         self.ha.update_lock = false
         self.assertEqual(self.ha.run_cycle(), 'demoted self because failed to update leader lock in DCS')
         with patch.object(Ha, '_get_node_to_follow', Mock(side_effect=DCSError('foo'))):
             self.assertEqual(self.ha.run_cycle(), 'demoted self because failed to update leader lock in DCS')
         self.p.is_leader = false
         self.assertEqual(self.ha.run_cycle(), 'not promoting because failed to update leader lock in DCS')
 
+    @patch.object(Cluster, 'is_unlocked', Mock(return_value=False))
     def test_follow(self):
-        self.ha.cluster.is_unlocked = false
         self.p.is_leader = false
         self.assertEqual(self.ha.run_cycle(), 'no action. I am (postgresql0), a secondary, and following a leader ()')
         self.ha.patroni.replicatefrom = "foo"
         self.p.config.check_recovery_conf = Mock(return_value=(True, False))
         self.ha.cluster.config.data.update({'slots': {'l': {'database': 'a', 'plugin': 'b'}}})
         self.ha.cluster.members[1].data['tags']['replicatefrom'] = 'postgresql0'
         self.ha.patroni.nofailover = True
         self.assertEqual(self.ha.run_cycle(), 'no action. I am (postgresql0), a secondary, and following a leader ()')
         del self.ha.cluster.config.data['slots']
         self.ha.cluster.config.data.update({'postgresql': {'use_slots': False}})
         self.assertEqual(self.ha.run_cycle(), 'no action. I am (postgresql0), a secondary, and following a leader ()')
         del self.ha.cluster.config.data['postgresql']['use_slots']
 
+    @patch.object(Cluster, 'is_unlocked', Mock(return_value=False))
     def test_follow_in_pause(self):
-        self.ha.cluster.is_unlocked = false
         self.ha.is_paused = true
         self.assertEqual(self.ha.run_cycle(), 'PAUSE: continue to run as primary without lock')
         self.p.is_leader = false
         self.assertEqual(self.ha.run_cycle(), 'PAUSE: no action. I am (postgresql0)')
 
     @patch.object(Rewind, 'rewind_or_reinitialize_needed_and_possible', Mock(return_value=True))
     @patch.object(Rewind, 'can_rewind', PropertyMock(return_value=True))
@@ -484,14 +480,15 @@
         self.assertEqual(self.ha.run_cycle(), 'demoting self because DCS is not accessible and I was a leader')
         self.ha._async_executor.schedule('dummy')
         self.assertEqual(self.ha.run_cycle(), 'demoted self because DCS is not accessible and I was a leader')
 
     def test_check_failsafe_topology(self):
         self.ha.load_cluster_from_dcs = Mock(side_effect=DCSError('Etcd is not responding properly'))
         self.ha.cluster = get_cluster_initialized_with_leader_and_failsafe()
+        self.ha.global_config = self.ha.patroni.config.get_global_config(self.ha.cluster)
         self.ha.dcs._last_failsafe = self.ha.cluster.failsafe
         self.assertEqual(self.ha.run_cycle(), 'demoting self because DCS is not accessible and I was a leader')
         self.ha.state_handler.name = self.ha.cluster.leader.name
         self.assertFalse(self.ha.failsafe_is_active())
         self.assertEqual(self.ha.run_cycle(),
                          'continue to run as a leader because failsafe mode is enabled and all members are accessible')
         self.assertTrue(self.ha.failsafe_is_active())
@@ -503,14 +500,15 @@
         self.ha.dcs._last_failsafe[self.ha.cluster.leader.name] = self.ha.cluster.leader.member.api_url
         self.assertEqual(self.ha.run_cycle(),
                          'continue to run as a leader because failsafe mode is enabled and all members are accessible')
 
     def test_no_dcs_connection_primary_failsafe(self):
         self.ha.load_cluster_from_dcs = Mock(side_effect=DCSError('Etcd is not responding properly'))
         self.ha.cluster = get_cluster_initialized_with_leader_and_failsafe()
+        self.ha.global_config = self.ha.patroni.config.get_global_config(self.ha.cluster)
         self.ha.dcs._last_failsafe = self.ha.cluster.failsafe
         self.ha.state_handler.name = self.ha.cluster.leader.name
         self.assertEqual(self.ha.run_cycle(),
                          'continue to run as a leader because failsafe mode is enabled and all members are accessible')
 
     def test_readonly_dcs_primary_failsafe(self):
         self.ha.cluster = get_cluster_initialized_with_leader_and_failsafe()
@@ -519,14 +517,15 @@
         self.ha.state_handler.name = self.ha.cluster.leader.name
         self.assertEqual(self.ha.run_cycle(),
                          'continue to run as a leader because failsafe mode is enabled and all members are accessible')
 
     def test_no_dcs_connection_replica_failsafe(self):
         self.ha.load_cluster_from_dcs = Mock(side_effect=DCSError('Etcd is not responding properly'))
         self.ha.cluster = get_cluster_initialized_with_leader_and_failsafe()
+        self.ha.global_config = self.ha.patroni.config.get_global_config(self.ha.cluster)
         self.ha.update_failsafe({'name': 'leader', 'api_url': 'http://127.0.0.1:8008/patroni',
                                  'conn_url': 'postgres://127.0.0.1:5432/postgres', 'slots': {'foo': 1000}})
         self.p.is_leader = false
         self.assertEqual(self.ha.run_cycle(), 'DCS is not accessible')
 
     def test_no_dcs_connection_replica_failsafe_not_enabled_but_active(self):
         self.ha.load_cluster_from_dcs = Mock(side_effect=DCSError('Etcd is not responding properly'))
@@ -651,18 +650,20 @@
             self.assertEqual(self.ha.run_cycle(), 'restart in progress')
 
             self.ha.has_lock = true
             self.assertEqual(self.ha.run_cycle(), 'updated leader lock during restart')
 
             self.ha.update_lock = false
             self.p.set_role('primary')
-            with patch('patroni.async_executor.CriticalTask.cancel', Mock(return_value=False)):
-                with patch('patroni.postgresql.Postgresql.terminate_starting_postmaster') as mock_terminate:
-                    self.assertEqual(self.ha.run_cycle(), 'lost leader lock during restart')
-                    mock_terminate.assert_called()
+            with patch('patroni.async_executor.CriticalTask.cancel', Mock(return_value=False)),\
+                    patch('patroni.async_executor.CriticalTask.result',
+                          PropertyMock(return_value=PostmasterProcess(os.getpid())), create=True),\
+                    patch('patroni.postgresql.Postgresql.terminate_starting_postmaster') as mock_terminate:
+                self.assertEqual(self.ha.run_cycle(), 'lost leader lock during restart')
+                mock_terminate.assert_called()
 
             self.ha.is_paused = true
             self.assertEqual(self.ha.run_cycle(), 'PAUSE: restart in progress')
 
     @patch('patroni.postgresql.citus.CitusHandler.is_coordinator', Mock(return_value=False))
     def test_manual_failover_from_leader(self):
         self.ha.fetch_node_status = get_node_status()
@@ -681,14 +682,16 @@
         self.ha.fetch_node_status = get_node_status(nofailover=True)
         self.assertEqual(self.ha.run_cycle(), 'no action. I am (postgresql0), the leader with the lock')
         self.ha.fetch_node_status = get_node_status(watchdog_failed=True)
         self.assertEqual(self.ha.run_cycle(), 'no action. I am (postgresql0), the leader with the lock')
         self.ha.fetch_node_status = get_node_status(timeline=1)
         self.assertEqual(self.ha.run_cycle(), 'no action. I am (postgresql0), the leader with the lock')
         self.ha.fetch_node_status = get_node_status(wal_position=1)
+        self.ha.cluster.config.data.update({'maximum_lag_on_failover': 5})
+        self.ha.global_config = self.ha.patroni.config.get_global_config(self.ha.cluster)
         self.assertEqual(self.ha.run_cycle(), 'no action. I am (postgresql0), the leader with the lock')
         # manual failover from the previous leader to us won't happen if we hold the nofailover flag
         self.ha.cluster = get_cluster_initialized_with_leader(Failover(0, 'blabla', self.p.name, None))
         self.assertEqual(self.ha.run_cycle(), 'no action. I am (postgresql0), the leader with the lock')
 
         # Failover scheduled time must include timezone
         scheduled = datetime.datetime.now()
@@ -731,15 +734,14 @@
         self.ha.cluster = get_cluster_initialized_with_leader(Failover(0, self.p.name, 'a', None), (self.p.name, 'a'))
         self.ha.is_failover_possible = true
         self.assertEqual('manual failover: demoting myself', self.ha.run_cycle())
 
     def test_manual_failover_process_no_leader(self):
         self.p.is_leader = false
         self.ha.cluster = get_cluster_initialized_without_leader(failover=Failover(0, '', self.p.name, None))
-        self.assertEqual(self.ha.run_cycle(), 'promoted self to leader by acquiring session lock')
         self.ha.cluster = get_cluster_initialized_without_leader(failover=Failover(0, '', 'leader', None))
         self.p.set_role('replica')
         self.assertEqual(self.ha.run_cycle(), 'promoted self to leader by acquiring session lock')
         self.ha.fetch_node_status = get_node_status()  # accessible, in_recovery
         self.assertEqual(self.ha.run_cycle(), 'following a different leader because i am not the healthiest node')
         self.ha.cluster = get_cluster_initialized_without_leader(failover=Failover(0, self.p.name, '', None))
         self.assertEqual(self.ha.run_cycle(), 'following a different leader because i am not the healthiest node')
@@ -786,31 +788,32 @@
         self.ha.cluster = get_cluster_initialized_without_leader(failover=Failover(0, '', 'other', None),
                                                                  sync=('leader1', 'blabla'))
         self.assertEqual(self.ha.run_cycle(), 'following a different leader because i am not the healthiest node')
 
         # manual failover when the `other` node isn't available but our name is in the /sync key
         self.ha.cluster = get_cluster_initialized_without_leader(failover=Failover(0, '', 'other', None),
                                                                  sync=('leader1', 'postgresql0'))
-        self.p.sync_handler.current_state = Mock(return_value=([], []))
-        self.ha.dcs.write_sync_state = true
+        self.p.sync_handler.current_state = Mock(return_value=(CaseInsensitiveSet(), CaseInsensitiveSet()))
+        self.ha.dcs.write_sync_state = Mock(return_value=SyncState.empty())
         self.assertEqual(self.ha.run_cycle(), 'promoted self to leader by acquiring session lock')
 
         # manual failover to our node (postgresql0),
         # which name is not in sync nodes list (the leader and all sync nodes are not available)
         self.p.set_role('replica')
         self.ha.cluster = get_cluster_initialized_without_leader(failover=Failover(0, '', 'postgresql0', None),
                                                                  sync=('leader1', 'other'))
         self.assertEqual(self.ha.run_cycle(), 'promoted self to leader by acquiring session lock')
 
         # manual failover to our node (postgresql0),
         # which name is not in sync nodes list (some sync nodes are available)
         self.ha.cluster = get_cluster_initialized_without_leader(failover=Failover(0, '', 'postgresql0', None),
                                                                  sync=('leader1', 'other'))
         self.p.set_role('replica')
-        self.p.sync_handler.current_state = Mock(return_value=(['leader1'], ['leader1']))
+        self.p.sync_handler.current_state = Mock(return_value=(CaseInsensitiveSet(['leader1']),
+                                                               CaseInsensitiveSet(['leader1'])))
         self.assertEqual(self.ha.run_cycle(), 'promoted self to leader by acquiring session lock')
 
     def test_manual_failover_process_no_leader_in_pause(self):
         self.ha.is_paused = true
         self.ha.cluster = get_cluster_initialized_without_leader(failover=Failover(0, '', 'other', None))
         self.assertEqual(self.ha.run_cycle(), 'PAUSE: continue to run as primary without lock')
         self.ha.cluster = get_cluster_initialized_without_leader(failover=Failover(0, 'leader', '', None))
@@ -836,28 +839,33 @@
             self.assertFalse(self.ha.is_healthiest_node())
         with patch('patroni.postgresql.Postgresql.is_starting', return_value=True):
             self.assertFalse(self.ha.is_healthiest_node())
         self.ha.is_paused = true
         self.assertFalse(self.ha.is_healthiest_node())
 
     def test__is_healthiest_node(self):
+        self.p.is_leader = false
         self.ha.cluster = get_cluster_initialized_without_leader(sync=('postgresql1', self.p.name))
+        self.ha.global_config = self.ha.patroni.config.get_global_config(self.ha.cluster)
         self.assertTrue(self.ha._is_healthiest_node(self.ha.old_cluster.members))
-        self.p.is_leader = false
         self.ha.fetch_node_status = get_node_status()  # accessible, in_recovery
         self.assertTrue(self.ha._is_healthiest_node(self.ha.old_cluster.members))
         self.ha.fetch_node_status = get_node_status(in_recovery=False)  # accessible, not in_recovery
         self.assertFalse(self.ha._is_healthiest_node(self.ha.old_cluster.members))
         self.ha.fetch_node_status = get_node_status(wal_position=11)  # accessible, in_recovery, wal position ahead
         self.assertFalse(self.ha._is_healthiest_node(self.ha.old_cluster.members))
         # in synchronous_mode consider itself healthy if the former leader is accessible in read-only and ahead of us
         with patch.object(Ha, 'is_synchronous_mode', Mock(return_value=True)):
             self.assertTrue(self.ha._is_healthiest_node(self.ha.old_cluster.members))
+        self.ha.cluster.config.data.update({'maximum_lag_on_failover': 5})
+        self.ha.global_config = self.ha.patroni.config.get_global_config(self.ha.cluster)
         with patch('patroni.postgresql.Postgresql.last_operation', return_value=1):
             self.assertFalse(self.ha._is_healthiest_node(self.ha.old_cluster.members))
+        with patch('patroni.postgresql.Postgresql.replica_cached_timeline', return_value=None):
+            self.assertFalse(self.ha._is_healthiest_node(self.ha.old_cluster.members))
         with patch('patroni.postgresql.Postgresql.replica_cached_timeline', return_value=1):
             self.assertFalse(self.ha._is_healthiest_node(self.ha.old_cluster.members))
         self.ha.patroni.nofailover = True
         self.assertFalse(self.ha._is_healthiest_node(self.ha.old_cluster.members))
         self.ha.patroni.nofailover = False
 
     def test_fetch_node_status(self):
@@ -958,47 +966,46 @@
         self.p.name = 'replica'
         self.ha.cluster = get_standby_cluster_initialized_with_only_leader()
         self.assertEqual(self.ha.run_cycle(),
                          'no action. I am (replica), a secondary, and following a standby leader (leader)')
         with patch.object(Leader, 'conn_url', PropertyMock(return_value='')):
             self.assertEqual(self.ha.run_cycle(), 'continue following the old known standby leader')
 
+    @patch.object(Cluster, 'is_unlocked', Mock(return_value=True))
     def test_process_unhealthy_standby_cluster_as_standby_leader(self):
         self.p.is_leader = false
         self.p.name = 'leader'
         self.ha.cluster = get_standby_cluster_initialized_with_only_leader()
-        self.ha.cluster.is_unlocked = true
         self.ha.sysid_valid = true
         self.p._sysid = True
         self.assertEqual(self.ha.run_cycle(), 'promoted self to a standby leader by acquiring session lock')
 
     @patch.object(Rewind, 'rewind_or_reinitialize_needed_and_possible', Mock(return_value=True))
     @patch.object(Rewind, 'can_rewind', PropertyMock(return_value=True))
     def test_process_unhealthy_standby_cluster_as_cascade_replica(self):
         self.p.is_leader = false
         self.p.name = 'replica'
         self.ha.cluster = get_standby_cluster_initialized_with_only_leader()
-        self.ha.is_unlocked = true
         self.assertTrue(self.ha.run_cycle().startswith('running pg_rewind from remote_member:'))
 
     def test_recover_unhealthy_leader_in_standby_cluster(self):
         self.p.is_leader = false
         self.p.name = 'leader'
         self.p.is_running = false
         self.p.follow = false
         self.ha.cluster = get_standby_cluster_initialized_with_only_leader()
         self.assertEqual(self.ha.run_cycle(), 'starting as a standby leader because i had the session lock')
 
+    @patch.object(Cluster, 'is_unlocked', Mock(return_value=True))
     def test_recover_unhealthy_unlocked_standby_cluster(self):
         self.p.is_leader = false
         self.p.name = 'leader'
         self.p.is_running = false
         self.p.follow = false
         self.ha.cluster = get_standby_cluster_initialized_with_only_leader()
-        self.ha.cluster.is_unlocked = true
         self.ha.has_lock = false
         self.assertEqual(self.ha.run_cycle(), 'trying to follow a remote member because standby cluster is unhealthy')
 
     def test_failed_to_update_lock_in_pause(self):
         self.ha.update_lock = false
         self.ha.is_paused = true
         self.p.name = 'leader'
@@ -1063,31 +1070,32 @@
         self.ha.fetch_node_status = get_node_status()  # accessible, in_recovery
         self.assertEqual(self.ha.run_cycle(), 'manual failover: demoting myself')
 
     @patch('patroni.ha.Ha.demote')
     def test_failover_immediately_on_zero_primary_start_timeout(self, demote):
         self.p.is_running = false
         self.ha.cluster = get_cluster_initialized_with_leader(sync=(self.p.name, 'other'))
-        self.ha.cluster.config.data['synchronous_mode'] = True
-        self.ha.patroni.config.set_dynamic_configuration({'primary_start_timeout': 0})
+        self.ha.cluster.config.data.update({'synchronous_mode': True, 'primary_start_timeout': 0})
+        self.ha.global_config = self.ha.patroni.config.get_global_config(self.ha.cluster)
         self.ha.has_lock = true
         self.ha.update_lock = true
         self.ha.fetch_node_status = get_node_status()  # accessible, in_recovery
         self.assertEqual(self.ha.run_cycle(), 'stopped PostgreSQL to fail over after a crash')
         demote.assert_called_once()
 
     def test_primary_stop_timeout(self):
         self.assertEqual(self.ha.primary_stop_timeout(), None)
-        self.ha.patroni.config.set_dynamic_configuration({'primary_stop_timeout': 30})
+        self.ha.cluster.config.data.update({'primary_stop_timeout': 30})
+        self.ha.global_config = self.ha.patroni.config.get_global_config(self.ha.cluster)
         with patch.object(Ha, 'is_synchronous_mode', Mock(return_value=True)):
             self.assertEqual(self.ha.primary_stop_timeout(), 30)
-        self.ha.patroni.config.set_dynamic_configuration({'primary_stop_timeout': 30})
         with patch.object(Ha, 'is_synchronous_mode', Mock(return_value=False)):
             self.assertEqual(self.ha.primary_stop_timeout(), None)
-            self.ha.patroni.config.set_dynamic_configuration({'primary_stop_timeout': None})
+            self.ha.cluster.config.data['primary_stop_timeout'] = None
+            self.ha.global_config = self.ha.patroni.config.get_global_config(self.ha.cluster)
             self.assertEqual(self.ha.primary_stop_timeout(), None)
 
     @patch('patroni.postgresql.Postgresql.follow')
     def test_demote_immediate(self, follow):
         self.ha.has_lock = true
         self.e.get_cluster = Mock(return_value=get_cluster_initialized_without_leader())
         self.ha.demote('immediate')
@@ -1099,102 +1107,105 @@
         self.p.name = 'leader'
 
         # Test sync key removed when sync mode disabled
         self.ha.cluster = get_cluster_initialized_with_leader(sync=('leader', 'other'))
         with patch.object(self.ha.dcs, 'delete_sync_state') as mock_delete_sync:
             self.ha.run_cycle()
             mock_delete_sync.assert_called_once()
-            mock_set_sync.assert_called_once_with([])
+            mock_set_sync.assert_called_once_with(CaseInsensitiveSet())
 
         mock_set_sync.reset_mock()
         # Test sync key not touched when not there
         self.ha.cluster = get_cluster_initialized_with_leader()
         with patch.object(self.ha.dcs, 'delete_sync_state') as mock_delete_sync:
             self.ha.run_cycle()
             mock_delete_sync.assert_not_called()
-            mock_set_sync.assert_called_once_with([])
+            mock_set_sync.assert_called_once_with(CaseInsensitiveSet())
 
         mock_set_sync.reset_mock()
 
         self.ha.is_synchronous_mode = true
 
         # Test sync standby not touched when picking the same node
-        self.p.sync_handler.current_state = Mock(return_value=(['other'], ['other']))
+        self.p.sync_handler.current_state = Mock(return_value=(CaseInsensitiveSet(['other']),
+                                                               CaseInsensitiveSet(['other'])))
         self.ha.cluster = get_cluster_initialized_with_leader(sync=('leader', 'other'))
         self.ha.run_cycle()
         mock_set_sync.assert_not_called()
 
         mock_set_sync.reset_mock()
 
         # Test sync standby is replaced when switching standbys
-        self.p.sync_handler.current_state = Mock(return_value=(['other2'], []))
-        self.ha.dcs.write_sync_state = Mock(return_value=True)
+        self.p.sync_handler.current_state = Mock(return_value=(CaseInsensitiveSet(['other2']), CaseInsensitiveSet()))
+        self.ha.dcs.write_sync_state = Mock(return_value=SyncState.empty())
         self.ha.run_cycle()
-        mock_set_sync.assert_called_once_with(['other2'])
+        mock_set_sync.assert_called_once_with(CaseInsensitiveSet(['other2']))
 
         # Test sync standby is replaced when new standby is joined
-        self.p.sync_handler.current_state = Mock(return_value=(['other2', 'other3'], ['other2']))
-        self.ha.dcs.write_sync_state = Mock(return_value=True)
+        self.p.sync_handler.current_state = Mock(return_value=(CaseInsensitiveSet(['other2', 'other3']),
+                                                               CaseInsensitiveSet(['other2'])))
+        self.ha.dcs.write_sync_state = Mock(return_value=SyncState.empty())
         self.ha.run_cycle()
-        self.assertEqual(mock_set_sync.call_args_list[0][0], (['other2'],))
-        self.assertEqual(mock_set_sync.call_args_list[1][0], (['other2', 'other3'],))
+        self.assertEqual(mock_set_sync.call_args_list[0][0], (CaseInsensitiveSet(['other2']),))
+        self.assertEqual(mock_set_sync.call_args_list[1][0], (CaseInsensitiveSet(['other2', 'other3']),))
 
         mock_set_sync.reset_mock()
         # Test sync standby is not disabled when updating dcs fails
-        self.ha.dcs.write_sync_state = Mock(return_value=False)
+        self.ha.dcs.write_sync_state = Mock(return_value=None)
         self.ha.run_cycle()
         mock_set_sync.assert_not_called()
 
         mock_set_sync.reset_mock()
         # Test changing sync standby
-        self.ha.dcs.write_sync_state = Mock(return_value=True)
+        self.ha.dcs.write_sync_state = Mock(return_value=SyncState.empty())
         self.ha.dcs.get_cluster = Mock(return_value=get_cluster_initialized_with_leader(sync=('leader', 'other')))
         # self.ha.cluster = get_cluster_initialized_with_leader(sync=('leader', 'other'))
-        self.p.sync_handler.current_state = Mock(return_value=(['other2'], ['other2']))
+        self.p.sync_handler.current_state = Mock(return_value=(CaseInsensitiveSet(['other2']),
+                                                               CaseInsensitiveSet(['other2'])))
         self.ha.run_cycle()
-        self.ha.dcs.get_cluster.assert_called_once()
         self.assertEqual(self.ha.dcs.write_sync_state.call_count, 2)
 
         # Test updating sync standby key failed due to race
-        self.ha.dcs.write_sync_state = Mock(side_effect=[True, False])
+        self.ha.dcs.write_sync_state = Mock(side_effect=[SyncState.empty(), None])
         self.ha.run_cycle()
         self.assertEqual(self.ha.dcs.write_sync_state.call_count, 2)
 
         # Test updating sync standby key failed due to DCS being not accessible
-        self.ha.dcs.write_sync_state = Mock(return_value=True)
+        self.ha.dcs.write_sync_state = Mock(return_value=SyncState.empty())
         self.ha.dcs.get_cluster = Mock(side_effect=DCSError('foo'))
         self.ha.run_cycle()
 
         # Test changing sync standby failed due to race
+        self.ha.dcs.write_sync_state = Mock(return_value=SyncState.empty())
         self.ha.dcs.get_cluster = Mock(return_value=get_cluster_initialized_with_leader(sync=('somebodyelse', None)))
         self.ha.run_cycle()
         self.assertEqual(self.ha.dcs.write_sync_state.call_count, 2)
 
         # Test sync set to '*' when synchronous_mode_strict is enabled
         mock_set_sync.reset_mock()
-        self.ha.is_synchronous_mode_strict = true
-        self.p.sync_handler.current_state = Mock(return_value=([], []))
-        self.ha.run_cycle()
-        mock_set_sync.assert_called_once_with(['*'])
+        self.p.sync_handler.current_state = Mock(return_value=(CaseInsensitiveSet(), CaseInsensitiveSet()))
+        with patch('patroni.config.GlobalConfig.is_synchronous_mode_strict', PropertyMock(return_value=True)):
+            self.ha.run_cycle()
+        mock_set_sync.assert_called_once_with(CaseInsensitiveSet('*'))
 
     def test_sync_replication_become_primary(self):
         self.ha.is_synchronous_mode = true
 
         mock_set_sync = self.p.sync_handler.set_synchronous_standby_names = Mock()
         self.p.is_leader = false
         self.p.set_role('replica')
         self.ha.has_lock = true
-        mock_write_sync = self.ha.dcs.write_sync_state = Mock(return_value=True)
+        mock_write_sync = self.ha.dcs.write_sync_state = Mock(return_value=SyncState.empty())
         self.p.name = 'leader'
         self.ha.cluster = get_cluster_initialized_with_leader(sync=('other', None))
 
         # When we just became primary nobody is sync
         self.assertEqual(self.ha.enforce_primary_role('msg', 'promote msg'), 'promote msg')
-        mock_set_sync.assert_called_once_with([])
-        mock_write_sync.assert_called_once_with('leader', None, index=0)
+        mock_set_sync.assert_called_once_with(CaseInsensitiveSet())
+        mock_write_sync.assert_called_once_with('leader', None, version=0)
 
         mock_set_sync.reset_mock()
 
         # When we just became primary nobody is sync
         self.p.set_role('replica')
         mock_write_sync.return_value = False
         self.assertTrue(self.ha.enforce_primary_role('msg', 'promote msg') != 'promote msg')
@@ -1203,15 +1214,15 @@
     def test_unhealthy_sync_mode(self):
         self.ha.is_synchronous_mode = true
 
         self.p.is_leader = false
         self.p.set_role('replica')
         self.p.name = 'other'
         self.ha.cluster = get_cluster_initialized_without_leader(sync=('leader', 'other2'))
-        mock_write_sync = self.ha.dcs.write_sync_state = Mock(return_value=True)
+        mock_write_sync = self.ha.dcs.write_sync_state = Mock(return_value=SyncState.empty())
         mock_acquire = self.ha.acquire_lock = Mock(return_value=True)
         mock_follow = self.p.follow = Mock()
         mock_promote = self.p.promote = Mock()
 
         # If we don't match the sync replica we are not allowed to acquire lock
         self.ha.run_cycle()
         mock_acquire.assert_not_called()
@@ -1224,15 +1235,15 @@
         self.ha._is_healthiest_node = true
 
         self.ha.cluster = get_cluster_initialized_without_leader(sync=('leader', 'other'))
         self.ha.run_cycle()
         mock_acquire.assert_called_once()
         mock_follow.assert_not_called()
         mock_promote.assert_called_once()
-        mock_write_sync.assert_called_once_with('other', None, index=0)
+        mock_write_sync.assert_called_once_with('other', None, version=0)
 
     def test_disable_sync_when_restarting(self):
         self.ha.is_synchronous_mode = true
 
         self.p.name = 'other'
         self.p.is_leader = false
         self.p.set_role('replica')
@@ -1269,18 +1280,18 @@
         self.ha._disable_sync = True
         self.assertEqual(self.ha.get_effective_tags(), {'foo': 'bar', 'nosync': True})
         self.ha._disable_sync = False
         self.assertEqual(self.ha.get_effective_tags(), {'foo': 'bar'})
 
     @patch('patroni.postgresql.mtime', Mock(return_value=1588316884))
     @patch('builtins.open', Mock(side_effect=Exception))
+    @patch.object(Cluster, 'is_unlocked', Mock(return_value=False))
     def test_restore_cluster_config(self):
         self.ha.cluster.config.data.clear()
         self.ha.has_lock = true
-        self.ha.cluster.is_unlocked = false
         self.assertEqual(self.ha.run_cycle(), 'no action. I am (postgresql0), the leader with the lock')
 
     def test_watch(self):
         self.ha.cluster = get_cluster_initialized_with_leader()
         self.ha.watch(0)
 
     def test_wakup(self):
@@ -1323,31 +1334,31 @@
         # will not say bootstrap because data directory is not accessible
         self.assertEqual(self.ha.run_cycle(),
                          "data directory is not accessible: [Errno 5] Input/output error: '{}'".format(self.p.data_dir))
 
     @patch('patroni.postgresql.mtime', Mock(return_value=1588316884))
     @patch('builtins.open', mock_open(read_data=('1\t0/40159C0\tno recovery target specified\n\n'
                                                  '2\t1/40159C0\tno recovery target specified\n')))
+    @patch.object(Cluster, 'is_unlocked', Mock(return_value=False))
     def test_update_cluster_history(self):
         self.ha.has_lock = true
-        self.ha.cluster.is_unlocked = false
         for tl in (1, 3):
             self.p.get_primary_timeline = Mock(return_value=tl)
             self.assertEqual(self.ha.run_cycle(), 'no action. I am (postgresql0), the leader with the lock')
 
     @patch('sys.exit', return_value=1)
     def test_abort_join(self, exit_mock):
         self.ha.cluster = get_cluster_not_initialized_without_leader()
         self.p.is_leader = false
         self.ha.run_cycle()
         exit_mock.assert_called_once_with(1)
 
+    @patch.object(Cluster, 'is_unlocked', Mock(return_value=False))
     def test_after_pause(self):
         self.ha.has_lock = true
-        self.ha.cluster.is_unlocked = false
         self.ha.is_paused = true
         self.assertEqual(self.ha.run_cycle(), 'PAUSE: no action. I am (postgresql0), the leader with the lock')
         self.ha.is_paused = false
         self.assertEqual(self.ha.run_cycle(), 'no action. I am (postgresql0), the leader with the lock')
 
     @patch('patroni.psycopg.connect', psycopg_connect)
     def test_permanent_logical_slots_after_promote(self):
@@ -1391,24 +1402,18 @@
     @patch('os.rename', Mock())
     @patch('patroni.postgresql.Postgresql.is_starting', Mock(return_value=False))
     @patch('builtins.open', mock_open())
     @patch.object(ConfigHandler, 'check_recovery_conf', Mock(return_value=(False, False)))
     @patch.object(Postgresql, 'major_version', PropertyMock(return_value=130000))
     @patch.object(SlotsHandler, 'sync_replication_slots', Mock(return_value=['ls']))
     def test_follow_copy(self):
-        self.ha.cluster.is_unlocked = false
         self.ha.cluster.config.data['slots'] = {'ls': {'database': 'a', 'plugin': 'b'}}
         self.p.is_leader = false
         self.assertTrue(self.ha.run_cycle().startswith('Copying logical slots'))
 
-    def test_is_failover_possible(self):
-        self.ha.fetch_node_status = Mock(return_value=_MemberStatus(self.ha.cluster.members[0],
-                                                                    True, True, 0, 2, None, {}, False))
-        self.assertFalse(self.ha.is_failover_possible(self.ha.cluster.members))
-
     def test_acquire_lock(self):
         self.ha.dcs.attempt_to_acquire_leader = Mock(side_effect=[DCSError('foo'), Exception])
         self.assertRaises(DCSError, self.ha.acquire_lock)
         self.assertFalse(self.ha.acquire_lock())
 
     @patch('patroni.postgresql.citus.CitusHandler.is_coordinator', Mock(return_value=False))
     def test_notify_citus_coordinator(self):
```

### Comparing `patroni-3.0.2/tests/test_kubernetes.py` & `patroni-3.0.3/tests/test_kubernetes.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,19 +1,20 @@
 import base64
 import datetime
 import json
 import mock
 import socket
 import time
 import unittest
+import urllib3
 
 from mock import Mock, PropertyMock, mock_open, patch
 from patroni.dcs.kubernetes import Cluster, k8s_client, k8s_config, K8sConfig, K8sConnectionFailed,\
-        K8sException, K8sObject, Kubernetes, KubernetesError, KubernetesRetriableException,\
-        Retry, RetryFailedError, SERVICE_HOST_ENV_NAME, SERVICE_PORT_ENV_NAME
+    K8sException, K8sObject, Kubernetes, KubernetesError, KubernetesRetriableException,\
+    Retry, RetryFailedError, SERVICE_HOST_ENV_NAME, SERVICE_PORT_ENV_NAME
 from threading import Thread
 from . import MockResponse, SleepException
 
 
 def mock_list_namespaced_config_map(*args, **kwargs):
     metadata = {'resource_version': '1', 'labels': {'f': 'b'}, 'name': 'test-config',
                 'annotations': {'initialize': '123', 'config': '{}'}}
@@ -30,14 +31,17 @@
     items.append(k8s_client.V1ConfigMap(metadata=k8s_client.V1ObjectMeta(**metadata)))
     metadata.update({'name': 'test-0-config', 'labels': {Kubernetes._CITUS_LABEL: '0'},
                      'annotations': {'initialize': '123', 'config': '{}'}})
     items.append(k8s_client.V1ConfigMap(metadata=k8s_client.V1ObjectMeta(**metadata)))
     metadata.update({'name': 'test-1-leader', 'labels': {Kubernetes._CITUS_LABEL: '1'},
                      'annotations': {'leader': 'p-3', 'ttl': '30s'}})
     items.append(k8s_client.V1ConfigMap(metadata=k8s_client.V1ObjectMeta(**metadata)))
+    metadata.update({'name': 'test-2-config', 'labels': {Kubernetes._CITUS_LABEL: '2'}, 'annotations': {}})
+    items.append(k8s_client.V1ConfigMap(metadata=k8s_client.V1ObjectMeta(**metadata)))
+
     metadata = k8s_client.V1ObjectMeta(resource_version='1')
     return k8s_client.V1ConfigMapList(metadata=metadata, items=items, kind='ConfigMapList')
 
 
 def mock_read_namespaced_endpoints(*args, **kwargs):
     target_ref = k8s_client.V1ObjectReference(kind='Pod', resource_version='10', name='p-0',
                                               namespace='default', uid='964dfeae-e79b-4476-8a5a-1920b5c2a69d')
@@ -307,15 +311,15 @@
                   Mock(side_effect=k8s_client.rest.ApiException(403, '')), create=True)
     def test_delete_cluster(self):
         self.k.delete_cluster()
 
     def test_watch(self):
         self.k.set_ttl(10)
         self.k.watch(None, 0)
-        self.k.watch(None, 0)
+        self.k.watch('5', 0)
 
     def test_set_history_value(self):
         self.k.set_history_value('{}')
 
     @patch('patroni.dcs.kubernetes.logger.warning')
     def test_reload_config(self, mock_warning):
         self.k.reload_config({'loop_wait': 10, 'ttl': 30, 'retry_timeout': 10, 'retriable_http_codes': '401, 403 '})
@@ -332,49 +336,55 @@
 
     @patch.object(k8s_client.CoreV1Api, 'list_namespaced_endpoints', mock_list_namespaced_endpoints, create=True)
     def setUp(self, config=None):
         super(TestKubernetesEndpoints, self).setUp({'use_endpoints': True, 'pod_ip': '10.0.0.0'})
 
     @patch.object(k8s_client.CoreV1Api, 'patch_namespaced_endpoints', create=True)
     def test_update_leader(self, mock_patch_namespaced_endpoints):
-        self.assertIsNotNone(self.k.update_leader('123', failsafe={'foo': 'bar'}))
+        leader = self.k.get_cluster().leader
+        self.assertIsNotNone(self.k.update_leader(leader, '123', failsafe={'foo': 'bar'}))
         args = mock_patch_namespaced_endpoints.call_args[0]
         self.assertEqual(args[2].subsets[0].addresses[0].target_ref.resource_version, '10')
         self.k._kinds._object_cache['test'].subsets[:] = []
-        self.assertIsNotNone(self.k.update_leader('123'))
+        self.assertIsNotNone(self.k.update_leader(leader, '123'))
         self.k._kinds._object_cache['test'].metadata.annotations['leader'] = 'p-1'
-        self.assertFalse(self.k.update_leader('123'))
+        self.assertFalse(self.k.update_leader(leader, '123'))
 
     @patch.object(k8s_client.CoreV1Api, 'read_namespaced_endpoints', create=True)
     @patch.object(k8s_client.CoreV1Api, 'patch_namespaced_endpoints', create=True)
     def test__update_leader_with_retry(self, mock_patch, mock_read):
+        leader = self.k.get_cluster().leader
         mock_read.return_value = mock_read_namespaced_endpoints()
         mock_patch.side_effect = k8s_client.rest.ApiException(502, '')
-        self.assertFalse(self.k.update_leader('123'))
+        self.assertFalse(self.k.update_leader(leader, '123'))
         mock_patch.side_effect = RetryFailedError('')
-        self.assertRaises(KubernetesError, self.k.update_leader, '123')
+        self.assertRaises(KubernetesError, self.k.update_leader, leader, '123')
         mock_patch.side_effect = k8s_client.rest.ApiException(409, '')
         with patch('time.time', Mock(side_effect=[0, 100, 200, 0, 0, 0, 0, 100, 200])):
-            self.assertFalse(self.k.update_leader('123'))
-            self.assertFalse(self.k.update_leader('123'))
-        self.assertFalse(self.k.update_leader('123'))
+            self.assertFalse(self.k.update_leader(leader, '123'))
+            self.assertFalse(self.k.update_leader(leader, '123'))
+        self.assertFalse(self.k.update_leader(leader, '123'))
         mock_patch.side_effect = [k8s_client.rest.ApiException(409, ''), mock_namespaced_kind()]
         mock_read.return_value.metadata.resource_version = '2'
         self.assertIsNotNone(self.k._update_leader_with_retry({}, '1', []))
         mock_patch.side_effect = k8s_client.rest.ApiException(409, '')
         mock_read.side_effect = RetryFailedError('')
-        self.assertRaises(KubernetesError, self.k.update_leader, '123')
+        self.assertRaises(KubernetesError, self.k.update_leader, leader, '123')
         mock_read.side_effect = Exception
-        self.assertFalse(self.k.update_leader('123'))
+        self.assertFalse(self.k.update_leader(leader, '123'))
 
-    @patch.object(k8s_client.CoreV1Api, 'create_namespaced_endpoints',
+    @patch.object(k8s_client.CoreV1Api, 'patch_namespaced_endpoints',
                   Mock(side_effect=[k8s_client.rest.ApiException(500, ''),
                                     k8s_client.rest.ApiException(502, '')]), create=True)
     def test_delete_sync_state(self):
-        self.assertFalse(self.k.delete_sync_state())
+        self.assertFalse(self.k.delete_sync_state(1))
+
+    @patch.object(k8s_client.CoreV1Api, 'patch_namespaced_endpoints', mock_namespaced_kind, create=True)
+    def test_write_sync_state(self):
+        self.assertIsNotNone(self.k.write_sync_state('a', ['b'], 1))
 
     @patch.object(k8s_client.CoreV1Api, 'patch_namespaced_pod', mock_namespaced_kind, create=True)
     @patch.object(k8s_client.CoreV1Api, 'create_namespaced_endpoints', mock_namespaced_kind, create=True)
     @patch.object(k8s_client.CoreV1Api, 'create_namespaced_service',
                   Mock(side_effect=[True,
                                     False,
                                     k8s_client.rest.ApiException(409, ''),
@@ -399,21 +409,26 @@
         mock_logger_exception.reset_mock()
 
         self.k.touch_member({'state': 'running', 'role': 'replica'})
         mock_logger_exception.assert_called_once()
         self.assertEqual(('create_config_service failed',), mock_logger_exception.call_args[0])
 
 
+def mock_watch(*args):
+    return urllib3.HTTPResponse()
+
+
 class TestCacheBuilder(BaseTestKubernetes):
 
     @patch.object(k8s_client.CoreV1Api, 'list_namespaced_config_map', mock_list_namespaced_config_map, create=True)
-    @patch('patroni.dcs.kubernetes.ObjectCache._watch')
-    def test__build_cache(self, mock_response):
+    @patch('patroni.dcs.kubernetes.ObjectCache._watch', mock_watch)
+    @patch.object(urllib3.HTTPResponse, 'read_chunked')
+    def test__build_cache(self, mock_read_chunked):
         self.k._citus_group = '0'
-        mock_response.return_value.read_chunked.return_value = [json.dumps(
+        mock_read_chunked.return_value = [json.dumps(
             {'type': 'MODIFIED', 'object': {'metadata': {
                 'name': self.k.config_path, 'resourceVersion': '2', 'annotations': {self.k._CONFIG: 'foo'}}}}
         ).encode('utf-8'), ('\n' + json.dumps(
             {'type': 'DELETED', 'object': {'metadata': {
                 'name': self.k.config_path, 'resourceVersion': '3'}}}
         ) + '\n' + json.dumps(
             {'type': 'MDIFIED', 'object': {'metadata': {'name': self.k.config_path}}}
@@ -431,16 +446,17 @@
         self.assertRaises(Exception, self.k._pods._list)
 
     @patch('patroni.dcs.kubernetes.ObjectCache._watch', Mock(return_value=None))
     def test__do_watch(self):
         self.assertRaises(AttributeError, self.k._kinds._do_watch, '1')
 
     @patch.object(k8s_client.CoreV1Api, 'list_namespaced_config_map', mock_list_namespaced_config_map, create=True)
-    @patch('patroni.dcs.kubernetes.ObjectCache._watch')
-    def test_kill_stream(self, mock_watch):
-        self.k._kinds.kill_stream()
-        mock_watch.return_value.read_chunked.return_value = []
-        mock_watch.return_value.connection.sock.close.side_effect = Exception
-        self.k._kinds._do_watch('1')
-        self.k._kinds.kill_stream()
-        type(mock_watch.return_value).connection = PropertyMock(side_effect=Exception)
+    @patch('patroni.dcs.kubernetes.ObjectCache._watch', mock_watch)
+    @patch.object(urllib3.HTTPResponse, 'read_chunked', Mock(return_value=[]))
+    def test_kill_stream(self):
         self.k._kinds.kill_stream()
+        with patch.object(urllib3.HTTPResponse, 'connection') as mock_connection:
+            mock_connection.sock.close.side_effect = Exception
+            self.k._kinds._do_watch('1')
+            self.k._kinds.kill_stream()
+        with patch.object(urllib3.HTTPResponse, 'connection', PropertyMock(side_effect=Exception)):
+            self.k._kinds.kill_stream()
```

### Comparing `patroni-3.0.2/tests/test_log.py` & `patroni-3.0.3/tests/test_log.py`

 * *Files identical despite different names*

### Comparing `patroni-3.0.2/tests/test_patroni.py` & `patroni-3.0.3/tests/test_patroni.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 from patroni.api import RestApiServer
 from patroni.async_executor import AsyncExecutor
 from patroni.dcs.etcd import AbstractEtcdClientWithFailover
 from patroni.exceptions import DCSError
 from patroni.postgresql import Postgresql
 from patroni.postgresql.config import ConfigHandler
 from patroni import check_psycopg
-from patroni.__main__ import Patroni, main as _main, patroni_main
+from patroni.__main__ import Patroni, main as _main
 from threading import Thread
 
 from . import psycopg_connect, SleepException
 from .test_etcd import etcd_read, etcd_write
 from .test_postgresql import MockPostmaster
 
 
@@ -48,29 +48,30 @@
 @patch.object(AsyncExecutor, 'run', Mock())
 @patch.object(etcd.Client, 'write', etcd_write)
 @patch.object(etcd.Client, 'read', etcd_read)
 class TestPatroni(unittest.TestCase):
 
     @patch('sys.argv', ['patroni.py'])
     def test_no_config(self):
-        self.assertRaises(SystemExit, patroni_main)
+        self.assertRaises(SystemExit, _main)
 
     @patch('sys.argv', ['patroni.py', '--validate-config', 'postgres0.yml'])
     @patch('socket.socket.connect_ex', Mock(return_value=1))
     def test_validate_config(self):
-        self.assertRaises(SystemExit, patroni_main)
+        self.assertRaises(SystemExit, _main)
         with patch.object(config.Config, '__init__', Mock(return_value=None)):
-            self.assertRaises(SystemExit, patroni_main)
+            self.assertRaises(SystemExit, _main)
 
     @patch('pkgutil.iter_importers', Mock(return_value=[MockFrozenImporter()]))
     @patch('sys.frozen', Mock(return_value=True), create=True)
     @patch.object(HTTPServer, '__init__', Mock())
     @patch.object(etcd.Client, 'read', etcd_read)
     @patch.object(Thread, 'start', Mock())
     @patch.object(AbstractEtcdClientWithFailover, '_get_machines_list', Mock(return_value=['http://remotehost:2379']))
+    @patch.object(Postgresql, '_get_gucs', Mock(return_value={'foo': True, 'bar': True}))
     def setUp(self):
         self._handlers = logging.getLogger().handlers[:]
         RestApiServer._BaseServer__is_shut_down = Mock()
         RestApiServer._BaseServer__shutdown_request = True
         RestApiServer.socket = 0
         os.environ['PATRONI_POSTGRESQL_DATA_DIR'] = 'data/test0'
         conf = config.Config('postgres0.yml')
@@ -86,23 +87,24 @@
         self.p.load_dynamic_configuration()
 
     @patch('sys.argv', ['patroni.py', 'postgres0.yml'])
     @patch('time.sleep', Mock(side_effect=SleepException))
     @patch.object(etcd.Client, 'delete', Mock())
     @patch.object(AbstractEtcdClientWithFailover, '_get_machines_list', Mock(return_value=['http://remotehost:2379']))
     @patch.object(Thread, 'join', Mock())
+    @patch.object(Postgresql, '_get_gucs', Mock(return_value={'foo': True, 'bar': True}))
     def test_patroni_patroni_main(self):
         with patch('subprocess.call', Mock(return_value=1)):
             with patch.object(Patroni, 'run', Mock(side_effect=SleepException)):
                 os.environ['PATRONI_POSTGRESQL_DATA_DIR'] = 'data/test0'
-                self.assertRaises(SleepException, patroni_main)
+                self.assertRaises(SleepException, _main)
             with patch.object(Patroni, 'run', Mock(side_effect=KeyboardInterrupt())):
                 with patch('patroni.ha.Ha.is_paused', Mock(return_value=True)):
                     os.environ['PATRONI_POSTGRESQL_DATA_DIR'] = 'data/test0'
-                    patroni_main()
+                    _main()
 
     @patch('os.getpid')
     @patch('multiprocessing.Process')
     @patch('patroni.__main__.patroni_main', Mock())
     def test_patroni_main(self, mock_process, mock_getpid):
         mock_getpid.return_value = 2
         _main()
```

### Comparing `patroni-3.0.2/tests/test_postgresql.py` & `patroni-3.0.3/tests/test_postgresql.py`

 * *Files 15% similar despite different names*

```diff
@@ -6,24 +6,30 @@
 import time
 
 from mock import Mock, MagicMock, PropertyMock, patch, mock_open
 
 import patroni.psycopg as psycopg
 
 from patroni.async_executor import CriticalTask
+from patroni.collections import CaseInsensitiveSet
+from patroni.config import GlobalConfig
 from patroni.dcs import RemoteMember
 from patroni.exceptions import PostgresConnectionException, PatroniException
 from patroni.postgresql import Postgresql, STATE_REJECT, STATE_NO_RESPONSE
 from patroni.postgresql.bootstrap import Bootstrap
 from patroni.postgresql.callback_executor import CallbackAction
 from patroni.postgresql.postmaster import PostmasterProcess
+from patroni.postgresql.validator import (ValidatorFactoryNoType, ValidatorFactoryInvalidType,
+                                          ValidatorFactoryInvalidSpec, ValidatorFactory, InvalidGucValidatorsFile,
+                                          _get_postgres_guc_validators, _read_postgres_gucs_validators_file,
+                                          _load_postgres_gucs_validators, Bool, Integer, Real, Enum, EnumBool, String)
 from patroni.utils import RetryFailedError
 from threading import Thread, current_thread
 
-from . import BaseTestPostgresql, MockCursor, MockPostmaster, psycopg_connect
+from . import BaseTestPostgresql, MockCursor, MockPostmaster, psycopg_connect, mock_available_gucs
 
 
 mtime_ret = {}
 
 
 def mock_mtime(filename):
     if filename not in mtime_ret:
@@ -86,21 +92,23 @@
 Float8 argument passing:              by value
 Data page checksum version:           0
 """
 
 
 @patch('subprocess.call', Mock(return_value=0))
 @patch('patroni.psycopg.connect', psycopg_connect)
+@patch.object(Postgresql, 'available_gucs', mock_available_gucs)
 class TestPostgresql(BaseTestPostgresql):
 
     @patch('subprocess.call', Mock(return_value=0))
     @patch('os.rename', Mock())
     @patch('patroni.postgresql.CallbackExecutor', Mock())
     @patch.object(Postgresql, 'get_major_version', Mock(return_value=140000))
     @patch.object(Postgresql, 'is_running', Mock(return_value=True))
+    @patch.object(Postgresql, 'available_gucs', mock_available_gucs)
     def setUp(self):
         super(TestPostgresql, self).setUp()
         self.p.config.write_postgresql_conf()
 
     @patch('subprocess.Popen')
     @patch.object(Postgresql, 'wait_for_startup')
     @patch.object(Postgresql, 'wait_for_port_open')
@@ -165,15 +173,16 @@
         # cancelled
         self.p.cancellable.cancel()
         self.assertFalse(self.p.wait_for_port_open(mock_postmaster, 1))
 
     @patch('time.sleep', Mock())
     @patch.object(Postgresql, 'is_running')
     @patch.object(Postgresql, '_wait_for_connection_close', Mock())
-    def test_stop(self, mock_is_running):
+    @patch('patroni.postgresql.cancellable.CancellableSubprocess.call')
+    def test_stop(self, mock_cancellable_call, mock_is_running):
         # Postmaster is not running
         mock_callback = Mock()
         mock_is_running.return_value = None
         self.assertTrue(self.p.stop(on_safepoint=mock_callback))
         mock_callback.assert_called()
 
         # Is running, stopped successfully
@@ -190,14 +199,27 @@
         mock_callback.assert_called()
         mock_postmaster.signal_stop.assert_called()
 
         # Immediate shutdown succeeded
         mock_postmaster.wait.side_effect = [psutil.TimeoutExpired(30), Mock()]
         self.assertTrue(self.p.stop(on_safepoint=mock_callback, stop_timeout=30))
 
+        # Ensure before_stop script is called when configured to
+        self.p.config._config['before_stop'] = ':'
+        mock_postmaster.wait.side_effect = [psutil.TimeoutExpired(30), Mock()]
+        mock_cancellable_call.return_value = 0
+        with patch('patroni.postgresql.logger.info') as mock_logger:
+            self.p.stop(on_safepoint=mock_callback, stop_timeout=30)
+            self.assertEqual(mock_logger.call_args[0], ('before_stop script `%s` exited with %s', ':', 0))
+        mock_postmaster.wait.side_effect = [psutil.TimeoutExpired(30), Mock()]
+        mock_cancellable_call.side_effect = Exception
+        with patch('patroni.postgresql.logger.error') as mock_logger:
+            self.p.stop(on_safepoint=mock_callback, stop_timeout=30)
+            self.assertEqual(mock_logger.call_args_list[1][0][0], 'Exception when calling `%s`: %r')
+
         # Stop signal failed
         mock_postmaster.signal_stop.return_value = False
         self.assertFalse(self.p.stop())
 
         # Stop signal failed to find process
         mock_postmaster.signal_stop.return_value = True
         mock_callback.reset_mock()
@@ -320,15 +342,16 @@
         self.p.config.write_recovery_conf({'foo': 'bar'})
         self.p.config.write_postgresql_conf()
 
     @patch.object(Postgresql, 'is_running', Mock(return_value=False))
     @patch.object(Postgresql, 'start', Mock())
     def test_follow(self):
         self.p.call_nowait(CallbackAction.ON_START)
-        m = RemoteMember('1', {'restore_command': '2', 'primary_slot_name': 'foo', 'conn_kwargs': {'host': 'bar'}})
+        m = RemoteMember.from_name_and_data('1', {'restore_command': '2', 'primary_slot_name': 'foo',
+                                                  'conn_kwargs': {'host': 'bar'}})
         self.p.follow(m)
         with patch.object(Postgresql, 'ensure_major_version_is_known', Mock(return_value=False)):
             self.assertIsNone(self.p.follow(m))
 
     @patch.object(MockCursor, 'execute', Mock(side_effect=psycopg.OperationalError))
     def test__query(self):
         self.assertRaises(PostgresConnectionException, self.p._query, 'blabla')
@@ -350,37 +373,41 @@
     @patch.object(Postgresql, 'controldata', Mock(return_value={'Database cluster state': 'shut down',
                                                                 'Latest checkpoint location': '0/1ADBC18',
                                                                 "Latest checkpoint's TimeLineID": '1'}))
     @patch('subprocess.Popen')
     def test_latest_checkpoint_location(self, mock_popen):
         mock_popen.return_value.communicate.return_value = (None, None)
         self.assertEqual(self.p.latest_checkpoint_location(), 28163096)
+        with patch.object(Postgresql, 'controldata', Mock(return_value={'Database cluster state': 'shut down',
+                                                                        'Latest checkpoint location': 'k/1ADBC18',
+                                                                        "Latest checkpoint's TimeLineID": '1'})):
+            self.assertIsNone(self.p.latest_checkpoint_location())
         # 9.3 and 9.4 format
         mock_popen.return_value.communicate.side_effect = [
-            (b'rmgr: XLOG        len (rec/tot):     72/   104, tx:          0, lsn: 0/01ADBC18, prev 0/01ADBBB8, ' +
-             b'bkp: 0000, desc: checkpoint: redo 0/1ADBC18; tli 1; prev tli 1; fpw true; xid 0/727; oid 16386; multi' +
-             b' 1; offset 0; oldest xid 715 in DB 1; oldest multi 1 in DB 1; oldest running xid 0; shutdown', None),
-            (b'rmgr: Transaction len (rec/tot):     64/    96, tx:        726, lsn: 0/01ADBBB8, prev 0/01ADBB70, ' +
-             b'bkp: 0000, desc: commit: 2021-02-26 11:19:37.900918 CET; inval msgs: catcache 11 catcache 10', None)]
+            (b'rmgr: XLOG        len (rec/tot):     72/   104, tx:          0, lsn: 0/01ADBC18, prev 0/01ADBBB8, '
+             + b'bkp: 0000, desc: checkpoint: redo 0/1ADBC18; tli 1; prev tli 1; fpw true; xid 0/727; oid 16386; multi'
+             + b' 1; offset 0; oldest xid 715 in DB 1; oldest multi 1 in DB 1; oldest running xid 0; shutdown', None),
+            (b'rmgr: Transaction len (rec/tot):     64/    96, tx:        726, lsn: 0/01ADBBB8, prev 0/01ADBB70, '
+             + b'bkp: 0000, desc: commit: 2021-02-26 11:19:37.900918 CET; inval msgs: catcache 11 catcache 10', None)]
         self.assertEqual(self.p.latest_checkpoint_location(), 28163096)
         mock_popen.return_value.communicate.side_effect = [
-            (b'rmgr: XLOG        len (rec/tot):     72/   104, tx:          0, lsn: 0/01ADBC18, prev 0/01ADBBB8, ' +
-             b'bkp: 0000, desc: checkpoint: redo 0/1ADBC18; tli 1; prev tli 1; fpw true; xid 0/727; oid 16386; multi' +
-             b' 1; offset 0; oldest xid 715 in DB 1; oldest multi 1 in DB 1; oldest running xid 0; shutdown', None),
-            (b'rmgr: XLOG        len (rec/tot):      0/    32, tx:          0, lsn: 0/01ADBBB8, prev 0/01ADBBA0, ' +
-             b'bkp: 0000, desc: xlog switch ', None)]
+            (b'rmgr: XLOG        len (rec/tot):     72/   104, tx:          0, lsn: 0/01ADBC18, prev 0/01ADBBB8, '
+             + b'bkp: 0000, desc: checkpoint: redo 0/1ADBC18; tli 1; prev tli 1; fpw true; xid 0/727; oid 16386; multi'
+             + b' 1; offset 0; oldest xid 715 in DB 1; oldest multi 1 in DB 1; oldest running xid 0; shutdown', None),
+            (b'rmgr: XLOG        len (rec/tot):      0/    32, tx:          0, lsn: 0/01ADBBB8, prev 0/01ADBBA0, '
+             + b'bkp: 0000, desc: xlog switch ', None)]
         self.assertEqual(self.p.latest_checkpoint_location(), 28163000)
         # 9.5+ format
         mock_popen.return_value.communicate.side_effect = [
-            (b'rmgr: XLOG        len (rec/tot):    114/   114, tx:          0, lsn: 0/01ADBC18, prev 0/018260F8, ' +
-             b'desc: CHECKPOINT_SHUTDOWN redo 0/1825ED8; tli 1; prev tli 1; fpw true; xid 0:494; oid 16387; multi 1' +
-             b'; offset 0; oldest xid 479 in DB 1; oldest multi 1 in DB 1; oldest/newest commit timestamp xid: 0/0;' +
-             b' oldest running xid 0; shutdown', None),
-            (b'rmgr: XLOG        len (rec/tot):     24/    24, tx:          0, lsn: 0/018260F8, prev 0/01826080, ' +
-             b'desc: SWITCH ', None)]
+            (b'rmgr: XLOG        len (rec/tot):    114/   114, tx:          0, lsn: 0/01ADBC18, prev 0/018260F8, '
+             + b'desc: CHECKPOINT_SHUTDOWN redo 0/1825ED8; tli 1; prev tli 1; fpw true; xid 0:494; oid 16387; multi 1'
+             + b'; offset 0; oldest xid 479 in DB 1; oldest multi 1 in DB 1; oldest/newest commit timestamp xid: 0/0;'
+             + b' oldest running xid 0; shutdown', None),
+            (b'rmgr: XLOG        len (rec/tot):     24/    24, tx:          0, lsn: 0/018260F8, prev 0/01826080, '
+             + b'desc: SWITCH ', None)]
         self.assertEqual(self.p.latest_checkpoint_location(), 25321720)
 
     def test_reload(self):
         self.assertTrue(self.p.reload())
 
     @patch.object(Postgresql, 'is_running')
     def test_is_healthy(self, mock_is_running):
@@ -499,18 +526,18 @@
     @patch('os.path.isfile', Mock(side_effect=[False, True]))
     @patch('shutil.copy', Mock(side_effect=IOError))
     def test_restore_configuration_files(self):
         self.p.config.restore_configuration_files()
 
     def test_can_create_replica_without_replication_connection(self):
         self.p.config._config['create_replica_method'] = []
-        self.assertFalse(self.p.can_create_replica_without_replication_connection())
+        self.assertFalse(self.p.can_create_replica_without_replication_connection(None))
         self.p.config._config['create_replica_method'] = ['wale', 'basebackup']
         self.p.config._config['wale'] = {'command': 'foo', 'no_leader': 1}
-        self.assertTrue(self.p.can_create_replica_without_replication_connection())
+        self.assertTrue(self.p.can_create_replica_without_replication_connection(None))
 
     def test_replica_method_can_work_without_replication_connection(self):
         self.assertFalse(self.p.replica_method_can_work_without_replication_connection('basebackup'))
         self.assertFalse(self.p.replica_method_can_work_without_replication_connection('foobar'))
         self.p.config._config['foo'] = {'command': 'bar', 'no_leader': 1}
         self.assertTrue(self.p.replica_method_can_work_without_replication_connection('foo'))
         self.p.config._config['foo'] = {'command': 'bar'}
@@ -640,20 +667,21 @@
 
         with patch.object(Postgresql, 'check_startup_state_changed', Mock(return_value=False)):
             self.p.cancellable.cancel()
             self.p._state = 'starting'
             self.assertIsNone(self.p.wait_for_startup())
 
     def test_get_server_parameters(self):
-        config = {'synchronous_mode': True, 'parameters': {'wal_level': 'hot_standby'}, 'listen': '0'}
+        config = {'parameters': {'wal_level': 'hot_standby'}, 'listen': '0'}
+        self.p._global_config = GlobalConfig({'synchronous_mode': True})
         self.p.config.get_server_parameters(config)
-        config['synchronous_mode_strict'] = True
+        self.p._global_config = GlobalConfig({'synchronous_mode': True, 'synchronous_mode_strict': True})
         self.p.config.get_server_parameters(config)
         self.p.config.set_synchronous_standby_names('foo')
-        self.assertTrue(str(self.p.config.get_server_parameters(config)).startswith('{'))
+        self.assertTrue(str(self.p.config.get_server_parameters(config)).startswith('<CaseInsensitiveDict'))
 
     @patch('time.sleep', Mock())
     def test__wait_for_connection_close(self):
         mock_postmaster = MockPostmaster()
         with patch.object(Postgresql, 'is_running', Mock(return_value=mock_postmaster)):
             mock_postmaster.is_running.side_effect = [True, False, False]
             mock_callback = Mock()
@@ -714,7 +742,216 @@
     def test_set_enforce_hot_standby_feedback(self):
         self.p.set_enforce_hot_standby_feedback(True)
 
     @patch.object(Postgresql, 'major_version', PropertyMock(return_value=140000))
     @patch.object(Postgresql, '_cluster_info_state_get', Mock(return_value=True))
     def test_handle_parameter_change(self):
         self.p.handle_parameter_change()
+
+    def test_validator_factory(self):
+        # validator with no type
+        validator = {
+            'version_from': 90300,
+            'version_till': None,
+        }
+        with self.assertRaises(ValidatorFactoryNoType) as e:
+            ValidatorFactory(validator)
+        self.assertEqual(str(e.exception), 'Validator contains no type.')
+
+        # validator with invalid type
+        validator = {
+            'type': 'Random',
+            'version_from': 90300,
+            'version_till': None,
+        }
+        with self.assertRaises(ValidatorFactoryInvalidType) as e:
+            ValidatorFactory(validator)
+        self.assertEqual(str(e.exception), f'Unexpected validator type: `{validator["type"]}`.')
+
+        # validator with missing attributes
+        validator = {
+            'type': 'Integer',
+            'version_from': 90300,
+            'min_val': 0,
+        }
+        with self.assertRaises(ValidatorFactoryInvalidSpec) as e:
+            ValidatorFactory(validator)
+        type_ = validator.pop('type')
+        self.assertRegex(
+            str(e.exception),
+            rf"Failed to parse `{type_}` validator \(`{validator}`\): `(Number\.)?__init__\(\) missing 1 "
+            "required keyword-only argument: 'max_val'`."
+        )
+
+        # valid validators
+        # Bool
+        validator = {
+            'type': 'Bool',
+            'version_from': 90300,
+            'version_till': None,
+        }
+        ret = ValidatorFactory(validator)
+        self.assertIsInstance(ret, Bool)
+        self.assertEqual(
+            ret.__dict__,
+            Bool(version_from=validator['version_from'], version_till=validator['version_till']).__dict__,
+        )
+
+        # Integer
+        validator = {
+            'type': 'Integer',
+            'version_from': 90300,
+            'version_till': None,
+            'min_val': 1,
+            'max_val': 100,
+            'unit': None,
+        }
+        ret = ValidatorFactory(validator)
+        self.assertIsInstance(ret, Integer)
+        self.assertEqual(
+            ret.__dict__,
+            Integer(version_from=validator['version_from'], version_till=validator['version_till'],
+                    min_val=validator['min_val'], max_val=validator['max_val'], unit=validator['unit']).__dict__,
+        )
+
+        # Real
+        validator = {
+            'type': 'Real',
+            'version_from': 90300,
+            'version_till': None,
+            'min_val': 1.0,
+            'max_val': 100.0,
+            'unit': None,
+        }
+        ret = ValidatorFactory(validator)
+        self.assertIsInstance(ret, Real)
+        self.assertEqual(
+            ret.__dict__,
+            Real(version_from=validator['version_from'], version_till=validator['version_till'],
+                 min_val=validator['min_val'], max_val=validator['max_val'], unit=validator['unit']).__dict__,
+        )
+
+        # Enum
+        validator = {
+            'type': 'Enum',
+            'version_from': 90300,
+            'version_till': None,
+            'possible_values': ('abc', 'def'),
+        }
+        ret = ValidatorFactory(validator)
+        self.assertIsInstance(ret, Enum)
+        self.assertEqual(
+            ret.__dict__,
+            Enum(version_from=validator['version_from'], version_till=validator['version_till'],
+                 possible_values=validator['possible_values']).__dict__,
+        )
+
+        # EnumBool
+        validator = {
+            'type': 'EnumBool',
+            'version_from': 90300,
+            'version_till': None,
+            'possible_values': ('abc', 'def'),
+        }
+        ret = ValidatorFactory(validator)
+        self.assertIsInstance(ret, EnumBool)
+        self.assertEqual(
+            ret.__dict__,
+            EnumBool(version_from=validator['version_from'], version_till=validator['version_till'],
+                     possible_values=validator['possible_values']).__dict__,
+        )
+
+        # String
+        validator = {
+            'type': 'String',
+            'version_from': 90300,
+            'version_till': None,
+        }
+        ret = ValidatorFactory(validator)
+        self.assertIsInstance(ret, String)
+        self.assertEqual(
+            ret.__dict__,
+            String(version_from=validator['version_from'], version_till=validator['version_till']).__dict__,
+        )
+
+    def test__get_postgres_guc_validators(self):
+        # normal run
+        parameter = 'my_parameter'
+
+        config = {
+            parameter: [{
+                'type': 'Bool',
+                'version_from': 90300,
+                'version_till': 90500,
+            }, {
+                'type': 'EnumBool',
+                'version_from': 90500,
+                'version_till': 90600,
+                'possible_values': [
+                    'always',
+                ],
+            }]
+        }
+        ret = _get_postgres_guc_validators(config, parameter)
+        self.assertIsInstance(ret, tuple)
+        self.assertEqual(len(ret), 2)
+        self.assertIsInstance(ret[0], Bool)
+        self.assertIsInstance(ret[1], EnumBool)
+
+        # log exceptions
+        del config[parameter][0]['type']
+
+        with patch('patroni.postgresql.validator.logger.warning') as mock_logger:
+            ret = _get_postgres_guc_validators(config, parameter)
+            self.assertIsInstance(ret, tuple)
+            self.assertEqual(len(ret), 1)
+            self.assertIsInstance(ret[0], EnumBool)
+
+            mock_logger.assert_called_once()
+            mock_call = mock_logger.call_args[0]
+            self.assertEqual(mock_call[0], 'Faced an issue while parsing a validator for parameter `%s`: `%r`')
+            self.assertEqual(mock_call[1], parameter)
+            self.assertIsInstance(mock_call[2], ValidatorFactoryNoType)
+
+    def test__read_postgres_gucs_validators_file(self):
+        # raise exception
+        with self.assertRaises(InvalidGucValidatorsFile) as exc:
+            _read_postgres_gucs_validators_file('random_file.yaml')
+        self.assertEqual(
+            str(exc.exception),
+            "Unexpected issue while reading parameters file `random_file.yaml`: `[Errno 2] No such file or directory: "
+            "'random_file.yaml'`."
+        )
+
+    def test__load_postgres_gucs_validators(self):
+        # log messages
+        with patch('os.walk', Mock(return_value=iter([('.', [], ['file.txt', 'random.yaml'])]))), \
+             patch('patroni.postgresql.validator.logger.info') as mock_info, \
+             patch('patroni.postgresql.validator.logger.warning') as mock_warning:
+            _load_postgres_gucs_validators()
+            mock_info.assert_called_once_with('Ignored a non-YAML file found under `available_parameters` directory: '
+                                              '`%s`.', os.path.join('.', 'file.txt'))
+            mock_warning.assert_called_once()
+            self.assertIn(
+                "Unexpected issue while reading parameters file `{0}`: `[Errno 2] No such file or "
+                "directory:".format(os.path.join('.', 'random.yaml')),
+                mock_warning.call_args[0][0]
+            )
+
+
+@patch('subprocess.call', Mock(return_value=0))
+@patch('patroni.psycopg.connect', psycopg_connect)
+class TestPostgresql2(BaseTestPostgresql):
+
+    @patch('subprocess.call', Mock(return_value=0))
+    @patch('os.rename', Mock())
+    @patch('patroni.postgresql.CallbackExecutor', Mock())
+    @patch.object(Postgresql, 'get_major_version', Mock(return_value=140000))
+    @patch.object(Postgresql, 'is_running', Mock(return_value=True))
+    def setUp(self):
+        super(TestPostgresql2, self).setUp()
+
+    @patch('subprocess.check_output', Mock(return_value='\n'.join(mock_available_gucs.return_value).encode('utf-8')))
+    def test_available_gucs(self):
+        gucs = self.p.available_gucs
+        self.assertIsInstance(gucs, CaseInsensitiveSet)
+        self.assertEqual(gucs, mock_available_gucs.return_value)
```

### Comparing `patroni-3.0.2/tests/test_postmaster.py` & `patroni-3.0.3/tests/test_postmaster.py`

 * *Files identical despite different names*

### Comparing `patroni-3.0.2/tests/test_raft.py` & `patroni-3.0.3/tests/test_raft.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 import os
 import unittest
 import tempfile
 import time
 
 from mock import Mock, PropertyMock, patch
 from patroni.dcs.raft import Cluster, DynMemberSyncObj, KVStoreTTL,\
-        Raft, RaftError, SyncObjUtility, TCPTransport, _TCPTransport
+    Raft, RaftError, SyncObjUtility, TCPTransport, _TCPTransport
 from pysyncobj import SyncObjConf, FAIL_REASON
 
 
 def remove_files(prefix):
     for f in ('journal', 'journal.meta', 'dump'):
         f = prefix + f
         if os.path.isfile(f):
@@ -134,23 +134,24 @@
         raft.reload_config({'retry_timeout': 20, 'ttl': 60, 'loop_wait': 10})
         self.assertTrue(raft._sync_obj.set(raft.members_path + 'legacy', '{"version":"2.0.0"}'))
         self.assertTrue(raft.touch_member(''))
         self.assertTrue(raft.initialize())
         self.assertTrue(raft.cancel_initialization())
         self.assertTrue(raft.set_config_value('{}'))
         self.assertTrue(raft.write_sync_state('foo', 'bar'))
+        self.assertFalse(raft.write_sync_state('foo', 'bar', 1))
         raft._citus_group = '1'
         self.assertTrue(raft.manual_failover('foo', 'bar'))
         raft._citus_group = '0'
         cluster = raft.get_cluster()
         self.assertIsInstance(cluster, Cluster)
         self.assertIsInstance(cluster.workers[1], Cluster)
         self.assertTrue(raft._sync_obj.set(raft.status_path, '{"optime":1234567,"slots":{"ls":12345}}'))
-        raft.get_cluster()
-        self.assertTrue(raft.update_leader('1', failsafe={'foo': 'bat'}))
+        leader = raft.get_cluster().leader
+        self.assertTrue(raft.update_leader(leader, '1', failsafe={'foo': 'bat'}))
         self.assertTrue(raft._sync_obj.set(raft.failsafe_path, '{"foo"}'))
         self.assertTrue(raft._sync_obj.set(raft.status_path, '{'))
         raft.get_citus_coordinator()
         self.assertTrue(raft.delete_sync_state())
         self.assertTrue(raft.delete_leader())
         self.assertTrue(raft.set_history_value(''))
         self.assertTrue(raft.delete_cluster())
```

### Comparing `patroni-3.0.2/tests/test_raft_controller.py` & `patroni-3.0.3/tests/test_raft_controller.py`

 * *Files identical despite different names*

### Comparing `patroni-3.0.2/tests/test_rewind.py` & `patroni-3.0.3/tests/test_rewind.py`

 * *Files 1% similar despite different names*

```diff
@@ -16,16 +16,16 @@
     def start(self):
         self._target(*self._args)
 
 
 def mock_cancellable_call(*args, **kwargs):
     communicate = kwargs.pop('communicate', None)
     if isinstance(communicate, dict):
-        communicate.update(stdout=b'', stderr=b'pg_rewind: error: could not open file ' +
-                                              b'"data/postgresql0/pg_xlog/000000010000000000000003": No such file')
+        communicate.update(stdout=b'', stderr=b'pg_rewind: error: could not open file '
+                           + b'"data/postgresql0/pg_xlog/000000010000000000000003": No such file')
     return 1
 
 
 def mock_cancellable_call0(*args, **kwargs):
     communicate = kwargs.pop('communicate', None)
     if isinstance(communicate, dict):
         communicate.update(stdout=b'', stderr=b'')
@@ -218,37 +218,37 @@
         self.r.cleanup_archive_status()
 
     @patch('os.path.isfile', Mock(return_value=True))
     @patch('shutil.move', Mock(side_effect=OSError))
     @patch('patroni.postgresql.rewind.logger.info')
     def test_archive_ready_wals(self, mock_logger_info):
         with patch('os.listdir', Mock(side_effect=OSError)), \
-              patch.object(Postgresql, 'get_guc_value', Mock(side_effect=['on', 'command %f'])):
+             patch.object(Postgresql, 'get_guc_value', Mock(side_effect=['on', 'command %f'])):
             self.r._archive_ready_wals()
             mock_logger_info.assert_not_called()
 
         # each assert_not_called() calls get_guc_value('archive_mode') + get_guc_value('archive_command')
         get_guc_value_res = [
             '', 'command %f',
             'on', '',
         ]
         with patch.object(Postgresql, 'get_guc_value', Mock(side_effect=get_guc_value_res)):
-            for _ in range(len(get_guc_value_res)//2):
+            for _ in range(len(get_guc_value_res) // 2):
                 self.r._archive_ready_wals()
                 mock_logger_info.assert_not_called()
 
         with patch('os.listdir', Mock(return_value=['000000000000000000000000.ready'])):
             # successful archive_command call
-            with patch.object(CancellableSubprocess, 'call',  Mock(return_value=0)):
+            with patch.object(CancellableSubprocess, 'call', Mock(return_value=0)):
                 get_guc_value_res = [
                     'on', 'command %f',
                     'always', 'command %f',
                 ]
                 with patch.object(Postgresql, 'get_guc_value', Mock(side_effect=get_guc_value_res)):
-                    for _ in range(len(get_guc_value_res)//2):
+                    for _ in range(len(get_guc_value_res) // 2):
                         self.r._archive_ready_wals()
                         mock_logger_info.assert_called_once()
                         self.assertEqual(('Trying to archive %s: %s',
                                           '000000000000000000000000', 'command 000000000000000000000000'),
                                          mock_logger_info.call_args[0])
                         mock_logger_info.reset_mock()
```

### Comparing `patroni-3.0.2/tests/test_slots.py` & `patroni-3.0.3/tests/test_slots.py`

 * *Files 2% similar despite different names*

```diff
@@ -107,14 +107,16 @@
             self.assertEqual(self.s.sync_replication_slots(self.cluster, False), ['ls'])
 
     def test_copy_logical_slots(self):
         self.cluster.config.data['slots']['ls']['database'] = 'b'
         self.s.copy_logical_slots(self.cluster, ['ls'])
         with patch.object(MockCursor, 'execute', Mock(side_effect=psycopg.OperationalError)):
             self.s.copy_logical_slots(self.cluster, ['foo'])
+        with patch.object(Cluster, 'leader', PropertyMock(return_value=None)):
+            self.s.copy_logical_slots(self.cluster, ['foo'])
 
     @patch.object(Postgresql, 'stop', Mock(return_value=True))
     @patch.object(Postgresql, 'start', Mock(return_value=True))
     @patch.object(Postgresql, 'is_leader', Mock(return_value=False))
     def test_check_logical_slots_readiness(self):
         self.s.copy_logical_slots(self.cluster, ['ls'])
         with patch.object(MockCursor, '__iter__', Mock(return_value=iter([('postgresql0', None)]))),\
```

### Comparing `patroni-3.0.2/tests/test_sync.py` & `patroni-3.0.3/tests/test_sync.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,29 +1,34 @@
 import os
 
 from mock import Mock, patch
 
+from patroni.collections import CaseInsensitiveSet
+from patroni.config import GlobalConfig
 from patroni.dcs import Cluster, SyncState
 from patroni.postgresql import Postgresql
 
-from . import BaseTestPostgresql, psycopg_connect
+from . import BaseTestPostgresql, psycopg_connect, mock_available_gucs
 
 
 @patch('subprocess.call', Mock(return_value=0))
 @patch('patroni.psycopg.connect', psycopg_connect)
+@patch.object(Postgresql, 'available_gucs', mock_available_gucs)
 class TestSync(BaseTestPostgresql):
 
     @patch('subprocess.call', Mock(return_value=0))
     @patch('os.rename', Mock())
     @patch('patroni.postgresql.CallbackExecutor', Mock())
     @patch.object(Postgresql, 'get_major_version', Mock(return_value=140000))
     @patch.object(Postgresql, 'is_running', Mock(return_value=True))
+    @patch.object(Postgresql, 'available_gucs', mock_available_gucs)
     def setUp(self):
         super(TestSync, self).setUp()
         self.p.config.write_postgresql_conf()
+        self.p._global_config = GlobalConfig({'synchronous_mode': True})
         self.s = self.p.sync_handler
 
     @patch.object(Postgresql, 'last_operation', Mock(return_value=1))
     def test_pick_sync_standby(self):
         cluster = Cluster(True, None, self.leader, 0, [self.me, self.other, self.leadermem], None,
                           SyncState(0, self.me.name, self.leadermem.name), None, None, None)
 
@@ -31,59 +36,67 @@
             {'pid': 100, 'application_name': self.leadermem.name, 'sync_state': 'sync', 'flush_lsn': 1},
             {'pid': 101, 'application_name': self.me.name, 'sync_state': 'async', 'flush_lsn': 2},
             {'pid': 102, 'application_name': self.other.name, 'sync_state': 'async', 'flush_lsn': 2}]
 
         # sync node is a bit behind of async, but we prefer it anyway
         with patch.object(Postgresql, "_cluster_info_state_get", side_effect=[self.leadermem.name,
                                                                               'on', pg_stat_replication]):
-            self.assertEqual(self.s.current_state(cluster), ([self.leadermem.name], [self.leadermem.name]))
+            self.assertEqual(self.s.current_state(cluster), (CaseInsensitiveSet([self.leadermem.name]),
+                                                             CaseInsensitiveSet([self.leadermem.name])))
 
         # prefer node with sync_state='potential', even if it is slightly behind of async
         pg_stat_replication[0]['sync_state'] = 'potential'
         for r in pg_stat_replication:
             r['write_lsn'] = r.pop('flush_lsn')
         with patch.object(Postgresql, "_cluster_info_state_get", side_effect=['', 'remote_write', pg_stat_replication]):
-            self.assertEqual(self.s.current_state(cluster), ([self.leadermem.name], []))
+            self.assertEqual(self.s.current_state(cluster), (CaseInsensitiveSet([self.leadermem.name]),
+                                                             CaseInsensitiveSet()))
 
         # when there are no sync or potential candidates we pick async with the minimal replication lag
         for i, r in enumerate(pg_stat_replication):
             r.update(replay_lsn=3 - i, application_name=r['application_name'].upper())
         missing = pg_stat_replication.pop(0)
         with patch.object(Postgresql, "_cluster_info_state_get", side_effect=['', 'remote_apply', pg_stat_replication]):
-            self.assertEqual(self.s.current_state(cluster), ([self.me.name], []))
+            self.assertEqual(self.s.current_state(cluster), (CaseInsensitiveSet([self.me.name]), CaseInsensitiveSet()))
 
         # unknown sync node is ignored
         missing.update(application_name='missing', sync_state='sync')
         pg_stat_replication.insert(0, missing)
         with patch.object(Postgresql, "_cluster_info_state_get", side_effect=['', 'remote_apply', pg_stat_replication]):
-            self.assertEqual(self.s.current_state(cluster), ([self.me.name], []))
+            self.assertEqual(self.s.current_state(cluster), (CaseInsensitiveSet([self.me.name]), CaseInsensitiveSet()))
 
         # invalid synchronous_standby_names and empty pg_stat_replication
         with patch.object(Postgresql, "_cluster_info_state_get", side_effect=['a b', 'remote_apply', None]):
             self.p._major_version = 90400
-            self.assertEqual(self.s.current_state(cluster), ([], []))
+            self.assertEqual(self.s.current_state(cluster), (CaseInsensitiveSet(), CaseInsensitiveSet()))
 
     def test_set_sync_standby(self):
         def value_in_conf():
             with open(os.path.join(self.p.data_dir, 'postgresql.conf')) as f:
                 for line in f:
                     if line.startswith('synchronous_standby_names'):
                         return line.strip()
 
         mock_reload = self.p.reload = Mock()
-        self.s.set_synchronous_standby_names(['n1'])
+        self.s.set_synchronous_standby_names(CaseInsensitiveSet(['n1']))
         self.assertEqual(value_in_conf(), "synchronous_standby_names = 'n1'")
         mock_reload.assert_called()
 
         mock_reload.reset_mock()
-        self.s.set_synchronous_standby_names(['n1'])
+        self.s.set_synchronous_standby_names(CaseInsensitiveSet(['n1']))
         mock_reload.assert_not_called()
         self.assertEqual(value_in_conf(), "synchronous_standby_names = 'n1'")
 
-        self.s.set_synchronous_standby_names(['n1', 'n2'])
+        self.s.set_synchronous_standby_names(CaseInsensitiveSet(['n1', 'n2']))
         mock_reload.assert_called()
         self.assertEqual(value_in_conf(), "synchronous_standby_names = '2 (n1,n2)'")
 
         mock_reload.reset_mock()
-        self.s.set_synchronous_standby_names([])
+        self.s.set_synchronous_standby_names(CaseInsensitiveSet([]))
         mock_reload.assert_called()
         self.assertEqual(value_in_conf(), None)
+
+        mock_reload.reset_mock()
+        self.p._global_config = GlobalConfig({'synchronous_mode': True})
+        self.s.set_synchronous_standby_names(CaseInsensitiveSet('*'))
+        mock_reload.assert_called()
+        self.assertEqual(value_in_conf(), "synchronous_standby_names = '*'")
```

### Comparing `patroni-3.0.2/tests/test_utils.py` & `patroni-3.0.3/tests/test_utils.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 import unittest
 
 from mock import Mock, patch
+
 from patroni.exceptions import PatroniException
-from patroni.utils import Retry, RetryFailedError, enable_keepalive, polling_loop, validate_directory
+from patroni.utils import Retry, RetryFailedError, enable_keepalive, polling_loop, validate_directory, unquote
 
 
 class TestUtils(unittest.TestCase):
 
     def test_polling_loop(self):
         self.assertEqual(list(polling_loop(0.001, interval=0.001)), [0])
 
@@ -31,20 +32,45 @@
     @patch('os.path.exists', Mock(return_value=True))
     @patch('os.path.isdir', Mock(return_value=False))
     def test_validate_directory_is_not_a_directory(self):
         self.assertRaises(PatroniException, validate_directory, "/tmp")
 
     def test_enable_keepalive(self):
         with patch('socket.SIO_KEEPALIVE_VALS', 1, create=True):
-            self.assertIsNotNone(enable_keepalive(Mock(), 10, 5))
+            self.assertIsNone(enable_keepalive(Mock(), 10, 5))
         with patch('socket.SIO_KEEPALIVE_VALS', None, create=True):
             for platform in ('linux2', 'darwin', 'other'):
                 with patch('sys.platform', platform):
                     self.assertIsNone(enable_keepalive(Mock(), 10, 5))
 
+    def test_unquote(self):
+        self.assertEqual(unquote('value'), 'value')
+        self.assertEqual(unquote('value with spaces'), "value with spaces")
+        self.assertEqual(unquote(
+            '"double quoted value"'),
+            'double quoted value')
+        self.assertEqual(unquote(
+            '\'single quoted value\''),
+            'single quoted value')
+        self.assertEqual(unquote(
+            'value "with" double quotes'),
+            'value "with" double quotes')
+        self.assertEqual(unquote(
+            '"value starting with" double quotes'),
+            '"value starting with" double quotes')
+        self.assertEqual(unquote(
+            '\'value starting with\' single quotes'),
+            '\'value starting with\' single quotes')
+        self.assertEqual(unquote(
+            'value with a \' single quote'),
+            'value with a \' single quote')
+        self.assertEqual(unquote(
+            '\'value with a \'"\'"\' single quote\''),
+            'value with a \' single quote')
+
 
 @patch('time.sleep', Mock())
 class TestRetrySleeper(unittest.TestCase):
 
     @staticmethod
     def _fail(times=1):
         scope = dict(times=0)
```

### Comparing `patroni-3.0.2/tests/test_validator.py` & `patroni-3.0.3/tests/test_validator.py`

 * *Files 12% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 import socket
 import tempfile
 import unittest
 
 from io import StringIO
 from mock import Mock, patch, mock_open
 from patroni.dcs import dcs_modules
-from patroni.validator import schema
+from patroni.validator import schema, Directory, Schema
 
 available_dcs = [m.split(".")[-1] for m in dcs_modules()]
 config = {
     "name": "string",
     "scope": "string",
     "restapi": {
         "listen": "127.0.0.2:800",
@@ -19,16 +19,15 @@
     },
     "bootstrap": {
         "dcs": {
             "ttl": 1000,
             "loop_wait": 1000,
             "retry_timeout": 1000,
             "maximum_lag_on_failover": 1000
-            },
-        "pg_hba": ["string"],
+        },
         "initdb": ["string", {"key": "value"}]
     },
     "consul": {
         "host": "127.0.0.1:5000"
     },
     "etcd": {
         "hosts": "127.0.0.1:2379,127.0.0.1:2380"
@@ -45,15 +44,15 @@
         "self_addr": "127.0.0.1:2222",
         "bind_addr": "0.0.0.0:2222",
         "partner_addrs": ["127.0.0.1:2223", "127.0.0.1:2224"],
         "data_dir": "/",
         "password": "12345"
     },
     "zookeeper": {
-        "hosts":  "127.0.0.1:3379,127.0.0.1:3380"
+        "hosts": "127.0.0.1:3379,127.0.0.1:3380"
     },
     "kubernetes": {
         "namespace": "string",
         "labels": {},
         "scope_label": "string",
         "role_label": "string",
         "use_endpoints": False,
@@ -81,21 +80,31 @@
         "use_pg_rewind": False
     },
     "watchdog": {
         "mode": "off",
         "device": "string"
     },
     "tags": {
-      "nofailover": False,
-      "clonefrom": False,
-      "noloadbalance": False,
-      "nosync": False
+        "nofailover": False,
+        "clonefrom": False,
+        "noloadbalance": False,
+        "nosync": False
     }
 }
 
+config_2 = {
+    "some_dir": "very_interesting_dir"
+}
+
+schema2 = Schema({
+    "some_dir": Directory(contains=["very_interesting_subdir", "another_interesting_subdir"])
+})
+
+required_binaries = ["pg_ctl", "initdb", "pg_controldata", "pg_basebackup", "postgres", "pg_isready"]
+
 directories = []
 files = []
 binaries = []
 
 
 def isfile_side_effect(arg):
     return arg in files
@@ -186,67 +195,114 @@
         c["postgresql"]["listen"] = "127.0.0.1:5432"
         with patch('patroni.validator.open', mock_open(read_data='9')):
             errors = schema(c)
         output = "\n".join(errors)
         self.assertEqual(['consul.host', 'etcd.host', 'postgresql.bin_dir', 'postgresql.data_dir', 'postgresql.listen',
                           'raft.bind_addr', 'raft.self_addr', 'restapi.connect_address'], parse_output(output))
 
+    def test_bin_dir_is_empty_string_excutables_in_path(self, mock_out, mock_err):
+        binaries.extend(required_binaries)
+        c = copy.deepcopy(config)
+        c["postgresql"]["bin_dir"] = ""
+        errors = schema(c)
+        output = "\n".join(errors)
+        self.assertEqual(['raft.bind_addr', 'raft.self_addr'], parse_output(output))
+
     @patch('subprocess.check_output', Mock(return_value=b"postgres (PostgreSQL) 12.1"))
     def test_data_dir_contains_pg_version(self, mock_out, mock_err):
         directories.append(config["postgresql"]["data_dir"])
         directories.append(config["postgresql"]["bin_dir"])
         directories.append(os.path.join(config["postgresql"]["data_dir"], "pg_wal"))
         files.append(os.path.join(config["postgresql"]["data_dir"], "global", "pg_control"))
         files.append(os.path.join(config["postgresql"]["data_dir"], "PG_VERSION"))
-        binaries.append(os.path.join(config["postgresql"]["bin_dir"], "pg_ctl"))
-        binaries.append(os.path.join(config["postgresql"]["bin_dir"], "initdb"))
-        binaries.append(os.path.join(config["postgresql"]["bin_dir"], "pg_controldata"))
-        binaries.append(os.path.join(config["postgresql"]["bin_dir"], "pg_basebackup"))
-        binaries.append(os.path.join(config["postgresql"]["bin_dir"], "postgres"))
-        binaries.append(os.path.join(config["postgresql"]["bin_dir"], "pg_isready"))
+        binaries.extend(required_binaries)
+        c = copy.deepcopy(config)
+        c["postgresql"]["bin_dir"] = ""  # to cover postgres --version call from PATH
         with patch('patroni.validator.open', mock_open(read_data='12')):
-            errors = schema(config)
+            errors = schema(c)
         output = "\n".join(errors)
         self.assertEqual(['raft.bind_addr', 'raft.self_addr'], parse_output(output))
 
     @patch('subprocess.check_output', Mock(return_value=b"postgres (PostgreSQL) 12.1"))
     def test_pg_version_missmatch(self, mock_out, mock_err):
         directories.append(config["postgresql"]["data_dir"])
         directories.append(config["postgresql"]["bin_dir"])
         directories.append(os.path.join(config["postgresql"]["data_dir"], "pg_wal"))
         files.append(os.path.join(config["postgresql"]["data_dir"], "global", "pg_control"))
         files.append(os.path.join(config["postgresql"]["data_dir"], "PG_VERSION"))
+        binaries.extend([os.path.join(config["postgresql"]["bin_dir"], i) for i in required_binaries])
         c = copy.deepcopy(config)
         c["etcd"]["hosts"] = []
         c["postgresql"]["listen"] = '127.0.0.2,*:543'
-        del c["postgresql"]["bin_dir"]
         with patch('patroni.validator.open', mock_open(read_data='11')):
             errors = schema(c)
         output = "\n".join(errors)
         self.assertEqual(['etcd.hosts', 'postgresql.data_dir', 'postgresql.listen',
                           'raft.bind_addr', 'raft.self_addr'], parse_output(output))
 
     @patch('subprocess.check_output', Mock(return_value=b"postgres (PostgreSQL) 12.1"))
     def test_pg_wal_doesnt_exist(self, mock_out, mock_err):
+        binaries.extend([os.path.join(config["postgresql"]["bin_dir"], i) for i in required_binaries])
         directories.append(config["postgresql"]["data_dir"])
         directories.append(config["postgresql"]["bin_dir"])
         files.append(os.path.join(config["postgresql"]["data_dir"], "global", "pg_control"))
         files.append(os.path.join(config["postgresql"]["data_dir"], "PG_VERSION"))
         c = copy.deepcopy(config)
-        del c["postgresql"]["bin_dir"]
         with patch('patroni.validator.open', mock_open(read_data='11')):
             errors = schema(c)
         output = "\n".join(errors)
         self.assertEqual(['postgresql.data_dir', 'raft.bind_addr', 'raft.self_addr'], parse_output(output))
 
     def test_data_dir_is_empty_string(self, mock_out, mock_err):
+        binaries.extend(required_binaries)
         directories.append(config["postgresql"]["data_dir"])
         directories.append(config["postgresql"]["bin_dir"])
         c = copy.deepcopy(config)
         c["kubernetes"] = False
         c["postgresql"]["pg_hba"] = ""
         c["postgresql"]["data_dir"] = ""
         c["postgresql"]["bin_dir"] = ""
         errors = schema(c)
         output = "\n".join(errors)
-        self.assertEqual(['kubernetes', 'postgresql.bin_dir', 'postgresql.data_dir',
+        self.assertEqual(['kubernetes', 'postgresql.data_dir',
                           'postgresql.pg_hba', 'raft.bind_addr', 'raft.self_addr'], parse_output(output))
+
+    def test_directory_contains(self, mock_out, mock_err):
+        directories.extend([config_2["some_dir"], os.path.join(config_2["some_dir"], "very_interesting_subdir")])
+        errors = schema2(config_2)
+        output = "\n".join(errors)
+        self.assertEqual(['some_dir'], parse_output(output))
+
+    def test_validate_binary_name(self, mock_out, mock_err):
+        r = copy.copy(required_binaries)
+        r.remove('postgres')
+        r.append('fake-postgres')
+        binaries.extend(r)
+        c = copy.deepcopy(config)
+        c["postgresql"]["bin_name"] = {"postgres": "fake-postgres"}
+        del c["postgresql"]["bin_dir"]
+        errors = schema(c)
+        output = "\n".join(errors)
+        self.assertEqual(['raft.bind_addr', 'raft.self_addr'], parse_output(output))
+
+    def test_validate_binary_name_missing(self, mock_out, mock_err):
+        r = copy.copy(required_binaries)
+        r.remove('postgres')
+        binaries.extend(r)
+        c = copy.deepcopy(config)
+        c["postgresql"]["bin_name"] = {"postgres": "fake-postgres"}
+        del c["postgresql"]["bin_dir"]
+        errors = schema(c)
+        output = "\n".join(errors)
+        self.assertEqual(['postgresql.bin_dir', 'postgresql.bin_name.postgres', 'raft.bind_addr', 'raft.self_addr'],
+                         parse_output(output))
+
+    def test_validate_binary_name_empty_string(self, mock_out, mock_err):
+        r = copy.copy(required_binaries)
+        binaries.extend(r)
+        c = copy.deepcopy(config)
+        c["postgresql"]["bin_name"] = {"postgres": ""}
+        del c["postgresql"]["bin_dir"]
+        errors = schema(c)
+        output = "\n".join(errors)
+        self.assertEqual(['postgresql.bin_dir', 'postgresql.bin_name.postgres', 'raft.bind_addr', 'raft.self_addr'],
+                         parse_output(output))
```

### Comparing `patroni-3.0.2/tests/test_wale_restore.py` & `patroni-3.0.3/tests/test_wale_restore.py`

 * *Files 1% similar despite different names*

```diff
@@ -119,15 +119,15 @@
 
         with patch.object(WALERestore, 'run', Mock(return_value=0)):
             self.assertEqual(_main(), 0)
 
         with patch.object(WALERestore, 'run', Mock(return_value=1)), \
                 patch('time.sleep', mock_sleep):
             self.assertEqual(_main(), 1)
-            self.assertTrue(sleeps[0], WALE_TEST_RETRIES)
+            self.assertEqual(sleeps[0], WALE_TEST_RETRIES)
 
     @patch('os.path.isfile', Mock(return_value=True))
     def test_get_major_version(self):
         with patch('builtins.open', mock_open(read_data='9.4')):
             self.assertEqual(get_major_version("data"), 9.4)
         with patch('builtins.open', side_effect=OSError):
             self.assertEqual(get_major_version("data"), 0.0)
```

### Comparing `patroni-3.0.2/tests/test_watchdog.py` & `patroni-3.0.3/tests/test_watchdog.py`

 * *Files 2% similar despite different names*

```diff
@@ -33,15 +33,15 @@
     assert 0 < fd < len(mock_devices)
     dev = mock_devices[fd]
     sys.stderr.write("Ioctl %d %d %r\n" % (fd, op, arg))
     if op == linuxwd.WDIOC_GETSUPPORT:
         sys.stderr.write("Get support\n")
         assert (mutate_flag is True)
         arg.options = sum(map(linuxwd.WDIOF.get, ['SETTIMEOUT', 'KEEPALIVEPING']))
-        arg.identity = (ctypes.c_ubyte*32)(*map(ord, 'Mock Watchdog'))
+        arg.identity = (ctypes.c_ubyte * 32)(*map(ord, 'Mock Watchdog'))
     elif op == linuxwd.WDIOC_GETTIMEOUT:
         arg.value = dev.timeout
     elif op == linuxwd.WDIOC_SETTIMEOUT:
         sys.stderr.write("Set timeout called with %s\n" % arg.value)
         assert 0 < arg.value < 65535
         dev.timeout = arg.value - 1
     else:
@@ -106,14 +106,19 @@
         self.assertTrue(device.open)
 
         self.assertEqual(device.timeout, 24)
 
         watchdog.keepalive()
         self.assertEqual(len(device.writes), 1)
 
+        watchdog.impl._fd, fd = None, watchdog.impl._fd
+        watchdog.keepalive()
+        self.assertEqual(len(device.writes), 1)
+        watchdog.impl._fd = fd
+
         watchdog.disable()
         self.assertFalse(device.open)
         self.assertEqual(device.writes[-1], b'V')
 
     def test_invalid_timings(self):
         watchdog = Watchdog({'ttl': 30, 'loop_wait': 20, 'watchdog': {'mode': 'automatic', 'safety_margin': -1}})
         watchdog.activate()
```

### Comparing `patroni-3.0.2/tests/test_zookeeper.py` & `patroni-3.0.3/tests/test_zookeeper.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,19 +4,20 @@
 from kazoo.client import KazooClient, KazooState
 from kazoo.exceptions import NoNodeError, NodeExistsError
 from kazoo.handlers.threading import SequentialThreadingHandler
 from kazoo.protocol.states import KeeperState, ZnodeStat
 from kazoo.retry import RetryFailedError
 from mock import Mock, PropertyMock, patch
 from patroni.dcs.zookeeper import Cluster, Leader, PatroniKazooClient,\
-        PatroniSequentialThreadingHandler, ZooKeeper, ZooKeeperError
+    PatroniSequentialThreadingHandler, ZooKeeper, ZooKeeperError
 
 
 class MockKazooClient(Mock):
 
+    handler = PatroniSequentialThreadingHandler(10)
     leader = False
     exists = True
 
     def __init__(self, *args, **kwargs):
         super(MockKazooClient, self).__init__()
         self._session_timeout = 30000
 
@@ -150,19 +151,14 @@
         self.zk = ZooKeeper({'hosts': ['localhost:2181'], 'scope': 'test',
                              'name': 'foo', 'ttl': 30, 'retry_timeout': 10, 'loop_wait': 10,
                              'set_acls': {'CN=principal2': ['ALL']}})
 
     def test_session_listener(self):
         self.zk.session_listener(KazooState.SUSPENDED)
 
-    def test_members_watcher(self):
-        self.zk._fetch_cluster = False
-        self.zk.members_watcher(None)
-        self.assertTrue(self.zk._fetch_cluster)
-
     def test_reload_config(self):
         self.zk.reload_config({'ttl': 20, 'retry_timeout': 10, 'loop_wait': 10})
         self.zk.reload_config({'ttl': 20, 'retry_timeout': 10, 'loop_wait': 5})
 
     def test_get_node(self):
         self.assertIsNone(self.zk.get_node('/no_node'))
 
@@ -219,14 +215,16 @@
         self.zk.set_config_value('Exception')
 
     def test_initialize(self):
         self.assertFalse(self.zk.initialize())
 
     def test_cancel_initialization(self):
         self.zk.cancel_initialization()
+        with patch.object(MockKazooClient, 'delete', Mock()):
+            self.zk.cancel_initialization()
 
     def test_touch_member(self):
         self.zk._name = 'buzz'
         self.zk.get_cluster()
         self.zk.touch_member({'new': 'new'})
         self.zk._name = 'bar'
         self.zk.touch_member({'new': 'new'})
@@ -248,22 +246,23 @@
 
     def test_take_leader(self):
         self.zk.take_leader()
         with patch.object(MockKazooClient, 'create', Mock(side_effect=Exception)):
             self.zk.take_leader()
 
     def test_update_leader(self):
-        self.assertFalse(self.zk.update_leader(12345))
+        leader = self.zk.get_cluster().leader
+        self.assertFalse(self.zk.update_leader(leader, 12345))
         with patch.object(MockKazooClient, 'delete', Mock(side_effect=RetryFailedError)):
-            self.assertRaises(ZooKeeperError, self.zk.update_leader, 12345)
+            self.assertRaises(ZooKeeperError, self.zk.update_leader, leader, 12345)
         with patch.object(MockKazooClient, 'delete', Mock(side_effect=NoNodeError)):
-            self.assertTrue(self.zk.update_leader(12345, failsafe={'foo': 'bar'}))
+            self.assertTrue(self.zk.update_leader(leader, 12345, failsafe={'foo': 'bar'}))
             with patch.object(MockKazooClient, 'create', Mock(side_effect=[RetryFailedError, Exception])):
-                self.assertRaises(ZooKeeperError, self.zk.update_leader, 12345)
-                self.assertFalse(self.zk.update_leader(12345))
+                self.assertRaises(ZooKeeperError, self.zk.update_leader, leader, 12345)
+                self.assertFalse(self.zk.update_leader(leader, 12345))
 
     @patch.object(Cluster, 'min_version', PropertyMock(return_value=(2, 0)))
     def test_write_leader_optime(self):
         self.zk.last_lsn = '0'
         self.zk.write_leader_optime('1')
         with patch.object(MockKazooClient, 'create_async', Mock()):
             self.zk.write_leader_optime('1')
```

