# Comparing `tmp/kaiju_tools-2.1.1-py3-none-any.whl.zip` & `tmp/kaiju_tools-2.1.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,56 +1,55 @@
-Zip file size: 91787 bytes, number of entries: 54
--rw-r--r--  2.0 unx      232 b- defN 23-Jun-14 19:33 kaiju_tools/__init__.py
--rw-r--r--  2.0 unx    15027 b- defN 23-Jun-14 19:33 kaiju_tools/annotations.py
--rw-r--r--  2.0 unx    23424 b- defN 23-Jun-14 19:33 kaiju_tools/app.py
--rw-r--r--  2.0 unx     8518 b- defN 23-Jun-14 19:33 kaiju_tools/cache.py
--rw-r--r--  2.0 unx     5430 b- defN 23-Jun-14 19:33 kaiju_tools/class_registry.py
--rw-r--r--  2.0 unx    10838 b- defN 23-Jun-14 19:33 kaiju_tools/config.py
--rw-r--r--  2.0 unx    12468 b- defN 23-Jun-14 19:33 kaiju_tools/encoding.py
--rw-r--r--  2.0 unx     6036 b- defN 23-Jun-14 19:33 kaiju_tools/exceptions.py
--rw-r--r--  2.0 unx      997 b- defN 23-Jun-14 19:33 kaiju_tools/fixtures.py
--rw-r--r--  2.0 unx     8706 b- defN 23-Jun-14 19:33 kaiju_tools/functions.py
--rw-r--r--  2.0 unx    10436 b- defN 23-Jun-14 19:33 kaiju_tools/http.py
--rw-r--r--  2.0 unx    10067 b- defN 23-Jun-14 19:33 kaiju_tools/jsonschema.py
--rw-r--r--  2.0 unx     8912 b- defN 23-Jun-14 19:33 kaiju_tools/locks.py
--rw-r--r--  2.0 unx    10306 b- defN 23-Jun-14 19:33 kaiju_tools/logging.py
--rw-r--r--  2.0 unx      195 b- defN 23-Jun-14 19:33 kaiju_tools/loop.py
--rw-r--r--  2.0 unx     7822 b- defN 23-Jun-14 19:33 kaiju_tools/mapping.py
--rw-r--r--  2.0 unx    39398 b- defN 23-Jun-14 19:33 kaiju_tools/rpc.py
--rw-r--r--  2.0 unx     4416 b- defN 23-Jun-14 19:33 kaiju_tools/rpc_client_gen.py
--rw-r--r--  2.0 unx      110 b- defN 23-Jun-14 19:33 kaiju_tools/serialization.py
--rw-r--r--  2.0 unx      434 b- defN 23-Jun-14 19:33 kaiju_tools/services.py
--rw-r--r--  2.0 unx     8898 b- defN 23-Jun-14 19:33 kaiju_tools/streams.py
--rw-r--r--  2.0 unx    14337 b- defN 23-Jun-14 19:33 kaiju_tools/templates.py
--rw-r--r--  2.0 unx    14700 b- defN 23-Jun-14 19:33 kaiju_tools/types.py
--rw-r--r--  2.0 unx      276 b- defN 23-Jun-14 19:33 kaiju_tools/docker/__init__.py
--rw-r--r--  2.0 unx     9769 b- defN 23-Jun-14 19:33 kaiju_tools/docker/containers.py
--rw-r--r--  2.0 unx     7728 b- defN 23-Jun-14 19:33 kaiju_tools/docker/images.py
--rw-r--r--  2.0 unx      103 b- defN 23-Jun-14 19:33 kaiju_tools/docker/services.py
--rw-r--r--  2.0 unx    11140 b- defN 23-Jun-14 19:33 kaiju_tools/docker/stack.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-14 19:33 kaiju_tools/docker/tests/__init__.py
--rw-r--r--  2.0 unx     1625 b- defN 23-Jun-14 19:33 kaiju_tools/docker/tests/fixtures.py
--rw-r--r--  2.0 unx      575 b- defN 23-Jun-14 19:33 kaiju_tools/docker/tests/test_containers.py
--rw-r--r--  2.0 unx      359 b- defN 23-Jun-14 19:33 kaiju_tools/docker/tests/test_images.py
--rw-r--r--  2.0 unx      371 b- defN 23-Jun-14 19:33 kaiju_tools/docker/tests/test_stack.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-14 19:33 kaiju_tools/tests/__init__.py
--rw-r--r--  2.0 unx     6146 b- defN 23-Jun-14 19:33 kaiju_tools/tests/fixtures.py
--rw-r--r--  2.0 unx     9192 b- defN 23-Jun-14 19:33 kaiju_tools/tests/test_annotation_parser.py
--rw-r--r--  2.0 unx     2155 b- defN 23-Jun-14 19:33 kaiju_tools/tests/test_cache.py
--rw-r--r--  2.0 unx     1408 b- defN 23-Jun-14 19:33 kaiju_tools/tests/test_class_registry.py
--rw-r--r--  2.0 unx      555 b- defN 23-Jun-14 19:33 kaiju_tools/tests/test_configurator.py
--rw-r--r--  2.0 unx     3752 b- defN 23-Jun-14 19:33 kaiju_tools/tests/test_http.py
--rw-r--r--  2.0 unx     1885 b- defN 23-Jun-14 19:33 kaiju_tools/tests/test_locks.py
--rw-r--r--  2.0 unx     1777 b- defN 23-Jun-14 19:33 kaiju_tools/tests/test_loggers.py
--rw-r--r--  2.0 unx     3385 b- defN 23-Jun-14 19:33 kaiju_tools/tests/test_mapping.py
--rw-r--r--  2.0 unx     8614 b- defN 23-Jun-14 19:33 kaiju_tools/tests/test_rpc_server.py
--rw-r--r--  2.0 unx     1590 b- defN 23-Jun-14 19:33 kaiju_tools/tests/test_scheduler.py
--rw-r--r--  2.0 unx      481 b- defN 23-Jun-14 19:33 kaiju_tools/tests/test_serialization.py
--rw-r--r--  2.0 unx     1896 b- defN 23-Jun-14 19:33 kaiju_tools/tests/test_serializers.py
--rw-r--r--  2.0 unx     2787 b- defN 23-Jun-14 19:33 kaiju_tools/tests/test_services.py
--rw-r--r--  2.0 unx     1692 b- defN 23-Jun-14 19:33 kaiju_tools/tests/test_templates.py
--rw-rw-rw-  2.0 unx      610 b- defN 23-Jun-14 19:33 kaiju_tools-2.1.1.dist-info/LICENSE
--rw-r--r--  2.0 unx     3306 b- defN 23-Jun-14 19:33 kaiju_tools-2.1.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jun-14 19:33 kaiju_tools-2.1.1.dist-info/WHEEL
--rw-r--r--  2.0 unx       12 b- defN 23-Jun-14 19:33 kaiju_tools-2.1.1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     4606 b- defN 23-Jun-14 19:33 kaiju_tools-2.1.1.dist-info/RECORD
-54 files, 309594 bytes uncompressed, 84473 bytes compressed:  72.7%
+Zip file size: 96942 bytes, number of entries: 53
+-rw-r--r--  2.0 unx      231 b- defN 23-Jun-22 14:41 kaiju_tools/__init__.py
+-rw-r--r--  2.0 unx    15027 b- defN 23-Jun-22 14:41 kaiju_tools/annotations.py
+-rw-r--r--  2.0 unx    37350 b- defN 23-Jun-22 14:41 kaiju_tools/app.py
+-rw-r--r--  2.0 unx     4699 b- defN 23-Jun-22 14:41 kaiju_tools/cache.py
+-rw-r--r--  2.0 unx     5430 b- defN 23-Jun-22 14:41 kaiju_tools/class_registry.py
+-rw-r--r--  2.0 unx    12475 b- defN 23-Jun-22 14:41 kaiju_tools/encoding.py
+-rw-r--r--  2.0 unx     5310 b- defN 23-Jun-22 14:41 kaiju_tools/exceptions.py
+-rw-r--r--  2.0 unx     8706 b- defN 23-Jun-22 14:41 kaiju_tools/functions.py
+-rw-r--r--  2.0 unx    11388 b- defN 23-Jun-22 14:41 kaiju_tools/http.py
+-rw-r--r--  2.0 unx    10411 b- defN 23-Jun-22 14:41 kaiju_tools/interfaces.py
+-rw-r--r--  2.0 unx    10067 b- defN 23-Jun-22 14:41 kaiju_tools/jsonschema.py
+-rw-r--r--  2.0 unx     7548 b- defN 23-Jun-22 14:41 kaiju_tools/locks.py
+-rw-r--r--  2.0 unx    10446 b- defN 23-Jun-22 14:41 kaiju_tools/logging.py
+-rw-r--r--  2.0 unx      210 b- defN 23-Jun-22 14:41 kaiju_tools/loop.py
+-rw-r--r--  2.0 unx     7822 b- defN 23-Jun-22 14:41 kaiju_tools/mapping.py
+-rw-r--r--  2.0 unx    34431 b- defN 23-Jun-22 14:41 kaiju_tools/rpc.py
+-rw-r--r--  2.0 unx     4416 b- defN 23-Jun-22 14:41 kaiju_tools/rpc_client_gen.py
+-rw-r--r--  2.0 unx       95 b- defN 23-Jun-22 14:41 kaiju_tools/serialization.py
+-rw-r--r--  2.0 unx      702 b- defN 23-Jun-22 14:41 kaiju_tools/services.py
+-rw-r--r--  2.0 unx    16658 b- defN 23-Jun-22 14:41 kaiju_tools/sessions.py
+-rw-r--r--  2.0 unx     7216 b- defN 23-Jun-22 14:41 kaiju_tools/streams.py
+-rw-r--r--  2.0 unx    14337 b- defN 23-Jun-22 14:41 kaiju_tools/templates.py
+-rw-r--r--  2.0 unx    12410 b- defN 23-Jun-22 14:41 kaiju_tools/types.py
+-rw-r--r--  2.0 unx      276 b- defN 23-Jun-22 14:41 kaiju_tools/docker/__init__.py
+-rw-r--r--  2.0 unx     9782 b- defN 23-Jun-22 14:41 kaiju_tools/docker/containers.py
+-rw-r--r--  2.0 unx     7723 b- defN 23-Jun-22 14:41 kaiju_tools/docker/images.py
+-rw-r--r--  2.0 unx      103 b- defN 23-Jun-22 14:41 kaiju_tools/docker/services.py
+-rw-r--r--  2.0 unx    11216 b- defN 23-Jun-22 14:41 kaiju_tools/docker/stack.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-22 14:41 kaiju_tools/docker/tests/__init__.py
+-rw-r--r--  2.0 unx     1625 b- defN 23-Jun-22 14:41 kaiju_tools/docker/tests/fixtures.py
+-rw-r--r--  2.0 unx      575 b- defN 23-Jun-22 14:41 kaiju_tools/docker/tests/test_containers.py
+-rw-r--r--  2.0 unx      359 b- defN 23-Jun-22 14:41 kaiju_tools/docker/tests/test_images.py
+-rw-r--r--  2.0 unx      371 b- defN 23-Jun-22 14:41 kaiju_tools/docker/tests/test_stack.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-22 14:41 kaiju_tools/tests/__init__.py
+-rw-r--r--  2.0 unx    18531 b- defN 23-Jun-22 14:41 kaiju_tools/tests/fixtures.py
+-rw-r--r--  2.0 unx     9209 b- defN 23-Jun-22 14:41 kaiju_tools/tests/test_annotation_parser.py
+-rw-r--r--  2.0 unx     1469 b- defN 23-Jun-22 14:41 kaiju_tools/tests/test_cache.py
+-rw-r--r--  2.0 unx     1436 b- defN 23-Jun-22 14:41 kaiju_tools/tests/test_class_registry.py
+-rw-r--r--  2.0 unx      552 b- defN 23-Jun-22 14:41 kaiju_tools/tests/test_configurator.py
+-rw-r--r--  2.0 unx     2849 b- defN 23-Jun-22 14:41 kaiju_tools/tests/test_http.py
+-rw-r--r--  2.0 unx     2097 b- defN 23-Jun-22 14:41 kaiju_tools/tests/test_locks.py
+-rw-r--r--  2.0 unx     3407 b- defN 23-Jun-22 14:41 kaiju_tools/tests/test_mapping.py
+-rw-r--r--  2.0 unx     9174 b- defN 23-Jun-22 14:41 kaiju_tools/tests/test_rpc.py
+-rw-r--r--  2.0 unx     1555 b- defN 23-Jun-22 14:41 kaiju_tools/tests/test_scheduler.py
+-rw-r--r--  2.0 unx     2800 b- defN 23-Jun-22 14:41 kaiju_tools/tests/test_serializers.py
+-rw-r--r--  2.0 unx     2207 b- defN 23-Jun-22 14:41 kaiju_tools/tests/test_services.py
+-rw-r--r--  2.0 unx     3213 b- defN 23-Jun-22 14:41 kaiju_tools/tests/test_streams.py
+-rw-r--r--  2.0 unx     1692 b- defN 23-Jun-22 14:41 kaiju_tools/tests/test_templates.py
+-rw-rw-rw-  2.0 unx      610 b- defN 23-Jun-22 14:41 kaiju_tools-2.1.2.dist-info/LICENSE
+-rw-r--r--  2.0 unx     3360 b- defN 23-Jun-22 14:41 kaiju_tools-2.1.2.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jun-22 14:41 kaiju_tools-2.1.2.dist-info/WHEEL
+-rw-r--r--  2.0 unx       12 b- defN 23-Jun-22 14:41 kaiju_tools-2.1.2.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     4510 b- defN 23-Jun-22 14:41 kaiju_tools-2.1.2.dist-info/RECORD
+53 files, 338190 bytes uncompressed, 89788 bytes compressed:  73.5%
```

## zipnote {}

```diff
@@ -9,32 +9,29 @@
 
 Filename: kaiju_tools/cache.py
 Comment: 
 
 Filename: kaiju_tools/class_registry.py
 Comment: 
 
-Filename: kaiju_tools/config.py
-Comment: 
-
 Filename: kaiju_tools/encoding.py
 Comment: 
 
 Filename: kaiju_tools/exceptions.py
 Comment: 
 
-Filename: kaiju_tools/fixtures.py
-Comment: 
-
 Filename: kaiju_tools/functions.py
 Comment: 
 
 Filename: kaiju_tools/http.py
 Comment: 
 
+Filename: kaiju_tools/interfaces.py
+Comment: 
+
 Filename: kaiju_tools/jsonschema.py
 Comment: 
 
 Filename: kaiju_tools/locks.py
 Comment: 
 
 Filename: kaiju_tools/logging.py
@@ -54,14 +51,17 @@
 
 Filename: kaiju_tools/serialization.py
 Comment: 
 
 Filename: kaiju_tools/services.py
 Comment: 
 
+Filename: kaiju_tools/sessions.py
+Comment: 
+
 Filename: kaiju_tools/streams.py
 Comment: 
 
 Filename: kaiju_tools/templates.py
 Comment: 
 
 Filename: kaiju_tools/types.py
@@ -117,47 +117,44 @@
 
 Filename: kaiju_tools/tests/test_http.py
 Comment: 
 
 Filename: kaiju_tools/tests/test_locks.py
 Comment: 
 
-Filename: kaiju_tools/tests/test_loggers.py
-Comment: 
-
 Filename: kaiju_tools/tests/test_mapping.py
 Comment: 
 
-Filename: kaiju_tools/tests/test_rpc_server.py
+Filename: kaiju_tools/tests/test_rpc.py
 Comment: 
 
 Filename: kaiju_tools/tests/test_scheduler.py
 Comment: 
 
-Filename: kaiju_tools/tests/test_serialization.py
-Comment: 
-
 Filename: kaiju_tools/tests/test_serializers.py
 Comment: 
 
 Filename: kaiju_tools/tests/test_services.py
 Comment: 
 
+Filename: kaiju_tools/tests/test_streams.py
+Comment: 
+
 Filename: kaiju_tools/tests/test_templates.py
 Comment: 
 
-Filename: kaiju_tools-2.1.1.dist-info/LICENSE
+Filename: kaiju_tools-2.1.2.dist-info/LICENSE
 Comment: 
 
-Filename: kaiju_tools-2.1.1.dist-info/METADATA
+Filename: kaiju_tools-2.1.2.dist-info/METADATA
 Comment: 
 
-Filename: kaiju_tools-2.1.1.dist-info/WHEEL
+Filename: kaiju_tools-2.1.2.dist-info/WHEEL
 Comment: 
 
-Filename: kaiju_tools-2.1.1.dist-info/top_level.txt
+Filename: kaiju_tools-2.1.2.dist-info/top_level.txt
 Comment: 
 
-Filename: kaiju_tools-2.1.1.dist-info/RECORD
+Filename: kaiju_tools-2.1.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## kaiju_tools/__init__.py

```diff
@@ -1,11 +1,8 @@
-from .app import *
-from .http import *
-from .rpc import *
-from .cache import *
-from .locks import *
-from .streams import *
-from .logging import *
+from kaiju_tools.interfaces import *
+from kaiju_tools.types import *
+from kaiju_tools.app import *
+from kaiju_tools.encoding import Serializable
 
-__version__ = '2.1.1'
+__version__ = '2.1.2'
 __python_version__ = '3.8'
 __author__ = 'antonnidhoggr@me.com'
```

## kaiju_tools/app.py

```diff
@@ -1,48 +1,410 @@
+"""Application essential services and loaders."""
+
 import abc
 import asyncio
 import errno
 import logging
+import os
+import pathlib
 import uuid
+from ast import literal_eval
 from argparse import ArgumentParser
 from enum import Enum
+from functools import partial
 from time import time
-from typing import Union, List, Type, TypeVar, Optional, Iterable, cast, TypedDict, Awaitable, Callable
-from contextvars import ContextVar  # noqa pycharm
+from typing import (
+    Union,
+    List,
+    Type,
+    TypeVar,
+    Optional,
+    Iterable,
+    cast,
+    TypedDict,
+    Awaitable,
+    Callable,
+    Collection,
+    NewType,
+)
+from contextvars import ContextVar  # noqa: pycharm
 from weakref import proxy
+from warnings import warn
 
+import yaml
 from aiohttp.web import Application, run_app, AppRunner
 
-from kaiju_tools.config import ConfigLoader, ProjectSettings
+import kaiju_tools.jsonschema as js
+from kaiju_tools.interfaces import App, ServiceManagerInterface
 from kaiju_tools.class_registry import AbstractClassRegistry
+from kaiju_tools.encoding import load
 from kaiju_tools.functions import retry, timeout
-from kaiju_tools.types import App, Scope, Session, RequestContext, ServiceConfig, Namespace, SHARED_NS, SortedStack
 from kaiju_tools.loop import loop
 from kaiju_tools.logging import Adapter, HANDLERS, FORMATTERS, Logger
+from kaiju_tools.mapping import recursive_update
+from kaiju_tools.templates import Template
+from kaiju_tools.types import Namespace, SortedStack, Session, RequestContext
 
 __all__ = [
-    'App',
     'Service',
     'ContextableService',
     'ServiceClassRegistry',
     'service_class_registry',
     'ServiceConfigurationError',
+    'ServiceNotAvailableError',
     'ServiceContextManager',
-    'Scope',
-    'Session',
     'RequestContext',
     'run_command',
     'Commands',
-    'AbstractCommand',
+    'COMMANDS',
+    'BaseCommand',
     'LoggingService',
     'Scheduler',
     'ExecPolicy',
+    'RequestContext',
+    'ServiceConfig',
+    'REQUEST_SESSION',
+    'REQUEST_CONTEXT',
+    'HandlerSettings',
+    'LoggerSettings',
+    'ConfigLoader',
+    'get_cli_parser',
+    'ConfigurationError',
+    'Settings',
+    'AppSettings',
+    'RunSettings',
+    'ProjectSettings',
+    'MainSettings',
+    'ServiceSettings',
+    'init_app',
+    'run_server',
 ]
 
 
+class ServiceConfig(TypedDict, total=False):
+    """Service configuration parameters."""
+
+    cls: str  #: service class name as in :py:class:`~kaiju_tools.services.service_class_registry`
+    name: str  #: unique service name, each service should have a default value for this
+    enabled: bool  #: disable service
+    required: bool  #: skip a service and proceed on initialization error
+    override: bool  #: replace an existing service with the same name
+    settings: dict  #: custom settings, unpacked to a service's __init__
+
+
+REQUEST_CONTEXT: ContextVar[Optional[RequestContext]] = ContextVar('RequestContext', default=None)
+REQUEST_SESSION: ContextVar[Optional['Session']] = ContextVar('RequestSession', default=None)
+
+
+class ConfigurationError(KeyError):
+    """Configuration key not found."""
+
+
+class Settings(dict):
+    """Settings object."""
+
+    validator: js.Object = None
+
+    def __init__(self, seq):
+        """Initialize."""
+        if self.validator:
+            seq = js.compile_schema(self.validator)(seq)
+        super().__init__(seq)
+
+    def __getattr__(self, item):
+        """Get a parameter from settings dict."""
+        try:
+            return self[item]
+        except KeyError:
+            raise ConfigurationError(f'No such config value: {item}')
+
+
+class AppSettings(Settings):
+    """Web application init settings."""
+
+    validator = js.Object(
+        {'debug': js.Boolean(default=False), 'client_max_size': js.Integer(minimum=1024, default=1024**2)},
+        additionalProperties=False,
+        required=[],
+    )
+
+
+class RunSettings(Settings):
+    """Server run settings."""
+
+    validator = js.Object(
+        {
+            'host': js.Nullable(js.String(minLength=1, default=None)),
+            'port': js.Nullable(js.Integer(minimum=1, maximum=65535, default=None)),
+            'path': js.Nullable(js.String(minLength=1, default=None)),
+            'shutdown_timeout': js.Integer(minimum=0, default=30),
+            'keepalive_timeout': js.Integer(minimum=0, default=60),
+        },
+        additionalProperties=False,
+        required=[],
+    )
+
+
+class MainSettings(Settings):
+    """Main project settings."""
+
+    validator = js.Object(
+        {
+            'name': js.String(minLength=1),
+            'version': js.String(minLength=1),
+            'env': js.String(minLength=1),
+            'loglevel': js.Enumerated(enum=['DEBUG', 'INFO', 'WARNING', 'ERROR'], default='INFO'),
+        },
+        additionalProperties=False,
+        required=['name', 'version', 'env'],
+    )
+
+
+class ServiceSettings(Settings):
+    """Service configuration."""
+
+    validator = js.Object(
+        {
+            'cls': js.String(minLength=1),
+            'name': js.String(minLength=1),
+            'enabled': js.Boolean(default=True),
+            'required': js.Boolean(default=True),
+            'override': js.Boolean(default=False),
+            'loglevel': js.JSONSchemaObject(enum=['DEBUG', 'INFO', 'WARNING', 'ERROR'], default=None),
+            'settings': js.Object(),
+        },
+        additionalProperties=False,
+        required=['cls'],
+    )
+
+    def __init__(self, seq):
+        if type(seq) is str:
+            seq = {'cls': seq}
+        super().__init__(seq)
+
+
+class ProjectSettings(Settings):
+    """Validation schema for project settings."""
+
+    def __init__(self, app: dict, run: dict, main: dict, etc: dict, services: list):
+        """Initialize."""
+        super().__init__(
+            dict(
+                app=AppSettings(app),
+                run=RunSettings(run),
+                main=MainSettings(main),
+                etc=Settings(etc),
+                services=tuple(ServiceSettings(srv) for srv in services),
+            )
+        )
+
+
+def get_cli_parser() -> ArgumentParser:
+    """Parse application CLI args."""
+    _parser = ArgumentParser(prog='aiohttp web application', description='web application run settings')
+    _parser.add_argument('--host', dest='host', default=None, help='web app host (default - from settings)')
+    _parser.add_argument('--port', dest='port', type=int, default=None, help='web app port (default - from settings)')
+    _parser.add_argument(
+        '--path', dest='path', default=None, metavar='FILE', help='socket path (default - from settings)'
+    )
+    _parser.add_argument(
+        '--debug', dest='debug', action='store_true', default=None, help='run in debug mode (default - from settings)'
+    )
+    _parser.add_argument(
+        '-c',
+        '--config',
+        dest='cfg',
+        default=[],
+        metavar='FILE',
+        action='append',
+        help='yaml config paths, use multiple times to merge multiple configs (default - settings/config.yml)',
+    )
+    _parser.add_argument(
+        '-l',
+        '--log',
+        dest='loglevel',
+        default=None,
+        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
+        help='log level',
+    )
+    _parser.add_argument(
+        '-f',
+        '--env-file',
+        dest='env_file',
+        default=[],
+        metavar='FILE',
+        action='append',
+        help='env file paths (default to ./settings/env.json + ./settings/env.local.json)',
+    )
+    _parser.add_argument(
+        '-e',
+        '--env',
+        dest='env',
+        default=[],
+        metavar='KEY=VALUE',
+        action='append',
+        help='overrides env variable (may be used multiple times)',
+    )
+    _parser.add_argument(
+        '--no-os-env', dest='no_os_env', action='store_true', default=False, help='do not use OS environment variables'
+    )
+    _parser.add_argument(
+        'cmd', metavar='COMMAND', default=None, nargs='?', help='optional management command to execute'
+    )
+    return _parser
+
+
+class ConfigLoader:
+    """Config loader class. It is intended to be used before the app start."""
+
+    CmdName = NewType('CmdName', str)
+    config_class = ProjectSettings
+    _file_loaders = {
+        '.json': load,
+        '.yml': partial(yaml.load, Loader=yaml.SafeLoader),
+        '.yaml': partial(yaml.load, Loader=yaml.SafeLoader),
+    }
+
+    def __init__(
+        self,
+        base_config_paths: Iterable[str] = tuple(),
+        base_env_paths: Iterable[str] = tuple(),
+        default_env_paths: Iterable[str] = tuple(),
+    ):
+        """Initialize.
+
+        :param base_config_paths: list of base paths to .yml config files in sequential order
+        :param base_env_paths: list of base paths to .env files in sequential order
+        :param default_env_paths: list of paths to .env files which may be overriden by the `--env-file` flag
+        """
+        self.base_config_paths = base_config_paths
+        self.base_env_paths = base_env_paths
+        self.default_env_paths = default_env_paths
+        self.logger = logging.getLogger('loader')
+
+    def configure(self) -> (Optional[CmdName], ProjectSettings):
+        """Load project config and command.
+
+        Loading order:
+
+        - ./settings/config.yml
+        - .yml files, first to last
+        - ./settings/env.json
+        - .env files, first to last
+        - os env vars (unless --no-os-env specified)
+        - CLI env vars
+
+        """
+        parser = get_cli_parser()
+        args = parser.parse_known_args()[0].__dict__
+        config_paths = [*self.base_config_paths, *args.get('cfg', [])]
+        _paths = args.get('env_file', [])
+        if not _paths:
+            _paths = self.default_env_paths
+        env_paths = [*self.base_env_paths, *_paths]
+        config, env = {}, {}
+        for cfg_path in config_paths:
+            _data = self._from_file(cfg_path)
+            _services = _data.pop('services', [])
+            config = recursive_update(config, _data)
+            if 'services' in config:
+                config['services'].extend(_services)
+            else:
+                config['services'] = _services
+        for env_path in env_paths:
+            env.update(self._from_file(env_path))
+        if not args['no_os_env']:
+            self._update_env_from_os(env)
+        self._update_env_from_cli(env, args)
+        config = self._from_dict(config, env)
+        if not args['no_os_env']:
+            self._update_config_from_os(config)
+        self._update_config_from_cli(config, args)
+        command = args.get('cmd')
+        config = ProjectSettings(**config)
+        return command, config
+
+    def _from_file(self, path) -> dict:
+        """Load data from a config file."""
+        self.logger.info('Loading %s', path)
+        path = pathlib.Path(path)
+        if path.suffix not in self._file_loaders:
+            raise ConfigurationError(f'Unknown config file format: {path}')
+        loader = self._file_loaders[path.suffix]
+        if not path.exists() or path.is_dir():
+            warn('Config path does not exist or it\'s a directory.' ' "%s" - not found!' % path)
+            return {}
+        with open(path) as f:
+            data = loader(f)
+        return data
+
+    def _from_dict(self, data: dict, env: dict) -> ProjectSettings:
+        """Create config from a template and env map."""
+        data = Template(data).fill(env)
+        config = self.config_class(**data)
+        return config
+
+    def _update_env_from_os(self, env: dict) -> None:
+        """Get env arguments from OS env."""
+        self.logger.debug('Loading OS')
+        for key in list(env.keys()):
+            value = os.getenv(key)
+            if value:
+                self.logger.info('From OS: %s', key)
+                env[key] = self._init_env_value(value)
+
+    def _update_env_from_cli(self, env: dict, args: dict) -> None:
+        """Update env map from CLI arguments."""
+        self.logger.debug('Loading CLI')
+        for record in args.get('env', []):
+            k, v = record.split('=')
+            if v:
+                self.logger.info('From CLI: %s', k)
+                env[k] = self._init_env_value(v)
+
+    def _update_config_from_os(self, config: ProjectSettings):
+        """Set OS args."""
+
+    @staticmethod
+    def _update_config_from_cli(config: ProjectSettings, args: dict):
+        """Set CLI arguments."""
+        for key in ('host', 'port', 'path'):
+            value = args.get(key)
+            if value is not None:
+                config.run[key] = value
+        debug = args.get('debug')
+        if debug is not None:
+            config.app['debug'] = debug
+        log = args.get('loglevel')
+        if log is not None:
+            config.main['loglevel'] = log
+
+    @staticmethod
+    def _init_env_value(value: str):
+        """Parse env arg from --env or unix environment."""
+        if value is None:
+            return None
+        value = value.strip()
+        if not value:
+            return None
+        _value = value.lower()
+        if _value == 'true':
+            value = True
+        elif _value == 'false':
+            value = False
+        elif _value == 'none':
+            value = None
+        else:
+            try:
+                value = literal_eval(value)
+            except Exception:  # noqa: reasonable
+                pass
+        return value
+
+
 class Service(abc.ABC):
     """Base service class."""
 
     service_name = None  #: you may define a custom service name here
 
     def __init__(self, app: App = None, logger=None):
         """Initialize.
@@ -53,19 +415,19 @@
         self.app = app
         self.log_ctx = {}
         if logger is None:
             if app:
                 logger = logging.getLogger(self.app.name)
             else:
                 logger = logging.getLogger('root')
-        self.logger = Adapter(logger, self.log_ctx)
+        self.logger = Adapter(logger=logger, extra=self.log_ctx, app=self.app)
 
     def discover_service(
         self,
-        name: Union[str, 'Service', None],
+        name: Union[str, object, None],
         cls: Union[Union[str, Type], Iterable[Union[str, Type]]] = None,
         required=True,
     ):
         """Discover a service using specified name and/or service class.
 
         :param name: specify a service name or service instance (in latter case
             it will be returned as is)
@@ -78,15 +440,15 @@
             otherwise in this case None will be returned
         """
         if name is False and not required:
             return
         elif isinstance(name, Service):
             return name
         else:
-            return self.app.services.discover_service(name=name, cls=cls, required=required)  # noqa ?
+            return self.app.services.discover_service(name=name, cls=cls, required=required)
 
 
 class ContextableService(Service):
     """A service which must be asynchronously initialized after it was created."""
 
     async def init(self):
         """Define your asynchronous initialization here."""
@@ -110,30 +472,30 @@
 Contextable = ContextableService
 
 
 class ServiceConfigurationError(RuntimeError):
     """An error during services configuration or initialization."""
 
 
-class ServiceNotAvailableError(KeyError):
+class ServiceNotAvailableError(ValueError):
     """Service with such name doesn't exist."""
 
 
 class ServiceClassRegistry(AbstractClassRegistry):
     """Class registry for service classes."""
 
     base_classes = [Service]
 
 
 service_class_registry = ServiceClassRegistry(raise_if_exists=False)  #: default service class registry object
 
 _Service = TypeVar('_Service', bound=Service)
 
 
-class ServiceContextManager(ContextableService):
+class ServiceContextManager(ContextableService, ServiceManagerInterface):
     """Services manager."""
 
     service_name = 'srv'
 
     def __init__(
         self,
         app: App,
@@ -162,14 +524,26 @@
                     self.logger.error('Service failed', service=name, exc_info=exc)
 
     async def close(self):
         for name in self._running_services[::-1]:
             await self.terminate_service(name)
         self._running_services.clear()
 
+    def add_service(self, service: Service, required: bool = True, name: str = None) -> None:
+        """Add a service directly to the service map.
+
+        This method is supposed to be used in testing / debugging, not intended for production use.
+        """
+        name = self._get_service_name(type(service), name)
+        service.service_name = name
+        self._services[name] = service
+        service.logger = self.app.logger.getChild(name)
+        if required:
+            self._required.add(service.service_name)
+
     async def start_service(self, name: str) -> None:
         """Start an idle service."""
         service = self._services[name]
         if name in self._running_services:
             return
         if isinstance(service, ContextableService):
             self.logger.debug('Starting service', service=service.service_name)
@@ -224,40 +598,47 @@
             priority order one by one.
         :param required: means that an exception will rise if service doesn't exist
             otherwise in this case None will be returned
         """
         if isinstance(name, Service):
             return name
 
-        if name and name in self._services:
+        if name:
+            if name not in self._services:
+                raise ServiceNotAvailableError(f'Service not found: "{name}".')
             service = self._services[name]
             if not isinstance(service, cls):
-                raise ValueError('Service class mismatch.')
+                raise ServiceNotAvailableError(f'Service class mismatch: {cls} vs {type(service)}.')
             return service
 
         service = next((service for service in self._services.values() if isinstance(service, cls)), None)
         if service:
             return service
         elif required:
-            raise ValueError('Service not found.')
+            raise ServiceNotAvailableError(f'Service not found: {name}/{cls}.')
+
+    @staticmethod
+    def _get_service_name(service_cls: Type, name: Optional[str]) -> str:
+        if not name:
+            name = getattr(service_cls, 'service_name', None)
+        if name is None:
+            name = service_cls.__name__
+        return name
 
     def _create_services(self) -> None:
-        self._services.clear()
         for settings in self._settings:
             if type(settings) is str:
                 settings = ServiceConfig(cls=settings)
             if settings.get('enabled', True):
                 cls = self._registry[settings['cls']]
-                name = settings.get('name', getattr(cls, 'service_name', None))
-                if not name:
-                    name = cls.__name__
+                name = self._get_service_name(cls, settings.get('name'))
                 if name in self._services:
                     if not settings.get('override'):
                         raise ServiceConfigurationError('Service with name "%s" already registered.' % name)
-                service = cls(app=self.app, **settings.get('settings', {}), logger=self.logger.getChild(name))
+                service = cls(app=self.app, **settings.get('settings', {}), logger=self.app.logger.getChild(name))
                 service.service_name = name
                 self._services[name] = service
                 if settings.get('required', True):
                     self._required.add(name)
                 loglevel = settings.get('loglevel')
                 if loglevel:
                     service.logger.setLevel(loglevel)
@@ -319,15 +700,15 @@
 
     @enabled.setter
     def enabled(self, value: bool) -> None:
         """Enable or disable task."""
         self._enabled = value
         if value is True:
             t_ex = self.called_at + self.interval
-            self.scheduler._stack.insert(t_ex, self)  # noqa
+            self._scheduler._stack.insert(t_ex, self)  # noqa
 
     @property
     def max_timeout(self) -> float:
         if self.policy is ExecPolicy.CANCEL:
             return max(0.0, self.interval)
         else:
             return max(0.0, self.interval * 4.0)
@@ -397,14 +778,15 @@
         if name is None:
             name = f'scheduled:{method.__name__}'
         if params is None:
             params = {}
         self.logger.debug('schedule', task_name=name, interval=interval, policy=policy.value)
         task = _ScheduledTask(self, name, method, params, interval, policy, retries)
         self._tasks.append(task)
+        self.refresh_rate = min(self.refresh_rate, interval, 0.1)
         t_ex = time() + interval
         self._stack.insert(t_ex, task)
         return task
 
     async def _iter(self) -> None:
         """Iterate over the tasks ready to run."""
         while 1:
@@ -414,18 +796,20 @@
                 if not scheduled.enabled:
                     continue
                 if scheduled.executed and not (scheduled.executed.done() or scheduled.executed.cancelled()):
                     if scheduled.policy is ExecPolicy.CANCEL:
                         scheduled.executed.cancel(msg='Cancelled by the scheduler')
                     elif scheduled.policy is ExecPolicy.WAIT:
                         continue
+                    else:
+                        raise RuntimeError(f'Unsupported exec policy: {scheduled.policy}')
 
                 scheduled.executed = task = asyncio.create_task(self._run_task(scheduled))
                 scheduled.called_at = time()
-                task._scheduled = proxy(scheduled)
+                task._scheduled = scheduled
                 task.add_done_callback(self._task_callback)
                 task.set_name(scheduled.name)
 
             await asyncio.sleep(self._get_sleep_interval())
 
     async def _run_task(self, task: _ScheduledTask) -> None:
         """Run task in a wrapper."""
@@ -454,25 +838,29 @@
             self.logger.error(str(result), exc_info=result)
         scheduled = task._scheduled  # noqa
         self._stack.insert(scheduled.called_at + scheduled.interval, scheduled)
         scheduled.executed = None
         task._scheduled = None
 
 
-class _HandlerSettings(TypedDict, total=False):
+class HandlerSettings(TypedDict, total=False):
+    """Log handler settings in a config file."""
+
     cls: str
     name: str
     formatter: str
     enabled: bool
     settings: dict
     formatter_settings: dict
     loglevel: str
 
 
-class _LoggerSettings(TypedDict, total=False):
+class LoggerSettings(TypedDict, total=False):
+    """Logger settings in a config file."""
+
     name: str
     enabled: bool
     handlers: Union[List[str], bool]
     loglevel: str
 
 
 class LoggingService(ContextableService):
@@ -480,30 +868,30 @@
 
     handler_classes = HANDLERS
     formatter_classes = FORMATTERS
 
     def __init__(
         self,
         *args,
-        loggers: Iterable[_LoggerSettings] = None,
-        handlers: Iterable[_HandlerSettings] = None,
+        loggers: Iterable[LoggerSettings] = None,
+        handlers: Iterable[HandlerSettings] = None,
         loglevel: str = None,
         **kws,
     ):
         """Initialize."""
         super().__init__(*args, **kws)
         self.loggers = loggers
         self.handlers = handlers
         self.loglevel = loglevel if loglevel else getattr(self.app, 'loglevel', 'INFO')
         self.clear_root_logger()
         _handlers = {handler['name']: handler for handler in self.handlers} if self.handlers else {}
         _loggers = {logger['name']: logger for logger in self.loggers} if self.loggers else {}
         app_logger_name = self.app.logger.name
         if app_logger_name not in _loggers:
-            _loggers[app_logger_name] = _LoggerSettings(name=app_logger_name, enabled=True, handlers=True)
+            _loggers[app_logger_name] = LoggerSettings(name=app_logger_name, enabled=True, handlers=True)
         self._handlers = {  # noqa
             name: self._init_handler(handler) for name, handler in _handlers.items() if handler.get('enabled')
         }
         self._loggers = {  # noqa
             name: self._init_logger(logger) for name, logger in _loggers.items() if logger.get('enabled')
         }
 
@@ -522,56 +910,60 @@
     @staticmethod
     def clear_root_logger():
         """Remove all existing handlers from the root logger."""
         logger = logging.getLogger()
         logger.handlers.clear()
         logger.setLevel(logging.NOTSET)
 
-    def _init_handler(self, handler: _HandlerSettings) -> logging.Handler:
+    def _init_handler(self, handler: HandlerSettings) -> logging.Handler:
         """Initialize a handler with handler settings."""
         if isinstance(handler['cls'], str):
             handler['cls'] = self.handler_classes[handler['cls']]
         if isinstance(handler['formatter'], str):
             handler['formatter'] = self.formatter_classes[handler['formatter']]
-        _handler = handler['cls'](self.app, **handler.get('settings', {}))
-        formatter = handler['formatter'](**handler.get('formatter_settings', {}))
+        _handler = handler['cls'](self.app, **handler.get('settings', {}))  # noqa
+        formatter = handler['formatter'](**handler.get('formatter_settings', {}))  # noqa
         _handler.setFormatter(formatter)
         loglevel = handler.get('loglevel', self.loglevel)
         _handler.setLevel(loglevel)
         return _handler
 
-    def _init_logger(self, logger: _LoggerSettings) -> logging.Logger:
+    def _init_logger(self, logger: LoggerSettings) -> logging.Logger:
         """Initialize a logger with logger settings."""
         _logger = logging.getLogger(logger['name'])
         _logger.handlers = []
         if logger['handlers'] is True:
             _handlers = self._handlers.values()
         else:
             _handlers = (self._handlers[name] for name in logger['handlers'])
         for handler in _handlers:
             _logger.addHandler(handler)
         loglevel = logger.get('loglevel', self.loglevel)
         _logger.setLevel(loglevel)
         return _logger
 
 
-class AbstractCommand(ContextableService, abc.ABC):
+class BaseCommand(ContextableService, abc.ABC):
     """Base application command, recognized by CLI."""
 
     service_name = None  #: command name after `python -m my_app [...]`
     run_app = True
 
     def __init__(self, app: App, logger=None):
+        """Initialize."""
         super().__init__(app=app, logger=logger)
         self._runner = AppRunner(app)
         self._closed = True
 
     @classmethod
     def get_parser(cls) -> ArgumentParser:
-        """Get an argument parser for CLI command arguments."""
+        """Get an argument parser for CLI command arguments.
+
+        You can provide additional CLI args for this particular command here.
+        """
         return ArgumentParser()
 
     @abc.abstractmethod
     async def command(self, **kws):
         """Run a specific command."""
 
     async def init(self):
@@ -605,61 +997,64 @@
                 loop.run_until_complete(self._runner.cleanup())
             loop.close()
             self._closed = True
             return result
 
 
 class Commands(AbstractClassRegistry):
-    """Map of all available commands. User MUST register it in this class."""
+    """Map of all available commands."""
 
-    base_classes = (AbstractCommand,)
+    base_classes = (BaseCommand,)
 
     @staticmethod
     def class_key(obj) -> str:
         return obj.service_name
 
 
-commands = Commands()
+COMMANDS = Commands()  # : default commands registry
 
 
-def run_command(app: App, command: str, commands_registry=commands) -> int:
+def run_command(app: App, command: ConfigLoader.CmdName, commands_registry: Commands = COMMANDS) -> int:
+    """Run an app with a CLI command.
+
+    :param app: application object
+    :param command: command name after python -m app ...
+    :param commands_registry: provide an instance of command class registry
+    :returns: unix exit code
+    """
     if command in commands_registry:
         cmd = commands_registry[command]
         cmd = cmd(app=app, logger=app.logger.getChild('CLI'))
         result = cmd.run()
     else:
         app.logger.error('Unknown command "%s".', command)
         result = errno.ENOENT
     return result
 
 
-def init_config(base_config_paths=None, base_env_paths=None, default_env_paths=None) -> (str, ProjectSettings):
-    """Configure."""
-    if base_config_paths is None:
-        base_config_paths = ['./settings/config.yml']
-    if base_env_paths is None:
-        base_env_paths = ['./settings/env.json']
-    if default_env_paths is None:
-        default_env_paths = ['./settings/env.local.json']
-    logging.basicConfig(level='INFO')
-    config_loader = ConfigLoader(
-        base_config_paths=base_config_paths, base_env_paths=base_env_paths, default_env_paths=default_env_paths
-    )
-    command, config = config_loader.configure()
-    logging.root.handlers = []
-    return command, config
+def init_app(settings: ProjectSettings, attrs: dict = None, middlewares: Collection = None) -> App:
+    """Create a web application object.
 
+    :param settings: project settings, usually provided by the configuration service.
+    :param attrs: additional application attributes
+    :param middlewares: list of additional aiohttp middlewares
+    """
+    from kaiju_tools.http import error_middleware, session_middleware, JSONRPCView
 
-def init_app(settings: ProjectSettings, attrs: dict = None, middlewares: list = None) -> App:
     if middlewares is None:
         middlewares = []
+
+    middlewares = [error_middleware, session_middleware, *middlewares]
     app = Application(middlewares=middlewares, logger=logging.getLogger(settings.main.name), **settings.app)
+    app.router.add_route('*', JSONRPCView.route, JSONRPCView)
+    app.request_context = REQUEST_CONTEXT
+    app.request_session = REQUEST_SESSION
     app = cast(App, app)
-    app.ns = Namespace(env=app.env, name=app.name)
-    app.ns_shared = Namespace(env=app.env, name=SHARED_NS)
+    app.namespace = Namespace(env=app.env, name=app.name)
+    app.namespace_shared = Namespace(env=app.env, name='__shared__')
     app.id = uuid.uuid4()
     app.loglevel = settings.main['loglevel']
     for key, value in settings.main.items():
         app[key] = value
         setattr(app, key, value)
     if attrs:
         for key, value in attrs.items():
@@ -669,16 +1064,35 @@
     app.services = services = ServiceContextManager(
         app=app, settings=settings.services, class_registry=service_class_registry, logger=app.logger
     )
     app.cleanup_ctx.append(services.cleanup_context)
     return app
 
 
-def main(_init_app, **config_settings):
-    command, config = init_config(**config_settings)
-    app: App = _init_app(config)
+def run_server(
+    init_app_func=init_app,
+    base_config_paths: Collection[str] = None,
+    base_env_paths: Collection[str] = None,
+    default_env_paths: Collection[str] = None,
+) -> None:
+    """Run aiohttp server or command.
+
+    You should use this in the `__main__` section of the project to start the app.
+
+    :param init_app_func: app initialization function
+    :param base_config_paths: list of base paths to .yml config files in sequential order
+    :param base_env_paths: list of base paths to .env files in sequential order
+    :param default_env_paths: list of paths to .env files which may be overriden by the `--env-file` flag
+    """
+    logging.basicConfig(level='INFO')
+    config_loader = ConfigLoader(
+        base_config_paths=base_config_paths, base_env_paths=base_env_paths, default_env_paths=default_env_paths
+    )
+    command, config = config_loader.configure()
+    logging.root.handlers = []
+    app: App = init_app_func(config)
     if config.app.debug:
-        print('\n-- RUNNING IN DEBUG MODE --\n')
+        warn('Running in debug mode')
     if command:
         run_command(app, command)
     else:
-        run_app(app, access_log=False, **config.run)  # noqa
+        run_app(app, access_log=None, **config.run)
```

## kaiju_tools/cache.py

```diff
@@ -1,228 +1,136 @@
 """Shared cache services and classes."""
 
 import abc
-import asyncio
-from typing import Union, Collection, FrozenSet, List, Dict, Any
+from typing import Collection, FrozenSet, List, Dict, Any, Optional, Type
 
 from kaiju_tools.app import ContextableService, Service
 from kaiju_tools.encoding import MimeTypes, serializers
+from kaiju_tools.interfaces import Cache
 from kaiju_tools.functions import RETRY_EXCEPTION_CLASSES
 from kaiju_tools.types import NSKey
 
-__all__ = ['BaseCacheService']
+__all__ = ['BaseCacheService', 'Cache']
 
 
-class BaseCacheService(ContextableService, abc.ABC):
+class BaseCacheService(ContextableService, Cache, abc.ABC):
     """Base class for shared cache."""
 
     service_name = 'cache'
     CONNECTION_ERROR_CLASSES = RETRY_EXCEPTION_CLASSES
-    DEFAULT_TTL = 0
     DEFAULT_SERIALIZER_TYPE = MimeTypes.msgpack
-    IGNORE_CONN_ERRORS = False
-    NOWAIT = True
-    transport_cls = None
 
     def __init__(
         self,
-        app,
-        transport: Union[str, Service] = None,
-        default_ttl: int = DEFAULT_TTL,
-        serializer_type: str = DEFAULT_SERIALIZER_TYPE,
+        *args,
+        transport: Service = None,
+        serializer_type: Optional[str] = DEFAULT_SERIALIZER_TYPE,
         encoders=serializers,
-        logger=None,
+        shared: bool = False,
+        **kws,
     ):
         """Initialize.
 
-        :param app: web app (provided by the service manager)
-        :param transport: transport service (may be Redis, DB or similar)
-        :param default_ttl:  default key lifetime in seconds (0 for infinite)
+        :param transport: transport service (Redis, DB or similar)
         :param serializer_type: you may specify a serializer type from `kaiju-tools.encoding`
         :param encoders: serializers registry with all serializers classes
-        :param logger: logger instance (provided by the service manager)
+        :param shared: use a shared namespace (i.e. between multiple apps)
         """
-        Service.__init__(self, app=app, logger=logger)
+        super().__init__(*args, **kws)
+        self.namespace = self.app.namespace_shared if shared else self.app.namespace
+        self.namespace = self.namespace / '_cache'
         self._transport_name = transport
-        self._default_ttl = max(self.DEFAULT_TTL, int(default_ttl))
-        self._serializer = encoders[serializer_type]()
+        self._serializer = encoders[serializer_type]() if serializer_type else None
         self._transport = None
         self._queue = None
 
     async def init(self):
         """Service context init."""
-        self._transport = self.discover_service(self._transport_name, cls=self.transport_cls)
+        self._transport = self.discover_service(self._transport_name, cls=self.get_transport_cls())
 
-    async def exists(self, key: NSKey, ignore_conn_errors=IGNORE_CONN_ERRORS) -> bool:
-        """Check if key is present in the cache."""
-        result = await self._wrap_exec(self._exists(key), ignore_conn_errors)
-        if result is None:
-            self.logger.info('key not found', key=key)
-        return bool(result)
-
-    async def _exists(self, key: NSKey) -> bool:
-        """Check if such key present and has not expired."""
-
-    async def m_exists(self, keys: Collection[NSKey], ignore_conn_errors=IGNORE_CONN_ERRORS) -> FrozenSet[NSKey]:
-        """Return a set of existing keys."""
-        self.logger.debug('m_exists', num_keys=len(keys))
-        results = await self._wrap_exec(self._m_exists(*keys), ignore_conn_errors)
-        if results:
-            return frozenset(NSKey(key) for key, result in zip(keys, results) if bool(result))
-        else:
-            return frozenset()
+    @classmethod
+    @abc.abstractmethod
+    def get_transport_cls(cls) -> Type:
+        """Get transport class required for this service."""
 
     @abc.abstractmethod
-    async def _m_exists(self, *keys: NSKey) -> List[NSKey]:
-        """Return a list of 0 and 1 (0 for not existing True for existing)."""
+    async def exists(self, id: NSKey) -> bool:
+        """Check if key is present in the cache."""
 
-    async def get(self, key: NSKey, use_serializer=True, ignore_conn_errors=IGNORE_CONN_ERRORS):
-        """Get value of a key or None if not found.
+    @abc.abstractmethod
+    async def m_exists(self, id: Collection[NSKey]) -> FrozenSet[str]:
+        """Return a set of existing keys."""
 
-        :param key: string only
-        :param use_serializer: to use serializer for value decoding (False = return raw)
-        :param ignore_conn_errors: set True to ignore connection errors and skip the operation
-        """
-        self.logger.debug('get', key=key)
-        value = await self._wrap_exec(self._get(key), ignore_conn_errors)
-        value = self._load_value(value, use_serializer)
-        if value is None:
-            self.logger.info('key not found', key=key)
+    async def get(self, id: NSKey):
+        """Get value of a key or None if not found."""
+        self.logger.debug('get', key=id)
+        value = await self._get(id)
+        value = self._load_value(value)
         return value
 
     @abc.abstractmethod
     async def _get(self, key: NSKey):
         """Return a key value or None if not found."""
 
-    async def m_get(
-        self, keys: Collection[NSKey], use_serializer=True, ignore_conn_errors=IGNORE_CONN_ERRORS
-    ) -> Dict[NSKey, Any]:
-        """Get values of multiple keys.
-
-        :param keys: list of keys
-        :param use_serializer: use a serializer for value decoding (False = return raw)
-        :param ignore_conn_errors: set True to ignore connection errors and skip the operation
-        """
-        self.logger.debug('m_get', num_keys=len(keys))
-        values = await self._wrap_exec(self._m_get(*keys), ignore_conn_errors)
+    async def m_get(self, id: Collection[NSKey]) -> Dict[NSKey, Any]:
+        """Get values of multiple keys."""
+        self.logger.debug('m_get', keys=id)
+        values = await self._m_get(*id)
         if values:
-            result = {k: self._load_value(v, use_serializer) for k, v in zip(keys, values) if v}
-        else:
-            result = {}
-        return result
+            return {k: self._load_value(v) for k, v in zip(id, values) if v}  # noqa
+        return {}
 
     @abc.abstractmethod
     async def _m_get(self, *keys: NSKey) -> List[NSKey]:
         """Return a list of values for given keys."""
 
-    async def set(
-        self,
-        key: NSKey,
-        value: Any,
-        ttl: int = None,
-        use_serializer=True,
-        ignore_conn_errors=IGNORE_CONN_ERRORS,
-        nowait=NOWAIT,
-    ) -> None:
-        """Set a single key.
-
-        :param key: string only
-        :param value: any serializable value
-        :param ttl: key lifetime in seconds, 0 for infinite, None for default
-        :param use_serializer: use a serializer for value encoding (False = return raw)
-        :param ignore_conn_errors: set True to ignore connection errors and skip the operation
-        :param nowait: set operation in background (don't wait for response)
-        """
-        if ttl is None:
-            _ttl = self._default_ttl
-        else:
-            _ttl = ttl
-        self.logger.info('set', key=key, ttl=ttl)
-        value = self._dump_value(value, use_serializer)
-        _task = self._wrap_exec(self._set(key, value, _ttl), ignore_conn_errors)
-        if nowait:
-            asyncio.create_task(_task)
-        else:
-            await _task
+    async def set(self, id: NSKey, value: Any, ttl: int = None) -> None:
+        """Set a single key."""
+        self.logger.info('set', key=id)
+        value = self._dump_value(value)
+        await self._set(id, value, ttl)
 
     @abc.abstractmethod
-    async def _set(self, key: NSKey, value: bytes, ttl: int):
+    async def _set(self, key: NSKey, value: bytes, ttl: Optional[int]):
         """Set a key value with ttl in sec (0 for infinite)."""
 
-    async def m_set(
-        self,
-        keys: Dict[NSKey, Any],
-        ttl: int = None,
-        use_serializer=True,
-        ignore_conn_errors=IGNORE_CONN_ERRORS,
-        nowait=NOWAIT,
-    ):
-        """Set multiple keys.
-
-        :param keys: <key>: <value>
-        :param ttl: lifetime in seconds, 0 for infinite, None for default
-        :param use_serializer: use a serializer for value encoding (False = return raw)
-        :param ignore_conn_errors: set True to ignore connection errors and skip the operation
-        :param nowait: set operation in background (don't wait for response)
-        """
-        if ttl is None:
-            _ttl = self._default_ttl
-        else:
-            _ttl = ttl
-        key_dict = {k: self._dump_value(v, use_serializer) for k, v in keys.items()}
-        self.logger.info('m_set', keys=list(keys.keys()), ttl=ttl)
-        _task = self._wrap_exec(self._m_set(key_dict, _ttl), ignore_conn_errors)
-        if nowait:
-            asyncio.create_task(_task)
-        else:
-            await _task
+    async def m_set(self, data: Dict[NSKey, Any], ttl: int = None) -> None:
+        """Set multiple keys."""
+        key_dict = {k: self._dump_value(v) for k, v in data.items()}
+        self.logger.info('m_set', keys=data.keys(), ttl=ttl)
+        await self._m_set(key_dict, ttl)
 
     @abc.abstractmethod
     async def _m_set(self, keys: Dict[NSKey, bytes], ttl: int):
         """Set multiple keys at once with ttl in sec (0 for inf)."""
 
-    async def delete(self, key: NSKey, ignore_conn_errors=IGNORE_CONN_ERRORS, nowait=NOWAIT) -> None:
+    async def delete(self, id: NSKey) -> None:
         """Remove a key from cache."""
-        self.logger.info('delete', key=key)
-        _task = self._wrap_exec(self._delete(key), ignore_conn_errors)
-        if nowait:
-            asyncio.create_task(_task)
-        else:
-            await _task
+        self.logger.info('delete', key=id)
+        await self._delete(id)
 
     @abc.abstractmethod
     async def _delete(self, key: str):
         """Remove one key at once."""
 
-    async def m_delete(self, keys: Collection[NSKey], ignore_conn_errors=IGNORE_CONN_ERRORS, nowait=NOWAIT) -> None:
+    async def m_delete(self, id: Collection[NSKey]) -> None:
         """Remove multiple keys at once."""
-        self.logger.info('m_delete', keys=keys)
-        _task = self._wrap_exec(self._m_delete(*keys), ignore_conn_errors)
-        if nowait:
-            asyncio.create_task(_task)
-        else:
-            await _task
+        self.logger.info('m_delete', keys=id)
+        await self._m_delete(*id)
 
     @abc.abstractmethod
     async def _m_delete(self, *keys: NSKey):
         """Remove multiple keys at once."""
 
-    async def _wrap_exec(self, f, ignore_conn_errors: bool):
-        try:
-            return await f
-        except tuple(self.CONNECTION_ERROR_CLASSES):
-            if not ignore_conn_errors:
-                raise
-
-    def _load_value(self, value, use_serializer: bool):
+    def _load_value(self, value):
         if value is None:
             return
-        elif use_serializer:
+        elif self._serializer:
             return self._serializer.loads(value)
         else:
             return value
 
-    def _dump_value(self, value, use_serializer: bool):
-        if use_serializer:
+    def _dump_value(self, value):
+        if self._serializer:
             return self._serializer.dumps(value)
         else:
             return value
```

## kaiju_tools/encoding.py

```diff
@@ -33,22 +33,15 @@
     'msgpack_types',
     'msgpack_dumps',
     'msgpack_loads',
     'serializers',
 ]
 
 
-class MimeTypes:
-    """Standard message type headers."""
-
-    json = 'application/json'
-    msgpack = 'application/msgpack'
-
-
-class Serializable(abc.ABC):
+class Serializable:
     """Class which supports serialization of its attributes."""
 
     serializable_attrs = None  #: Should be a frozenset or None. If None, then all will be used for serialization.
     include_null_values = True  #: include null values in a representation
 
     def repr(self) -> dict:
         """Must return a representation of object __init__ arguments."""
@@ -107,14 +100,21 @@
         return {'__cls': self.__class__.__name__, '__attrs': super().repr()}
 
     @classmethod
     def from_repr(cls, attrs: dict):
         return cls(**attrs)  # noqa
 
 
+class MimeTypes:
+    """Standard message type headers."""
+
+    json = 'application/json'
+    msgpack = 'application/msgpack'
+
+
 class SerializedClasses(AbstractClassRegistry):
     """Serialized class."""
 
     base_classes = [SerializedClass]
 
 
 class SerializerInterface(abc.ABC):
@@ -277,20 +277,20 @@
             'You either need to inherit from `kaiju_tools.serialization.Serializable`'
             ' or to set up your own `repr()` method or to set up you own'
             ' `pack_b` and `unpack_b` methods.'
         )
 
     def to_bytes(self) -> bytes:
         """Pack object to bytes (you can use a struct here to optimize size)."""
-        return dumps(self.repr())  # noqa
+        return msgpack_dumps(self.repr())  # noqa
 
     @classmethod
     def from_bytes(cls, data: bytes) -> 'MsgpackType':
         """Unpack bytes into object."""
-        return cls(**loads(data))  # noqa
+        return cls(**msgpack_loads(data))  # noqa
 
 
 class ReservedClassIDs:
     """Msgpack ids reserved by the library."""
 
     # reserved from 0 to 16 (incl.)
```

## kaiju_tools/exceptions.py

```diff
@@ -1,20 +1,17 @@
 """API exceptions."""
 
-import sys
-import types
 from typing import Union
 
 from aiohttp.client import ClientResponseError
 from fastjsonschema import JsonSchemaValueException
 
 from kaiju_tools.serialization import Serializable
 
 __all__ = [
-    'ensure_traceback',
     'APIException',
     'InternalError',
     'ClientError',
     'ValidationError',
     'NotFound',
     'MethodNotFound',
     'InvalidParams',
@@ -28,32 +25,14 @@
     'NotAuthorized',
     'InvalidLicense',
     'HTTPRequestError',
     'JSONRPCError',
 ]
 
 
-def ensure_traceback(exc: Exception):
-    """Add an empty traceback to the exception which points at this line of code.
-
-    This code is required only for Sentry (raven) client to actually recognize an exception. It should be removed
-    once a custom Sentry client is implemented.
-    """
-    if exc.__traceback__:
-        return exc
-    tb = None
-    base_exc = getattr(exc, 'base_exc')
-    if base_exc and base_exc.__traceback__:
-        tb = base_exc.__traceback__
-    if not tb:
-        frame = sys._getframe(0)  # noqa
-        tb = types.TracebackType(None, frame, frame.f_lasti, frame.f_lineno)  # artificial traceback
-    return exc.with_traceback(tb)
-
-
 class APIException(Serializable, Exception):
     """Base exception class.
 
     Also see :class:`.Serializable`.
 
     :param message: user-friendly message or message template
     :param code: app friendly error code string, which can be used for localization
@@ -64,15 +43,15 @@
     __slots__ = ('message', 'status', 'data', 'id', 'debug', 'base_exc')
 
     status_code: int = 500
 
     def __init__(
         self,
         message: str = '',
-        id: Union[int, None] = None,  # noqa I know
+        id: Union[int, None] = None,
         base_exc: Exception = None,
         debug: bool = False,
         data: dict = None,
         **extras,
     ):
         """Initialize."""
         self.id = id
```

## kaiju_tools/functions.py

```diff
@@ -165,15 +165,15 @@
 
     return wrapper
 
 
 async def async_run_in_thread(f, args: tuple = None, kws: dict = None, max_timeout: float = None):
     """Run a synchronous function in a separate thread as an async function. Use with caution.
 
-    :param f: function
+    :param f: callable object
     :param args: function arguments
     :param kws: function keyword arguments
     :param max_timeout: max execution time in seconds (None for no limit)
 
     :return: function result
     :raises ConcurrentTimeoutError: on execution timeout
     """
@@ -190,15 +190,15 @@
             if max_timeout:
                 async with timeout(max_timeout):
                     result = await future
             else:
                 result = await future
         except ConcurrentTimeoutError:
             tp.shutdown(wait=False)
-            for t in tp._threads:  # noqa  can't do otherwise
+            for t in tp._threads:  # noqa: reasonable
                 terminate_thread(t)
             raise
         else:
             return result
 
 
 def async_(__f):
@@ -245,15 +245,15 @@
 
         return _wrap
 
     return __params
 
 
 def debug_only(f):
-    """Decorate a debug-only method (will not be available in the production mode."""
+    """Decorate a debug-only method (will not be available in the production mode)."""
     f._debug_only_ = True
     return f
 
 
 class _Timeout:
     __slots__ = ('_timeout', '_loop', '_task', '_handler')
```

## kaiju_tools/http.py

```diff
@@ -10,27 +10,29 @@
 from aiohttp_cors import CorsViewMixin
 from aiohttp.client import ClientSession, TCPConnector, ClientResponseError
 from aiohttp.cookiejar import CookieJar
 from aiohttp.http_websocket import WSMessage
 from aiohttp.web import Request, Response, View, json_response, WebSocketResponse, WSMsgType, middleware
 from aiohttp.web_exceptions import HTTPClientError
 
-from kaiju_tools.rpc import BaseRPCClientService, RPCRequest, RPCError, BaseSessionService, JSONRPCServer
+from kaiju_tools.interfaces import SessionInterface, App
 from kaiju_tools.types import Session, Scope
+from kaiju_tools.rpc import BaseRPCClient, RPCRequest, RPCError, JSONRPCServer, JSONRPCHeaders
 from kaiju_tools.exceptions import HTTPRequestError, InternalError, ClientError
 from kaiju_tools.app import ContextableService
 from kaiju_tools.serialization import dumps, loads
+from kaiju_tools.sessions import AuthenticationService, SessionService
 
 __all__ = [
     'HTTPService',
     'RPCClientService',
     'error_middleware',
     'JSONRPCView',
-    'jsonrpc_websocket_handler',
     'RPCClientService',
+    'session_middleware',
 ]
 
 
 class HTTPService(ContextableService):
     """HTTP transport."""
 
     UPLOAD_CHUNK_SIZE = 4096 * 1024
@@ -139,15 +141,15 @@
         t0 = time()
         async with self.session.request(
             method,
             url,
             params=params,
             headers=headers,
             data=data,
-            cookies=self.session.cookie_jar._cookies,  # noqa ? pycharm
+            # cookies=self.session.cookie_jar._cookies,  # noqa ? pycharm
             json=json,
             **kws,
         ) as response:
             response.encoding = 'utf-8'
             text = await response.text()
             t = int((time() - t0) * 1000)
             if response.status >= 400:
@@ -160,15 +162,15 @@
                     history=response.history,
                     status=response.status,
                 )
                 exc.params = params
                 exc.took_ms = t
                 exc.request = json if json else None
                 exc.response = text
-                raise HTTPRequestError(base_exc=exc, message='HTTP request error')
+                raise HTTPRequestError(base_exc=exc, message=str(exc))
 
         if text is not None and accept_json:
             text = loads(text)
         if self._response_logs:
             self.logger.info(
                 'Response',
                 method=method,
@@ -180,15 +182,15 @@
             )
         return text
 
     def resolve(self, uri: str) -> str:
         return f"{self.host}/{uri.lstrip('/')}"
 
 
-class RPCClientService(BaseRPCClientService):
+class RPCClientService(BaseRPCClient):
     """HTTP JSONRPC client service."""
 
     transport_cls = HTTPService
     _transport: HTTPService
 
     def __init__(self, *args, base_uri: str = '/public/rpc', **kws):
         """Initialize."""
@@ -213,85 +215,104 @@
         error = InternalError(message='Internal error', base_exc=exc)
         request.app.logger.error(str(exc), exc_info=exc)
         return json_response(RPCError(id=None, error=error), dumps=dumps)
     else:
         return response
 
 
-async def jsonrpc_websocket_handler(
-    request: Request, rpc_server_name: str = None, session_service_name: str = None, validate_session: bool = True
-):
-    """Read from websocket."""
-    ws = WebSocketResponse()
-    counter = 0
-    services: ServiceContextManager = request.app.services  # noqa
-    rpc: JSONRPCServer = services.discover_service(rpc_server_name, cls=JSONRPCServer)
-    sessions: BaseSessionService = services.discover_service(session_service_name, cls=BaseSessionService)
-    session: Session = request.get('session', None)
-    scope: Scope = session.scope if session else Scope.GUEST
-    headers = dict(request.headers)
-
-    async def _send_response(_session: Session, headers: dict, result):  # noqa
-        nonlocal session, scope, request
-        session = request['session'] = _session
-        scope = session.scope
-        await ws.send_json(result.repr(), dumps=dumps)
-
-    await ws.prepare(request)
+@middleware
+async def session_middleware(request: Request, handler):
+    app = cast(App, request.app)
+    headers = request.headers
+    if JSONRPCHeaders.SESSION_ID_HEADER in headers:
+        request['session_id'] = headers[JSONRPCHeaders.SESSION_ID_HEADER]
+        return await handler(request)
+    else:
+        cookie_key = f'{app.env}-{app.name}-sid'
+        session_id = request.cookies.get(cookie_key)
+        if session_id:
+            request['session_id'] = session_id
+        response = await handler(request)
+        new_session_id = response.headers.get(JSONRPCHeaders.SESSION_ID_HEADER, '')
+        if session_id != new_session_id:
+            response.set_cookie(cookie_key, new_session_id, secure=not request.app.debug, httponly=True)  # noqa
+        return response
 
-    try:
-        async for msg in ws:
-            msg = cast(WSMessage, msg)
-            if msg.type == WSMsgType.ERROR:
-                request.app.logger.error('Websocket error: %s', ws.exception())
-            elif msg.type == WSMsgType.TEXT:
-                if msg.data == 'close':
-                    await ws.close()
-                    break
-
-                if validate_session:
-                    session_exists = await sessions.session_exists(session.id)
-                    if not session_exists:
-                        session = None  # noqa
-                        del request['session']
-                        await ws.close()
-                        break
-
-                data = loads(msg.data)
-                counter += 1
-                if 'id' not in data:
-                    data['id'] = counter
-                result = await rpc.call(
-                    data, headers=headers, session=session, scope=scope, nowait=True, callback=_send_response
-                )
-                if type(result) is not asyncio.Task:
-                    _headers, result = result
-                    if result:
-                        await ws.send_json(result, dumps=dumps)
-    except Exception as exc:
-        request.app.logger.error('Websocket error', exc_info=exc)
 
-    finally:
-        ws._headers[JSONRPCHeaders.SESSION_ID_HEADER] = session.id if session else ''  # noqa
-        if not ws.closed:
-            await ws.close()
-        return ws
+# async def jsonrpc_websocket_handler(
+#     request: Request, rpc_server_name: str = None, session_service_name: str = None, validate_session: bool = True
+# ):
+#     """Read from websocket."""
+#     ws = WebSocketResponse()
+#     counter = 0
+#     services: ServiceContextManager = request.app.services  # noqa
+#     rpc: JSONRPCServer = services.discover_service(rpc_server_name, cls=JSONRPCServer)
+#     sessions: SessionInterface = services.discover_service(session_service_name, cls=SessionInterface)
+#     session: Session = request.get('session', None)
+#     scope: Scope = session.scope if session else Scope.GUEST
+#     headers = dict(request.headers)
+#
+#     async def _send_response(_session: Session, headers: dict, result):  # noqa
+#         nonlocal session, scope, request
+#         session = request['session'] = _session
+#         scope = session.scope
+#         await ws.send_json(result.repr(), dumps=dumps)
+#
+#     await ws.prepare(request)
+#
+#     try:
+#         async for msg in ws:
+#             msg = cast(WSMessage, msg)
+#             if msg.type == WSMsgType.ERROR:
+#                 request.app.logger.error('Websocket error: %s', ws.exception())
+#             elif msg.type == WSMsgType.TEXT:
+#                 if msg.data == 'close':
+#                     await ws.close()
+#                     break
+#
+#                 if validate_session:
+#                     session_exists = await sessions.session_exists(session.id)
+#                     if not session_exists:
+#                         session = None  # noqa
+#                         del request['session']
+#                         await ws.close()
+#                         break
+#
+#                 data = loads(msg.data)
+#                 counter += 1
+#                 if 'id' not in data:
+#                     data['id'] = counter
+#                 result = await rpc.call(
+#                     data, headers=headers, session=session, scope=scope, nowait=True, callback=_send_response
+#                 )
+#                 if type(result) is not asyncio.Task:
+#                     _headers, result = result
+#                     if result:
+#                         await ws.send_json(result, dumps=dumps)
+#     except Exception as exc:
+#         request.app.logger.error('Websocket error', exc_info=exc)
+#
+#     finally:
+#         ws._headers[JSONRPCHeaders.SESSION_ID_HEADER] = session.id if session else ''  # noqa
+#         if not ws.closed:
+#             await ws.close()
+#         return ws
 
 
 class JSONRPCView(CorsViewMixin, View):
     """JSON RPC server endpoint."""
 
     route = '/public/rpc'
-    rpc_server_name = 'rpc'
 
     async def post(self):
         """Make an RPC request."""
         if not self.request.can_read_body:
             return Response()
-        data = await self.request.text()
-        session: Session = self.request.get('session', None)
-        scope: Scope = session.scope if session else Scope.GUEST
-        rpc: JSONRPCServer = getattr(self.request.app.services, self.rpc_server_name)  # noqa
-        headers, result = await rpc.call(
-            loads(data), headers=dict(self.request.headers), session=session, scope=scope, nowait=False
-        )
+        app = cast(App, self.request.app)
+        rpc = cast(JSONRPCServer, app.services['rpc'])
+        headers = dict(self.request.headers)
+        session_id = self.request.get('session_id')
+        if session_id:
+            headers[JSONRPCHeaders.SESSION_ID_HEADER] = session_id
+        data = await self.request.json(loads=loads)
+        headers, result = await rpc.call(data, headers=headers, nowait=False)
         return json_response(result, headers=headers, dumps=dumps)
```

## kaiju_tools/locks.py

```diff
@@ -1,28 +1,19 @@
-"""Shared locks management."""
+"""Shared locks services and types."""
 
 import abc
 import asyncio
 from time import time
-from typing import Optional, Union, List, NewType, FrozenSet, Dict
+from typing import Dict, Optional, List, FrozenSet, Type
 
-from kaiju_tools.app import ContextableService, Service, Scheduler
+from kaiju_tools.app import Service, ContextableService, Scheduler
+from kaiju_tools.interfaces import Locks
 from kaiju_tools.types import NSKey
 
-__all__ = [
-    'BaseLocksService',
-    'ErrorCode',
-    'LockError',
-    'LockExistsError',
-    'NotLockOwnerError',
-    'LockTimeout',
-    'LockId',
-]
-
-LockId = NewType('LockId', str)
+__all__ = ['BaseLocksService', 'LockError', 'LockTimeout', 'NotLockOwnerError', 'LockExistsError', 'Scheduler', 'Locks']
 
 
 class LockError(Exception):
     """Base class for lock related errors."""
 
 
 class LockExistsError(LockError):
@@ -33,232 +24,197 @@
     """A lock can be released only by its owner."""
 
 
 class LockTimeout(LockError, TimeoutError):
     """Timeout when trying to acquire a lock."""
 
 
-class ErrorCode:
-    """Status and error codes for locks."""
-
-    LOCK_EXISTS = 'LOCK_EXISTS'  #: the lock already present in the db
-    NOT_LOCK_OWNER = 'NOT_OWNER'  #: service trying to release a lock is not a lock owner
-    RUNTIME_ERROR = 'RUNTIME_ERROR'  #: any other error
-    LOCK_ACQUIRE_TIMEOUT = 'LOCK_ACQUIRE_TIMEOUT'
-    OK = 'OK'  #: OK
+class BaseLocksService(ContextableService, Locks, abc.ABC):
+    """Base class for managing shared locks."""
 
+    class ErrorCode:
+        """Status and error codes for locks."""
 
-class BaseLocksService(ContextableService, abc.ABC):
-    """Base class for managing shared locks."""
+        LOCK_EXISTS = 'LOCK_EXISTS'  #: the lock already present in the db
+        NOT_LOCK_OWNER = 'NOT_OWNER'  #: service trying to release a lock is not a lock owner
+        RUNTIME_ERROR = 'RUNTIME_ERROR'  #: any other error
+        LOCK_ACQUIRE_TIMEOUT = 'LOCK_ACQUIRE_TIMEOUT'
+        OK = 'OK'  #: OK
 
     service_name = 'locks'
-    WAIT_RELEASE_REFRESH_INTERVAL = 1  #: (s) interval between tries to acquire a used lock
-    MIN_REFRESH_INTERVAL = 1  #: (s) minimal allowed refresh interval for the daemon
-    REFRESH_INTERVAL = 60  #: (s) how often locks will be renewed by the daemon
-    BASE_TTL = 3 * REFRESH_INTERVAL  #: (s) lifetime of a lock after each renewal
-    DELIMITER = '-'
-    transport_cls = None
 
     def __init__(
         self,
-        app,
+        *args,
         transport: Service = None,
-        refresh_interval: int = REFRESH_INTERVAL,
+        refresh_interval: int = 30,
         scheduler: str = None,
-        logger=None,
+        shared: bool = False,
+        **kws,
     ):
         """Initialize.
 
-        :param app: web app (provided by the service manager)
         :param transport: db / redis connector
         :param refresh_interval:  how often locks will be renewed
-        :param logger: logger instance (provided by the service manager)
+        :param shared: shared between apps
         """
-        super().__init__(app=app, logger=logger)
+        super().__init__(*args, **kws)
+        self.namespace = self.app.namespace_shared if shared else self.app.namespace
+        self.namespace = self.namespace / '_lock'
         self._transport_name = transport
-        self._refresh_interval = max(self.MIN_REFRESH_INTERVAL, int(refresh_interval))
-        self._scheduler: Union[Scheduler, None] = scheduler
-        self._keys: Dict[NSKey, int] = {}
+        self._refresh_interval = max(1, int(refresh_interval))
+        self._base_ttl = self._refresh_interval * 3
+        self._scheduler = scheduler
+        self._keys: Dict[NSKey, float] = {}
         self._closing = False
         self._transport = None
         self._task = None
 
     async def init(self):
         """Initialize."""
-        self._transport = self.discover_service(self._transport_name, cls=self.transport_cls)
+        self._transport = self.discover_service(self._transport_name, cls=self.get_transport_cls())
         self._scheduler = self.discover_service(self._scheduler, cls=Scheduler)
         self._closing = False
         self._task = self._scheduler.schedule_task(
             self._renew_keys,
             self._refresh_interval,
             name=f'{self.service_name}._renew_keys',
-            policy=self._scheduler.ExecPolicy.WAIT,
+            policy=self._scheduler.ExecPolicy.CANCEL,
         )
         self._keys = {}
 
     async def close(self):
         """Close."""
         self._closing = True
         self._task.enabled = False
 
-    async def wait(self, key: NSKey, timeout: float = None) -> None:
-        """Wait for a lock and return when it's released.
-
-        :param key: lock key name
-        :param timeout: optional max wait time in seconds
-
-        :raises LockAcquireTimeout: when timeout reached
-        :raises LockError: any internal error
-        """
-        t = 0
-        while 1:
-            if key in self._keys or key in (await self.m_exists([key])):
-                self.logger.debug('wait', key=key)
-                await asyncio.sleep(self.WAIT_RELEASE_REFRESH_INTERVAL)
-                t += self.WAIT_RELEASE_REFRESH_INTERVAL
-                if timeout and t > timeout:
-                    raise LockTimeout(ErrorCode.LOCK_ACQUIRE_TIMEOUT)
-            else:
-                break
+    @classmethod
+    @abc.abstractmethod
+    def get_transport_cls(cls) -> Type:
+        """Get transport class required for this service."""
 
     async def acquire(
-        self, key: NSKey, identifier: LockId = None, ttl: int = None, wait=True, timeout: float = None
-    ) -> LockId:
+        self, id: NSKey, identifier: Locks.LockId = None, ttl: int = None, wait=True, timeout: float = None
+    ) -> Locks.LockId:
         """Wait for lock and acquire it.
 
-        :param key: lock name
+        :param id: lock name
         :param identifier: service/owner identifier, if id is None then the app['id'] will be used
         :param ttl: optional ttl in seconds, None for eternal (until app exists)
         :param wait: wait for a lock to release (if False then it will raise a `LockError`
-            if lock with such key already exists
+            if lock with such key already exists)
         :param timeout: optional max wait time in seconds
 
         :raises LockExistsError:
         :raises LockAcquireTimeout: when timeout's reached
         :raises LockError: any internal error
         """
-
-        def _wait():
-            if wait:
-                return asyncio.sleep(self.WAIT_RELEASE_REFRESH_INTERVAL)
-            raise LockExistsError(ErrorCode.LOCK_EXISTS)
-
         if identifier is None:
-            identifier = LockId(str(self.app['id']))
-
-        t0 = 0
-
-        while 1:
-            if ttl is None:
-                new_ttl = self.BASE_TTL
-            else:
-                new_ttl = min(self.BASE_TTL, int(ttl))
-
-            t = int(time()) + 1
-
-            if timeout and t0 > timeout:
-                raise LockTimeout(ErrorCode.LOCK_ACQUIRE_TIMEOUT)
-
-            t0 += self.WAIT_RELEASE_REFRESH_INTERVAL
-
-            if self._closing:
-                await _wait()
+            identifier = Locks.LockId(str(self.app.id))
+        if ttl is None:
+            raise ValueError('TTL must be set')
+
+        t0 = time()
+
+        while not self._closing:
+            t = time()
+            deadline = self._keys.get(id)
+            if deadline and deadline > t:
+                if not wait:
+                    raise LockExistsError(id)
+                elif timeout and t - t0 > timeout:
+                    raise LockTimeout(id)
+                await asyncio.sleep(1)
                 continue
 
-            if key in self._keys:
-                _deadline = self._keys[key]
-                if _deadline is None or _deadline > t:
-                    await _wait()
-                    continue
-                else:
-                    del self._keys[key]
-
+            actual_ttl = min(self._base_ttl, ttl)
             try:
-                await self._acquire([key], identifier, new_ttl)
+                await self._acquire([id], identifier=identifier, ttl=actual_ttl)
             except LockExistsError:
-                await _wait()
-            except Exception as exc:
-                raise LockError(ErrorCode.RUNTIME_ERROR) from exc
-            else:
-                self.logger.info('locked', key=key)
-                self._keys[key] = t + new_ttl if ttl else None
-                return identifier
+                if not wait:
+                    raise LockExistsError(id)
+                elif timeout and t - t0 > timeout:
+                    raise LockTimeout(id)
+                await asyncio.sleep(1)
+                continue
+
+            self._keys[id] = time() + ttl
+            self.logger.info('locked', key=id)
+            return identifier
 
-    async def release(self, key: NSKey, identifier: LockId) -> None:
+        raise LockTimeout(id)
+
+    async def release(self, id: NSKey, identifier: Locks.LockId) -> None:
         """Release a lock.
 
-        :param key: lock name
+        :param id: lock name
         :param identifier: service/owner identifier
         :raises LockError: if the lock can't be released by this service
         """
-        self.logger.debug('release', key=key)
+        self.logger.debug('release', key=id)
         try:
-            await self._release([key], identifier)
+            await self._release([id], identifier)
         except NotLockOwnerError as exc:
             raise exc
         except Exception as exc:
-            raise LockError(ErrorCode.RUNTIME_ERROR) from exc
-        self.logger.info('released', key=key)
-        if key in self._keys:
-            del self._keys[key]
+            raise LockError(self.ErrorCode.RUNTIME_ERROR) from exc
+        self.logger.info('released', key=id)
+        if id in self._keys:
+            del self._keys[id]
 
-    async def owner(self, key: NSKey) -> Optional[LockId]:
+    async def owner(self, id: NSKey) -> Optional[Locks.LockId]:
         """Return a current lock owner identifier or None if not found / has no owner."""
-        owner = await self._owner(key)
-        return LockId(owner)
+        owner = await self._owner(id)
+        return Locks.LockId(owner)
 
-    async def is_owner(self, key: NSKey) -> bool:
+    async def is_owner(self, id: NSKey) -> bool:
         """Return `True` if the current instance is an owner of this lock."""
-        owner = await self._owner(key)
+        owner = await self._owner(id)
         return str(owner) == str(self.app['id'])
 
     @abc.abstractmethod
-    async def m_exists(self, keys: List[NSKey]) -> FrozenSet[NSKey]:
+    async def m_exists(self, id: List[NSKey]) -> FrozenSet[NSKey]:
         """Check if locks with such keys exist. Return a set of existing keys."""
 
     @abc.abstractmethod
-    async def _acquire(self, keys: List[NSKey], identifier: LockId, ttl: int):
+    async def _acquire(self, keys: List[NSKey], identifier: Locks.LockId, ttl: int):
         """Set a list of specified keys. Also keep in mind that the operation must be atomic or transactional.
 
-        :param keys: a list of keys
-        :param identifier: key value
-        :param ttl: key ttl in sec
-        :raises `LockExistsError` if lock exists
+        :raises LockExistsError:
         """
 
     @abc.abstractmethod
-    async def _release(self, keys: List[NSKey], identifier: LockId) -> None:
+    async def _release(self, keys: List[NSKey], identifier: Locks.LockId) -> None:
         """Release a lock.
 
-        Must raise `NotALockOwnerError` if identifier doesn't match the stored one.
-        Also keep in mind that the operation must be atomic or transactional.
+        :raises NotALockOwnerError: provided identifier doesn't match with the stored value
         """
 
     @abc.abstractmethod
     async def _renew(self, keys: List[NSKey], values: List[int]) -> None:
         """Renew keys TTLs with the new provided values (in sec)."""
 
     @abc.abstractmethod
-    async def _owner(self, key: NSKey) -> LockId:
+    async def _owner(self, key: NSKey) -> Locks.LockId:
         """Return a key owner or None if there's no key or owner."""
 
     async def _renew_keys(self):
         """Renew existing locks."""
-        t = int(time()) + 1
+        t = time()
         keys, values, to_remove = [], [], []
 
         for key, deadline in self._keys.items():
             if deadline is None:
                 keys.append(key)
-                values.append(self.BASE_TTL)
+                values.append(self._refresh_interval)
             elif deadline <= t:
                 to_remove.append(key)
             else:
                 keys.append(key)
-                ttl = min(deadline - t, self.BASE_TTL)
+                ttl = min(deadline - t, self._refresh_interval)
                 values.append(ttl)
 
         for key in to_remove:
             del self._keys[key]
 
         if keys:
             await self._renew(keys, values)
```

## kaiju_tools/logging.py

```diff
@@ -1,21 +1,21 @@
 """Logging handlers, formatters and interfaces."""
 
 import abc
 import inspect
 import logging
 import sys
-from contextvars import ContextVar  # noqa pycharm bug?
+from contextvars import ContextVar  # noqa: pycharm
 from typing import TypedDict, Union
 from logging import DEBUG, INFO, WARNING, ERROR
+from weakref import proxy
 
 from kaiju_tools.class_registry import AbstractClassRegistry
 from kaiju_tools.encoding import serializers
 from kaiju_tools.exceptions import APIException, InternalError
-from kaiju_tools.types import REQUEST_CONTEXT
 
 __all__ = [
     'LogTrace',
     'LogException',
     'LogExceptionTrace',
     'LogMessage',
     'TextFormatter',
@@ -122,15 +122,15 @@
     ):
         if extra is None:
             extra = {}
         extra['_data'] = kws
         extra['_cid'] = _cid
         extra['_sid'] = _sid
         extra['_dline'] = _dline
-        super()._log(  # noqa
+        super()._log(  # noqa: reasonable
             level=level,
             msg=msg,
             args=args,
             exc_info=exc_info,
             extra=extra,
             stack_info=stack_info,
             stacklevel=stacklevel,
@@ -143,24 +143,28 @@
 
 class Adapter(logging.LoggerAdapter):
     """Logging adapter and log context manager.
 
     It is used to provide contextual information to log records.
     """
 
+    def __init__(self, app, *args, **kws):
+        super().__init__(*args, **kws)
+        self._app = proxy(app)
+
     def process(self, msg: str, kwargs: dict) -> (str, dict):
         """Process the logging message and keyword arguments."""
-        ctx = REQUEST_CONTEXT.get()
+        ctx = self._app.request_context.get()
         if ctx:
             kwargs['_cid'] = ctx['correlation_id']
             kwargs['_sid'] = ctx['session_id']
             kwargs['_dline'] = ctx['request_deadline']
         return msg, kwargs
 
-    def getChild(self, suffix):  # noqa python fails to follow the standards here
+    def getChild(self, suffix):  # noqa: python fails to follow the naming standards here
         """Get child logger.
 
         Compatibility method for `Logged` class.
         """
         return Adapter(self.logger.getChild(suffix), self.extra)
 
 
@@ -264,32 +268,33 @@
             msg = str(msg)
         if self.colored_mode:
             self.set_color(record, msg)
         return msg
 
     def formatMessage(self, record) -> str:
         """Format log message."""
-        msg = self.create_message(record)  # noqa (pycharm)
+        msg = self.create_message(record)  # noqa
         return str(msg)
 
     def formatException(self, ei):
         """Format exception (skip it)."""
         return
 
     def create_message(self, record: _LogRecord) -> LogMessage:
         """Create log message dict from a log record."""
         msg = {'t': record.created, 'name': record.name, 'lvl': record.levelname, 'msg': record.getMessage()}
-        if record._cid:
-            msg['cid'] = record._cid
-        if record._sid:
-            msg['sid'] = record._sid
-        if record._dline:
-            msg['t_max'] = record._dline
-        if record._data:
-            msg['data'] = record._data
+        cid, sid, dline, data = record._cid, record._sid, record._dline, record._data  # noqa
+        if cid:
+            msg['cid'] = cid
+        if sid:
+            msg['sid'] = sid
+        if dline:
+            msg['t_max'] = dline
+        if data:
+            msg['data'] = data
         if record.exc_info:
             error_cls, error, stack = record.exc_info
             if not isinstance(error, APIException):
                 error = InternalError(message=str(error), base_exc=error)
             error.debug = self._debug
             error.debug = True  # a little hack to enable trace info (probably there's a better way)
             msg['error'] = error.repr()
```

## kaiju_tools/loop.py

```diff
@@ -1,12 +1,11 @@
-"""
-Asyncio loop initialization.
-"""
+"""Asyncio loop initialization."""
 
 import asyncio
 import os
 
 if os.name != 'nt':
-    import uvloop
+    import uvloop  # noqa: legacy
+
     asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
 
 loop = asyncio.get_event_loop()
```

## kaiju_tools/rpc.py

```diff
@@ -1,51 +1,58 @@
 """RPC services and classes."""
 
 import abc
 import asyncio
 import os
-import uuid
 import warnings
 from binascii import b2a_hex
-from datetime import datetime
+from enum import Enum
 from fnmatch import fnmatch
-from hashlib import blake2b
-from secrets import randbits
 from time import time
 from typing import cast, List, Union, TypedDict, Callable, Dict, Optional, Tuple, Awaitable, NewType, Any
 
-import fastjsonschema  # type: ignore
-
+from kaiju_tools.interfaces import (
+    AbstractRPCCompatible,
+    SessionInterface,
+    PublicInterface,
+    RPCServer,
+    RPCClient,
+    AuthenticationInterface,
+)
 from kaiju_tools.annotations import AnnotationParser, FunctionAnnotation
-from kaiju_tools.cache import BaseCacheService
 from kaiju_tools.functions import get_short_uid, timeout, retry
 from kaiju_tools.jsonschema import compile_schema
 from kaiju_tools.templates import Template
-from kaiju_tools.app import Service, ContextableService
+from kaiju_tools.app import Service, ContextableService, Scheduler
 from kaiju_tools.serialization import Serializable
 from kaiju_tools.encoding import MsgpackType, msgpack_types, ReservedClassIDs, msgpack_dumps, msgpack_loads
-from kaiju_tools.types import REQUEST_CONTEXT, REQUEST_SESSION, RequestContext, Session, Scope
+from kaiju_tools.app import REQUEST_CONTEXT, REQUEST_SESSION, RequestContext
+from kaiju_tools.sessions import TokenClientService
+from kaiju_tools.types import Scope, Session
 from kaiju_tools.exceptions import *
 
 __all__ = (
     'JSONRPC',
     'RPCMessage',
     'RPCRequest',
     'RPCResponse',
     'RPCError',
     'JSONRPCHeaders',
-    'BaseSessionService',
     'AbstractRPCCompatible',
     'PermissionKeys',
     'AbstractTokenInterface',
     'RPCClientError',
-    'BaseRPCClientService',
+    'RPCServer',
+    'RPCClient',
     'RequestHeaders',
     'MethodInfo',
     'JSONRPCServer',
+    'Scope',
+    'Session',
+    'BaseRPCClient',
 )
 
 JSONRPC = '2.0'  #: protocol version
 
 
 class RPCMessage(Serializable, MsgpackType, abc.ABC):
     """Base JSONRPC message class."""
@@ -156,215 +163,22 @@
     CALLBACK_ID = 'RPC-Callback'
     ABORT_ON_ERROR = 'RPC-Batch-Abort-Error'
     USE_TEMPLATE = 'RPC-Batch-Template'
 
     SESSION_ID_HEADER = 'Session-Id'
 
 
-class BaseSessionService(Service, abc.ABC):
-    """Session store interface used by the rpc server."""
-
-    service_name = 'sessions'
-    session_cls = Session
-
-    def __init__(
-        self,
-        app,
-        cache_service: BaseCacheService = None,
-        session_idle_timeout: int = 24 * 3600,
-        exp_renew_interval: int = 3600,
-        salt: str = 'SALT',
-        logger=None,
-    ):
-        """Initialize.
-
-        :param app: web app
-        :param cache_service: cache service instance
-        :param session_idle_timeout: (s) Idle lifetime for each session.
-        :param exp_renew_interval: (s)
-        :param salt: salt for user agent hashing, change it to invalidate all current sessions
-        :param logger:
-        """
-        Service.__init__(self, app, logger=logger)
-        self._ns = self.app.ns / '_session'
-        self._cache: BaseCacheService = self.discover_service(cache_service, cls=BaseCacheService)
-        self.session_idle_timeout = session_idle_timeout
-        self.exp_renew_interval = exp_renew_interval
-        self.salt = salt.encode('utf-8')
-
-    def get_new_session(self, data: dict, *, user_agent: Union[str, bytes] = '') -> Session:
-        """Create and return a new session (not stored yet).
-
-        :param data: session data
-        :param user_agent: user agent or client id or hash to match session in subsequent requests
-        """
-        h_agent = self._get_agent_hash(user_agent) if type(user_agent) is str else user_agent
-        session = self._create_new_session(data, h_agent)
-        self.logger.debug('new session', session_id=session.id)
-        return session
-
-    async def session_exists(self, session_id: str, /) -> bool:
-        """Check if session exists in the session cache."""
-        return await self._cache.exists(self._ns.get_key(session_id))
-
-    async def save_session(self, session: Session, /) -> None:
-        """Save session to the storage.
-
-        The session will be stored only if it is marked as stored, and it has been changed.
-        Token-auth sessions and initial sessions without data won't be stored.
-        """
-        if not session or not session.stored:
-            return
-
-        key = self._ns.get_key(session.id)
-        exp = int(time()) + self.session_idle_timeout
-        if session.changed:
-            self.logger.info('saving session', session_id=session.id)
-            await self._cache.set(key, session.repr(), ttl=exp, nowait=True)
-            data = session.repr()
-            data['expires'] = exp
-            await self._save_session(data)
-        elif session.loaded and session.expires - time() < self.exp_renew_interval:
-            asyncio.create_task(self._cache._transport.expire(key, exp))  # noqa
-            await self._update_session_exp(session.id, exp)
-
-    @abc.abstractmethod
-    async def _save_session(self, session_data: dict) -> None:
-        """Save session in database backend."""
-
-    @abc.abstractmethod
-    async def _update_session_exp(self, session_id, exp) -> None:
-        """Save session in database backend."""
-
-    async def delete_session(self, session: Session, /) -> None:
-        """Delete session from the storage."""
-        if session and session.stored and session.loaded:
-            self.logger.info('removing session', session_id=session.id)
-            key = self._ns.get_key(session.id)
-            await self._cache.delete(key, nowait=True)
-            try:
-                await self._delete_session(session.id)
-            except NotFound:
-                pass
-
-    @abc.abstractmethod
-    async def _delete_session(self, session_id) -> None:
-        """Delete session in database."""
-
-    async def load_session(self, session_id: str, /, *, user_agent: str = '') -> Optional[Session]:
-        """Load session from the storage.
-
-        :param session_id: unique session id
-        :param user_agent: user agent or client id for security purposes
-        :return: returns None when session is not available
-        """
-        key = self._ns.get_key(session_id)
-        session = cached = await self._cache.get(key)
-        if not session:
-            try:
-                session = await self._get_session(session_id)
-            except NotFound:
-                self.logger.info('session not found', session_id=session_id)
-                return
-
-            if session['expires'] < time():
-                self.logger.debug('session expired', session_id=session_id)
-                await self._cache.delete(key, nowait=True)
-                await self._delete_session(session_id)
-                return
-
-        agent_hash = self._get_agent_hash(user_agent)
-        session = self.session_cls(**session, _stored=True, _changed=False, _loaded=True)
-        if session.h_agent != agent_hash:
-            self.logger.info('user agent mismatch', session_id=session_id)
-            return
-
-        self.logger.debug('session loaded', session_id=session_id)
-        if not cached:
-            await self._cache.set(key, session.repr(), nowait=True)
-        return session
-
-    @abc.abstractmethod
-    async def _get_session(self, session_id) -> dict:
-        """Get session."""
-
-    def _create_new_session(self, data: dict, h_agent: bytes) -> Session:
-        """Create a new session object."""
-        return self.session_cls(
-            id=uuid.UUID(int=randbits(128)).hex,
-            user_id=None,
-            permissions=frozenset(),
-            data=data,
-            expires=int(time()) + self.session_idle_timeout,
-            created=datetime.now(),
-            h_agent=h_agent,
-            _changed=bool(data),
-            _stored=True,
-            _loaded=False,
-        )
-
-    def _get_agent_hash(self, user_agent: str) -> bytes:
-        return blake2b(user_agent.encode('utf-8'), digest_size=16, salt=self.salt).digest()
-
-
 class PermissionKeys:
     """Permission scopes."""
 
     GLOBAL_SYSTEM_PERMISSION = Scope.SYSTEM
     GLOBAL_USER_PERMISSION = Scope.USER
     GLOBAL_GUEST_PERMISSION = Scope.GUEST
 
 
-class AbstractRPCCompatible(abc.ABC):
-    """Class with an RPC interface."""
-
-    DEFAULT_PERMISSION = '*'
-    PermissionKeys = PermissionKeys
-
-    @staticmethod
-    def get_session() -> Union[Session, None]:
-        """Get current user session."""
-        return REQUEST_SESSION.get()
-
-    @staticmethod
-    def get_request_context() -> Union[RequestContext, None]:
-        """Get current user request context."""
-        return REQUEST_CONTEXT.get()
-
-    def get_user_id(self):
-        """Return current session user id."""
-        session = self.get_session()
-        return session.user_id if session else None
-
-    def has_permission(self, permission: str) -> bool:
-        """Check  if a user session has a particular permission."""
-        session = self.get_session()
-        return permission in session.permissions or self.system_user() if session else True
-
-    def system_user(self) -> bool:
-        """Check if user session has the system scope."""
-        session = self.get_session()
-        return PermissionKeys.GLOBAL_SYSTEM_PERMISSION.value >= session.scope.value if session else None
-
-    @property
-    def routes(self) -> dict:
-        """List RPC routes."""
-        return {}
-
-    @property
-    def permissions(self) -> dict:
-        """List RPC routes permissions."""
-        return {}
-
-    @property
-    def validators(self) -> dict:
-        """List of RPC routes validation schemas."""
-        return {}
-
-
 class AbstractTokenInterface(abc.ABC):
     """Describes a token provider service methods to be able to be used by the :class:`.AbstractRPCClientService`."""
 
     @abc.abstractmethod
     async def get_token(self) -> str:
         """Must always return a valid auth token."""
 
@@ -376,113 +190,14 @@
         super().__init__(*args, **kws)
         self.response = response
 
     def __str__(self):
         return self.message
 
 
-class BaseRPCClientService(ContextableService, abc.ABC):
-    """JSONRPC client."""
-
-    transport_cls = Service
-
-    def __init__(self, app, transport: str, logger=None):
-        super().__init__(app=app, logger=logger)
-        self._transport = transport
-
-    async def init(self):
-        self._transport = self.discover_service(self._transport, cls=self.transport_cls)
-
-    async def call(
-        self,
-        method: str,
-        params: Union[dict, None] = None,
-        nowait: bool = False,
-        request_id: int = 0,
-        max_timeout: int = None,
-        use_context: bool = True,
-    ) -> Union[Any, None]:
-        """Make an RPC call.
-
-        :param method: rpc method name
-        :param params: method call arguments
-        :param nowait: create a 'notify' request - do not wait for the result
-        :param request_id: optional request id (usually you don't need to set it)
-        :param max_timeout: request timeout in sec
-        :param use_context: use app request context such as correlation id and request chain deadline
-        """
-        headers = self._create_request_headers(max_timeout, use_context, nowait)
-        _id = None if nowait else request_id
-        body = RPCRequest(id=_id, method=method, params=params)
-        response = await self._request(body, headers)
-        result = self._process_response(response)
-        if isinstance(result, Exception):
-            raise result
-        return result
-
-    async def call_multiple(
-        self,
-        *requests: dict,
-        raise_exception: bool = True,
-        nowait: bool = False,
-        max_timeout: int = None,
-        use_context: bool = True,
-    ) -> Union[List, None]:
-        """Make an RPC batch call.
-
-        :param requests: list of request dicts
-        :param nowait: create a 'notify' request - do not wait for the result
-        :param max_timeout: request timeout in sec
-        :param use_context: use app request context such as correlation id and request chain deadline
-        :param raise_exception: raise exception instead of returning error objects in the list
-        """
-        headers = self._create_request_headers(max_timeout, use_context, nowait)
-        body = [RPCRequest(id=n, **req) for n, req in enumerate(requests)]
-        response = await self._request(body, headers)
-        if response is None:  # for notify requests
-            return
-        results = []
-        for resp in response:
-            resp = self._process_response(resp)
-            if isinstance(resp, Exception) and raise_exception:
-                raise resp
-            results.append(resp)
-        return results
-
-    @abc.abstractmethod
-    async def _request(self, body: Union[RPCRequest, List[RPCRequest]], headers: dict):
-        """Make an external requests via transport service."""
-
-    @staticmethod
-    def _create_request_headers(max_timeout, use_context, nowait) -> dict:
-        headers = {}
-        ctx = REQUEST_CONTEXT.get() if use_context else None
-        if ctx:
-            headers[JSONRPCHeaders.CORRELATION_ID_HEADER] = ctx['correlation_id']
-            if not nowait:
-                headers[JSONRPCHeaders.REQUEST_DEADLINE_HEADER] = ctx['request_deadline']
-        else:
-            headers[JSONRPCHeaders.CORRELATION_ID_HEADER] = get_short_uid()
-        if max_timeout:
-            headers[JSONRPCHeaders.REQUEST_TIMEOUT_HEADER] = max_timeout
-        return headers
-
-    def _process_response(self, response: dict):
-        if 'error' in response:
-            return self._create_exception(response['error'])
-        else:
-            return response['result']
-
-    @staticmethod
-    def _create_exception(error_data: dict) -> RPCClientError:
-        exc = RPCClientError(message=error_data['message'], data=error_data['data'])
-        exc.status_code = error_data['code']
-        return exc
-
-
 _RequestId = NewType('_RequestId', Union[int, None])
 
 
 class _Task(asyncio.Task):
     deadline: int
     started: int
 
@@ -507,29 +222,30 @@
     f: Callable
     signature: FunctionAnnotation
     service_name: str
     permission: Scope
     validator: Callable
 
 
-class JSONRPCServer(ContextableService, AbstractRPCCompatible):
+class JSONRPCServer(ContextableService, AbstractRPCCompatible, RPCServer):
     """A simple JSON RPC interface with method execution and management tasks."""
 
     service_name = 'rpc'
     _permission_levels = {
         AbstractRPCCompatible.PermissionKeys.GLOBAL_SYSTEM_PERMISSION: Scope.SYSTEM,
         AbstractRPCCompatible.PermissionKeys.GLOBAL_USER_PERMISSION: Scope.USER,
         AbstractRPCCompatible.PermissionKeys.GLOBAL_GUEST_PERMISSION: Scope.GUEST,
     }
 
     def __init__(
         self,
         app,
         *,
-        session_service: BaseSessionService = None,
+        session_service: SessionInterface = None,
+        auth_service: AuthenticationInterface = None,
         max_parallel_tasks: int = 64,
         default_request_time: int = 120,
         max_request_time: int = 600,
         enable_permissions: bool = True,
         request_logs: bool = False,
         response_logs: bool = False,
         blacklist_routes: List[str] = None,
@@ -550,14 +266,15 @@
         :param blacklist_scope: integer value to blacklist permission scopes lower or equal to this value
         :param use_annotation_parser: annotation parser for non-validated method will be used and will try to
             create a method validator from its annotations
         :param logger:
         """
         ContextableService.__init__(self, app=app, logger=logger)
         self._sessions = session_service
+        self._auth = auth_service
         self._max_parallel_tasks = max(1, int(max_parallel_tasks))
         self._default_request_time = max(1, int(default_request_time))
         self._max_request_time = max(self._default_request_time, int(max_request_time))
         self._enable_permissions = enable_permissions
         self._debug = self.app.debug
         self._request_logs = request_logs
         self._response_logs = response_logs
@@ -573,17 +290,18 @@
 
     async def init(self):
         if not self._enable_permissions:
             warnings.warn('Server permissions are disabled.')
         self._counter = self._max_parallel_tasks
         self._empty.set()
         self._not_full.set()
-        self._sessions = self.discover_service(self._sessions, cls=BaseSessionService, required=False)
+        self._sessions = self.discover_service(self._sessions, cls=SessionInterface, required=False)
+        self._auth = self.discover_service(self._auth, cls=AuthenticationInterface, required=False)
         for service_name, service in self.app.services.items():
-            if isinstance(service, AbstractRPCCompatible):
+            if isinstance(service, PublicInterface):
                 self.register_service(service_name, service)
         await super().init()
 
     async def close(self):
         await self._empty.wait()
         await super().close()
 
@@ -647,15 +365,15 @@
         session = self.get_session()
         routes = [
             {
                 'route': route,
                 'signature': method['signature'],
             }
             for route, method in self._methods.items()
-            if method['permission'].value >= session.scope.value and fnmatch(route, pattern)
+            if session and method['permission'].value >= session.scope.value and fnmatch(route, pattern)
         ]
         routes.sort(key=lambda o: o['route'])
         return {'api': 'jsonrpc', 'version': JSONRPC, 'spec': 'https://www.jsonrpc.org/specification', 'routes': routes}
 
     def register_service(self, service_name: str, service: AbstractRPCCompatible) -> None:
         """Register an RPC compatible service and its methods."""
         if not isinstance(service, AbstractRPCCompatible):
@@ -677,15 +395,18 @@
             except Exception as exc:
                 warnings.warn(f'Cannot automatically create validator for {route}: {exc}')
                 annotation = None
             validator = None
             if route in validators:
                 params = validators[route]
                 if params:
-                    validator = compile_schema(params)
+                    if isinstance(params, Callable):
+                        validator = params
+                    else:
+                        validator = compile_schema(params)
             elif self._use_annotation_parser and annotation:
                 params = annotation['params']
                 if params:
                     params = params.repr()
                     if params:
                         validator = compile_schema(params)
             # signature = inspect.signature(f)
@@ -706,28 +427,38 @@
                 return True
         return False
 
     async def call(
         self,
         body: Union[List, Dict],
         headers: dict = None,
-        session: Session = None,
         nowait: bool = False,
-        scope: Scope = Scope.SYSTEM,
         callback: Callable[..., Awaitable] = None,
-    ) -> (dict, RequestHeaders):
+    ):
         """Call a server command.
 
         :param body: request body
         :param headers: request headers (optional)
-        :param session: client session object
-        :param scope: user scope
         :param nowait: do not wait for the result
         :param callback: optional response callback which should contain (session, headers, result) input params
         """
+        session = None
+        if headers is None:
+            headers = {}
+        if self._sessions and headers.get(JSONRPCHeaders.SESSION_ID_HEADER):
+            session = await self._sessions.load_session(headers[JSONRPCHeaders.SESSION_ID_HEADER])
+        if not session and self._auth and JSONRPCHeaders.AUTHORIZATION in headers:
+            try:
+                session = await self._auth.header_auth(headers[JSONRPCHeaders.AUTHORIZATION])
+            except NotAuthorized as exc:
+                headers = self._get_request_headers(headers)
+                headers = self._get_response_headers(headers['correlation_id'], session)
+                return headers, RPCError(id=None, error=exc)
+
+        scope = session.scope if session else Scope.GUEST
         headers = self._get_request_headers(headers)
         if type(body) in {list, tuple}:
             try:
                 reqs = [
                     self._prepare_request(req, session, scope, n, use_template=headers['use_template'])
                     for n, req in enumerate(body)
                 ]
@@ -777,15 +508,15 @@
             self._empty.clear()
         if self._counter <= 0:
             self._counter = 0
             self._not_full.clear()
         if not return_result:
             return self._get_response_headers(headers['correlation_id'], session), None
         elif nowait:
-            return task
+            return headers, task
         else:
             return await task
 
     @staticmethod
     def _get_int_header(value: Union[str, None], default) -> int:
         """Parse an integer header value.
 
@@ -958,36 +689,35 @@
 
         method_name = body.get('method')
         if not method_name:
             raise InvalidRequest(id=_id, message='Request "method" must be a string', request_method=method_name)
         try:
             method = self._methods[method_name]
         except KeyError:
-            raise MethodNotFound(id=_id, message='Method not found', request_method=method_name)
+            raise MethodNotFound(id=_id, message='Method not found', request_method=method_name) from None
         else:
             if self._enable_permissions:
                 if all(
                     (
                         method['permission'].value < scope.value,
-                        session and method_name not in session.permissions,
-                        session and method['service_name'] not in session.permissions,
+                        not session or method_name not in session.permissions,
+                        not session or method['service_name'] not in session.permissions,
                     )
                 ):
                     raise PermissionDenied(id=_id, message='Method not found', request_method=method_name)
 
         # request params validation
 
         params = body.get('params')
         if params is None:
             params = {}
-        else:
-            params = {k: v for k, v in params.items() if not k.startswith('_')}
+        if not type(params) is dict:
+            raise InvalidParams(id=_id, message='Request "params" must be an object', request_params=params)
         if params:
-            if not type(params) is dict:
-                raise InvalidParams(id=_id, message='Request "params" must be an object', request_params=params)
+            params = {k: v for k, v in params.items() if not k.startswith('_')}
             if not use_template and method['validator']:  # template validation is postponed
                 try:
                     params = method['validator'](params)
                 except Exception as exc:
                     raise InvalidParams(id=_id, message=str(exc), base_exc=exc)
 
         return _RequestId(_id), method['f'], params, body
@@ -1014,24 +744,156 @@
             result = RPCError(id=request_id, error=exc)
             self.logger.error('Internal error', request=request, exc_info=exc)
         except Exception as exc:
             result = RPCError(id=request_id, error=InternalError(base_exc=exc, message='Internal error'))
             self.logger.error('Internal error', request=request, exc_info=exc)
         else:
             result = RPCResponse(id=request_id, result=result)
-            if self._request_logs:
-                self.logger.info('Request', request=request, took_ms=int((time() - t0) * 1000))
-            elif self._response_logs:
-                self.logger.info('Request', request=request, result=result.result, took_ms=int((time() - t0) * 1000))
+            if self._response_logs:
+                self.logger.info('request', request=request, result=result.result, took_ms=int((time() - t0) * 1000))
+            elif self._request_logs:
+                self.logger.info('request', request=request, took_ms=int((time() - t0) * 1000))
         return result
 
     def _request_done_cb(self, task: asyncio.Task) -> None:
         """Increment the counter when a request is finished."""
         self._counter += 1
         if self._counter >= self._max_parallel_tasks:
             self._counter = self._max_parallel_tasks
             self._empty.set()
         if not self._not_full.is_set():
             self._not_full.set()
         exc = task.exception()
         if exc:
             self.logger.error('task execution error', exc_info=exc)
+
+
+class BaseRPCClient(ContextableService, RPCClient, abc.ABC):
+    """JSONRPC client."""
+
+    transport_cls = Service
+
+    class Topic(Enum):
+        """Default topic names."""
+
+        RPC = 'rpc'
+        MANAGER = 'manager'
+        EXECUTOR = 'executor'
+
+    def __init__(
+        self,
+        *args,
+        transport: str,
+        request_logs: bool = True,
+        response_logs: bool = False,
+        auth_str: str = None,
+        scheduler: Scheduler = None,
+        token_client: TokenClientService = False,
+        **kws,
+    ):
+        super().__init__(*args, **kws)
+        self._transport = transport
+        self._scheduler = scheduler
+        self._request_logs = request_logs
+        self._response_logs = response_logs
+        self._auth_str = auth_str
+        self._token_client = token_client
+
+    async def init(self):
+        self._transport = self.discover_service(self._transport, cls=self.transport_cls)
+        self._token_client = self.discover_service(self._token_client, cls=TokenClientService, required=False)
+
+    async def call(
+        self,
+        method: str,
+        params: Union[dict, None] = None,
+        nowait: bool = False,
+        request_id: int = 0,
+        max_timeout: int = None,
+        use_context: bool = True,
+    ) -> Union[Any, None]:
+        """Make an RPC call.
+
+        :param method: rpc method name
+        :param params: method call arguments
+        :param nowait: create a 'notify' request - do not wait for the result
+        :param request_id: optional request id (usually you don't need to set it)
+        :param max_timeout: request timeout in sec
+        :param use_context: use app request context such as correlation id and request chain deadline
+        """
+        t0 = time()
+        headers = self._create_request_headers(max_timeout, use_context, nowait)
+        _id = None if nowait else request_id
+        body = RPCRequest(id=_id, method=method, params=params)
+        response = await self._request(body, headers)
+        result = self._process_response(response) if response else None
+        if isinstance(result, Exception):
+            raise result
+        if self._response_logs:
+            self.logger.info('request', request=body, result=response, took_ms=int((time() - t0) * 1000))
+        elif self._request_logs:
+            self.logger.info('request', request=body, took_ms=int((time() - t0) * 1000))
+        return result
+
+    async def call_multiple(
+        self,
+        *requests: dict,
+        raise_exception: bool = True,
+        nowait: bool = False,
+        max_timeout: int = None,
+        use_context: bool = True,
+    ) -> Union[List, None]:
+        """Make an RPC batch call.
+
+        :param requests: list of request dicts
+        :param nowait: create a 'notify' request - do not wait for the result
+        :param max_timeout: request timeout in sec
+        :param use_context: use app request context such as correlation id and request chain deadline
+        :param raise_exception: raise exception instead of returning error objects in the list
+        """
+        headers = self._create_request_headers(max_timeout, use_context, nowait)
+        body = [RPCRequest(id=n, **req) for n, req in enumerate(requests)]
+        response = await self._request(body, headers)
+        if response is None:  # for notify requests
+            return
+        results = []
+        for resp in response:
+            resp = self._process_response(resp)
+            if isinstance(resp, Exception) and raise_exception:
+                raise resp
+            results.append(resp)
+        return results
+
+    @abc.abstractmethod
+    async def _request(self, body: Union[RPCRequest, List[RPCRequest]], headers: dict):
+        """Make an external requests via transport service."""
+
+    def _create_request_headers(self, max_timeout, use_context, nowait) -> dict:
+        headers = {}
+        ctx = REQUEST_CONTEXT.get() if use_context else None
+        if ctx:
+            headers[JSONRPCHeaders.CORRELATION_ID_HEADER] = ctx['correlation_id']
+            if not nowait:
+                headers[JSONRPCHeaders.REQUEST_DEADLINE_HEADER] = ctx['request_deadline']
+        else:
+            headers[JSONRPCHeaders.CORRELATION_ID_HEADER] = get_short_uid()
+        if max_timeout:
+            headers[JSONRPCHeaders.REQUEST_TIMEOUT_HEADER] = max_timeout
+        if self._auth_str:
+            headers[JSONRPCHeaders.AUTHORIZATION] = self._auth_str
+        elif self._token_client:
+            token = self._token_client.get_token()
+            if token:
+                headers[JSONRPCHeaders.AUTHORIZATION] = f'Bearer {token}'
+        return headers
+
+    def _process_response(self, response: dict):
+        if 'error' in response:
+            return self._create_exception(response['error'])
+        else:
+            return response['result']
+
+    @staticmethod
+    def _create_exception(error_data: dict) -> RPCClientError:
+        exc = RPCClientError(message=error_data['message'], data=error_data['data'])
+        exc.status_code = error_data['code']
+        return exc
```

## kaiju_tools/serialization.py

```diff
@@ -1,3 +1 @@
-"""Backward compatibility."""
-
-from kaiju_tools.encoding import Serializable, dumps, loads, load, dumps_bytes
+from kaiju_tools.encoding import Serializable, dumps, loads, load, dumps_bytes  # noqa: legacy
```

## kaiju_tools/services.py

```diff
@@ -1,10 +1,13 @@
-from kaiju_tools.app import *
-from kaiju_tools.rpc import JSONRPCServer
-from kaiju_tools.http import RPCClientService
-from kaiju_tools.streams import StreamRPCClient
+from kaiju_tools.app import *  # noqa: legacy
+from kaiju_tools.rpc import JSONRPCServer, AbstractRPCCompatible  # noqa: legacy
+from kaiju_tools.http import RPCClientService, HTTPService
+from kaiju_tools.sessions import SessionService, LoginService, AuthenticationService
 
 service_class_registry.register_class(LoggingService)
 service_class_registry.register_class(Scheduler)
+service_class_registry.register_class(SessionService)
+service_class_registry.register_class(LoginService)
+service_class_registry.register_class(AuthenticationService)
 service_class_registry.register_class(JSONRPCServer)
+service_class_registry.register_class(HTTPService)
 service_class_registry.register_class(RPCClientService)
-service_class_registry.register_class(StreamRPCClient)
```

## kaiju_tools/streams.py

```diff
@@ -1,251 +1,198 @@
 """Message stream services and classes."""
 
 import abc
 import asyncio
-from typing import Union, List, TypedDict, cast, Dict, Callable, Awaitable
+from typing import Union, List, Type, final
 
-import kaiju_tools.jsonschema as js
-from kaiju_tools.app import ContextableService, Scheduler
-from kaiju_tools.locks import BaseLocksService, LockId
-from kaiju_tools.rpc import RPCRequest, JSONRPCServer, BaseRPCClientService
-from kaiju_tools.types import NSKey, Namespace
+from kaiju_tools.app import ContextableService, Scheduler, Service
+from kaiju_tools.interfaces import Locks, SessionInterface, AuthenticationInterface
+from kaiju_tools.rpc import RPCRequest, RPCError, JSONRPCServer, BaseRPCClient
+from kaiju_tools.types import Namespace
 from kaiju_tools.logging import Adapter
+from kaiju_tools.encoding import MimeTypes, serializers
 
-__all__ = ['Listener', 'Consumer', 'TopicSettings', 'StreamRPCClient', 'Topic']
-
-
-topic_settings_schema = js.Object(
-    {
-        'key': js.String(minLength=1),
-        'app': js.String(minLength=1, default=None),
-        'enabled': js.Boolean(default=True),
-        'settings': js.Object(default={}),
-    },
-    required=['key'],
-    additionalProperties=False,
-)
-
-topics_settings_schema = js.Array(items=topic_settings_schema)
-_topics_settings_schema = js.compile_schema(topics_settings_schema)
+__all__ = ['Listener', 'StreamRPCClient', 'Topic']
 
 
+@final
 class Topic:
     """Default topic names."""
 
     RPC = 'rpc'
     MANAGER = 'manager'
     EXECUTOR = 'executor'
 
 
-class TopicSettings(TypedDict):
-    """Consumer configuration params."""
+class Listener(ContextableService, abc.ABC):
+    """Stream consumer assigned to a particular topic."""
+
+    content_type = MimeTypes.msgpack
+    lock_check_interval = 1
 
-    key: str  #: consumer key (topic name without the app / env prefix)
-    enabled: bool  #: enable / disable consumer, disabled consumer won't be initialized
-    settings: dict  #: `Consumer` class settings (unpacked as kws)
-
-
-class Consumer(abc.ABC):
-    """Stream consumer assigned to a particular topic.
-
-    Consumers are created by a listener from topic settings.
-    """
-
-    def __init__(self, key: str, topic_key: NSKey, lock_key: NSKey, rpc: JSONRPCServer, logger: Adapter, **_):
-        """Initialize."""
-        self.key = key
-        self.lock_key = lock_key
-        self.topic_key = topic_key
-        self.rpc = rpc
-        self.logger = logger
+    def __init__(
+        self,
+        app,
+        topic: str,
+        rpc_service: JSONRPCServer = None,
+        locks_service: Locks = None,
+        scheduler: Scheduler = None,
+        session_service: SessionInterface = None,
+        authentication_service: AuthenticationInterface = None,
+        transport: Service = None,
+        shared: bool = False,
+        max_parallel_batches: int = None,
+        logger: Adapter = None,
+    ):
+        super().__init__(app=app, logger=logger)
+        self._transport = transport
+        self.topic = topic
+        self.max_parallel_batches = max_parallel_batches
+        self._counter = asyncio.Semaphore(max_parallel_batches) if max_parallel_batches else None
+        _ns = self.app.namespace_shared if shared else self.app.namespace
+        _ns = _ns / '_stream'
+        self._key = _ns.get_key(self.topic)
+        self._lock_key = _ns.create_namespace('_lock').get_key(self.topic)
+        self._lock_id = None
+        self._scheduler = scheduler
+        self._rpc = rpc_service
+        self._locks = locks_service
+        self._sessions = session_service
+        self._auth = authentication_service
         self._unlocked = asyncio.Event()
         self._idle = asyncio.Event()
-        self._task = None
-        self._handler = None
-        self.unlock()
+        self._loop = None
+        self._lock_task = None
+        self._closing = True
+        self._encoder = serializers[self.content_type]()
 
     async def init(self):
-        self._task = asyncio.create_task(self._read(), name=f'streams.{self.key}._read')
+        self._closing = False
+        self._unlocked.set()
+        self._idle.set()
+        self._rpc = self.discover_service(self._rpc, cls=JSONRPCServer)
+        self._locks = self.discover_service(self._rpc, cls=Locks)
+        self._sessions = self.discover_service(self._sessions, cls=SessionInterface, required=False)
+        self._transport = self.discover_service(self._transport, cls=self.get_transport_cls())
+        self._scheduler = self.discover_service(self._scheduler, cls=Scheduler)
+        self._auth = self.discover_service(self._auth, cls=AuthenticationInterface, required=False)
+        self._loop = asyncio.create_task(self._read(), name=f'{self.service_name}.{self.topic}._read')
+        self._lock_task = self._scheduler.schedule_task(
+            self._check_lock, interval=self.lock_check_interval, name=f'{self.service_name}._check_lock'
+        )
 
     async def close(self):
-        self._task.cancel(msg='closing')
+        self._closing = True
+        self._lock_task.enabled = False
+        await self._idle.wait()
+
+    @classmethod
+    @abc.abstractmethod
+    def get_transport_cls(cls) -> Type:
+        ...
 
     async def lock(self) -> None:
-        self.logger.info('lock')
+        self.logger.info('lock', topic=self._key)
+        self._lock_id = await self._locks.acquire(self._lock_key)
         self._unlocked.clear()
-        await self._idle.wait()
-
-    def unlock(self) -> None:
-        self.logger.info('unlock')
+        await asyncio.sleep(self.lock_check_interval)  # ensure that everyone else is locked
+        if not self._idle.is_set():
+            await self._idle.wait()
+
+    async def unlock(self) -> None:
+        self.logger.info('unlock', topic=self._key)
+        if self._lock_id:
+            await self._locks.release(self._lock_key, self._lock_id)
+            self._lock_id = None
         self._unlocked.set()
 
+    @property
     def locked(self) -> bool:
         return not self._unlocked.is_set()
 
-    def add_handler(self, f: Callable[..., Awaitable]) -> None:
-        self._handler = f
-
     @abc.abstractmethod
     async def _read_batch(self) -> list:
         """Get messages from a stream."""
 
     @abc.abstractmethod
     async def _process_batch(self, batch: list) -> None:
         """Define your own message processing and commit here."""
 
-    async def _process_request(self, body, headers: dict) -> None:
+    async def _process_request(self, data) -> None:
         """Process a single request in a batch."""
-        if type(body) is RPCRequest:
-            await self.rpc.call(body=body, headers=headers, nowait=True)
-        elif self._handler:
-            try:
-                await self._handler(body)
-            except Exception as exc:
-                self.logger.error('Handler error', exc_info=exc, topic=self.topic_key, body=body)
+        body, headers = data
+        headers, result = await self._rpc.call(body=body, headers=headers, nowait=True)
+        if type(result) is RPCError:  # here it can be only a pre-request error due to nowait=False
+            self.logger.info('Client error', result=result)
 
     async def _read(self) -> None:
         """Read from a stream."""
-        self.logger.info('Starting', topic=self.topic_key)
-        while 1:
+        self.logger.info('Starting')
+        while not self._closing:
             await self._unlocked.wait()
-            self._idle.clear()
             try:
                 batch = await self._read_batch()
-                await self._process_batch(batch)
             except Exception as exc:
-                self.logger.error('Error in consumer loop', exc_info=exc, topic=self.topic_key)
-            finally:
-                self._idle.set()
+                self.logger.error('Read error', exc_info=exc, topic=self._key)
+                continue
 
+            self._idle.clear()
+            if self.max_parallel_batches:
+                async with self._counter.acquire():
+                    await self._process_batch(batch)
+            else:
+                await self._process_batch(batch)
+            self._idle.set()
 
-class Listener(ContextableService, abc.ABC):
-    """Stream listener capable of publishing and consuming messages."""
+    async def _check_lock(self) -> None:
+        """Check for existing shared lock and lock / unlock if needed."""
+        existing = await self._locks.m_exists([self._lock_key])
+        if self._lock_key in existing:
+            if not self.locked:
+                self._unlocked.clear()
+        elif self.locked:
+            self._unlocked.set()
 
-    service_name = 'streams'
-    consumer_cls = Consumer
-    CHECK_LOCKS_INTERVAL = 1  #: (s) interval to check shared locks for existing consumers
 
-    def __init__(
-        self,
-        *args,
-        rpc_service: str = None,
-        scheduler: str = None,
-        locks_service: str = None,
-        topics: List[dict] = None,
-        **kws,
-    ):
+class StreamRPCClient(BaseRPCClient, abc.ABC):
+    """Stream client for RPC requests."""
+
+    transport_cls: Type
+
+    def __init__(self, *args, app_name: str, topic: str = Topic.RPC, shared: bool = False, **kws):
         """Initialize.
 
-        :param rpc_service: rpc server name
-        :params scheduler: scheduler name
-        :param locks_service: locks service name
-        :param topics: topics to listen
+        :param app_name: application (topic) name
+        :param listener_service: stream listener service instance
+        :param topic: topic name
         """
         super().__init__(*args, **kws)
-        self.topics = cast(List[TopicSettings], _topics_settings_schema(topics))
-        self._ns = self.app.ns / self.service_name
-        self._ns_stream = self._ns / '_stream'
-        self._ns_lock = self._ns / '_lock'
-        self.consumers: Dict[str, Consumer] = {}
-        self._rpc = rpc_service
-        self._scheduler = scheduler
-        self._locks: BaseLocksService = locks_service  # noqa
-        self._lock_task = None
+        if shared:
+            app_name = '__shared__'
+        self._topic = Namespace(env=self.app.env, name=app_name).create_namespace('_stream').get_key(topic)
 
     async def init(self):
+        self._transport = self.discover_service(self._transport, cls=self.get_transport_cls())
         await super().init()
-        self._rpc: JSONRPCServer = self.discover_service(self._rpc, cls=JSONRPCServer)
-        self._scheduler: Scheduler = self.discover_service(self._scheduler, cls=Scheduler)
-        self._locks: BaseLocksService = self.discover_service(self._locks, cls=BaseLocksService)
-        self.consumers = {topic['key']: self._create_consumer(topic) for topic in self.topics if topic['enabled']}
-        self._lock_task = self._scheduler.schedule_task(
-            self._check_locks,
-            self.CHECK_LOCKS_INTERVAL,
-            policy=self._scheduler.ExecPolicy.WAIT,
-            name=f'{self.service_name}._check_locks',
-        )
-        await asyncio.gather(*(consumer.init() for consumer in self.consumers))
 
-    async def close(self):
-        await super().close()
-        await asyncio.wait(
-            *(consumer.close() for consumer in self.consumers.values()), return_when=asyncio.ALL_COMPLETED
-        )
-        self._lock_task.enabled = False
+    @classmethod
+    @abc.abstractmethod
+    def get_transport_cls(cls) -> Type:
+        """Get transport class required for this service."""
 
     @abc.abstractmethod
-    async def write(self, topic: str, body, *, headers: dict = None, key=None) -> None:
+    async def write(self, body, headers: dict = None, key=None) -> None:
         """Submit a message to a stream.
 
-        :param topic: topic name
         :param body: message body
         :param headers: message headers
         :param key: (optional) unique message id
         """
 
-    async def lock_topic(self, key: str) -> LockId:
-        """Lock a topic for all instances."""
-        consumer = self.consumers[key]
-        self.logger.info('lock_topic', topic=consumer.topic_key)
-        await consumer.lock()
-        return await self._locks.acquire(consumer.lock_key)
-
-    async def unlock_topic(self, key: str, identifier: LockId) -> None:
-        consumer = self.consumers[key]
-        self.logger.info('unlock_topic', topic=consumer.topic_key)
-        await self._locks.release(consumer.lock_key, identifier)
-        consumer.unlock()
-
-    def _create_consumer(self, topic: TopicSettings) -> Consumer:
-        """Create consumer for a particular topic."""
-        return self.consumer_cls(
-            key=topic['key'],
-            topic_key=self._ns_stream.get_key(topic['key']),
-            lock_key=self._ns_lock.get_key(topic['key']),
-            rpc=self._rpc,  # noqa
-            logger=self.logger.getChild(topic['key']),
-            **topic['settings'],
-        )
-
-    async def _check_locks(self) -> None:
-        """Check for existing topic locks and lock topics if needed."""
-        locks = [c.lock_key for c in self.consumers.values()]
-        existing = await self._locks.m_exists(locks)
-        to_lock = []
-        for c in self.consumers.values():
-            if c.lock_key in existing:
-                if not c.locked():
-                    to_lock.append(c.lock())
-            else:
-                if c.locked():
-                    c.unlock()
-        if to_lock:
-            await asyncio.gather(*to_lock)
-
-
-class StreamRPCClient(BaseRPCClientService):
-    """Stream client for RPC requests."""
-
-    def __init__(self, *args, app: str, listener_service: str, topic: str = Topic.RPC, **kws):
-        """Initialize.
-
-        :param app: application (topic) name
-        :param listener_service: stream listener service instance
-        :param topic: topic name
-        """
-        super().__init__(*args, **kws)
-        self._topic = Namespace(env=self.app.env, name=app).get_key(topic)
-        self._listener: Listener = listener_service  # noqa
-
-    async def init(self):
-        self._listener = self.discover_service(self._listener, cls=Listener)
-        await super().init()
-
     async def _request(self, body: Union[RPCRequest, List[RPCRequest]], headers: dict) -> None:
         """Send an RPC request via stream."""
         if type(body) is list:  # automatically make them notify because stream is a one-way interaction
             for row in body:
                 row.id = None
         else:
             body.id = None
-        await self._listener.write(self._topic, body, headers=headers)
+        await self.write(body, headers=headers)
```

## kaiju_tools/types.py

```diff
@@ -1,181 +1,47 @@
 """Basic types and constants."""
 
-import logging
-import sys
-from bisect import bisect, bisect_left, bisect_right
-from dataclasses import dataclass, field
+import uuid
+from bisect import bisect
 from datetime import datetime
+from dataclasses import dataclass, field
 from enum import Enum
 from time import time
 from typing import (
     Optional,
-    TypedDict,
-    FrozenSet,
     Mapping,
     Iterator,
     Sized,
     Iterable,
     Dict,
     cast,
     NewType,
     MutableMapping,
+    Tuple,
+    Hashable,
+    Any,
+    FrozenSet,
+    TypedDict,
 )
-from uuid import UUID
-from contextvars import ContextVar  # noqa pycharm
 
-from aiohttp.web import Application
-
-from kaiju_tools.serialization import Serializable
+from kaiju_tools.encoding import Serializable
 from kaiju_tools.functions import not_implemented
 
-
-SHARED_NS = '_shared'
-
-
-class Scope(Enum):
-    """Permission scope for application methods."""
-
-    SYSTEM = 0
-    ADMIN = 10
-    USER = 100
-    GUEST = 1000
-
-
-SCOPE_MAP = {Scope.SYSTEM: 'system', Scope.ADMIN: 'admin', Scope.USER: 'user'}  # used by permission services
-
-
-class Session(Serializable):
-    """User session data."""
-
-    __slots__ = ('id', 'h_agent', 'user_id', 'expires', 'permissions', 'data', 'created', '_stored', '_changed')
-
-    def __init__(
-        self,
-        *,
-        id: str,  # noqa
-        h_agent: bytes,
-        user_id: Optional[UUID],
-        expires: int,
-        permissions: FrozenSet[str],
-        data: dict,
-        created: datetime,
-        _stored: bool,
-        _changed: bool,
-        _loaded: bool,
-    ):
-        """Initialize.
-
-        :param id:
-        :param h_agent:
-        :param user_id:
-        :param expires:
-        :param permissions:
-        :param data:
-        :param created:
-        :param _stored:
-        :param _changed:
-        :param _loaded:
-        """
-        self.id = id
-        self.h_agent = h_agent
-        self.user_id = user_id
-        self.expires = expires
-        self.permissions = frozenset(permissions)
-        self.data = data
-        self.created = created
-        self._stored = _stored
-        self._changed = _changed
-        self._loaded = _loaded
-
-    def __getitem__(self, item):
-        return self.data.get(item)
-
-    def __setitem__(self, key, value):
-        self.update({key: value})
-
-    @property
-    def scope(self) -> Scope:
-        """Base user scope."""
-        if SCOPE_MAP[Scope.SYSTEM] in self.permissions:
-            return Scope.SYSTEM
-        elif SCOPE_MAP[Scope.USER] in self.permissions:
-            return Scope.USER
-        else:
-            return Scope.GUEST
-
-    @property
-    def stored(self) -> bool:
-        """Session should be stored."""
-        return self._stored
-
-    @property
-    def changed(self) -> bool:
-        """Session has changed."""
-        return self._changed
-
-    @property
-    def loaded(self) -> bool:
-        """Session has been loaded from db."""
-        return self._loaded
-
-    def update(self, data: dict):
-        """Update session data."""
-        self.data.update(data)
-        self._changed = True
-
-    def clear(self):
-        """Clear all session data."""
-        self.data.clear()
-        self._changed = True
-
-    def repr(self) -> dict:
-        """Get object representation."""
-        return {slot: getattr(self, slot) for slot in self.__slots__ if not slot.startswith('_')}
-
-
-class RequestContext(TypedDict):
-    """Request context stored for the request chain."""
-
-    correlation_id: str
-    session_id: Optional[str]
-    request_deadline: Optional[int]
-
-
-class ServiceConfig(TypedDict, total=False):
-    """Service configuration parameters."""
-
-    cls: str  #: service class name as in :py:class:`~kaiju_tools.services.service_class_registry`
-    name: str  #: unique service name, each service should have a default value for this
-    enabled: bool  #: disable service
-    required: bool  #: skip a service and proceed on initialization error
-    override: bool  #: replace an existing service with the same name
-    settings: dict  #: custom settings, unpacked to a service's __init__
-
-
-class App(Application):
-    """Web application interface."""
-
-    id: str
-    name: str
-    version: str
-    env: str
-    debug: bool
-    loglevel: str
-    logger: logging.Logger
-    services: dict
-    settings: dict
-    ns: 'Namespace'
-    ns_shared: 'Namespace'
-
-    def get_context(self) -> RequestContext:
-        ...
-
-    def get_session(self) -> Optional[Session]:
-        ...
+__all__ = [
+    'SortedStack',
+    'RestrictedDict',
+    'TTLDict',
+    'NSKey',
+    'Namespace',
+    'Session',
+    'Scope',
+    'SCOPES',
+    'SCOPE_MAP',
+    'RequestContext',
+]
 
 
 class SortedStack(Sized, Iterable):
     """A sorted collection (stack) of items.
 
     >>> stack = SortedStack({'dogs': 12, 'sobaki': 5})
     >>> stack = SortedStack(stack)
@@ -380,15 +246,29 @@
 
 
 NSKey = NewType('NSKey', str)  # namespace key
 
 
 @dataclass
 class Namespace(Mapping):
-    """Namespace can be used for shared key and name management."""
+    """Namespace can be used for shared key and name management.
+
+    Create namespaces.
+
+    >>> ns = Namespace(env='dev', name='app')
+    >>> sub_ns = ns / 'sub'  # eq to .create_namespace('sub')
+    >>> str(sub_ns)
+    'dev.app.sub'
+
+    Get keys.
+
+    >>> sub_ns.get_key('key')
+    'dev.app.sub.key'
+
+    """
 
     delimiter = '.'  # it is supported both by kafka topics and redis keys
 
     env: str
     name: str
     namespaces: Dict[str, 'Namespace'] = field(init=False, default_factory=RestrictedDict)
 
@@ -419,128 +299,183 @@
         return self.create_namespace(__other)
 
     def __str__(self):
         return self.delimiter.join((self.env, self.name))
 
 
 class TTLDict(MutableMapping):
-    """A simple TTL dict mostly compatible with a normal one."""
+    """A simple TTL dict mostly compatible with a normal one.
+
+    Similar to a normal dict, but with ttl in seconds:
+
+    >>> d = TTLDict()
+    >>> d.set('key', 'value', ttl=1)
+    >>> d.get('key')
+    'value'
+
+    With no ttl (infinite):
+
+    >>> d['key'] = 'value'
+
+    """
 
-    TTL = 60  #: default TTL in ms
-    __slots__ = ('_ttl', '_dict', '_ttls', '_keys')
+    __slots__ = ('_dict',)
 
     def __init__(self, *args, **kws):
-        self._ttl = int(self.TTL)
-        self._dict = dict()  # here key: (value, index) data will be stored
-        self._keys = []  # sorted list of keys
-        self._ttls = []  # sorted list of deadlines
+        self._dict: Dict[Hashable, Tuple[Any, float]] = dict()  # (value, exp)
         for key, value in dict(*args, **kws).items():
             self[key] = value
 
-    def __getitem__(self, key):
-        value = self._dict[key]
-        n = self._keys.index(key)
-        t = self._ttls[n]
-        if t > time():
+    def __getitem__(self, key: Hashable) -> Any:
+        value, t = self._dict[key]
+        if not t or t > time():
             return value
         else:
             del self[key]
             raise KeyError(key)
 
-    def __setitem__(self, key, value):
-        return self.set(key, value, self._ttl)
+    def __setitem__(self, key: Hashable, value: Any) -> None:
+        self.set(key, value, 0)
 
-    def __delitem__(self, key):
-        n = self._keys.index(key)
-        del self._keys[n]
-        del self._ttls[n]
+    def __delitem__(self, key: Hashable) -> None:
         del self._dict[key]
 
-    def __len__(self):
-        self.refresh()
+    def __len__(self) -> int:
         return len(self._dict)
 
-    def __bool__(self):
+    def __bool__(self) -> bool:
         return bool(len(self))
 
-    def __contains__(self, item):
-        return self.get(item) is not None
-
-    def __eq__(self, other):
-        self.refresh()
-        other.refresh()
+    def __eq__(self, other: 'TTLDict') -> bool:
         return other._dict == self._dict  # noqa
 
     def __iter__(self):
         return iter(self.keys())
 
-    def get(self, item, default=None):
-        """Similar to `dict().get`."""
+    def get(self, item: Hashable, default: Any = None):
         try:
             return self[item]
         except KeyError:
             return default
 
-    def set(self, key, value, ttl: int):
-        """Set key.
-
-        Similar to `__setitem__` but you may specify per-key ttl.
-        """
-        if key in self._dict:
-            del self[key]
-            self.set(key, value, ttl)
+    def get_exp(self, key: Hashable) -> float:
+        """Get exp time of a key."""
+        value, t = self._dict[key]
+        if not t or t > time():
+            return t
         else:
-            if ttl:
-                t = int(time() + ttl)
-            else:
-                t = cast(int, float('Inf'))
-            n = bisect_left(self._ttls, t)
-            self._ttls.insert(n, t)
-            self._keys.insert(n, key)
-            self._dict[key] = value
-
-    def values(self):
-        """Similar to `dict().values`."""
-        self.refresh()
-        return self._dict.values()
-
-    def keys(self):
-        """Similar to `dict().keys`."""
-        self.refresh()
-        return self._dict.keys()
-
-    def items(self):
-        """Similar to `dict().items`."""
-        return zip(self._dict.keys(), self.values())
+            del self[key]
+            raise KeyError(key)
 
-    def set_ttl(self, ttl: int):
-        """Set default ttl to a new value.
+    def set(self, key: Hashable, value: Any, ttl: float) -> None:
+        """Set a key."""
+        self._dict[key] = (value, time() + ttl)
+
+
+class Scope(Enum):
+    """Session permission scopes for RPC methods."""
+
+    SYSTEM = 0
+    ADMIN = 10
+    USER = 100
+    GUEST = 1000
+
+
+SCOPES = {s.name: s for s in Scope}
+SCOPE_MAP = {Scope.SYSTEM: 'system', Scope.ADMIN: 'admin', Scope.USER: 'user'}  # used by permission servicesR
 
-        :param ttl: TTL value in seconds
-        """
-        if not ttl:
-            ttl = sys.maxsize
-        elif ttl < 0:
-            raise ValueError('TTL value must be greater than zero.')
-        ttl = int(ttl)
-        self._ttl = ttl
 
-    def refresh(self):
-        """Remove old records.
+class RequestContext(TypedDict):
+    """Request context for an RPC request."""
 
-        .. note::
+    correlation_id: str
+    session_id: Optional[str]
+    request_deadline: Optional[int]
 
-            This method is called each time one calls a `TTLDict.__len__`
-            or any other method that must provide an actual dictionary state.
 
+class Session(Serializable):
+    """User session data."""
+
+    __slots__ = ('id', 'h_agent', 'user_id', 'expires', 'permissions', 'data', 'created', '_stored', '_changed')
+
+    def __init__(
+        self,
+        *,
+        id: str,  # noqa
+        h_agent: Optional[bytes],
+        user_id: Optional[uuid.UUID],
+        expires: int,
+        permissions: FrozenSet[str],
+        data: dict,
+        created: datetime,
+        _stored: bool,
+        _changed: bool,
+        _loaded: bool,
+    ):
+        """Initialize.
+
+        :param id:
+        :param h_agent:
+        :param user_id:
+        :param expires:
+        :param permissions:
+        :param data:
+        :param created:
+        :param _stored:
+        :param _changed:
+        :param _loaded:
         """
-        t = int(time())
-        n = bisect_right(self._ttls, t)
-        if n:
-            self._ttls = self._ttls[n:]
-            keys, self._keys = self._keys[:n], self._keys[n:]
-            for key in keys:
-                del self._dict[key]
+        self.id = id
+        self.h_agent = h_agent
+        self.user_id = user_id
+        self.expires = expires
+        self.permissions = frozenset(permissions)
+        self.data = data
+        self.created = created
+        self._stored = _stored
+        self._changed = _changed
+        self._loaded = _loaded
+
+    def __getitem__(self, item):
+        return self.data.get(item)
+
+    def __setitem__(self, key, value):
+        self.update({key: value})
+
+    @property
+    def scope(self) -> Scope:
+        """Base user scope."""
+        if SCOPE_MAP[Scope.SYSTEM] in self.permissions:
+            return Scope.SYSTEM
+        elif SCOPE_MAP[Scope.USER] in self.permissions:
+            return Scope.USER
+        else:
+            return Scope.GUEST
+
+    @property
+    def stored(self) -> bool:
+        """Session should be stored."""
+        return self._stored
+
+    @property
+    def changed(self) -> bool:
+        """Session has changed."""
+        return self._changed
+
+    @property
+    def loaded(self) -> bool:
+        """Session has been loaded from db."""
+        return self._loaded
 
+    def update(self, data: dict):
+        """Update session data."""
+        self.data.update(data)
+        self._changed = True
 
-REQUEST_CONTEXT: ContextVar[Optional[RequestContext]] = ContextVar('RequestContext', default=None)
-REQUEST_SESSION: ContextVar[Optional[Session]] = ContextVar('RequestSession', default=None)
+    def clear(self):
+        """Clear all session data."""
+        self.data.clear()
+        self._changed = True
+
+    def repr(self) -> dict:
+        """Get object representation."""
+        return {slot: getattr(self, slot) for slot in self.__slots__ if not slot.startswith('_')}
```

## kaiju_tools/docker/containers.py

```diff
@@ -3,17 +3,17 @@
 import uuid
 from time import sleep
 from typing import Awaitable, Union
 
 import docker  # type: ignore
 from docker.errors import NotFound  # type: ignore
 
-from kaiju_tools.services import ContextableService
+from kaiju_tools.app import ContextableService
 from kaiju_tools.functions import async_run_in_thread
-from .images import DockerImage
+from kaiju_tools.docker.images import DockerImage
 
 __all__ = ['DockerContainer']
 
 
 class DockerContainer(ContextableService):
     """Single docker container management.
```

## kaiju_tools/docker/images.py

```diff
@@ -5,15 +5,15 @@
 from datetime import datetime
 from pathlib import Path
 from typing import Awaitable, Optional
 
 import docker  # type: ignore
 from docker.errors import NotFound  # type: ignore
 
-from kaiju_tools.services import ContextableService
+from kaiju_tools.app import ContextableService
 from kaiju_tools.functions import async_run_in_thread
 
 __all__ = ['DockerImage']
 
 
 class DockerImage(ContextableService):
     """Docker image builder. Builds docker images from dockerfiles.
```

## kaiju_tools/docker/stack.py

```diff
@@ -6,15 +6,15 @@
 import tempfile
 import textwrap
 import uuid
 from pathlib import Path
 from time import sleep
 from typing import Awaitable, List, Union
 
-from kaiju_tools.services import ContextableService
+from kaiju_tools.app import ContextableService
 from kaiju_tools.functions import async_run_in_thread
 
 __all__ = ['DockerStack']
 
 
 class DockerStack(ContextableService):
     """
@@ -56,18 +56,28 @@
     updated by other files. This is a standard docker-compose behaviour.
 
     """
 
     COMPOSE_FILE = './docker-compose.yaml'
 
     def __init__(
-        self, name: str, compose_files: Union[str, list] = None,
-        profiles: List[str] = None, services: List[str] = None, sleep_interval=0,
-        stop_before_run=True, remove_containers=True, wait_for_start=True, wait_for_stop=True, build=False,
-        max_timeout=10, app=None, logger=None
+        self,
+        name: str,
+        compose_files: Union[str, list] = None,
+        profiles: List[str] = None,
+        services: List[str] = None,
+        sleep_interval=0,
+        stop_before_run=True,
+        remove_containers=True,
+        wait_for_start=True,
+        wait_for_stop=True,
+        build=False,
+        max_timeout=10,
+        app=None,
+        logger=None,
     ):
         """
         :param app:
         :param compose_files: list of compose files (see official docs about compose file inheritance and format)
             by default `COMPOSE_FILE` is used
         :param name: custom docker stack (project) name
         :param profiles: list of compose profiles (see official docs about that)
@@ -168,15 +178,15 @@
         cmd = f"docker-compose {compose_files} ps | grep -E 'unhealthy|healthy|starting|running'"
         value = self._run_cmd(cmd, check=False)
         return bool(value)
 
     @property
     def exists(self) -> bool:
         compose_files = self._get_compose_files_cli()
-        cmd = f"docker-compose {compose_files} ps -q --all"
+        cmd = f'docker-compose {compose_files} ps -q --all'
         value = self._run_cmd(cmd, check=False)
         return bool(value)
 
     def start_async(self, *args, **kws) -> Awaitable:
         return async_run_in_thread(self.start, *args, **kws)
 
     def stop_async(self) -> Awaitable:
```

## kaiju_tools/tests/fixtures.py

```diff
@@ -1,190 +1,607 @@
 import asyncio
-import gc
 import logging
-import multiprocessing as mp
-import os
-import platform
-import queue
-import signal
-import traceback
 import uuid
-from datetime import timedelta, datetime
-from inspect import iscoroutinefunction
-from pathlib import Path
-from tempfile import TemporaryDirectory, NamedTemporaryFile
-from time import sleep
-from types import SimpleNamespace
-from typing import *
-
-import pytest
-
-import kaiju_tools.jsonschema as j
-from kaiju_tools.rpc import AbstractRPCCompatible, JSONRPCServer
-from kaiju_tools.services import Service, ServiceContextManager
-from kaiju_tools.logging import Logger
-from kaiju_tools.exceptions import ValidationError
-
-__all__ = ('logger', 'application', 'temp_dir', 'sample_file', 'rpc_interface', 'rpc_compatible_service')
+from datetime import datetime
+from time import time
+from typing import cast, Dict, List, FrozenSet, Optional, Collection, Hashable, Type
+
+import pytest  # noqa: pycharm
+import pytest_asyncio
+
+import kaiju_tools.jsonschema as js
+from kaiju_tools.exceptions import ValidationError, NotFound
+from kaiju_tools.app import (
+    HandlerSettings,
+    LoggerSettings,
+    LoggingService,
+    ServiceContextManager,
+    REQUEST_CONTEXT,
+    REQUEST_SESSION,
+    Scheduler,
+    Service,
+)
+from kaiju_tools.interfaces import DataStore, UserInterface, TokenInterface, _Columns, _Row, _User, App  # noqa
+from kaiju_tools.rpc import JSONRPCServer, AbstractRPCCompatible
+from kaiju_tools.cache import BaseCacheService
+from kaiju_tools.locks import BaseLocksService, NotLockOwnerError, LockExistsError
+from kaiju_tools.sessions import SessionService, AuthenticationService, LoginService
+from kaiju_tools.templates import Condition
+from kaiju_tools.types import NSKey, TTLDict, Namespace, Session
+from kaiju_tools.streams import Listener, StreamRPCClient
+from kaiju_tools.http import error_middleware
+
+__all__ = [
+    'logger',
+    'app',
+    'rpc',
+    'scheduler',
+    'mock_cache',
+    'mock_locks',
+    'mock_service',
+    'mock_session',
+    'mock_sessions',
+    'get_app',
+    'mock_auth',
+    'mock_tokens',
+    'mock_users',
+    'mock_data_store',
+    'mock_listener',
+    'mock_stream_client',
+    'mock_login',
+]
 
 
 @pytest.fixture(scope='session')
 def logger():
-    """Return a test logger preconfigured to DEBUG level."""
+    """Get a test logger preconfigured to DEBUG level."""
     logger = logging.getLogger('pytest')
     logger.setLevel('DEBUG')
     return logger
 
 
+def get_app(logger) -> App:
+    from aiohttp.web import Application
+
+    app = Application(middlewares=[error_middleware], logger=logger, debug=True)
+    app.id = str(uuid.uuid4())
+    app.name = 'pytest'
+    app.env = 'pytest'
+    app.namespace = Namespace(env=app.env, name=app.name)
+    app.namespace_shared = Namespace(env=app.env, name='__shared__')
+    app.request_context = REQUEST_CONTEXT
+    app.request_session = REQUEST_SESSION
+    app = cast(App, app)
+    app.services = ServiceContextManager(app, settings=[], logger=logger)
+    logger_settings = [LoggerSettings(name='root', enabled=True, handlers=['default'], loglevel='DEBUG')]
+    handler_settings = [
+        HandlerSettings(cls='StreamHandler', name='default', enabled=True, formatter='TextFormatter', loglevel='DEBUG')
+    ]
+    logging_service = LoggingService(
+        app=app, handlers=handler_settings, loggers=logger_settings, loglevel='DEBUG', logger=logger
+    )
+    app.services.add_service(logging_service)
+    app.cleanup_ctx.append(app.services.cleanup_context)
+    return app
+
+
 @pytest.fixture
-def application(logger):
-    """Return a sample aiohttp web app object to use in tests. Requires aiohttp.
+def app(logger) -> App:
+    """Create a web app.
+
+    Depends on:
 
-    You may pass a list of `Service` classes. They won't be initialized but they will be registered
-    in the pseudo-service context meaning you can use `app.services.<service_name>` inside you code
-    as if it's a normal initialized app.
+        aiohttp
     """
-    from aiohttp.web import Application
+    return get_app(logger)
 
-    def _application(*services: Service, name='pytest', id=str(uuid.uuid4()), **kws):
-        app = Application(logger=logger, **kws)
-        app['id'] = app.id = id
-        app['name'] = app.name = name
-        app['env'] = app.env = 'dev'
-        app.services = ServiceContextManager(app, settings=[])  # noqa
-        for service in services:
-            service.app = app
-        app.services._services = {service.service_name: service for service in services}
-        app.services._create_services = lambda: None
-        return app
 
-    return _application
+@pytest.fixture
+def rpc(app) -> JSONRPCServer:
+    service = JSONRPCServer(app=app, request_logs=True, response_logs=True)
+    app.services.add_service(service)
+    return service
+
+
+@pytest.fixture
+def scheduler(app) -> Scheduler:
+    service = Scheduler(app=app, refresh_rate=0.1)
+    app.services.add_service(service)
+    return service
+
+
+class _MockDataStore(Service, DataStore):
+    def __init__(self, *args, primary_key: str = 'id', **kws):
+        super().__init__(*args, **kws)
+        self.primary_key = primary_key
+        self._ext = {}
+
+    @staticmethod
+    def _filter_columns(row: dict, columns):
+        if not columns:
+            return
+        elif columns == '*':
+            return row
+        else:
+            return {key: row[key] for key in row if key in columns}
+
+    async def get(self, id: Hashable, columns: _Columns = '*', _connection=None) -> _Row:
+        await asyncio.sleep(0)
+        try:
+            row = self._ext[id]
+        except KeyError:
+            raise NotFound
+        else:
+            return self._filter_columns(row, columns)
+
+    async def m_get(self, id: Collection[Hashable], columns: _Columns = '*', _connection=None) -> Collection[_Row]:
+        await asyncio.sleep(0)
+        rows = [self._ext.get(key) for key in id if key in self._ext]
+        return [self._filter_columns(row, columns) for row in rows]
+
+    async def exists(self, id: Hashable, _connection=None) -> bool:
+        await asyncio.sleep(0)
+        return id in self._ext
+
+    async def m_exists(self, id: Collection[Hashable], _connection=None) -> FrozenSet[Hashable]:
+        await asyncio.sleep(0)
+        return frozenset(key for key in id if key in self._ext)
+
+    async def delete(self, id: Hashable, columns: _Columns = None, _connection=None) -> _Row:
+        await asyncio.sleep(0)
+        try:
+            row = self._ext.pop(id)
+        except KeyError:
+            raise NotFound
+        else:
+            if columns:
+                return self._filter_columns(row, columns)
+
+    async def m_delete(
+        self, id: Collection[Hashable] = None, conditions=None, columns: _Columns = None, _connection=None
+    ) -> Collection[_Row]:
+        await asyncio.sleep(0)
+        if conditions:
+            conditions = Condition(conditions)
+        rows = []
+        keys = id if id else tuple(self._ext.keys())
+        for key in keys:
+            if key in self._ext and (conditions is None or conditions(key)):
+                row = self._ext.pop(key)
+                rows.append(self._filter_columns(row, columns))
+        if columns:
+            return rows
+
+    async def create(
+        self,
+        data: dict,
+        columns: _Columns = '*',
+        _connection=None,
+        on_conflict: str = None,
+        on_conflict_keys: Collection = None,
+        on_conflict_values=None,
+    ) -> _Row:
+        await asyncio.sleep(0)
+        self._ext[data[self.primary_key]] = data
+        if columns:
+            return self._filter_columns(data, columns)
+
+    async def m_create(
+        self,
+        data: Collection,
+        columns: _Columns = '*',
+        _connection=None,
+        on_conflict: str = None,
+        on_conflict_keys: Collection = None,
+        on_conflict_values: dict = None,
+    ) -> Collection[_Row]:
+        await asyncio.sleep(0)
+        rows = []
+        for row in data:
+            self._ext[row[self.primary_key]] = row
+            rows.append(self._filter_columns(row, columns))
+        if columns:
+            return rows
+
+    async def update(self, id: Hashable, data, columns: _Columns = '*', _connection=None) -> _Row:
+        try:
+            self._ext[id].update(data)
+        except KeyError:
+            raise NotFound
+        else:
+            if columns:
+                return self._filter_columns(self._ext[id], columns)
+
+    async def m_update(
+        self, id: Collection[Hashable], data, conditions=None, columns: _Columns = '*', _connection=None
+    ) -> Collection[_Row]:
+        if conditions:
+            conditions = Condition(conditions)
+        rows = []
+        keys = id if id else tuple(self._ext.keys())
+        for key in keys:
+            if key in self._ext and (conditions is None or conditions(key)):
+                self._ext[id].update(data)
+                row = self._ext[id]
+                rows.append(self._filter_columns(row, columns))
+        if columns:
+            return rows
 
 
 @pytest.fixture
-def temp_dir():
-    with TemporaryDirectory(prefix='pytest') as d:
-        yield Path(d)
+def mock_data_store(app) -> _MockDataStore:
+    service = _MockDataStore(app=app)
+    app.services.add_service(service)
+    return service
+
+
+class _MockCacheService(BaseCacheService):
+    class MockTransport(Service):
+        pass
+
+    def __init__(self, *args, **kws):
+        super().__init__(*args, **kws)
+        self._ext = TTLDict()
+
+    @classmethod
+    def get_transport_cls(cls) -> Type:
+        return cls.MockTransport
+
+    async def exists(self, id: NSKey) -> bool:
+        return id in self._ext
+
+    async def m_exists(self, id: NSKey):
+        return frozenset([key for key in id if key in self._ext])
+
+    async def _get(self, key: NSKey):
+        return self._ext.get(key)
+
+    async def _m_get(self, *keys: NSKey) -> List[NSKey]:
+        return [self._ext.get(key) for key in keys]
+
+    async def _set(self, key: NSKey, value: bytes, ttl: int):
+        self._ext.set(key, value, ttl)
+
+    async def _m_set(self, keys: Dict[NSKey, bytes], ttl: int):
+        for key, value in keys.items():
+            self._ext.set(key, value, ttl)
+
+    async def _delete(self, key: str):
+        if key in self._ext:
+            del self._ext[key]
+
+    async def _m_delete(self, *keys: NSKey):
+        for key in keys:
+            await self._delete(key)
 
 
 @pytest.fixture
-def sample_file():
-    with NamedTemporaryFile(prefix='pytest', delete=False) as f:
-        name = f.name
-        f.write('test')
-    yield Path(name)
-    os.remove(name)
+def mock_cache(app) -> _MockCacheService:
+    service = _MockCacheService(app=app, transport=_MockCacheService.MockTransport(app))
+    app.services.add_service(service)
+    return service
 
 
 @pytest.fixture
-def rpc_interface(application, logger):
-    app = application(debug=True)
-    return JSONRPCServer(app=app, session_service=False, logger=logger)
+def mock_sessions(app, mock_cache, mock_data_store) -> SessionService:
+    service = SessionService(app=app, cache_service=mock_cache, store_service=mock_data_store)
+    app.services.add_service(service)
+    return service
+
+
+@pytest_asyncio.fixture
+async def mock_session(mock_sessions) -> Session:
+    _session = Session(
+        id='mock_session',
+        user_id=uuid.uuid4(),
+        permissions=frozenset({'mock_permission', 'user'}),
+        data={},
+        created=datetime.now(),
+        h_agent=None,
+        expires=int(time() + 1000),
+        _stored=True,
+        _changed=False,
+        _loaded=False,
+    )
+    _session._changed = True
+    await mock_sessions.save_session(_session)
+    _session._changed = False
+    return _session
+
+
+class _MockTokenService(Service, TokenInterface):
+    token: str
+    refresh_token: str
+    user_id: uuid.UUID
+    permissions: frozenset
+
+    def __init__(self, *args, session: Session, **kws):
+        super().__init__(*args, **kws)
+        self.token = uuid.uuid4().hex
+        self.refresh_token = uuid.uuid4().hex
+        self.user_id = session.user_id
+        self.permissions = session.permissions
+
+    async def auth(self, token: str, /) -> Optional[TokenInterface.TokenClaims]:
+        if token == self.token:
+            return TokenInterface.TokenClaims(id=self.user_id, permissions=self.permissions)
+
+    async def get(self, claims: TokenInterface.TokenClaims, /) -> TokenInterface.TokenInfo:
+        return TokenInterface.TokenInfo(access=self.token, refresh=self.refresh_token)
+
+    async def refresh(self, token: str, /) -> Optional[TokenInterface.TokenInfo]:
+        if token == self.refresh_token:
+            return TokenInterface.TokenInfo(access=self.token, refresh=self.refresh_token)
 
 
 @pytest.fixture
-def rpc_compatible_service():
-    class TestService(Service, AbstractRPCCompatible):
-        service_name = 'm'
+def mock_tokens(app, mock_session) -> TokenInterface:
+    service = _MockTokenService(app=app, session=mock_session)
+    app.services.add_service(service)
+    return service
+
+
+class _MockUserService(Service, UserInterface):
+    def __init__(self, *args, session: Session, **kws):
+        super().__init__(*args, **kws)
+        self.username = uuid.uuid4().hex
+        self.password = uuid.uuid4().hex
+        self.session = session
+
+    async def auth(self, username: str, password: str) -> Optional[_User]:
+        if username == self.username and password == self.password:
+            return TokenInterface.TokenClaims(id=self.session.user_id, permissions=self.session.permissions)
 
-        def __init__(self, *args, **kws):
-            super().__init__(*args, **kws)
-            self.retry_counter = 0
 
-        @property
-        def validators(self) -> dict:
-            return {'validated': j.Object({'a': j.Integer(), 'b': j.Integer()}, required=['a', 'b'])}
-
-        @property
-        def permissions(self) -> Optional[dict]:
-            return {
-                '*': self.PermissionKeys.GLOBAL_GUEST_PERMISSION,
-                'method_with_user_permission': self.PermissionKeys.GLOBAL_USER_PERMISSION,
-                'method_with_user_permission_2': self.PermissionKeys.GLOBAL_USER_PERMISSION,
-            }
-
-        @property
-        def routes(self) -> dict:
-            return {
-                'echo': self.echo,
-                'aecho': self.async_echo,
-                'sum': self.sum,
-                'fail': self.failed,
-                'long_echo': self.async_long_echo,
-                'split': self.split,
-                'standard_echo': self.async_standard_echo,
-                'validated': self.validated_method,
-                'uses_context': self.uses_context,
-                'method_with_user_permission': self.echo_true,
-                'method_with_user_permission_2': self.echo_true,
-                'method_with_retry': self.retry_method,
-            }
-
-        async def retry_method(self, when: int) -> bool:
-            self.retry_counter += 1
-            if self.retry_counter < when:
-                raise TimeoutError('Simulated timeout')
+@pytest.fixture
+def mock_users(app, mock_session) -> _MockUserService:
+    service = _MockUserService(app=app, session=mock_session)
+    app.services.add_service(service)
+    return service
+
+
+@pytest.fixture
+def mock_auth(app, mock_sessions, mock_users, mock_tokens) -> AuthenticationService:
+    service = AuthenticationService(
+        app=app,
+        session_service=mock_sessions,
+        user_service=mock_users,
+        token_service=mock_tokens,
+        enable_token_auth=True,
+        enable_basic_auth=True,
+    )
+    app.services.add_service(service)
+    return service
+
+
+@pytest.fixture
+def mock_login(app, rpc, mock_sessions, mock_users, mock_auth, mock_tokens) -> LoginService:
+    service = LoginService(app=app)
+    app.services.add_service(service)
+    return service
+
+
+class _MockLockService(BaseLocksService):
+    class MockTransport(Service):
+        pass
+
+    def __init__(self, *args, **kws):
+        super().__init__(*args, **kws)
+        self._ext = TTLDict()
+
+    @classmethod
+    def get_transport_cls(cls) -> Type:
+        return cls.MockTransport
+
+    async def m_exists(self, keys: List[NSKey]) -> FrozenSet[NSKey]:
+        s = frozenset(key for key in keys if key in self._ext)
+        return s
+
+    async def _acquire(self, keys: List[NSKey], identifier, ttl: int):
+        for key in keys:
+            if key in self._ext:
+                raise LockExistsError(key)
+        for key in keys:
+            self._ext.set(key, identifier, ttl=ttl)
+
+    async def _release(self, keys: List[NSKey], identifier) -> None:
+        for key in keys:
+            existing = self._ext.get(key)
+            if existing and existing != identifier:
+                raise NotLockOwnerError()
+            del self._ext[key]
+
+    async def _renew(self, keys: List[NSKey], values: List[int]) -> None:
+        for key, ttl in zip(keys, values):
+            existing = self._ext.get(key)
+            if existing:
+                self._ext.set(key, existing, ttl=ttl)
+
+    async def _owner(self, key: NSKey):
+        return self._ext.get(key)
+
+
+@pytest.fixture
+def mock_locks(app, scheduler) -> _MockLockService:
+    service = _MockLockService(app=scheduler.app, transport=_MockLockService.MockTransport(scheduler.app))
+    app.services.add_service(service)
+    return service
+
+
+class _MockService(Service, AbstractRPCCompatible):
+    """Mocked service with RPC interface."""
+
+    service_name = 'do'
+
+    def __init__(self, *args, **kws):
+        super().__init__(*args, **kws)
+        self.retry_counter = 0
+        self.stored = None
+
+    @property
+    def routes(self) -> dict:
+        return {
+            'guest': self.guest,
+            'echo': self.echo,
+            'echo_validated': self.echo_validated,
+            'echo_custom_validated': self.echo_custom_validated,
+            'echo_user': self.echo_user,
+            'echo_session': self.echo_session,
+            'write_session': self.write_session,
+            'call_system_method': self.call_system_method,
+            'fail': self.fail,
+            'retry': self.retry,
+            'store': self.store,
+        }
+
+    @property
+    def permissions(self) -> dict:
+        return {
+            '*': self.PermissionKeys.GLOBAL_USER_PERMISSION,
+            'guest': self.PermissionKeys.GLOBAL_GUEST_PERMISSION,
+            'call_system_method': self.PermissionKeys.GLOBAL_SYSTEM_PERMISSION,
+        }
+
+    @property
+    def validators(self) -> dict:
+        return {
+            'echo_validated': js.Object(
+                {'data': js.Boolean(), 't': js.Number()}, required=['data'], additionalProperties=False
+            ),
+            'echo_custom_validated': self._validate,
+        }
+
+    @staticmethod
+    async def guest() -> bool:
+        return True
+
+    async def store(self, data):
+        self.stored = data
+
+    @staticmethod
+    async def echo(data=None, t: float = 0):
+        await asyncio.sleep(t)
+        return data
+
+    @staticmethod
+    async def fail():
+        raise RuntimeError('Destined to fail!')
+
+    async def retry(self, n: int = 0):
+        self.retry_counter += 1
+        if self.retry_counter >= n:
             self.retry_counter = 0
             return True
+        raise TimeoutError('Needs retry!')
 
-        async def echo_true(self, *args, **kws):
-            return True
+    async def echo_validated(self, data: bool, t: float = 0):
+        return await self.echo(data, t)
+
+    @staticmethod
+    def _validate(params: dict) -> dict:
+        if params['data'] is not True:
+            raise ValidationError
+        return params
+
+    async def echo_custom_validated(self, data: bool, t: float = 0):
+        return await self.echo(data, t)
+
+    async def echo_user(self):
+        await asyncio.sleep(0)
+        return self.get_user_id()
+
+    async def echo_session(self):
+        await asyncio.sleep(0)
+        return self.get_session().id
+
+    async def write_session(self, data):
+        await asyncio.sleep(0)
+        session = self.get_session()
+        session.update(data)
+
+    @staticmethod
+    async def call_system_method():
+        await asyncio.sleep(0)
+        return True
+
+
+@pytest.fixture
+def mock_service(app) -> _MockService:
+    service = _MockService(app=app)
+    app.services.add_service(service)
+    return service
 
-        async def uses_context(self):
-            """Check if a session matches the context."""
-            return await self._uses_context()
-
-        async def _uses_context(self):
-            await asyncio.sleep(0.1)
-            stored_session = self.get_session()
-            return stored_session
-
-        async def sum(self, x: float, y: float) -> float:
-            """Sum something.
-
-            :param x: first value
-            :example x: 7
-            :param y: second value
-            :example y: 6
-            :returns: sum of two values
-            """
-            return x + y
-
-        async def split(self, value: str, delimiter: str) -> List[str]:
-            """Split a string value by delimiter.
-
-            :returns: split parts
-            """
-            return value.split(delimiter)
-
-        async def failed(self):
-            """Do wrong command."""
-            raise ValueError('Something bad happened.')
-
-        async def echo(self, *args, **kws):
-            """Echo command which accepts any arguments."""
-            self.logger.info('Executing echo.')
-            return args, kws
-
-        async def async_echo(self, *args, **kws):
-            """Echo command which accepts any arguments."""
-            self.logger.info('Executing async echo.')
-            await asyncio.sleep(0.01)
-            return args, kws
-
-        async def async_standard_echo(self, *args, **kws):
-            """Echo command which accepts any arguments."""
-            self.logger.info('Executing echo.')
-            await asyncio.sleep(0.01)
-            return args, kws
-
-        async def async_long_echo(self, *args, **kws):
-            """Echo command which accepts any arguments."""
-            self.logger.info('Executing long echo.')
-            await asyncio.sleep(2.2)
-            return args, kws
-
-        async def validated_method(self, a: int, b: int):
-            """Call a method with a validated input."""
-            return a * b
 
-    return TestService
+class _MockListener(Listener):
+    class MockTransport(Service):
+        def __init__(self, *args, **kws):
+            super().__init__(*args, **kws)
+            self.stream = asyncio.LifoQueue()
+
+        async def add(self, body, headers):
+            await self.stream.put((body, headers))
+
+    _transport: MockTransport
+
+    async def close(self):
+        await self._transport.stream.join()
+        await super().close()
+
+    @classmethod
+    def get_transport_cls(cls) -> Type:
+        return _MockListener.MockTransport
+
+    async def _read_batch(self) -> list:
+        msg = await self._transport.stream.get()
+        return [msg]
+
+    async def _process_batch(self, batch: list) -> None:
+        for msg in batch:
+            await self._process_request(msg)
+            self._transport.stream.task_done()
+
+
+@pytest.fixture
+def mock_listener(app, rpc, scheduler, mock_locks, mock_auth) -> _MockListener:
+    transport = _MockListener.MockTransport(app)
+    service = _MockListener(
+        app=app,
+        topic='test',
+        transport=transport,
+        authentication_service=mock_auth,
+        rpc_service=rpc,
+        scheduler=scheduler,
+        locks_service=mock_locks,
+    )
+    app.services.add_service(transport)
+    app.services.add_service(service)
+    return service
+
+
+class _MockStreamClient(StreamRPCClient):
+    _transport: _MockListener.MockTransport
+
+    @classmethod
+    def get_transport_cls(cls) -> Type:
+        return _MockListener.MockTransport
+
+    async def write(self, body, headers: dict = None, key=None) -> None:
+        if isinstance(body, list):
+            body = [req.repr() for req in body]
+        else:
+            body = body.repr()
+        await self._transport.stream.put((body, headers))
+
+
+@pytest.fixture
+def mock_stream_client(app, mock_listener, mock_users) -> _MockStreamClient:
+    service = _MockStreamClient(
+        app=app,
+        app_name=app.name,
+        topic='test',
+        transport=mock_listener._transport,  # noqa
+        auth_str=f'Basic {mock_users.username}:{mock_users.password}',
+        request_logs=True,
+        response_logs=True,
+    )
+    app.services.add_service(service)
+    return service
```

## kaiju_tools/tests/test_annotation_parser.py

```diff
@@ -15,15 +15,15 @@
     NewType,
     TypeVar,
     Literal,
     Generic,
 )
 from uuid import UUID
 
-import pytest
+import pytest  # noqa: pycharm
 
 from kaiju_tools.annotations import AnnotationParser, MethodSignatureError
 from kaiju_tools.jsonschema import compile_schema
 from kaiju_tools.rpc import AbstractRPCCompatible
 
 
 class _TReq(TypedDict):
```

## kaiju_tools/tests/test_cache.py

```diff
@@ -1,64 +1,49 @@
-import asyncio
 import uuid
 
-import pytest
+import pytest  # noqa: pycharm
+import pytest_asyncio
 
-from kaiju_tools.cache import *
+from kaiju_tools.interfaces import Cache
 
-__all__ = ['cache_service_test']
+__all__ = ['TestCache']
 
 
-async def cache_service_test(cache, logger):
-    logger.info('Testing basic operations')
-
-    async with cache:
-        key = str(uuid.uuid4())
-        await cache.set(key, key, ttl=10, nowait=False)
-        _value = await cache.get(key)
-        assert _value == key
-        await cache.delete(key, nowait=False)
-        _value = await cache.exists(key)
-        assert _value is False
-
-        logger.info('Testing bulk operations')
-
-        key1, key2 = str(uuid.uuid4()), str(uuid.uuid4())
-
-        await cache.m_set({key1: key1, key2: key2}, nowait=False)
-        values = await cache.m_exists([key1, key2])
-        assert key1 in values
-        assert key2 in values
-
-        values = await cache.m_get([key1, key2, 'key3'])
-        assert values[key1] == key1
-        assert values[key2] == key2
-        assert 'key3' not in values
-
-        await cache.m_delete([key1, key2], nowait=False)
-        values = await cache.m_exists([key1, key2])
-        assert key1 not in values
-        assert key2 not in values
-
-        await cache.m_set({key1: key1, key2: key2}, ttl=10, nowait=False)
-        values = await cache.m_get([key1, key2])
-        assert values[key1] == key1
-        assert values[key2] == key2
-
-        logger.info('Testing json dumps')
-
-        await cache.set(key, {'data': True}, nowait=False)
-        result = await cache.get(key)
-        assert result['data'] is True
-
-        logger.info('Testing nowait operations')
-
-        nowait_key_1, nowait_key_2 = 'nowait_key_1', 'nowait_key_2'
-        await cache.m_set({nowait_key_1: nowait_key_1, nowait_key_2: nowait_key_2}, ttl=10, nowait=True)
-        await cache.delete(nowait_key_1, nowait=True)
-
-        await asyncio.sleep(0.1)  #: giving background tasks a chance to execute
-
-        nowait_key_cached = await cache.get(nowait_key_2)
-        assert nowait_key_cached == nowait_key_2, 'nowait key should be set in background'
-        nowait_key_exists = await cache.exists(nowait_key_1)
-        assert not nowait_key_exists, 'nowait key should be removed in background'
+@pytest.mark.asyncio
+class TestCache:
+    @staticmethod
+    def get_key():
+        return str(uuid.uuid4())
+
+    @staticmethod
+    def get_value():
+        return {'arg_1': True, 'arg_2': uuid.uuid4()}
+
+    @pytest_asyncio.fixture
+    async def _cache(self, app, mock_cache):
+        async with app.services:
+            yield mock_cache
+
+    async def test_singular_keys(self, _cache: Cache):
+        key, value = self.get_key(), self.get_value()
+        key = _cache.namespace.get_key(key)
+        await _cache.set(key, value, ttl=1)
+        exists = await _cache.exists(key)
+        assert exists
+        _value = await _cache.get(key)
+        assert _value == value
+        await _cache.delete(key)
+        exists = await _cache.exists(key)
+        assert not exists
+
+    async def test_multi_keys(self, _cache: Cache):
+        key_1 = _cache.namespace.get_key(self.get_key())
+        key_2 = _cache.namespace.get_key(self.get_key())
+        data = {key_1: self.get_value(), key_2: self.get_value()}
+        await _cache.m_set(data, ttl=1)
+        exists = await _cache.m_exists(data.keys())
+        assert set(exists) == set(data)
+        _data = await _cache.m_get(data.keys())
+        assert _data == data
+        await _cache.m_delete(data.keys())
+        exists = await _cache.m_exists(data.keys())
+        assert not exists
```

## kaiju_tools/tests/test_class_registry.py

```diff
@@ -1,11 +1,12 @@
 import abc
-import pytest
 
-from ..class_registry import AbstractClassRegistry, ClassRegistrationError
+import pytest  # noqa: pycharm
+
+from kaiju_tools.class_registry import AbstractClassRegistry, ClassRegistrationError
 
 
 def test_class_manager():
     class Base:
         pass
 
     class Registry(AbstractClassRegistry):
```

## kaiju_tools/tests/test_configurator.py

```diff
@@ -1,8 +1,8 @@
-from kaiju_tools.config import ConfigLoader
+from kaiju_tools.app import ConfigLoader
 
 
 def test_settings(logger):
     test_config = {
         'main': {'name': '[APP_NAME]', 'version': '0.0', 'env': 'dev', 'loglevel': 'DEBUG'},
         'app': {},
         'run': {'host': 'localhost', 'port': 8080},
```

## kaiju_tools/tests/test_http.py

```diff
@@ -1,94 +1,74 @@
-from types import SimpleNamespace
-from uuid import uuid4
+from base64 import b64encode
+from time import time
 
-import pytest
+import pytest  # noqa: pycharm
+import pytest_asyncio
 
-from kaiju_tools.serialization import dumps, loads
-from kaiju_tools.tests.fixtures import *
-from kaiju_tools.rpc import JSONRPCHeaders
-from kaiju_tools.http import *
+from kaiju_tools.rpc import JSONRPCServer
+from kaiju_tools.http import RPCClientService, JSONRPCView, HTTPService
+from kaiju_tools.sessions import TokenClientService
 
+from kaiju_tools.tests.fixtures import get_app
 
-@pytest.mark.asyncio
-async def test_rpc_http_client(rpc_interface, aiohttp_server, application, rpc_compatible_service, logger):
-    port = 7677
-    application = application(debug=True)
-
-    async with rpc_interface as rpc:
-        service = rpc_compatible_service(logger=logger)
-        rpc.register_service('do', service)
-        application.router.add_view(JSONRPCView.route, JSONRPCView)
-        application.services = SimpleNamespace(rpc=rpc)
-        server = await aiohttp_server(application, port=port)
-
-        try:
-            async with HTTPService(application, host=f'http://localhost:{port}', logger=logger) as http_client:
-                async with RPCClientService(
-                    app=None, transport=http_client, base_uri=JSONRPCView.route, logger=logger
-                ) as client:
-                    args, kws = await client.call('do.echo', {'value': True})
-                    assert kws['value']
-
-                    result = await client.call_multiple(
-                        {'method': 'do.echo', 'params': {'value': 1}},
-                        {'method': 'do.echo', 'params': {'value': 2}},
-                        {'method': 'do.echo', 'params': {'value': 3}},
-                    )
-
-                    assert [r[1]['value'] for r in result] == [1, 2, 3]
-
-        finally:
-            await server.close()
+__all__ = ['TestRPCClientService']
 
 
 @pytest.mark.asyncio
-async def test_rpc_rest_view(rpc_interface, aiohttp_client, application, rpc_compatible_service, logger):
-    logger.info('Testing service context initialization.')
-    application = application()
-
-    async with rpc_interface as rpc:
-        service = rpc_compatible_service(logger=logger)
-        rpc.register_service(service.service_name, service)
-        application.router.add_view(JSONRPCView.route, JSONRPCView)
-        application.services = SimpleNamespace(rpc=rpc)
-        client = await aiohttp_client(application)
-
-        logger.info('Testing basic functionality.')
-
-        app_id = uuid4()
-        correlation_id = uuid4()
-        headers = {
-            JSONRPCHeaders.APP_ID_HEADER: str(app_id),
-            JSONRPCHeaders.CORRELATION_ID_HEADER: str(correlation_id),
-            'Content-Type': 'application/json',
-        }
-        data = {'id': uuid4().int, 'method': 'm.echo', 'params': {'a': 1, 'b': 2, 'c': 3}}
-        data = dumps(data)
-        response = await client.post(JSONRPCView.route, data=data, headers=headers)
-        assert response.status == 200
-        text = await response.text()
-        body = loads(text)
-        logger.info(body)
-        assert body['result'][1] == {'a': 1, 'b': 2, 'c': 3}
-
-        logger.info('Testing batch functionality.')
-
-        headers = {
-            JSONRPCHeaders.APP_ID_HEADER: str(app_id),
-            JSONRPCHeaders.CORRELATION_ID_HEADER: str(correlation_id),
-            'Content-Type': 'application/json',
-        }
-        data = [
-            {'id': uuid4().int, 'method': 'm.echo', 'params': {'a': 1}},
-            {'id': uuid4().int, 'method': 'm.echo', 'params': None},
-            {'id': uuid4().int, 'method': 'm.echo'},
-        ]
-        headers[JSONRPCHeaders.CORRELATION_ID_HEADER] = str(uuid4())
-        data = dumps(data)
-        response = await client.post(JSONRPCView.route, data=data, headers=headers)
-        assert response.status == 200
-        text = await response.text()
-        body = loads(text)
-        logger.info(body)
-        assert [r['result'] for r in body] == [[[], {'a': 1}], [[], {}], [[], {}]]
-        logger.info('All tests finished.')
+class TestRPCClientService:
+    class _MockTokenClient(TokenClientService):
+        def _get_exp_time(self, token: str) -> int:
+            return int(time() + 1000)
+
+    @pytest.fixture
+    def _sessions(self, mock_sessions):
+        return mock_sessions
+
+    @pytest_asyncio.fixture
+    async def _server(
+        self, app, rpc, mock_service, mock_login, _sessions, aiohttp_server, mock_session
+    ) -> JSONRPCServer:
+        app.router.add_route('*', '/public/rpc', JSONRPCView)
+        server = await aiohttp_server(app, port=20020)  # TODO: random port ?
+        async with server:
+            async with app.services:
+                yield rpc
+
+    @pytest_asyncio.fixture
+    async def _client(self, _server, mock_users, scheduler, logger):
+        app = get_app(logger)
+        transport = HTTPService(app=app, host='http://localhost:20020')
+        client = RPCClientService(app=app, transport=transport)
+        app.services.add_service(client)
+        token_client = self._MockTokenClient(
+            app=app, rpc_client=client, username=mock_users.username, password=mock_users.password, scheduler=scheduler
+        )
+        app.services.add_service(token_client)
+        client._token_client = token_client
+        async with app.services:
+            yield client
+
+    async def test_request(self, _server: JSONRPCServer, _client: RPCClientService, rpc):
+        rpc._enable_permissions = False
+        result = await _client.call('do.echo', {'data': True})
+        assert result is True
+
+    async def test_request_with_basic_auth(self, _server, _client, rpc, mock_users):
+        code = b64encode(f'{mock_users.username}:{mock_users.password}'.encode()).decode('utf-8')
+        _client._auth_str = f'Basic {code}'
+        result = await _client.call('do.echo', {'data': True})
+        assert result is True
+
+    async def test_request_with_token_auth(self, _server: JSONRPCServer, _client: RPCClientService, rpc):
+        result = await _client.call('do.echo', {'data': True})
+        assert result is True
+
+    async def test_batch_request(self, _server: JSONRPCServer, _client: RPCClientService, rpc):
+        rpc._enable_permissions = False
+        result = await _client.call_multiple(
+            *(
+                {'method': 'do.echo', 'params': {'data': 1}},
+                {'method': 'do.echo', 'params': {'data': 2}},
+                {'method': 'do.echo', 'params': {'data': 3}},
+            )
+        )
+        assert result == [1, 2, 3]
```

## kaiju_tools/tests/test_locks.py

```diff
@@ -1,66 +1,55 @@
 import asyncio
 import uuid
 
-import pytest
+import pytest  # noqa: pycharm
+import pytest_asyncio
 
-from kaiju_tools.locks import *
+from kaiju_tools.locks import BaseLocksService, NotLockOwnerError, LockTimeout, LockExistsError, Locks
 
-__all__ = ['locks_service_test']
+__all__ = ['TestLocks']
 
 
-async def locks_service_test(container, locks_service, logger):
-    key = 'some_group:some_service'
-    value = str(uuid.uuid4())
-
-    logger.info('Initialization.')
-
-    async with locks_service as locks:
-        locks: BaseLocksService = locks
-
-        logger.info('Testing basic locking.')
-
-        await locks.acquire(key, value)
-        owner = await locks.owner(key)
-        assert owner == value
-        await locks.release(key, value)
-
-        logger.info('Testing locking/unlocking with ttl')
-
-        await locks.acquire(key, value, ttl=1)
-        await locks.acquire(key, value)
-
-        logger.info('Testing waiting')
-
-        with pytest.raises(LockTimeout):
-            await locks.wait(key, timeout=0.001)
-
-        logger.info('Trying to release a lock by a different owner should produce an error.')
-
+@pytest.mark.asyncio
+class TestLocks:
+    @staticmethod
+    def get_key():
+        return str(uuid.uuid4())
+
+    @pytest_asyncio.fixture
+    async def _locks(self, app, mock_locks):
+        mock_locks.WAIT_RELEASE_REFRESH_INTERVAL = 0.1
+        mock_locks._refresh_interval = 0.1
+        async with app.services:
+            yield mock_locks
+
+    async def test_acquire_release(self, _locks: BaseLocksService):
+        key = _locks.namespace.get_key(self.get_key())
+        lock_id = await _locks.acquire(key, ttl=1)
+        existing = await _locks.m_exists([key])
+        assert key in existing
+        await _locks.release(key, lock_id)
+
+    async def test_release_with_wrong_lock_id(self, _locks: BaseLocksService):
+        key = _locks.namespace.get_key(self.get_key())
+        await _locks.acquire(key, ttl=1)
         with pytest.raises(NotLockOwnerError):
-            new_value = str(uuid.uuid4())
-            await locks.release(key, new_value)
+            await _locks.release(key, Locks.LockId('Wrong!'))
 
-        logger.info('Trying to acquire an existing lock with wait=False should raise an error.')
+    async def test_acquire_timeout(self, _locks: BaseLocksService):
+        key = _locks.namespace.get_key(self.get_key())
+        await _locks.acquire(key, ttl=3)
+        with pytest.raises(LockTimeout):
+            await _locks.acquire(key, ttl=1, timeout=1)
 
+    async def test_acquire_lock_exists(self, _locks: BaseLocksService):
+        key = _locks.namespace.get_key(self.get_key())
+        await _locks.acquire(key, ttl=1)
         with pytest.raises(LockExistsError):
-            await locks.acquire(key, new_value, wait=False)
-
-        logger.info('Trying to set multiple locks at once (should raise an error for all except one)')
-
-        results = await asyncio.gather(
-            locks.acquire('key', str(uuid.uuid4()), wait=False),
-            locks.acquire('key', str(uuid.uuid4()), wait=False),
-            locks.acquire('key', str(uuid.uuid4()), wait=False),
-            return_exceptions=True,
-        )
-
-        counter = 0
-
-        for value in results:
-            logger.debug(value)
-            if isinstance(value, LockExistsError):
-                counter += 1
-
-        assert counter == 2
+            await _locks.acquire(key, ttl=3, wait=False)
 
-        logger.info('Terminating.')
+    async def test_for_parallel_acquire(self, _locks: BaseLocksService):
+        key = _locks.namespace.get_key(self.get_key())
+        commands = [_locks.acquire(key, ttl=1, wait=False) for _ in range(5)]
+        results = await asyncio.gather(*commands, return_exceptions=True)
+        counter = sum(1 for r in results if isinstance(r, LockExistsError))
+        assert counter == len(commands) - 1, 'all but one should fail'
```

## kaiju_tools/tests/test_mapping.py

```diff
@@ -1,10 +1,10 @@
-import pytest
+import pytest  # noqa: pytest
 
-from ..mapping import *
+from kaiju_tools.mapping import *
 
 
 @pytest.fixture
 def test_mapping():
     def _test_mapping(**kws):
         return {'come': {'get': {**kws}}}
 
@@ -24,15 +24,14 @@
     def _test_list_mapping(*args, **kws):
         return {'data': test_list(*args, **kws)}
 
     return _test_list_mapping
 
 
 def test_strip_fields(logger):
-
     obj = {'_id': 1, 'value': True}
     obj = strip_fields(obj)
     logger.debug(obj)
     assert '_id' not in obj
     assert obj['value'] is True
 
     obj = [{'_id': 1, 'value': True}, {'_id': 2, 'value': True}]
@@ -47,15 +46,14 @@
     logger.debug(obj)
     for _obj in obj:
         assert '_id' not in _obj['obj']
         assert _obj['obj']['value'] is True
 
 
 def test_flatten(logger, test_mapping, test_list, test_list_mapping):
-
     obj = test_mapping(some=True)
     _o = flatten(obj)
     logger.debug(_o)
     assert _o['come.get.some'] is True
 
     _obj = test_list(some=True)
     _o = flatten(_obj)
@@ -67,15 +65,14 @@
     _o = flatten(_obj)
     logger.debug(_o)
     for o in _o['data']:
         assert o['come.get.some'] is True
 
 
 def test_recursive_update(logger, test_mapping, test_list):
-
     obj1 = test_mapping(some=True)
     obj2 = test_mapping(another=False)
     recursive_update(obj1, obj2)
     logger.debug(obj1)
     assert obj1['come']['get']['another'] is False
 
     obj1 = test_list(some=True, another=False)
@@ -83,15 +80,14 @@
     recursive_update(obj1, obj2)
     logger.debug(obj1)
     assert obj1[0]['come']['get']['another'] is True
     assert obj1[1]['come']['get']['another'] is False
 
 
 def test_filter_fields(logger, test_mapping, test_list, test_list_mapping):
-
     obj = test_mapping(some=True, another=False)
     _o = filter_fields(obj, ['come.get.some', 'come.get.empty'])
     logger.debug(_o)
     assert _o['come']['get']['some'] is True
     assert 'another' not in _o['come']['get']
     assert _o['come']['get']['empty'] is None
```

## kaiju_tools/tests/test_scheduler.py

```diff
@@ -1,57 +1,40 @@
 import asyncio
 
-import pytest
+import pytest  # noqa: pycharm
 
-from kaiju_tools.app import Scheduler
-from kaiju_tools.fixtures import *
+__all__ = ['TestScheduler']
 
 
 @pytest.mark.asyncio
-async def test_scheduler_execution(application):
+class TestScheduler:
     counter = 0
 
-    async def _call_task(n=1):
-        nonlocal counter
-        counter += n
-
-    scheduler = Scheduler(app=application(), refresh_rate=0.1)
-    scheduler.schedule_task(_call_task, params={'n': 2}, interval=0.1, name='task_1')
-    scheduler.schedule_task(_call_task, interval=0.1, name='task_2')
-
-    async with scheduler:
-        await asyncio.sleep(0.15)
-        assert counter == 3, 'both tasks must be completed'
-
-
-@pytest.mark.asyncio
-async def test_scheduler_policy_wait(application):
-    counter = 0
-
-    async def _call_task():
-        await asyncio.sleep(0.1)
-        nonlocal counter
-        counter += 1
-
-    scheduler = Scheduler(app=application(), refresh_rate=0.1)
-    scheduler.schedule_task(_call_task, interval=0.1, policy=scheduler.ExecPolicy.WAIT)
-
-    async with scheduler:
-        await asyncio.sleep(0.35)
-        assert counter == 1, 'must wait, i.e. only one increment must happen'
-
-
-@pytest.mark.asyncio
-async def test_scheduler_policy_cancel(application):
-    counter = 0
-
-    async def _call_task():
-        await asyncio.sleep(0.1)
-        nonlocal counter
-        counter += 1
-
-    scheduler = Scheduler(app=application(), refresh_rate=0.1)
-    scheduler.schedule_task(_call_task, interval=0.1, policy=scheduler.ExecPolicy.CANCEL)
-
-    async with scheduler:
-        await asyncio.sleep(0.35)
-        assert counter == 0, 'no execution must happen due to cancellation'
+    async def _call(self, t: float = None):
+        if t:
+            await asyncio.sleep(t)
+        self.counter += 1
+
+    async def test_scheduler_execution(self, scheduler):
+        scheduler.schedule_task(self._call, interval=0.1, name='task_1')
+        scheduler.schedule_task(self._call, interval=0.1, name='task_2')
+        async with scheduler.app.services:
+            await asyncio.sleep(0.15)
+            assert self.counter == 2, 'both tasks must be completed'
+
+    async def test_scheduler_repeat(self, scheduler):
+        scheduler.schedule_task(self._call, interval=0.1, name='task_1')
+        async with scheduler.app.services:
+            await asyncio.sleep(0.4)
+            assert self.counter == 3, 'must repeat task'
+
+    async def test_scheduler_policy_wait(self, scheduler):
+        scheduler.schedule_task(self._call, params={'t': 0.1}, interval=0.1, policy=scheduler.ExecPolicy.WAIT)
+        async with scheduler.app.services:
+            await asyncio.sleep(0.35)
+            assert self.counter == 1, 'must wait, i.e. only one increment must happen'
+
+    async def test_scheduler_policy_cancel(self, scheduler):
+        scheduler.schedule_task(self._call, params={'t': 0.1}, interval=0.1, policy=scheduler.ExecPolicy.CANCEL)
+        async with scheduler.app.services:
+            await asyncio.sleep(0.35)
+            assert self.counter == 0, 'no execution must happen due to cancellation'
```

## kaiju_tools/tests/test_serializers.py

```diff
@@ -1,60 +1,85 @@
+import datetime
+import uuid
 from typing import cast, Type
 
 import pytest
 
-from kaiju_tools.encoding import serializers, SerializerInterface
-from .fixtures import *
+from kaiju_tools.encoding import serializers, SerializerInterface, Serializable
+from kaiju_tools.rpc import RPCRequest, RPCResponse, RPCError
+from kaiju_tools.exceptions import InternalError
 
 
 @pytest.fixture
 def serializable_data():
-    import datetime
-    import uuid
-
     data = {
         'int': 42,
         'str': 'some text',
         'unicode': '',
         'bool': True,
         'uuid': uuid.uuid4(),
         'list': ['some', 'text', 42],
         'time': datetime.datetime(2001, 1, 1, 1),
     }
     return data
 
 
 @pytest.fixture
 def serializable_special_objects():
-    from kaiju_tools.rpc import RPCRequest, RPCResponse, RPCError
-    from kaiju_tools.exceptions import InternalError
-
     data = {
         'request': RPCRequest(id=1, method='test', params=None),
         'response': RPCResponse(id=1, result=[1, 2, 3]),
         'error': RPCError(id=None, error=InternalError('Internal error', base_exc=ValueError('Sht!'))),
     }
     return data
 
 
+class _Serialized(Serializable):
+    fields = None
+
+    def __init__(self):
+        self.a = 1
+        self.b = None
+        self._c = 2
+
+
+@pytest.fixture(
+    params=[
+        {'fields': ['a', 'b'], '__slots__': None, 'serializable_attrs': None, 'include_null_values': True},
+        {'fields': ['b'], '__slots__': ['b', '_c'], 'serializable_attrs': None, 'include_null_values': True},
+        {'fields': ['b'], '__slots__': None, 'serializable_attrs': ['b'], 'include_null_values': True},
+        {'fields': ['a'], '__slots__': None, 'serializable_attrs': None, 'include_null_values': False},
+        {'fields': [], '__slots__': ['b', '_c'], 'serializable_attrs': None, 'include_null_values': False},
+    ]
+)
+def _serialized(request):
+    s = _Serialized
+    for key, value in request.param.items():
+        setattr(s, key, value)
+    return s()
+
+
+def test_serializable_objects(_serialized):
+    serialized = _serialized.repr()
+    assert list(serialized) == list(_serialized.fields)
+
+
 @pytest.mark.parametrize('serializer', tuple(serializers.values()), ids=tuple(serializers.keys()))
 def test_serializers(serializer, serializable_data, logger):
-    serializer = cast(Type[SerializerInterface], serializer)
-    serializer = serializer()
+    serializer = cast(Type[SerializerInterface], serializer)()
     s = serializer.dumps(serializable_data)
     logger.debug(s)
     data = serializer.loads(s)
     logger.debug(serializable_data)
     logger.debug(data)
     assert serializable_data == data
 
 
 @pytest.mark.parametrize('serializer', tuple(serializers.values()), ids=tuple(serializers.keys()))
 def test_serializers_for_special_objects(serializer, serializable_special_objects, logger):
-    serializer = cast(Type[SerializerInterface], serializer)
-    serializer = serializer()
+    serializer = cast(Type[SerializerInterface], serializer)()
     s = serializer.dumps(serializable_special_objects)
     logger.debug(s)
     data = serializer.loads(s)
     logger.debug(serializable_special_objects)
     logger.debug(data)
     assert {k: v.repr() for k, v in serializable_special_objects.items()} == data
```

## kaiju_tools/tests/test_services.py

```diff
@@ -1,88 +1,78 @@
 import pytest
 
-from ..services import *
+from kaiju_tools.app import Service, ContextableService, ServiceNotAvailableError
 
 
-@pytest.mark.asyncio
-async def test_service_context_manager_basic_functions(aiohttp_server, application, logger):
-    class SimpleUnnamedService(Service):
-        def __init__(self, x, *args, **kws):
-            super().__init__(*args, **kws)
-            self.x = x
-
-        def __call__(self, *args, **kwargs):
-            return self.x
-
-    class SimpleUnnamedServiceWithDefaults(Service):
-        def __init__(self, x, *args, **kws):
-            super().__init__(*args, **kws)
-            self.x = x
-
-        def __call__(self, *args, **kwargs):
-            return self.x
-
-    class _ContextableService(ContextableService):
-
-        service_name = 'contextable_service'
+class _Service(Service):
+    """Simple service."""
 
-        def __init__(self, x, *args, **kws):
-            super().__init__(*args, **kws)
-            self.x = x
-            self.y = None
+    service_name = '_service'
 
-        def closed(self) -> bool:
-            return self.y is None
 
-        async def init(self):
-            self.y = self.x
+class _OtherService(Service):
+    pass
 
-        async def close(self):
-            self.y = None
 
-        def call(self, *args, **kwargs):
-            return self.y
+class _ContextableService(ContextableService):
+    """Simple contextable service."""
 
-    class ContextableFailedService(_ContextableService):
+    service_name = '_ctx_service'
 
-        service_name = 'contextable_failed'
+    def __init__(self, *args, dependency=None, **kws):
+        super().__init__(*args, **kws)
+        self.ready = False
+        self.dependency = dependency
+        self.dep_required = True
+        self.dep_type = _Service
+        self.kws = kws
 
-        def __init__(self, *args, **kws):
-            super().__init__(*args, **kws)
-            self.y = 42
+    async def init(self):
+        self.dependency = self.discover_service(self.dependency, cls=self.dep_type, required=self.dep_required)
+        self.ready = True
 
-        async def init(self):
-            raise ValueError()
+    async def close(self):
+        self.ready = False
 
-    class ContextableDoubleFailedService(ContextableFailedService):
 
-        service_name = 'contextable_double_failed'
-
-        async def close(self):
-            raise ValueError()
-
-    settings = [
-        {'cls': 'SimpleUnnamedService', 'settings': {'x': 42}},
-        {'cls': 'SimpleUnnamedServiceWithDefaults', 'settings': {'x': 42}},
-        {'cls': 'SimpleUnnamedService', 'name': 'another_simple_unnamed', 'settings': {'x': 43}},
-        {'cls': '_ContextableService', 'settings': {'x': 44}},
-        {'cls': 'ContextableFailedService', 'required': False, 'settings': {'x': 44}},
-        {'cls': 'ContextableDoubleFailedService', 'required': False, 'settings': {'x': 44}},
-        {'cls': 'SimpleUnnamedService', 'name': 'unregistered', 'settings': {'x': 42}},
-    ]
-
-    application = application()
-    registry = ServiceClassRegistry()
-    registry.register_classes_from_namespace(locals())
+@pytest.mark.asyncio
+async def test_services_init(app):
+    ctx_service = _ContextableService(app=app)
+    ctx_service.dep_required = False
+    app.services.add_service(ctx_service)
+    async with app.services:
+        assert ctx_service.ready
+    assert not ctx_service.ready
 
-    manager = ServiceContextManager(application, class_registry=registry, settings=settings, logger=logger)
-    application.services = manager
 
-    # testing app initialization
+@pytest.mark.asyncio
+@pytest.mark.parametrize('dep', [_Service.service_name, None], ids=['by name', 'by type'])
+async def test_discover_dependency(app, dep):
+    service = _Service(app=app)
+    ctx_service = _ContextableService(app=app, dependency=dep)
+    app.services.add_service(service)
+    app.services.add_service(ctx_service)
+    async with app.services:
+        assert ctx_service.ready
+        assert ctx_service.dependency is service
 
-    application.cleanup_ctx.append(manager.cleanup_context)
-    await aiohttp_server(application)
 
-    # checking all services are OK
-    assert application.services.SimpleUnnamedService() == 42
-    assert application.services.another_simple_unnamed() == 43
-    assert application.services.contextable_service.call() == 44
+@pytest.mark.asyncio
+@pytest.mark.parametrize(
+    'kws',
+    [
+        {'dependency': 'other_service'},
+        {'dep_type': _OtherService},
+        {'dependency': _Service.service_name, 'dep_type': _OtherService},
+    ],
+    ids=['wrong name', 'wrong type', 'type mismatch'],
+)
+async def test_discover_dependency_errors(app, kws):
+    service = _Service(app=app)
+    ctx_service = _ContextableService(app=app)
+    for key, value in kws.items():
+        setattr(ctx_service, key, value)
+    app.services.add_service(service)
+    app.services.add_service(ctx_service)
+    with pytest.raises(ServiceNotAvailableError):
+        async with app.services:
+            pass
```

## Comparing `kaiju_tools/tests/test_rpc_server.py` & `kaiju_tools/tests/test_rpc.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,217 +1,220 @@
 import asyncio
-import logging
 from time import time
-from types import SimpleNamespace
-from uuid import uuid4
 
-import pytest
+import pytest  # noqa: pycharm
+import pytest_asyncio
 
-from kaiju_tools.rpc import JSONRPCServer, JSONRPCHeaders, RPCRequest, RPCResponse, RPCError, AbstractRPCCompatible
-from kaiju_tools.types import Scope
-from kaiju_tools.exceptions import *
-from .fixtures import *
+from kaiju_tools.rpc import JSONRPCHeaders, RPCError, RPCResponse, AbstractRPCCompatible, JSONRPCServer
+from kaiju_tools.exceptions import Aborted, RequestTimeout
 
-
-@pytest.mark.asyncio
-async def test_rpc_server_performance(rpc_interface, logger):
-    requests, parallel, n = 5000, 128, 5
-    counter = 0
-
-    async def _do_call(rpc):
-        nonlocal counter
-        data = {'id': 0, 'method': 'do.sleep', 'params': {'test': True}}
-        while counter < requests:
-            await rpc.call(data, {})
-            counter += 1
-
-    class _Service(AbstractRPCCompatible):
-        @property
-        def routes(self) -> dict:
-            return {'sleep': self.do_sleep}
-
-        async def do_sleep(self, test: bool):
-            return test
-
-    print(f'\nJSON RPC Queued Service simple benchmark (best of {n}).')
-    print(f'{parallel} connections\n')
-
-    async with rpc_interface as rpc:
-        rpc_interface._debug = False
-        rpc_interface._request_logs = False
-        rpc_interface._response_logs = False
-        rpc.register_service('do', _Service())
-        tasks = [asyncio.create_task(_do_call(rpc)) for _ in range(parallel)]
-        t0 = time()
-        while counter < requests:
-            await asyncio.sleep(0.1)
-        t1 = time()
-        await asyncio.gather(*tasks)
-
-    dt = t1 - t0
-    rps = round(counter / dt)
-    print(f'{dt}')
-    print(f'{counter} requests')
-    print(f'{rps} req/sec')
+__all__ = ['TestRPCServer']
 
 
 @pytest.mark.asyncio
-async def test_rpc_server_methods(rpc_interface, rpc_compatible_service, logger):
-    logger.info('Testing service context initialization.')
-
-    async with rpc_interface as rpc:
-        service = rpc_compatible_service(app=rpc.app, logger=logger)
-        rpc.register_service(service.service_name, service)
-
-        correlation_id = uuid4()
-
-        logger.info('Testing basic requests.')
-
-        data = {'method': 'm.echo'}
-        _headers, response = await rpc.call(data, {})
-        assert response.result == ((), {})
-
-        data = {'id': None, 'method': 'm.echo'}
-        _result = await rpc.call(data, {})
-        assert _result[1] is None
-
-        data = {'id': uuid4().int, 'method': 'm.echo', 'params': {'value': 42}}
-        _headers, response = await rpc.call(data, {})
-        assert response.result[1]['value'] == 42
-
-        data = {'id': uuid4().int, 'method': 'm.aecho', 'params': {'a': 1, 'b': 2, 'c': 3}}
-        _headers, response = await rpc.call(data, {})
-        assert response.result[1] == {'a': 1, 'b': 2, 'c': 3}
-
-        data = {'id': uuid4().int, 'method': 'm.echo', 'params': {'x': True}}
-        _headers, response = await rpc.call(data, {})
-        assert response.result == ((), {'x': True})
-
-        logger.info('Testing data validation.')
-
-        data = {'id': uuid4().int, 'method': 'm.validated', 'params': {'a': 11, 'b': 2}}
-        _headers, response = await rpc.call(data, {})
-        assert response.result == 22
-
-        data = {'id': uuid4().int, 'method': 'm.validated', 'params': {'a': 11, 'b': 's'}}
-        _headers, response = await rpc.call(data, {})
-        logger.debug(response.repr())
-        assert isinstance(response.error, InvalidParams)
-
-        data = {'id': uuid4().int, 'method': 'm.validated', 'params': {'a': 11}}
-        _headers, response = await rpc.call(data, {})
-        logger.debug(response.repr())
-        assert isinstance(response.error, InvalidParams)
-
-        logger.info('Testing batch requests.')
+class TestRPCServer:
+    @pytest_asyncio.fixture
+    async def _rpc(self, app, rpc, mock_service, mock_sessions, mock_session, mock_auth):
+        async with app.services:
+            yield rpc
+
+    @pytest.mark.parametrize('method', ['api', 'status', 'tasks'])
+    async def test_inspect_methods(self, _rpc: JSONRPCServer, mock_session, method: str):
+        _rpc._enable_permissions = False
+        _, response = await _rpc.call({'method': f'rpc.{method}'})
+        _rpc.logger.debug(response.result)
+
+    async def test_authentication(self, _rpc: JSONRPCServer, mock_users):
+        req = {'method': 'do.echo', 'params': {'data': True}}
+        req_headers = {JSONRPCHeaders.AUTHORIZATION: f'Basic {mock_users.username}:{mock_users.password}'}
+        headers, response = await _rpc.call(req, req_headers)
+        assert type(response) is RPCResponse
+        assert response.result is True
 
-        data = [
-            {'id': uuid4().int, 'method': 'm.echo', 'params': {'x': True}},
-            {'id': uuid4().int, 'method': 'm.echo', 'params': {'a': 1, 'b': 2}},
-            {'id': uuid4().int, 'method': 'm.aecho', 'params': {'a': 1, 'b': 2}},
-        ]
-        _headers, response = await rpc.call(data, {})
-        assert [r.result for r in response] == [((), {'x': True}), ((), {'a': 1, 'b': 2}), ((), {'a': 1, 'b': 2})]
+    @pytest.mark.parametrize(
+        'result, req',
+        [
+            (True, {'jsonrpc': '2.0', 'id': 421, 'method': 'do.echo', 'params': {'data': True}}),
+            (True, {'method': 'do.echo', 'params': {'data': True, '_skip': False}}),
+            (True, {'id': 421, 'method': 'do.echo', 'params': {'data': True}}),
+            (True, {'method': 'do.echo', 'params': {'data': True}}),
+            (None, {'method': 'do.echo'}),
+            (None, {'method': 'do.echo', 'params': None}),
+            ('mock_session', {'method': 'do.echo_session'}),
+        ],
+        ids=[
+            'normal',
+            'skip underscored',
+            'no protocol',
+            'default id',
+            'no params',
+            'null params',
+            'session context',
+        ],
+    )
+    async def test_valid_calls(self, _rpc: JSONRPCServer, mock_session, req, result):
+        req_headers = {
+            JSONRPCHeaders.CORRELATION_ID_HEADER: 'ffffffff',
+            JSONRPCHeaders.SESSION_ID_HEADER: mock_session.id,
+        }
+        headers, response = await _rpc.call(req, req_headers)
+        assert headers[JSONRPCHeaders.CORRELATION_ID_HEADER] == req_headers[JSONRPCHeaders.CORRELATION_ID_HEADER]
+        assert type(response) is RPCResponse
+        assert response.result == result
+
+    async def test_retries(self, _rpc: JSONRPCServer, mock_session):
+        req_headers = {JSONRPCHeaders.RETRIES: 3, JSONRPCHeaders.SESSION_ID_HEADER: mock_session.id}
+        req = {'method': 'do.retry', 'params': {'n': 3}}
+        headers, response = await _rpc.call(req, req_headers)
+        assert type(response) is RPCResponse
+        assert response.result is True
 
-        logger.info('Testing batch requests using templates.')
+    async def test_local_callback(self, _rpc: JSONRPCServer, mock_session):
+        counter = 0
 
-        data = [
-            {'id': 0, 'method': 'm.sum', 'params': {'x': 1, 'y': 1}},
-            {'id': 1, 'method': 'm.sum', 'params': {'x': 1, 'y': '[0]'}},
-            {'id': 2, 'method': 'm.sum', 'params': {'x': 1, 'y': '[1]'}},
+        async def _do_callback(mock_session, headers, response):
+            nonlocal counter
+            counter += response.result
+
+        req = {'method': 'do.echo', 'params': {'data': 1}}
+        headers = {JSONRPCHeaders.SESSION_ID_HEADER: mock_session.id}
+        await _rpc.call(req, headers, callback=_do_callback, nowait=True)
+        await asyncio.sleep(0.1)
+        assert counter == 1
+
+    async def test_session_store(self, _rpc: JSONRPCServer, mock_session):
+        req = {'method': 'do.write_session', 'params': {'data': {'value': 1}}}
+        headers = {JSONRPCHeaders.SESSION_ID_HEADER: mock_session.id}
+        headers, response = await _rpc.call(req, headers)
+        assert type(response) is RPCResponse
+        existing = await _rpc._sessions.load_session(mock_session.id)
+        assert 'value' in existing.data, 'value must be stored in the session'
+
+    @pytest.mark.parametrize(
+        'result, req',
+        [
+            (
+                [1, 2, 3],
+                [
+                    {'method': 'do.echo', 'params': {'data': 1}},
+                    {'method': 'do.echo', 'params': {'data': 2}},
+                    {'method': 'do.echo', 'params': {'data': 3}},
+                ],
+            )
+        ],
+        ids=['batch request'],
+    )
+    async def test_batch_calls(self, _rpc: JSONRPCServer, mock_session, req, result):
+        req_headers = {JSONRPCHeaders.SESSION_ID_HEADER: mock_session.id}
+        headers, response = await _rpc.call(req, req_headers)
+        assert type(response) is list
+        assert [r.result for r in response] == result
+        assert [r.id for r in response] == list(range(len(req)))
+
+    async def test_batch_abort_on_error(self, _rpc: JSONRPCServer, mock_session):
+        req = [
+            {'method': 'do.echo', 'params': {'data': True}},
+            {'method': 'do.fail'},
+            {'method': 'do.echo', 'params': {'data': True}},
         ]
-        _headers, response = await rpc.call(data, {JSONRPCHeaders.USE_TEMPLATE: '?1'})
-        assert response[-1].result == 4
-
-        logger.info('Testing request error handling.')
-
-        data = {'id': uuid4().int, 'method': 'm.unknown', 'params': {'x': True}}
-        _headers, response = await rpc.call(data, {})
-        assert isinstance(response.error, MethodNotFound)
-
-        data = {'id': uuid4().int}
-        _headers, response = await rpc.call(data, {})
-        assert isinstance(response.error, InvalidRequest)
-
-        # data = {'id': uuid4().int, 'method': 'm.unknown', 'params': {'value': True}, 'shit': 1}
-        # _headers, response = await rpc.call(data, {})
-        # assert isinstance(response.error, InvalidRequest)
-
-        data = {'id': uuid4().int, 'method': 'm.fail'}
-        _headers, response = await rpc.call(data, {})
-        assert isinstance(response.error, InternalError)
-
-        logger.info('Testing timeouts.')
-
-        headers = {}
-
-        data = {'id': uuid4().int, 'method': 'm.long_echo'}
-        headers[JSONRPCHeaders.REQUEST_TIMEOUT_HEADER] = 1
-        _headers, response = await rpc.call(data, headers)
-        assert isinstance(response.error, RequestTimeout)
-        #
-        # data = {'id': uuid4().int, 'method': 'm.long_echo'}
-        # headers[JSONRPCHeaders.REQUEST_TIMEOUT_HEADER] = 10
-        # _headers, response = await rpc.call(data, headers)
-        # assert isinstance(response, RPCResponse)
-
-        logger.info('Testing retries.')
-
-        data = {'method': 'm.method_with_retry', 'params': {'when': 2}}
-        _headers, response = await rpc.call(data, {JSONRPCHeaders.RETRIES: 2})
-        assert response.result is True
-
-        data = {'method': 'm.method_with_retry', 'params': {'when': 2}}
-        _headers, response = await rpc.call(data, {JSONRPCHeaders.RETRIES: 1})
-        assert response.error
-
-        logger.info('Testing for parallel task execution')
-
-        tasks = []
-
-        for _ in range(4):
-            data = {'id': uuid4().int, 'method': 'm.standard_echo'}
-            headers[JSONRPCHeaders.REQUEST_DEADLINE_HEADER] = int(time() + 1)
-            headers[JSONRPCHeaders.CORRELATION_ID_HEADER] = correlation_id
-            tasks.append(rpc.call(data, headers))
-
-        t = time()
-        await asyncio.gather(*tasks)
-        t = time() - t
-        assert t <= 1
-
-        logger.info('Testing separate bulk request error handling.')
-
-        data = [
-            {'method': 'm.echo'},
-            {'method': 'm.long_echo'},
-            {'method': 'm.fail'},
-            {'method': 'm.sum', 'params': {'x': 1, 'y': 2}},
+        req_headers = {JSONRPCHeaders.ABORT_ON_ERROR: '?1', JSONRPCHeaders.SESSION_ID_HEADER: mock_session.id}
+        headers, response = await _rpc.call(req, req_headers)
+        assert type(response[2].error) is Aborted
+
+    async def test_batch_abort_on_timeout(self, _rpc: JSONRPCServer, mock_session):
+        req = [
+            {'method': 'do.echo', 'params': {'t': 0}},
+            {'method': 'do.echo', 'params': {'t': 3}},
+            {'method': 'do.echo', 'params': {'t': 0}},
         ]
-        headers[JSONRPCHeaders.REQUEST_DEADLINE_HEADER] = int(time() + 1)
-        _headers, response = await rpc.call(data, headers)
-        assert isinstance(response[1].error, RequestTimeout)
-        assert isinstance(response[2], RPCError)
-
-        logger.info('Testing context handling.')
-
-        session_1 = SimpleNamespace(id='1', test=True, permissions=frozenset(), stored=True, loaded=False, changed=True)
-        session_2 = SimpleNamespace(
-            id='2', test=False, permissions=frozenset(), stored=True, loaded=True, changed=False
-        )
-        data = {'id': uuid4().int, 'method': 'm.uses_context'}
-        r1, r2 = await asyncio.gather(rpc.call(data, {}, session=session_1), rpc.call(data, {}, session=session_2))
-        assert r1[0][JSONRPCHeaders.SESSION_ID_HEADER] == session_1.id
-        assert r2[0][JSONRPCHeaders.SESSION_ID_HEADER] == session_2.id
-
-        logger.info('Testing permissions handling.')
-        session = SimpleNamespace(id='1', permissions=set(), stored=True, loaded=True, changed=True)
-        _, response = await rpc.call({'method': 'm.method_with_user_permission'}, session=session, scope=Scope.GUEST)
-        assert isinstance(response.error, PermissionDenied)
-
-        session.permissions.add('m.method_with_user_permission')
-        _, response = await rpc.call({'method': 'm.method_with_user_permission'}, session=session, scope=Scope.GUEST)
-        assert response.result is True
-        _, response = await rpc.call({'method': 'm.method_with_user_permission_2'}, session=session, scope=Scope.GUEST)
-        assert isinstance(response.error, PermissionDenied)
-
-        logger.info('All tests finished.')
+        req_headers = {JSONRPCHeaders.REQUEST_TIMEOUT_HEADER: 1, JSONRPCHeaders.SESSION_ID_HEADER: mock_session.id}
+        headers, response = await _rpc.call(req, req_headers)
+        assert type(response[1].error) is RequestTimeout
+        assert type(response[2].error) is RequestTimeout
+
+    @pytest.mark.parametrize(
+        'error, req',
+        [
+            ('InvalidParams', {'method': 'do.echo', 'params': {'not_data': True}}),
+            ('InvalidParams', {'method': 'do.echo', 'params': []}),
+            ('InvalidRequest', {}),
+            ('InvalidRequest', {'id': '0', 'method': 'do.echo'}),
+            ('InvalidRequest', {'id': '0', 'method': 'do.echo', 'something': True}),
+            ('MethodNotFound', {'method': 'do._non_existent'}),
+            ('MethodNotFound', {'method': 'do.call_system_method'}),
+            ('MethodNotFound', {'id': None, 'method': 'do.call_system_method'}),
+            ('RequestTimeout', {'method': 'do.echo', 'params': {'t': 3}}),
+        ],
+        ids=[
+            'wrong input param',
+            'not dict "params"',
+            'no method name',
+            'not integer "id"',
+            'non-standard request field',
+            'calling non-existent method',
+            'insufficient permissions',
+            'notify request pre-check',
+            'request timeout',
+        ],
+    )
+    async def test_errors(self, _rpc: JSONRPCServer, mock_session, req, error):
+        req_headers = {
+            JSONRPCHeaders.CORRELATION_ID_HEADER: 'ffffffff',
+            JSONRPCHeaders.REQUEST_TIMEOUT_HEADER: 1,
+            JSONRPCHeaders.SESSION_ID_HEADER: mock_session.id,
+        }
+        headers, response = await _rpc.call(req, req_headers)
+        assert headers[JSONRPCHeaders.CORRELATION_ID_HEADER] == req_headers[JSONRPCHeaders.CORRELATION_ID_HEADER]
+        assert type(response) is RPCError
+        assert response.repr()['error']['data']['type'] == error
+
+    @pytest.mark.parametrize(
+        'result, req',
+        [(1, [{'method': 'do.echo', 'params': {'data': 1}}, {'method': 'do.echo', 'params': {'data': '[0]'}}])],
+    )
+    async def test_batch_templates(self, _rpc: JSONRPCServer, mock_session, req, result):
+        headers = {JSONRPCHeaders.USE_TEMPLATE: '?1', JSONRPCHeaders.SESSION_ID_HEADER: mock_session.id}
+        headers, response = await _rpc.call(req, headers)
+        assert response[-1].result == result
+
+    @pytest.mark.benchmark
+    async def test_performance(self, rpc):
+        requests, parallel, n = 5000, 128, 5
+        counter = 0
+
+        async def _do_call(_rpc):
+            nonlocal counter
+            data = {'id': 0, 'method': 'do.sleep', 'params': {'test': True}}
+            while counter < requests:
+                await _rpc.call(data, {})
+                counter += 1
+
+        class _Service(AbstractRPCCompatible):
+            @property
+            def routes(self) -> dict:
+                return {'sleep': self.do_sleep}
+
+            @staticmethod
+            async def do_sleep(test: bool):
+                return test
+
+        print(f'\nJSON RPC Queued Service simple benchmark (best of {n}).')
+        print(f'{parallel} connections\n')
+
+        async with rpc:
+            rpc._debug = False
+            rpc._request_logs = False
+            rpc._response_logs = False
+            rpc._enable_permissions = False
+            rpc.register_service('do', _Service())
+            tasks = [asyncio.create_task(_do_call(rpc)) for _ in range(parallel)]
+            t0 = time()
+            while counter < requests:
+                await asyncio.sleep(0.1)
+            t1 = time()
+            await asyncio.gather(*tasks)
+
+        dt = t1 - t0
+        rps = round(counter / dt)
+        print(f'{dt}')
+        print(f'{counter} requests')
+        print(f'{rps} req/sec')
```

## Comparing `kaiju_tools-2.1.1.dist-info/LICENSE` & `kaiju_tools-2.1.2.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `kaiju_tools-2.1.1.dist-info/METADATA` & `kaiju_tools-2.1.2.dist-info/METADATA`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: kaiju-tools
-Version: 2.1.1
+Version: 2.1.2
 Summary: Base classes and services for a backend application.
 Home-page: https://gitlab.com/kaiju-python/kaiju-tools
 Author: antonnidhoggr@me.com
 Author-email: antonnidhoggr@me.com
 License: Apache Software License 2.0
 Classifier: Development Status :: 3 - Alpha
 Classifier: License :: OSI Approved :: Apache Software License
@@ -47,14 +47,15 @@
 Requires-Dist: python-docs-theme ; extra == 'docs'
 Provides-Extra: test
 Requires-Dist: pytest (>=7.2) ; extra == 'test'
 Requires-Dist: pytest-asyncio (>=0.20) ; extra == 'test'
 Requires-Dist: docker (>=6.0) ; extra == 'test'
 Requires-Dist: pytest-timeout (>=2.1) ; extra == 'test'
 Requires-Dist: pytest-aiohttp (>=1.0) ; extra == 'test'
+Requires-Dist: pytest-cov (>=4.1.0) ; extra == 'test'
 
 
 .. image:: https://badge.fury.io/py/kaiju-tools.svg
     :target: https://pypi.org/project/kaiju-tools
     :alt: Latest package version
 
 .. image:: https://readthedocs.org/projects/kaiju-tools/badge/?version=latest
```

## Comparing `kaiju_tools-2.1.1.dist-info/RECORD` & `kaiju_tools-2.1.2.dist-info/RECORD`

 * *Files 13% similar despite different names*

```diff
@@ -1,54 +1,53 @@
-kaiju_tools/__init__.py,sha256=9KAXG6Y-8SjH8qEjHuYotizC7WeJ2MjmWLoqn9yQq8s,232
+kaiju_tools/__init__.py,sha256=nCLkaPFqQYQ_knkOYClKffgt0QpgzgDCZrib7Z4ZfHA,231
 kaiju_tools/annotations.py,sha256=47ioFnGcZDvDElnCVRFlzkCU_A6a2UfWz_M8dfKBmpM,15027
-kaiju_tools/app.py,sha256=8_FKunyG2_-wip_7h0C1z5HHPWxP1jEoFt9dCjHX6n0,23424
-kaiju_tools/cache.py,sha256=hhlQMTyDb-2C6tz0_1_gNYxjgUodxnH-YrSmvvqa-rI,8518
+kaiju_tools/app.py,sha256=MwsaUs2cy2bJGUJ8Hrr648_1txYGHxamNT4mRzJ7Upc,37350
+kaiju_tools/cache.py,sha256=pzlK_GEYfp0dus4uZDov4cd5d7I9wbdS6Y4QQSo9vlo,4699
 kaiju_tools/class_registry.py,sha256=0duhDIB_za-SPLkxI7mK6NsjGodw99i-ldoOGFSI5Q4,5430
-kaiju_tools/config.py,sha256=diK17zC4cAZv7xph4cGc1Osvir2CxMBMZc4fKqm--f4,10838
-kaiju_tools/encoding.py,sha256=RlQizZ0KcPF0RnsfmbOx1TlTjKnuzjMoV_r1VFQYcBQ,12468
-kaiju_tools/exceptions.py,sha256=5rEyPjgPZp9VmuaeSXoGgHYKNQzXgBYZ6CeMPOvkcIs,6036
-kaiju_tools/fixtures.py,sha256=h1ElRgENikjde-uXvk_exA_SvMPuxqtxo0qaiYDhCWk,997
-kaiju_tools/functions.py,sha256=OzUtyDHfPHItc7u5cRbqr00kABQ-P682MUkzqkXn3Sk,8706
-kaiju_tools/http.py,sha256=Nlq9g3N4SSZitEfwBeruAkc4oD8KTuKuOEn63kBt-Ew,10436
+kaiju_tools/encoding.py,sha256=r0Jx4U5VLTCVswZC83lFDp5za1BY1jFJAm88w3rGEyg,12475
+kaiju_tools/exceptions.py,sha256=_z9yJd2zJv32Z76bDnlVCtBSN9b0FRWRsfJGf4xcB94,5310
+kaiju_tools/functions.py,sha256=ZeyIGCUqrYcNpBZ_dF530I7KB3f4lfT52TATAWfb8Xg,8706
+kaiju_tools/http.py,sha256=Ld9ETI13WjMtucqEOEmxGq3XxbYdVfbfazy3JurpN_E,11388
+kaiju_tools/interfaces.py,sha256=RnCSmy__6YbXZL01BtREzxlvu5qUHO3DsCNgF8qUMCE,10411
 kaiju_tools/jsonschema.py,sha256=cKn8bZ9pXZXgTgcj4hG9H8EWuHS7Q2QQmcN4k5eQMc4,10067
-kaiju_tools/locks.py,sha256=r11c5Ain0hpDhQ6dZvy9gC0DC5XVKBuRl_gKHyE8Mso,8912
-kaiju_tools/logging.py,sha256=eTu8BKD4ax2nK7ZwHylJLW_dGUYegyUEh5F2GCZPqwk,10306
-kaiju_tools/loop.py,sha256=Nkih75UgPen5SvZYSU0XFb9cbH6Zle-Nbto0B_wpn50,195
+kaiju_tools/locks.py,sha256=OGtfkqwECCMM1JK8Dtm2vOAsKJz4OK2NpH1SMQ30xeE,7548
+kaiju_tools/logging.py,sha256=ZG-ImBLpOvzB9MrSopVBlgNiCckO9QdXh7V_6iBGRGk,10446
+kaiju_tools/loop.py,sha256=DhiJ9jftB43RkeoTvOGKdwBbRFr4gj1Rz6uTLTYyF_U,210
 kaiju_tools/mapping.py,sha256=1ilBdLrpTEkGkMm6Ogv1L8lcHofRSelavhJnuiZraOM,7822
-kaiju_tools/rpc.py,sha256=sRFkiyRnVJhiJYWesT66WyZwBKgZIAUiZ9D-39C5rHM,39398
+kaiju_tools/rpc.py,sha256=zDbqkK3ofZ_jbYkX-ZHcCCMHGYwTWWbfH2WZhzOhksY,34431
 kaiju_tools/rpc_client_gen.py,sha256=8k31kUdwK2T4Zpwb-OBlNAoMK7k_2FDoX1LjhciD2yg,4416
-kaiju_tools/serialization.py,sha256=mTMlrikFyvyAChd8rTNUtiXsYqkSLsnP1ykJFWQ05WE,110
-kaiju_tools/services.py,sha256=T2uy_jXm9eLhji1j51Dpw9IjttJPmcvh40I245n1Uyo,434
-kaiju_tools/streams.py,sha256=aHZ3cdqlus2LTLZi3_gc4ZvNhJqGqh1Pnk7QGaJ0BJk,8898
+kaiju_tools/serialization.py,sha256=_TPV7HTUua5EEeUYEHCdqj_HHo2VdKpe2RusbeVIp8g,95
+kaiju_tools/services.py,sha256=hh7nSgw6cN59menn7LgeQ0R9gFmMQOrt1FLi22Rdhq8,702
+kaiju_tools/sessions.py,sha256=HcxcyX1gONcHXdTTfD53sI3OXTtB8Vb4Na9Av4UFi7w,16658
+kaiju_tools/streams.py,sha256=QG_V_Hv6C4ZT5gs-kT99XlifeoKSO1YsC3S27EKhonM,7216
 kaiju_tools/templates.py,sha256=DAwXGLYN7flm5hS4fKWpt5cJZuXK1x5YYgiflTvJKuU,14337
-kaiju_tools/types.py,sha256=CEtufhdHWQV2ISQzVQiIQyrAajqgTK1wn7xF2f5K9s8,14700
+kaiju_tools/types.py,sha256=9WE-S_6V63fO1y8E9F9NFjJEhayF6fUzEjjseyyquLU,12410
 kaiju_tools/docker/__init__.py,sha256=OAZ6r-UPPssXFhgcw2Xa0AW9N8ToR45ZVX6jQWpv3k0,276
-kaiju_tools/docker/containers.py,sha256=WJEcPUeznEalFek9dQKt1Z4qS9l_-lzoZjrNnbl9onY,9769
-kaiju_tools/docker/images.py,sha256=-U34Sp_vaAAVNhg2P0YQC3qxFoNnJPLdyUpJpGz6YHE,7728
+kaiju_tools/docker/containers.py,sha256=Vx-uMI7lHrQWRQAj2UFcjuVdVX3C7F43IJRoDebgrTY,9782
+kaiju_tools/docker/images.py,sha256=ZDzc2rQ9kL1ydWSco0r9JAIgHq-BVT1HmkDhXjzf1F4,7723
 kaiju_tools/docker/services.py,sha256=cS3Xl3EMOWv0y_WRCi5GKFYXFrcpkievTVSyFUt0p4I,103
-kaiju_tools/docker/stack.py,sha256=7OwZPpuL0nFMQC-8B1bJXCm8qVNP3tGd0mv0XdhwwbA,11140
+kaiju_tools/docker/stack.py,sha256=dtoS3veOln42dG-um9d1datTwMJIUkR8vWPgVySYB1Y,11216
 kaiju_tools/docker/tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 kaiju_tools/docker/tests/fixtures.py,sha256=0bx6BE7id5Rz52y3nEVXx0ZNjzPuDe_T9C6Jzq4iHaU,1625
 kaiju_tools/docker/tests/test_containers.py,sha256=Yugtil6wQs2DGp6W0lNJM0ZoihzG5d95z0u5StGIfJ0,575
 kaiju_tools/docker/tests/test_images.py,sha256=XDJ1Z3XQSQswiJxDO_QtKmw4NymJpXhSdvHtxCGg8rE,359
 kaiju_tools/docker/tests/test_stack.py,sha256=qa6UbjCBiVmAORKyWTs9HaArZF0ZLBDZSzGJlc3Bjhs,371
 kaiju_tools/tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-kaiju_tools/tests/fixtures.py,sha256=mavh-c91FsNVVWrD7MBt9GzmVV_QfWKkDvP-OjmA4TI,6146
-kaiju_tools/tests/test_annotation_parser.py,sha256=HtMEC3f_BVufBL1QIHfgzH5G1OvlyoXhfK8-WIdwr6M,9192
-kaiju_tools/tests/test_cache.py,sha256=WJ2nSI0DfsRUQss7fClgMct7IldETxxMcoMIuNOkVQE,2155
-kaiju_tools/tests/test_class_registry.py,sha256=D9mZu9sgY21gEWE8vp2HN0Vfb5WZqxWTNVRPWEPduhU,1408
-kaiju_tools/tests/test_configurator.py,sha256=ABGh7UMGZmx2yCdppGN3TaJKBFJlP-l37PaPF_1JIqY,555
-kaiju_tools/tests/test_http.py,sha256=9ANl3nuGv6236fa97h_fhj6vY5sbBpiy9a7ZENdM0Ss,3752
-kaiju_tools/tests/test_locks.py,sha256=0kTvmL-PfqXk8LtosIaMdR_RFwxM5Pg5sfGWyHaLNvQ,1885
-kaiju_tools/tests/test_loggers.py,sha256=NtFJBeetvdhFr1DBbCe1Er5c7uXrR5-oStd-VCI_BnQ,1777
-kaiju_tools/tests/test_mapping.py,sha256=HzxgQo-k2cBPE88F4AnhBv_Ixb0YS-dWL5rQtM_MRVE,3385
-kaiju_tools/tests/test_rpc_server.py,sha256=TjN-co3UPxVt_G6bQkOi-iGCQjZYm4lmG2Q0bBn3JlM,8614
-kaiju_tools/tests/test_scheduler.py,sha256=FSGW95IWnT2dsNfrBrNEXHYoHKbsYwcRvS9CRHtluVQ,1590
-kaiju_tools/tests/test_serialization.py,sha256=9ETlE5i8b2WFwKsTrECVuB14Pkxf4QgCc6oFX_yjMZw,481
-kaiju_tools/tests/test_serializers.py,sha256=goaWGwrsOwrL19Cib5xSJtezo9knbXK84cB8o11tCdU,1896
-kaiju_tools/tests/test_services.py,sha256=fYhWYsp-aAyIYwAcLx0ScZhBLBPEX6Apf2LRVWe-WX8,2787
+kaiju_tools/tests/fixtures.py,sha256=1qWxT4_wC7CgOzmmzn96QZpHLg-Fbt1GaUDz2ng609k,18531
+kaiju_tools/tests/test_annotation_parser.py,sha256=jCQ5DR_EJEcyH5L5YzhKGw01_UlUxeiSd2i6zJUqhOU,9209
+kaiju_tools/tests/test_cache.py,sha256=Ko4Lx4thNU4IQK575xRAws7XVaAjLG0i_nMVFvn1kBg,1469
+kaiju_tools/tests/test_class_registry.py,sha256=P-XQ4nbAgH6TMBHoIEKpAdlMd1FL9tM9zm4f7L7Mpaw,1436
+kaiju_tools/tests/test_configurator.py,sha256=BwOmU6H4szzWDpK0JNq1b7Q1mm-yWGwAZ3O200MNLJc,552
+kaiju_tools/tests/test_http.py,sha256=2gnS9pVZPCcf8imAjHm8b2TBXKCJdWLpzP0L3BwOvg8,2849
+kaiju_tools/tests/test_locks.py,sha256=-6h3amJnhWe5mf6uP1r2-laiaiXGMNfFpzThmyZfBlw,2097
+kaiju_tools/tests/test_mapping.py,sha256=V2821EkkXrzvQ5qKPHYADPRNmTrOa2LaNJv8y6rdar4,3407
+kaiju_tools/tests/test_rpc.py,sha256=_SEQNib-2fO-T_wgJlDUsUuUpPJ4RrUsZnULwjxX0jQ,9174
+kaiju_tools/tests/test_scheduler.py,sha256=jeOmW44btxdyeeORs_fPnK2Qzj1kENkFpLxJz5Zkkl4,1555
+kaiju_tools/tests/test_serializers.py,sha256=VcQkjaZe8gjjg4Thyf-cEXp8uHgntPcZKc0jURg66qo,2800
+kaiju_tools/tests/test_services.py,sha256=cCZecNOXsY-j91l07_rG3rfKpXQiu6DeMqBCrbaBsuY,2207
+kaiju_tools/tests/test_streams.py,sha256=WEVy4ctD3znw-qmZgM_j90mVPdoRcpXOKs3dIYi7A5U,3213
 kaiju_tools/tests/test_templates.py,sha256=CoQdRatGZhGzPl_y4MYXPTQMA9c_wn0ieVITQaamrL4,1692
-kaiju_tools-2.1.1.dist-info/LICENSE,sha256=XIlN2qA8UqpBDA-PteoYP4hTU0qBW0G9PRB__khO2zc,610
-kaiju_tools-2.1.1.dist-info/METADATA,sha256=YavI3lsqh8-fzSckhv_DZEjGB06KFs2kAOV-hYXXFH0,3306
-kaiju_tools-2.1.1.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-kaiju_tools-2.1.1.dist-info/top_level.txt,sha256=DIgXUScFnJtRLiRWfjo547JK4rMuTIJoioMPRe1Jn6Y,12
-kaiju_tools-2.1.1.dist-info/RECORD,,
+kaiju_tools-2.1.2.dist-info/LICENSE,sha256=XIlN2qA8UqpBDA-PteoYP4hTU0qBW0G9PRB__khO2zc,610
+kaiju_tools-2.1.2.dist-info/METADATA,sha256=-pXqeazEOvYFbZYJpufo3t7CtbF2gk9rtCv_3W4IjIY,3360
+kaiju_tools-2.1.2.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+kaiju_tools-2.1.2.dist-info/top_level.txt,sha256=DIgXUScFnJtRLiRWfjo547JK4rMuTIJoioMPRe1Jn6Y,12
+kaiju_tools-2.1.2.dist-info/RECORD,,
```

