# Comparing `tmp/dtlpy-1.78.17-py3-none-any.whl.zip` & `tmp/dtlpy-1.79.14-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,25 +1,25 @@
-Zip file size: 532756 bytes, number of entries: 229
--rw-r--r--  2.0 unx    19847 b- defN 23-May-23 16:53 dtlpy/__init__.py
--rw-r--r--  2.0 unx       20 b- defN 23-Jun-07 20:16 dtlpy/__version__.py
+Zip file size: 533686 bytes, number of entries: 229
+-rw-r--r--  2.0 unx    19847 b- defN 23-Jun-14 11:16 dtlpy/__init__.py
+-rw-r--r--  2.0 unx       20 b- defN 23-Jun-22 08:40 dtlpy/__version__.py
 -rw-r--r--  2.0 unx     2871 b- defN 23-May-15 16:11 dtlpy/exceptions.py
--rw-r--r--  2.0 unx     5654 b- defN 23-May-15 16:11 dtlpy/new_instance.py
+-rw-r--r--  2.0 unx     5654 b- defN 23-Jun-14 08:42 dtlpy/new_instance.py
 -rw-r--r--  2.0 unx      976 b- defN 23-May-15 16:11 dtlpy/assets/__init__.py
 -rw-r--r--  2.0 unx    24081 b- defN 23-May-15 16:11 dtlpy/assets/lock_open.png
 -rw-r--r--  2.0 unx     1380 b- defN 23-May-15 16:11 dtlpy/assets/main.py
 -rw-r--r--  2.0 unx      267 b- defN 23-May-15 16:11 dtlpy/assets/main_partial.py
 -rw-r--r--  2.0 unx      149 b- defN 23-May-15 16:11 dtlpy/assets/mock.json
 -rw-r--r--  2.0 unx     3168 b- defN 23-May-15 16:11 dtlpy/assets/model_adapter.py
 -rw-r--r--  2.0 unx     1259 b- defN 23-May-15 16:11 dtlpy/assets/package.json
 -rw-r--r--  2.0 unx      568 b- defN 23-May-15 16:11 dtlpy/assets/package_catalog.json
 -rw-r--r--  2.0 unx     4405 b- defN 23-May-15 16:11 dtlpy/assets/package_gitignore
 -rw-r--r--  2.0 unx    10807 b- defN 23-May-15 16:11 dtlpy/assets/project_dataset_recipe_ontology.png
 -rw-r--r--  2.0 unx      742 b- defN 23-May-15 16:11 dtlpy/assets/voc_annotation_template.xml
--rw-r--r--  2.0 unx       48 b- defN 23-May-15 16:11 dtlpy/assets/code_server/config.yaml
--rw-r--r--  2.0 unx      838 b- defN 23-May-15 16:11 dtlpy/assets/code_server/installation.sh
+-rw-r--r--  2.0 unx       48 b- defN 23-Jun-14 08:42 dtlpy/assets/code_server/config.yaml
+-rw-r--r--  2.0 unx      838 b- defN 23-Jun-14 08:42 dtlpy/assets/code_server/installation.sh
 -rw-r--r--  2.0 unx      331 b- defN 23-May-15 16:11 dtlpy/assets/code_server/launch.json
 -rw-r--r--  2.0 unx       64 b- defN 23-May-15 16:11 dtlpy/assets/code_server/settings.json
 -rw-r--r--  2.0 unx     1643 b- defN 23-May-15 16:11 dtlpy/assets/service_runners/__init__.py
 -rw-r--r--  2.0 unx     2819 b- defN 23-May-15 16:11 dtlpy/assets/service_runners/converter.py
 -rw-r--r--  2.0 unx     1200 b- defN 23-May-15 16:11 dtlpy/assets/service_runners/multi_method.py
 -rw-r--r--  2.0 unx     1548 b- defN 23-May-15 16:11 dtlpy/assets/service_runners/multi_method_annotation.py
 -rw-r--r--  2.0 unx     1592 b- defN 23-May-15 16:11 dtlpy/assets/service_runners/multi_method_dataset.py
@@ -43,73 +43,73 @@
 -rw-r--r--  2.0 unx      868 b- defN 23-May-15 16:11 dtlpy/dlp/__init__.py
 -rw-r--r--  2.0 unx    16037 b- defN 23-May-15 16:11 dtlpy/dlp/cli_utilities.py
 -rw-r--r--  2.0 unx    31529 b- defN 23-May-15 16:11 dtlpy/dlp/command_executor.py
 -rw-r--r--  2.0 unx       10 b- defN 23-May-15 16:11 dtlpy/dlp/dlp
 -rw-r--r--  2.0 unx       37 b- defN 23-May-15 16:11 dtlpy/dlp/dlp.bat
 -rw-r--r--  2.0 unx     4278 b- defN 23-May-15 16:11 dtlpy/dlp/dlp.py
 -rw-r--r--  2.0 unx    30551 b- defN 23-May-15 16:11 dtlpy/dlp/parser.py
--rw-r--r--  2.0 unx     4368 b- defN 23-May-23 16:53 dtlpy/entities/__init__.py
+-rw-r--r--  2.0 unx     4368 b- defN 23-Jun-14 08:42 dtlpy/entities/__init__.py
 -rw-r--r--  2.0 unx    11374 b- defN 23-May-15 16:11 dtlpy/entities/analytic.py
--rw-r--r--  2.0 unx    67242 b- defN 23-May-23 16:53 dtlpy/entities/annotation.py
--rw-r--r--  2.0 unx    29408 b- defN 23-May-15 16:11 dtlpy/entities/annotation_collection.py
--rw-r--r--  2.0 unx     4850 b- defN 23-May-15 16:11 dtlpy/entities/app.py
--rw-r--r--  2.0 unx     3645 b- defN 23-May-23 16:53 dtlpy/entities/app_module.py
+-rw-r--r--  2.0 unx    67242 b- defN 23-Jun-14 08:42 dtlpy/entities/annotation.py
+-rw-r--r--  2.0 unx    29408 b- defN 23-Jun-14 08:42 dtlpy/entities/annotation_collection.py
+-rw-r--r--  2.0 unx     4850 b- defN 23-Jun-14 08:42 dtlpy/entities/app.py
+-rw-r--r--  2.0 unx     3645 b- defN 23-Jun-14 08:42 dtlpy/entities/app_module.py
 -rw-r--r--  2.0 unx     5711 b- defN 23-May-15 16:11 dtlpy/entities/artifact.py
 -rw-r--r--  2.0 unx    14346 b- defN 23-May-15 16:11 dtlpy/entities/assignment.py
 -rw-r--r--  2.0 unx     7408 b- defN 23-May-15 16:11 dtlpy/entities/base_entity.py
 -rw-r--r--  2.0 unx     3819 b- defN 23-May-15 16:11 dtlpy/entities/bot.py
--rw-r--r--  2.0 unx     8999 b- defN 23-May-15 16:11 dtlpy/entities/codebase.py
--rw-r--r--  2.0 unx     4978 b- defN 23-May-15 16:11 dtlpy/entities/command.py
--rw-r--r--  2.0 unx    44074 b- defN 23-May-15 16:11 dtlpy/entities/dataset.py
+-rw-r--r--  2.0 unx     8999 b- defN 23-Jun-14 08:42 dtlpy/entities/codebase.py
+-rw-r--r--  2.0 unx     4978 b- defN 23-Jun-14 08:42 dtlpy/entities/command.py
+-rw-r--r--  2.0 unx    44528 b- defN 23-Jun-22 06:58 dtlpy/entities/dataset.py
 -rw-r--r--  2.0 unx     1186 b- defN 23-May-15 16:11 dtlpy/entities/directory_tree.py
--rw-r--r--  2.0 unx    12627 b- defN 23-May-23 16:53 dtlpy/entities/dpk.py
--rw-r--r--  2.0 unx     7197 b- defN 23-May-15 16:11 dtlpy/entities/driver.py
--rw-r--r--  2.0 unx    12377 b- defN 23-May-15 16:11 dtlpy/entities/execution.py
--rw-r--r--  2.0 unx     4192 b- defN 23-Jun-07 17:54 dtlpy/entities/feature.py
--rw-r--r--  2.0 unx     4587 b- defN 23-May-15 16:11 dtlpy/entities/feature_set.py
--rw-r--r--  2.0 unx    18485 b- defN 23-May-23 16:53 dtlpy/entities/filters.py
--rw-r--r--  2.0 unx     5366 b- defN 23-May-23 16:53 dtlpy/entities/integration.py
--rw-r--r--  2.0 unx    27922 b- defN 23-May-15 16:11 dtlpy/entities/item.py
+-rw-r--r--  2.0 unx    12634 b- defN 23-Jun-22 06:58 dtlpy/entities/dpk.py
+-rw-r--r--  2.0 unx     7197 b- defN 23-Jun-14 08:42 dtlpy/entities/driver.py
+-rw-r--r--  2.0 unx    12377 b- defN 23-Jun-14 08:42 dtlpy/entities/execution.py
+-rw-r--r--  2.0 unx     4192 b- defN 23-Jun-14 08:42 dtlpy/entities/feature.py
+-rw-r--r--  2.0 unx     4587 b- defN 23-Jun-14 08:42 dtlpy/entities/feature_set.py
+-rw-r--r--  2.0 unx    18574 b- defN 23-Jun-22 06:58 dtlpy/entities/filters.py
+-rw-r--r--  2.0 unx     5366 b- defN 23-Jun-14 08:42 dtlpy/entities/integration.py
+-rw-r--r--  2.0 unx    27922 b- defN 23-Jun-14 08:42 dtlpy/entities/item.py
 -rw-r--r--  2.0 unx     3869 b- defN 23-May-15 16:11 dtlpy/entities/label.py
 -rw-r--r--  2.0 unx     2516 b- defN 23-May-15 16:11 dtlpy/entities/links.py
--rw-r--r--  2.0 unx     5865 b- defN 23-May-23 16:53 dtlpy/entities/message.py
--rw-r--r--  2.0 unx    18385 b- defN 23-May-23 16:53 dtlpy/entities/model.py
--rw-r--r--  2.0 unx    36116 b- defN 23-Jun-07 17:54 dtlpy/entities/node.py
--rw-r--r--  2.0 unx    29275 b- defN 23-May-15 16:11 dtlpy/entities/ontology.py
--rw-r--r--  2.0 unx     9893 b- defN 23-May-15 16:11 dtlpy/entities/organization.py
--rw-r--r--  2.0 unx    26199 b- defN 23-Jun-07 17:54 dtlpy/entities/package.py
+-rw-r--r--  2.0 unx     5865 b- defN 23-Jun-14 08:42 dtlpy/entities/message.py
+-rw-r--r--  2.0 unx    19134 b- defN 23-Jun-22 06:58 dtlpy/entities/model.py
+-rw-r--r--  2.0 unx    36217 b- defN 23-Jun-22 06:58 dtlpy/entities/node.py
+-rw-r--r--  2.0 unx    29275 b- defN 23-Jun-14 08:42 dtlpy/entities/ontology.py
+-rw-r--r--  2.0 unx     9893 b- defN 23-Jun-14 08:42 dtlpy/entities/organization.py
+-rw-r--r--  2.0 unx    26199 b- defN 23-Jun-14 08:42 dtlpy/entities/package.py
 -rw-r--r--  2.0 unx      211 b- defN 23-May-15 16:11 dtlpy/entities/package_defaults.py
--rw-r--r--  2.0 unx     5917 b- defN 23-May-23 16:53 dtlpy/entities/package_function.py
+-rw-r--r--  2.0 unx     5917 b- defN 23-Jun-14 08:42 dtlpy/entities/package_function.py
 -rw-r--r--  2.0 unx     4035 b- defN 23-Jun-07 16:38 dtlpy/entities/package_module.py
 -rw-r--r--  2.0 unx     5694 b- defN 23-May-15 16:11 dtlpy/entities/package_slot.py
--rw-r--r--  2.0 unx     5848 b- defN 23-May-15 16:11 dtlpy/entities/paged_entities.py
--rw-r--r--  2.0 unx    19306 b- defN 23-Jun-07 17:54 dtlpy/entities/pipeline.py
--rw-r--r--  2.0 unx     7215 b- defN 23-May-15 16:11 dtlpy/entities/pipeline_execution.py
--rw-r--r--  2.0 unx    14301 b- defN 23-May-15 16:11 dtlpy/entities/project.py
--rw-r--r--  2.0 unx     9541 b- defN 23-May-15 16:11 dtlpy/entities/recipe.py
+-rw-r--r--  2.0 unx     5848 b- defN 23-Jun-14 08:42 dtlpy/entities/paged_entities.py
+-rw-r--r--  2.0 unx    19306 b- defN 23-Jun-14 08:42 dtlpy/entities/pipeline.py
+-rw-r--r--  2.0 unx     7215 b- defN 23-Jun-14 08:42 dtlpy/entities/pipeline_execution.py
+-rw-r--r--  2.0 unx    14301 b- defN 23-Jun-14 08:42 dtlpy/entities/project.py
+-rw-r--r--  2.0 unx     9541 b- defN 23-Jun-14 08:42 dtlpy/entities/recipe.py
 -rw-r--r--  2.0 unx     3273 b- defN 23-May-15 16:11 dtlpy/entities/reflect_dict.py
--rw-r--r--  2.0 unx     5033 b- defN 23-May-15 16:11 dtlpy/entities/resource_execution.py
--rw-r--r--  2.0 unx    27221 b- defN 23-May-15 16:11 dtlpy/entities/service.py
--rw-r--r--  2.0 unx     8467 b- defN 23-May-15 16:11 dtlpy/entities/setting.py
+-rw-r--r--  2.0 unx     5033 b- defN 23-Jun-14 08:42 dtlpy/entities/resource_execution.py
+-rw-r--r--  2.0 unx    27221 b- defN 23-Jun-14 08:42 dtlpy/entities/service.py
+-rw-r--r--  2.0 unx     8467 b- defN 23-Jun-14 08:42 dtlpy/entities/setting.py
 -rw-r--r--  2.0 unx     6067 b- defN 23-May-15 16:11 dtlpy/entities/similarity.py
--rw-r--r--  2.0 unx    18774 b- defN 23-May-15 16:11 dtlpy/entities/task.py
+-rw-r--r--  2.0 unx    18850 b- defN 23-Jun-22 06:58 dtlpy/entities/task.py
 -rw-r--r--  2.0 unx     4017 b- defN 23-May-15 16:11 dtlpy/entities/time_series.py
--rw-r--r--  2.0 unx    13984 b- defN 23-May-15 16:11 dtlpy/entities/trigger.py
+-rw-r--r--  2.0 unx    13984 b- defN 23-Jun-14 08:42 dtlpy/entities/trigger.py
 -rw-r--r--  2.0 unx     3865 b- defN 23-May-15 16:11 dtlpy/entities/user.py
--rw-r--r--  2.0 unx     3539 b- defN 23-May-15 16:11 dtlpy/entities/webhook.py
+-rw-r--r--  2.0 unx     3539 b- defN 23-Jun-14 08:42 dtlpy/entities/webhook.py
 -rw-r--r--  2.0 unx      572 b- defN 23-May-15 16:11 dtlpy/entities/annotation_definitions/__init__.py
 -rw-r--r--  2.0 unx     2201 b- defN 23-May-15 16:11 dtlpy/entities/annotation_definitions/base_annotation_definition.py
--rw-r--r--  2.0 unx     8622 b- defN 23-May-15 16:11 dtlpy/entities/annotation_definitions/box.py
+-rw-r--r--  2.0 unx     8622 b- defN 23-Jun-14 08:42 dtlpy/entities/annotation_definitions/box.py
 -rw-r--r--  2.0 unx     1559 b- defN 23-May-15 16:11 dtlpy/entities/annotation_definitions/classification.py
 -rw-r--r--  2.0 unx     1806 b- defN 23-May-15 16:11 dtlpy/entities/annotation_definitions/comparison.py
 -rw-r--r--  2.0 unx     8554 b- defN 23-May-15 16:11 dtlpy/entities/annotation_definitions/cube.py
 -rw-r--r--  2.0 unx     5805 b- defN 23-May-15 16:11 dtlpy/entities/annotation_definitions/cube_3d.py
 -rw-r--r--  2.0 unx      919 b- defN 23-May-15 16:11 dtlpy/entities/annotation_definitions/description.py
 -rw-r--r--  2.0 unx     4054 b- defN 23-May-15 16:11 dtlpy/entities/annotation_definitions/ellipse.py
--rw-r--r--  2.0 unx     4105 b- defN 23-May-15 16:11 dtlpy/entities/annotation_definitions/note.py
+-rw-r--r--  2.0 unx     4105 b- defN 23-Jun-14 08:42 dtlpy/entities/annotation_definitions/note.py
 -rw-r--r--  2.0 unx     3271 b- defN 23-May-15 16:11 dtlpy/entities/annotation_definitions/point.py
 -rw-r--r--  2.0 unx     6346 b- defN 23-May-15 16:11 dtlpy/entities/annotation_definitions/polygon.py
 -rw-r--r--  2.0 unx     3237 b- defN 23-May-15 16:11 dtlpy/entities/annotation_definitions/polyline.py
 -rw-r--r--  2.0 unx     2527 b- defN 23-May-15 16:11 dtlpy/entities/annotation_definitions/pose.py
 -rw-r--r--  2.0 unx     6475 b- defN 23-May-15 16:11 dtlpy/entities/annotation_definitions/segmentation.py
 -rw-r--r--  2.0 unx     1005 b- defN 23-May-15 16:11 dtlpy/entities/annotation_definitions/subtitle.py
 -rw-r--r--  2.0 unx     2559 b- defN 23-May-15 16:11 dtlpy/entities/annotation_definitions/text.py
@@ -136,96 +136,96 @@
 -rw-r--r--  2.0 unx      671 b- defN 23-May-15 16:11 dtlpy/examples/upload_batch_of_items.py
 -rw-r--r--  2.0 unx     2308 b- defN 23-May-15 16:11 dtlpy/examples/upload_items_and_custom_format_annotations.py
 -rw-r--r--  2.0 unx     1848 b- defN 23-May-15 16:11 dtlpy/examples/upload_items_with_modalities.py
 -rw-r--r--  2.0 unx     1388 b- defN 23-May-15 16:11 dtlpy/examples/upload_segmentation_annotations_from_mask_image.py
 -rw-r--r--  2.0 unx     2610 b- defN 23-May-15 16:11 dtlpy/examples/upload_yolo_format_annotations.py
 -rw-r--r--  2.0 unx      829 b- defN 23-May-15 16:11 dtlpy/miscellaneous/__init__.py
 -rw-r--r--  2.0 unx     3489 b- defN 23-May-15 16:11 dtlpy/miscellaneous/dict_differ.py
--rw-r--r--  2.0 unx     7971 b- defN 23-May-15 16:11 dtlpy/miscellaneous/git_utils.py
+-rw-r--r--  2.0 unx     7971 b- defN 23-Jun-14 08:42 dtlpy/miscellaneous/git_utils.py
 -rw-r--r--  2.0 unx      428 b- defN 23-May-15 16:11 dtlpy/miscellaneous/json_utils.py
 -rw-r--r--  2.0 unx     4808 b- defN 23-May-15 16:11 dtlpy/miscellaneous/list_print.py
--rw-r--r--  2.0 unx     5337 b- defN 23-Jun-07 17:54 dtlpy/miscellaneous/zipping.py
+-rw-r--r--  2.0 unx     5337 b- defN 23-Jun-14 08:42 dtlpy/miscellaneous/zipping.py
 -rw-r--r--  2.0 unx      799 b- defN 23-May-15 16:11 dtlpy/ml/__init__.py
--rw-r--r--  2.0 unx    35692 b- defN 23-Jun-07 17:54 dtlpy/ml/base_model_adapter.py
--rw-r--r--  2.0 unx    20037 b- defN 23-May-15 16:11 dtlpy/ml/metrics.py
--rw-r--r--  2.0 unx    12484 b- defN 23-May-15 16:11 dtlpy/ml/predictions_utils.py
+-rw-r--r--  2.0 unx    36060 b- defN 23-Jun-22 06:58 dtlpy/ml/base_model_adapter.py
+-rw-r--r--  2.0 unx    20037 b- defN 23-Jun-14 08:42 dtlpy/ml/metrics.py
+-rw-r--r--  2.0 unx    12484 b- defN 23-Jun-14 08:42 dtlpy/ml/predictions_utils.py
 -rw-r--r--  2.0 unx     2784 b- defN 23-May-15 16:11 dtlpy/ml/summary_writer.py
--rw-r--r--  2.0 unx     2444 b- defN 23-May-15 16:11 dtlpy/ml/train_utils.py
--rw-r--r--  2.0 unx     1883 b- defN 23-May-23 16:53 dtlpy/repositories/__init__.py
--rw-r--r--  2.0 unx     2966 b- defN 23-May-15 16:11 dtlpy/repositories/analytics.py
--rw-r--r--  2.0 unx    35175 b- defN 23-May-15 16:11 dtlpy/repositories/annotations.py
--rw-r--r--  2.0 unx    10299 b- defN 23-Jun-07 20:16 dtlpy/repositories/apps.py
--rw-r--r--  2.0 unx    19554 b- defN 23-Jun-07 17:54 dtlpy/repositories/artifacts.py
--rw-r--r--  2.0 unx    25410 b- defN 23-May-15 16:11 dtlpy/repositories/assignments.py
--rw-r--r--  2.0 unx     8195 b- defN 23-May-15 16:11 dtlpy/repositories/bots.py
--rw-r--r--  2.0 unx    25183 b- defN 23-May-15 16:11 dtlpy/repositories/codebases.py
--rw-r--r--  2.0 unx     5233 b- defN 23-May-15 16:11 dtlpy/repositories/commands.py
--rw-r--r--  2.0 unx    42482 b- defN 23-Jun-07 17:54 dtlpy/repositories/datasets.py
--rw-r--r--  2.0 unx    41101 b- defN 23-May-15 16:11 dtlpy/repositories/downloader.py
--rw-r--r--  2.0 unx    13793 b- defN 23-May-23 16:53 dtlpy/repositories/dpks.py
--rw-r--r--  2.0 unx    10284 b- defN 23-Jun-07 17:54 dtlpy/repositories/drivers.py
--rw-r--r--  2.0 unx    30254 b- defN 23-Jun-07 17:54 dtlpy/repositories/executions.py
--rw-r--r--  2.0 unx     7888 b- defN 23-May-15 16:11 dtlpy/repositories/feature_sets.py
--rw-r--r--  2.0 unx     9166 b- defN 23-Jun-07 17:54 dtlpy/repositories/features.py
--rw-r--r--  2.0 unx    11460 b- defN 23-May-23 16:53 dtlpy/repositories/integrations.py
--rw-r--r--  2.0 unx    37804 b- defN 23-May-15 16:11 dtlpy/repositories/items.py
--rw-r--r--  2.0 unx     3080 b- defN 23-May-23 16:53 dtlpy/repositories/messages.py
--rw-r--r--  2.0 unx    29021 b- defN 23-Jun-07 17:54 dtlpy/repositories/models.py
--rw-r--r--  2.0 unx     3061 b- defN 23-May-15 16:11 dtlpy/repositories/nodes.py
--rw-r--r--  2.0 unx    19525 b- defN 23-May-15 16:11 dtlpy/repositories/ontologies.py
--rw-r--r--  2.0 unx    22957 b- defN 23-Jun-07 17:54 dtlpy/repositories/organizations.py
--rw-r--r--  2.0 unx    86474 b- defN 23-May-23 16:53 dtlpy/repositories/packages.py
--rw-r--r--  2.0 unx    11873 b- defN 23-Jun-07 17:54 dtlpy/repositories/pipeline_executions.py
--rw-r--r--  2.0 unx    23316 b- defN 23-Jun-07 17:54 dtlpy/repositories/pipelines.py
--rw-r--r--  2.0 unx    22184 b- defN 23-Jun-07 17:54 dtlpy/repositories/projects.py
--rw-r--r--  2.0 unx    15703 b- defN 23-May-23 16:53 dtlpy/repositories/recipes.py
--rw-r--r--  2.0 unx     5374 b- defN 23-May-15 16:11 dtlpy/repositories/resource_executions.py
--rw-r--r--  2.0 unx    63949 b- defN 23-May-15 16:11 dtlpy/repositories/services.py
+-rw-r--r--  2.0 unx     2444 b- defN 23-Jun-14 08:42 dtlpy/ml/train_utils.py
+-rw-r--r--  2.0 unx     1883 b- defN 23-Jun-14 08:42 dtlpy/repositories/__init__.py
+-rw-r--r--  2.0 unx     2966 b- defN 23-Jun-14 08:42 dtlpy/repositories/analytics.py
+-rw-r--r--  2.0 unx    35352 b- defN 23-Jun-22 06:58 dtlpy/repositories/annotations.py
+-rw-r--r--  2.0 unx    10474 b- defN 23-Jun-22 06:58 dtlpy/repositories/apps.py
+-rw-r--r--  2.0 unx    19554 b- defN 23-Jun-14 08:42 dtlpy/repositories/artifacts.py
+-rw-r--r--  2.0 unx    25410 b- defN 23-Jun-14 08:42 dtlpy/repositories/assignments.py
+-rw-r--r--  2.0 unx     8195 b- defN 23-Jun-14 08:42 dtlpy/repositories/bots.py
+-rw-r--r--  2.0 unx    25183 b- defN 23-Jun-14 08:42 dtlpy/repositories/codebases.py
+-rw-r--r--  2.0 unx     5233 b- defN 23-Jun-14 08:42 dtlpy/repositories/commands.py
+-rw-r--r--  2.0 unx    42647 b- defN 23-Jun-22 06:58 dtlpy/repositories/datasets.py
+-rw-r--r--  2.0 unx    41300 b- defN 23-Jun-22 06:58 dtlpy/repositories/downloader.py
+-rw-r--r--  2.0 unx    13793 b- defN 23-Jun-14 08:42 dtlpy/repositories/dpks.py
+-rw-r--r--  2.0 unx    10284 b- defN 23-Jun-14 08:42 dtlpy/repositories/drivers.py
+-rw-r--r--  2.0 unx    30254 b- defN 23-Jun-14 08:42 dtlpy/repositories/executions.py
+-rw-r--r--  2.0 unx     7888 b- defN 23-Jun-14 08:42 dtlpy/repositories/feature_sets.py
+-rw-r--r--  2.0 unx     9343 b- defN 23-Jun-22 06:58 dtlpy/repositories/features.py
+-rw-r--r--  2.0 unx    11460 b- defN 23-Jun-14 08:42 dtlpy/repositories/integrations.py
+-rw-r--r--  2.0 unx    37937 b- defN 23-Jun-22 06:58 dtlpy/repositories/items.py
+-rw-r--r--  2.0 unx     3080 b- defN 23-Jun-14 08:42 dtlpy/repositories/messages.py
+-rw-r--r--  2.0 unx    30468 b- defN 23-Jun-22 06:58 dtlpy/repositories/models.py
+-rw-r--r--  2.0 unx     3061 b- defN 23-Jun-14 08:42 dtlpy/repositories/nodes.py
+-rw-r--r--  2.0 unx    19525 b- defN 23-Jun-14 08:42 dtlpy/repositories/ontologies.py
+-rw-r--r--  2.0 unx    22957 b- defN 23-Jun-14 08:42 dtlpy/repositories/organizations.py
+-rw-r--r--  2.0 unx    86474 b- defN 23-Jun-14 08:42 dtlpy/repositories/packages.py
+-rw-r--r--  2.0 unx    11873 b- defN 23-Jun-14 08:42 dtlpy/repositories/pipeline_executions.py
+-rw-r--r--  2.0 unx    23316 b- defN 23-Jun-14 08:42 dtlpy/repositories/pipelines.py
+-rw-r--r--  2.0 unx    22184 b- defN 23-Jun-14 08:42 dtlpy/repositories/projects.py
+-rw-r--r--  2.0 unx    15703 b- defN 23-Jun-14 08:42 dtlpy/repositories/recipes.py
+-rw-r--r--  2.0 unx     5374 b- defN 23-Jun-14 08:42 dtlpy/repositories/resource_executions.py
+-rw-r--r--  2.0 unx    63949 b- defN 23-Jun-14 08:42 dtlpy/repositories/services.py
 -rw-r--r--  2.0 unx    12296 b- defN 23-May-15 16:11 dtlpy/repositories/settings.py
--rw-r--r--  2.0 unx    46625 b- defN 23-Jun-07 17:54 dtlpy/repositories/tasks.py
--rw-r--r--  2.0 unx    11420 b- defN 23-May-15 16:11 dtlpy/repositories/times_series.py
--rw-r--r--  2.0 unx    21961 b- defN 23-May-15 16:11 dtlpy/repositories/triggers.py
+-rw-r--r--  2.0 unx    47401 b- defN 23-Jun-22 06:58 dtlpy/repositories/tasks.py
+-rw-r--r--  2.0 unx    11420 b- defN 23-Jun-14 08:42 dtlpy/repositories/times_series.py
+-rw-r--r--  2.0 unx    21961 b- defN 23-Jun-14 08:42 dtlpy/repositories/triggers.py
 -rw-r--r--  2.0 unx     9251 b- defN 23-May-15 16:11 dtlpy/repositories/upload_element.py
--rw-r--r--  2.0 unx    30682 b- defN 23-May-15 16:11 dtlpy/repositories/uploader.py
--rw-r--r--  2.0 unx     9033 b- defN 23-May-15 16:11 dtlpy/repositories/webhooks.py
--rw-r--r--  2.0 unx      904 b- defN 23-May-15 16:11 dtlpy/services/__init__.py
+-rw-r--r--  2.0 unx    30682 b- defN 23-Jun-14 08:42 dtlpy/repositories/uploader.py
+-rw-r--r--  2.0 unx     9033 b- defN 23-Jun-14 08:42 dtlpy/repositories/webhooks.py
+-rw-r--r--  2.0 unx      904 b- defN 23-Jun-14 08:42 dtlpy/services/__init__.py
 -rw-r--r--  2.0 unx     5022 b- defN 23-May-15 16:11 dtlpy/services/aihttp_retry.py
--rw-r--r--  2.0 unx    65015 b- defN 23-Jun-07 20:16 dtlpy/services/api_client.py
+-rw-r--r--  2.0 unx    65015 b- defN 23-Jun-21 11:27 dtlpy/services/api_client.py
 -rw-r--r--  2.0 unx     1515 b- defN 23-May-15 16:11 dtlpy/services/api_reference.py
 -rw-r--r--  2.0 unx     2810 b- defN 23-May-15 16:11 dtlpy/services/async_utils.py
 -rw-r--r--  2.0 unx     1036 b- defN 23-May-15 16:11 dtlpy/services/calls_counter.py
 -rw-r--r--  2.0 unx     3593 b- defN 23-May-15 16:11 dtlpy/services/check_sdk.py
 -rw-r--r--  2.0 unx     3694 b- defN 23-May-15 16:11 dtlpy/services/cookie.py
 -rw-r--r--  2.0 unx     6332 b- defN 23-May-15 16:11 dtlpy/services/create_logger.py
 -rw-r--r--  2.0 unx     3686 b- defN 23-May-15 16:11 dtlpy/services/events.py
 -rw-r--r--  2.0 unx     7906 b- defN 23-May-15 16:11 dtlpy/services/logins.py
 -rw-r--r--  2.0 unx     9122 b- defN 23-May-15 16:11 dtlpy/services/reporter.py
 -rw-r--r--  2.0 unx     3860 b- defN 23-May-15 16:11 dtlpy/services/service_defaults.py
 -rw-r--r--  2.0 unx      898 b- defN 23-May-15 16:11 dtlpy/utilities/__init__.py
 -rw-r--r--  2.0 unx     7514 b- defN 23-May-15 16:11 dtlpy/utilities/base_package_runner.py
--rw-r--r--  2.0 unx    73766 b- defN 23-May-15 16:11 dtlpy/utilities/converter.py
+-rw-r--r--  2.0 unx    73766 b- defN 23-Jun-14 08:42 dtlpy/utilities/converter.py
 -rw-r--r--  2.0 unx      728 b- defN 23-May-15 16:11 dtlpy/utilities/annotations/__init__.py
 -rw-r--r--  2.0 unx    10796 b- defN 23-May-15 16:11 dtlpy/utilities/annotations/annotation_converters.py
 -rw-r--r--  2.0 unx       65 b- defN 23-May-15 16:11 dtlpy/utilities/dataset_generators/__init__.py
--rw-r--r--  2.0 unx    31352 b- defN 23-May-15 16:11 dtlpy/utilities/dataset_generators/dataset_generator.py
+-rw-r--r--  2.0 unx    31352 b- defN 23-Jun-14 08:42 dtlpy/utilities/dataset_generators/dataset_generator.py
 -rw-r--r--  2.0 unx      716 b- defN 23-May-15 16:11 dtlpy/utilities/dataset_generators/dataset_generator_tensorflow.py
 -rw-r--r--  2.0 unx      536 b- defN 23-May-15 16:11 dtlpy/utilities/dataset_generators/dataset_generator_torch.py
 -rw-r--r--  2.0 unx       70 b- defN 23-May-15 16:11 dtlpy/utilities/local_development/__init__.py
 -rw-r--r--  2.0 unx     6451 b- defN 23-May-15 16:11 dtlpy/utilities/local_development/local_session.py
 -rw-r--r--  2.0 unx      124 b- defN 23-May-15 16:11 dtlpy/utilities/reports/__init__.py
 -rw-r--r--  2.0 unx     5927 b- defN 23-May-15 16:11 dtlpy/utilities/reports/figures.py
 -rw-r--r--  2.0 unx     2639 b- defN 23-May-15 16:11 dtlpy/utilities/reports/report.py
 -rw-r--r--  2.0 unx      734 b- defN 23-May-15 16:11 dtlpy/utilities/videos/__init__.py
 -rw-r--r--  2.0 unx    24072 b- defN 23-May-15 16:11 dtlpy/utilities/videos/video_player.py
 -rw-r--r--  2.0 unx    21875 b- defN 23-May-15 16:11 dtlpy/utilities/videos/videos.py
--rwxr-xr-x  2.0 unx       10 b- defN 23-May-15 16:11 dtlpy-1.78.17.data/scripts/dlp
--rwxr-xr-x  2.0 unx       37 b- defN 23-May-15 16:11 dtlpy-1.78.17.data/scripts/dlp.bat
--rwxr-xr-x  2.0 unx     4267 b- defN 23-Jun-07 20:17 dtlpy-1.78.17.data/scripts/dlp.py
+-rwxr-xr-x  2.0 unx       10 b- defN 23-May-15 16:11 dtlpy-1.79.14.data/scripts/dlp
+-rwxr-xr-x  2.0 unx       37 b- defN 23-May-15 16:11 dtlpy-1.79.14.data/scripts/dlp.bat
+-rwxr-xr-x  2.0 unx     4267 b- defN 23-Jun-22 08:47 dtlpy-1.79.14.data/scripts/dlp.py
 -rw-r--r--  2.0 unx        0 b- defN 23-May-15 16:11 tests/features/__init__.py
--rw-r--r--  2.0 unx     9395 b- defN 23-Jun-07 17:54 tests/features/environment.py
--rw-r--r--  2.0 unx    11356 b- defN 23-Jun-07 20:17 dtlpy-1.78.17.dist-info/LICENSE
--rw-r--r--  2.0 unx     3022 b- defN 23-Jun-07 20:17 dtlpy-1.78.17.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jun-07 20:17 dtlpy-1.78.17.dist-info/WHEEL
--rw-r--r--  2.0 unx       43 b- defN 23-Jun-07 20:17 dtlpy-1.78.17.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       12 b- defN 23-Jun-07 20:17 dtlpy-1.78.17.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    20660 b- defN 23-Jun-07 20:17 dtlpy-1.78.17.dist-info/RECORD
-229 files, 2158387 bytes uncompressed, 500004 bytes compressed:  76.8%
+-rw-r--r--  2.0 unx     9732 b- defN 23-Jun-22 06:58 tests/features/environment.py
+-rw-r--r--  2.0 unx    11356 b- defN 23-Jun-22 08:47 dtlpy-1.79.14.dist-info/LICENSE
+-rw-r--r--  2.0 unx     3022 b- defN 23-Jun-22 08:47 dtlpy-1.79.14.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jun-22 08:47 dtlpy-1.79.14.dist-info/WHEEL
+-rw-r--r--  2.0 unx       43 b- defN 23-Jun-22 08:47 dtlpy-1.79.14.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       12 b- defN 23-Jun-22 08:47 dtlpy-1.79.14.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    20660 b- defN 23-Jun-22 08:47 dtlpy-1.79.14.dist-info/RECORD
+229 files, 2163817 bytes uncompressed, 500934 bytes compressed:  76.9%
```

## zipnote {}

```diff
@@ -648,41 +648,41 @@
 
 Filename: dtlpy/utilities/videos/video_player.py
 Comment: 
 
 Filename: dtlpy/utilities/videos/videos.py
 Comment: 
 
-Filename: dtlpy-1.78.17.data/scripts/dlp
+Filename: dtlpy-1.79.14.data/scripts/dlp
 Comment: 
 
-Filename: dtlpy-1.78.17.data/scripts/dlp.bat
+Filename: dtlpy-1.79.14.data/scripts/dlp.bat
 Comment: 
 
-Filename: dtlpy-1.78.17.data/scripts/dlp.py
+Filename: dtlpy-1.79.14.data/scripts/dlp.py
 Comment: 
 
 Filename: tests/features/__init__.py
 Comment: 
 
 Filename: tests/features/environment.py
 Comment: 
 
-Filename: dtlpy-1.78.17.dist-info/LICENSE
+Filename: dtlpy-1.79.14.dist-info/LICENSE
 Comment: 
 
-Filename: dtlpy-1.78.17.dist-info/METADATA
+Filename: dtlpy-1.79.14.dist-info/METADATA
 Comment: 
 
-Filename: dtlpy-1.78.17.dist-info/WHEEL
+Filename: dtlpy-1.79.14.dist-info/WHEEL
 Comment: 
 
-Filename: dtlpy-1.78.17.dist-info/entry_points.txt
+Filename: dtlpy-1.79.14.dist-info/entry_points.txt
 Comment: 
 
-Filename: dtlpy-1.78.17.dist-info/top_level.txt
+Filename: dtlpy-1.79.14.dist-info/top_level.txt
 Comment: 
 
-Filename: dtlpy-1.78.17.dist-info/RECORD
+Filename: dtlpy-1.79.14.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## dtlpy/__version__.py

```diff
@@ -1 +1 @@
-version = '1.78.17'
+version = '1.79.14'
```

## dtlpy/entities/dataset.py

```diff
@@ -61,14 +61,16 @@
     export = attr.ib(repr=False)
     expiration_options = attr.ib()
     index_driver = attr.ib()
     enable_sync_with_cloned = attr.ib(repr=False)
 
     # name change when to_json
     created_at = attr.ib()
+    updated_at = attr.ib()
+    updated_by = attr.ib()
     items_url = attr.ib(repr=False)
     readable_type = attr.ib(repr=False)
     access_level = attr.ib(repr=False)
     driver = attr.ib(repr=False)
     src_dataset = attr.ib(repr=False)
     _readonly = attr.ib(repr=False)
 
@@ -149,14 +151,16 @@
         if expiration_options:
             expiration_options = ExpirationOptions.from_json(expiration_options)
         inst = cls(metadata=_json.get('metadata', None),
                    directoryTree=_json.get('directoryTree', None),
                    readable_type=_json.get('readableType', None),
                    access_level=_json.get('accessLevel', None),
                    created_at=_json.get('createdAt', None),
+                   updated_at=_json.get('updatedAt', None),
+                   updated_by=_json.get('updatedBy', None),
                    items_count=_json.get('itemsCount', None),
                    annotated=_json.get('annotated', None),
                    readonly=_json.get('readonly', None),
                    projects=projects,
                    creator=_json.get('creator', None),
                    items_url=_json.get('items', None),
                    export=_json.get('export', None),
@@ -190,24 +194,28 @@
                                                               attr.fields(Dataset)._labels,
                                                               attr.fields(Dataset)._recipe,
                                                               attr.fields(Dataset)._ontology,
                                                               attr.fields(Dataset)._directory_tree,
                                                               attr.fields(Dataset).access_level,
                                                               attr.fields(Dataset).readable_type,
                                                               attr.fields(Dataset).created_at,
+                                                              attr.fields(Dataset).updated_at,
+                                                              attr.fields(Dataset).updated_by,
                                                               attr.fields(Dataset).items_url,
                                                               attr.fields(Dataset).expiration_options,
                                                               attr.fields(Dataset).items_count,
                                                               attr.fields(Dataset).index_driver,
                                                               attr.fields(Dataset).enable_sync_with_cloned,
                                                               attr.fields(Dataset).src_dataset,
                                                               ))
         _json.update({'items': self.items_url})
         _json['readableType'] = self.readable_type
         _json['createdAt'] = self.created_at
+        _json['updatedAt'] = self.updated_at
+        _json['updatedBy'] = self.updated_by
         _json['accessLevel'] = self.access_level
         _json['readonly'] = self._readonly
         _json['itemsCount'] = self.items_count
         _json['indexDriver'] = self.index_driver
         if self.expiration_options and self.expiration_options.to_json():
             _json['expirationOptions'] = self.expiration_options.to_json()
         if self.enable_sync_with_cloned is not None:
```

## dtlpy/entities/dpk.py

```diff
@@ -141,15 +141,15 @@
     icon: str = entities.DlProperty(location=['icon'], _type=str)
     tags: list = entities.DlProperty(location=['tags'], _type=list)
     codebase: str = entities.DlProperty(location=['codebase'], _kls="Codebase")
     scope: dict = entities.DlProperty(location=['scope'], _type=dict)
 
     # defaults
     components: Components = entities.DlProperty(location=['components'], _kls='Components')
-    description: str = entities.DlProperty(location=['name'], _type=str)
+    description: str = entities.DlProperty(location=['description'], _type=str)
     url: str = entities.DlProperty(location=['url'], _type=str)
 
     # sdk
     client_api: ApiClient
     project: entities.Project
     _revisions = None
     __repositories = None
```

## dtlpy/entities/filters.py

```diff
@@ -81,15 +81,16 @@
             values=None,
             operator: FiltersOperations = None,
             method: FiltersMethod = None,
             custom_filter=None,
             resource: FiltersResource = FiltersResource.ITEM,
             use_defaults=True,
             context=None,
-            page_size=None
+            page_size=None,
+            user_query=True
     ):
         if page_size is None:
             if resource in [FiltersResource.EXECUTION, FiltersResource.PIPELINE_EXECUTION]:
                 page_size = 100
             else:
                 page_size = 1000
 
@@ -101,14 +102,15 @@
         self._resource = resource
         self.page = 0
         self.page_size = page_size
         self.method = FiltersMethod.AND
         self.sort = dict()
         self.join = None
         self.recursive = True
+        self.user_query = 'true' if user_query else 'false'
 
         # system only - task and assignment attributes
         self._ref_task = False
         self._ref_assignment = False
         self._ref_op = None
         self._ref_assignment_id = None
         self._ref_task_id = None
```

## dtlpy/entities/model.py

```diff
@@ -443,14 +443,28 @@
         Train the model in the cloud. This will create a service and will run the adapter's train function as an execution
 
         :param dict service_config : Service object as dict. Contains the spec of the default service to create.
         :return:
         """
         return self.models.train(model_id=self.id, service_config=service_config)
 
+    def evaluate(self, dataset_id, filters: entities.Filters = None, service_config=None):
+        """
+        Evaluate Model, provide data to evaluate the model on You can also provide specific config for the deployed service
+
+        :param dict service_config : Service object as dict. Contains the spec of the default service to create.
+        :param str dataset_id: ID of the dataset to evaluate
+        :param entities.Filters filters: dl.Filter entity to run the predictions on
+        :return:
+        """
+        return self.models.evaluate(model_id=self.id,
+                                    dataset_id=dataset_id,
+                                    filters=filters,
+                                    service_config=service_config)
+
     def predict(self, item_ids):
         """
         Run model prediction with items
 
         :param item_ids: a list of item id to run the prediction.
         :return:
         """
```

## dtlpy/entities/node.py

```diff
@@ -343,15 +343,15 @@
             target_port = node.inputs[0]
 
         if node.is_root():
             self._pipeline.set_start_node(self)
 
         source_connection = PipelineConnectionPort(node_id=self.node_id, port_id=source_port.port_id)
         target_connection = PipelineConnectionPort(node_id=node.node_id, port_id=target_port.port_id)
-        if action is None and source_port.actions is not None and source_port.actions is not []:
+        if action is None and source_port.actions is not None and source_port.actions != []:
             action = source_port.actions[0]
         connection = PipelineConnection(source=source_connection, target=target_connection, filters=filters,
                                         action=action)
         return connection
 
     def connect(self,
                 node,
@@ -877,15 +877,16 @@
         for single_input in function_io:
             pipeline_io.append(
                 PipelineNodeIO(port_id=str(uuid.uuid4()),
                                input_type=single_input.type,
                                name=single_input.name,
                                color=None,
                                display_name=single_input.name,
-                               default_value=single_input.value))
+                               default_value=single_input.value,
+                               actions=single_input.actions if single_input.actions is not None else []))
         return pipeline_io
 
     @staticmethod
     def from_json(_json: dict):
         parent = PipelineNode.from_json(_json)
         parent.__class__ = FunctionNode
         return parent
```

## dtlpy/entities/task.py

```diff
@@ -441,23 +441,23 @@
         """
         return self.tasks.remove_items(task=self,
                                        query=query,
                                        filters=filters,
                                        items=items,
                                        wait=wait)
 
-    def get_items(self, filters=None):
+    def get_items(self, filters=None, get_consensus_items: bool = False):
         """
         Get the task items
 
         :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
         :return: list of the items or PagedEntity output of items
         :rtype: list or dtlpy.entities.paged_entities.PagedEntities
         """
-        return self.tasks.get_items(task_id=self.id, dataset=self.dataset, filters=filters)
+        return self.tasks.get_items(task_id=self.id, dataset=self.dataset, filters=filters, get_consensus_items=get_consensus_items)
 
     def set_status(self, status: str, operation: str, item_ids: List[str]):
         """
         Update item status within task
 
         :param str status: string the describes the status
         :param str operation: the status action need 'create' or 'delete'
```

## dtlpy/ml/base_model_adapter.py

```diff
@@ -134,32 +134,38 @@
             Virtual method - need to implement
 
         :param batch: `np.ndarray`
         :return: `list[dl.AnnotationCollection]` each collection is per each image / item in the batch
         """
         raise NotImplementedError("Please implement 'predict' method in {}".format(self.__class__.__name__))
 
-    def evaluate(self, model: entities.Model, dataset: entities.Dataset):
+    def evaluate(self, model: entities.Model, dataset: entities.Dataset, filters: entities.Filters):
         """
         This function evaluates the model prediction on a dataset (with GT annotations).
         The evaluation process will upload the scores and metrics to the platform.
 
         :param model: The model to evaluate (annotation.metadata.system.model.name
         :param dataset: Dataset where the model predicted and uploaded its annotations
+        :param filters: Filters to query items on the dataset
         :return:
         """
         try:
             from dtlpymetrics.scoring import ScoringAndMetrics
         except (ImportError, ModuleNotFoundError):
-            logger.error('Import Error! Cant import dtlpymetrics. Need install the Dataloop Metrics App (from GitHub)')
-            raise
+            import subprocess
+            import sys
+            p_url = 'https://storage.googleapis.com/dtlpy/dtlpy-metrics/dtlpymetrics-latest-py3-none-any.whl'
+            subprocess.check_call([sys.executable, "-m", "pip", "install", p_url])
+            from dtlpymetrics.scoring import ScoringAndMetrics
         compare_types = model.output_type
         success, response = ScoringAndMetrics.create_model_score(model=model,
                                                                  dataset=dataset,
                                                                  compare_types=compare_types)
+        if not success:
+            raise ValueError(response)
 
     def convert_from_dtlpy(self, data_path, **kwargs):
         """ Convert Dataloop structure data to model structured
 
             Virtual method - need to implement
 
             e.g. take dlp dir structure and construct annotation file
@@ -310,43 +316,46 @@
     # ===============
     # SERVICE METHODS
     # ===============
 
     @entities.Package.decorators.function(display_name='Predict Items',
                                           inputs={'items': 'Item[]'},
                                           outputs={'items': 'Item[]', 'annotations': 'Annotation[]'})
-    def predict_items(self, items: list, upload_annotations=True, conf_threshold=0, batch_size=None, **kwargs):
+    def predict_items(self, items: list, upload_annotations=True, clean_annotations=True, batch_size=None, **kwargs):
         """
         Run the predict function on the input list of items (or single) and return the items and the predictions.
         Each prediction is by the model output type (package.output_type) and model_info in the metadata
 
         :param items: `List[dl.Item]` list of items to predict
         :param upload_annotations: `bool` uploads the predictions on the given items
-        :param conf_threshold: `float` returns and uploads annotation only above this threshold
+        :param clean_annotations: `bool` deletes previous model annotations (predictions) before uploading new ones
         :param batch_size: `int` size of batch to run a single inference
 
         :return: `List[dl.Item]`, `List[List[dl.Annotation]]`
         """
         if batch_size is None:
             batch_size = self.configuration.get('batch_size', 4)
         input_type = self.model_entity.input_type
         self.logger.debug(
             "Predicting {} items, using batch size {}. input type: {}".format(len(items), batch_size, input_type))
         pool = ThreadPoolExecutor(max_workers=16)
+
         annotations = list()
         for i_batch in tqdm.tqdm(range(0, len(items), batch_size), desc='predicting', unit='bt', leave=None):
             batch_items = items[i_batch: i_batch + batch_size]
             batch = list(pool.map(self.prepare_item_func, batch_items))
             batch_collections = self.predict(batch, **kwargs)
             if upload_annotations is True:
                 self.logger.debug(
                     "Uploading items' annotation for model {!r}.".format(self.model_entity.name))
                 try:
-                    batch_collections = list(pool.map(partial(self._upload_model_annotations),
-                                                      batch_items, batch_collections))
+                    batch_collections = list(pool.map(partial(self._upload_model_annotations,
+                                                              clean_annotations=clean_annotations),
+                                                      batch_items,
+                                                      batch_collections))
                 except Exception as err:
                     self.logger.exception("Failed to upload annotations items.")
 
             for collection in batch_collections:
                 # function needs to return `List[List[dl.Annotation]]`
                 # convert annotation collection to a list of dl.Annotation for each batch
                 if isinstance(collection, entities.AnnotationCollection):
@@ -418,15 +427,14 @@
             ##############
             # Set status #
             ##############
             model.status = 'training'
             if context is not None:
                 if 'system' not in model.metadata:
                     model.metadata['system'] = dict()
-                model.metadata['system']['trainExecutionId'] = context.execution_id
             model.update()
 
             ##########################
             # load model and weights #
             ##########################
             logger.info("Loading Adapter with: {n} ({i!r})".format(n=model.name, i=model.id))
             self.load_from_model(model_entity=model)
@@ -467,94 +475,92 @@
             # save also on fail
             if output_path is not None:
                 self.save_to_model(local_path=output_path, replace=True)
             logger.info('Execution failed. Setting model.status to failed')
             model.status = 'failed'
             model.update()
             raise
-        return model.id
+        return model
 
     @entities.Package.decorators.function(display_name='Evaluate a Model',
                                           inputs={'model': entities.Model,
-                                                  'dataset': entities.Dataset})
+                                                  'dataset': entities.Dataset,
+                                                  'filters': 'Json'})
     def evaluate_model(self,
                        model: entities.Model,
                        dataset: entities.Dataset,
-                       filters: entities.Filters,
+                       filters: entities.Filters = None,
                        #
                        progress: utilities.Progress = None,
                        context: utilities.Context = None):
         """
         Evaluate a model.
         data will be downloaded from the dataset and query
         configuration is as defined in dl.Model.configuration
         upload annotations and calculate metrics vs GT
-        """
-        try:
-            logger.info(
-                f"Received model: {model.id} for evaluation on dataset (name: {dataset.name}, id: {dataset.id}")
-            if context is not None:
-                if 'system' not in model.metadata:
-                    model.metadata['system'] = dict()
-                model.metadata['system']['evaluateExecutionId'] = context.execution_id
-            model.update()
-            ##########################
-            # load model and weights #
-            ##########################
-            logger.info(f"Loading Adapter with: {model.name} ({model.id!r})")
-            self.load_from_model(dataset=dataset,
-                                 model_entity=model)
 
-            ##############
-            # Predicting #
-            ##############
-            self.predict_dataset(dataset=dataset,
-                                 filters=filters,
-                                 with_upload=True)
-
-            ##############
-            # Evaluating #
-            ##############
-            if progress is not None:
-                progress.update(message='calculating metrics',
-                                progress=98)
-            self.evaluate(model=model, dataset=dataset)
-            #########
-            # Done! #
-            #########
-            if progress is not None:
-                progress.update(message='finishing evaluation',
-                                progress=99)
-
-        except Exception:
-            model.status = 'failed'
-            model.update()
-            raise
-        return self.model
+        :param model: Model entity to run prediction
+        :param dataset: Dataset to evaluate
+        :param filters: Filter for specific items from dataset
+        :param progress: dl.Progress for report FaaS progress
+        :param context:
+        :return:
+        """
+        logger.info(
+            f"Received model: {model.id} for evaluation on dataset (name: {dataset.name}, id: {dataset.id}")
+        ##########################
+        # load model and weights #
+        ##########################
+        logger.info(f"Loading Adapter with: {model.name} ({model.id!r})")
+        self.load_from_model(dataset=dataset,
+                             model_entity=model)
+
+        ##############
+        # Predicting #
+        ##############
+        logger.info(f"Calling prediction, dataset: {dataset.name!r} ({model.id!r}), filters: {filters}")
+        self.predict_dataset(dataset=dataset,
+                             filters=filters,
+                             with_upload=True)
+
+        ##############
+        # Evaluating #
+        ##############
+        logger.info(f"Starting adapter.evaluate()")
+        if progress is not None:
+            progress.update(message='calculating metrics',
+                            progress=98)
+        self.evaluate(model=model,
+                      dataset=dataset,
+                      filters=filters)
+        #########
+        # Done! #
+        #########
+        if progress is not None:
+            progress.update(message='finishing evaluation',
+                            progress=99)
+        return model
 
     # =============
     # INNER METHODS
     # =============
-
-    def _upload_model_annotations(self, item: entities.Item, predictions):
+    def _upload_model_annotations(self, item: entities.Item, predictions, clean_annotations):
         """
         Utility function that upload prediction to dlp platform based on the package.output_type
         :param predictions: `dl.AnnotationCollection`
         :param cleanup: `bool` if set removes existing predictions with the same package-model name
         """
         if not (isinstance(predictions, entities.AnnotationCollection) or isinstance(predictions, list)):
             raise TypeError('predictions was expected to be of type {}, but instead it is {}'.
                             format(entities.AnnotationCollection, type(predictions)))
-        model_info_name = "{}-{}".format(self.package_name, self.model_entity.name)
-        # if cleanup:
-        #     clean_filter = entities.Filters(field='type',
-        #                                     values=self.model_entity.output_type,
-        #                                     resource=entities.FiltersResource.ANNOTATION)
-        #     clean_filter.add(field='metadata.user.model.name', values=model_info_name)
-        #     item.annotations.delete(filters=clean_filter)
+        if clean_annotations:
+            clean_filter = entities.Filters(resource=entities.FiltersResource.ANNOTATION)
+            clean_filter.add(field='metadata.user.model.name', values=self.model_entity.name)
+            # clean_filter.add(field='type', values=self.model_entity.output_type,)
+            item.annotations.delete(filters=clean_filter)
         annotations = item.annotations.upload(annotations=predictions)
         return annotations
 
     @staticmethod
     def _item_to_image(item):
         """
         Preprocess items before cvalling the `predict` functions.
```

## dtlpy/repositories/annotations.py

```diff
@@ -129,15 +129,17 @@
 
         :return: json response
         :rtype: 
         """
         # prepare request
         success, response = self._client_api.gen_request(req_type="POST",
                                                          path="/datasets/{}/query".format(self._dataset_id),
-                                                         json_req=filters.prepare())
+                                                         json_req=filters.prepare(),
+                                                         headers={'user_query': filters.user_query}
+                                                         )
         if not success:
             raise exceptions.PlatformException(response)
         return response.json()
 
     @_api_reference.add(path='/datasets/{id}/query', method='post')
     def list(self, filters: entities.Filters = None, page_offset: int = None, page_size: int = None):
         """
@@ -161,15 +163,15 @@
                                          field='type',
                                          values='box'),
                       page_size=100,
                       page_offset=0)
         """
         if self._dataset_id is not None:
             if filters is None:
-                filters = entities.Filters(resource=entities.FiltersResource.ANNOTATION)
+                filters = entities.Filters(resource=entities.FiltersResource.ANNOTATION, user_query=False)
 
             if not filters.resource == entities.FiltersResource.ANNOTATION:
                 raise exceptions.PlatformException(error='400',
                                                    message='Filters resource must to be FiltersResource.ANNOTATION')
 
             if self._item is not None and not filters.has_field('itemId'):
                 filters = deepcopy(filters)
```

## dtlpy/repositories/apps.py

```diff
@@ -156,27 +156,30 @@
                                        page_offset=filters.page,
                                        page_size=filters.page_size,
                                        project_id=project_id,
                                        client_api=self._client_api)
         paged.get_page()
         return paged
 
-    def update(self, app: entities.App) -> bool:
+    def update(self, app: entities.App = None, app_id: str = None) -> bool:
         """
         Update the current app to the new configuration
 
         :param entities.App app: The app to update.
+        :param str app_id: The app id to update.
         :return bool whether the operation ran successfully or not
 
         **Example**
         .. code-block:: python
             succeed = dl.apps.update(app)
         """
+        if app_id is not None and app is None:
+            app = self.get(app_id=app_id)
         if app is None:
-            raise exceptions.PlatformException(error='400', message='You must provide app')
+            raise exceptions.PlatformException(error='400', message='You must provide app or app_id')
         success, response = self._client_api.gen_request(req_type='put',
                                                          path=f"/apps/{app.id}",
                                                          json_req=app.to_json())
         if success:
             return success
         raise exceptions.PlatformException(response)
```

## dtlpy/repositories/datasets.py

```diff
@@ -85,15 +85,15 @@
         elif len(datasets_by_name) > 1:
             raise Exception('Multiple datasets with this name exist')
         else:
             raise Exception("Dataset not found")
 
     def _bulid_folder_filter(self, folder_path, filters=None):
         if filters is None:
-            filters = entities.Filters()
+            filters = entities.Filters(user_query=False)
         if not folder_path.startswith('/'):
             folder_path = '/' + folder_path
         filters.add(field='dir', values=folder_path + '*')
         return filters
 
     @property
     def platform_url(self):
@@ -435,15 +435,15 @@
             dataset = project.datasets.clone(dataset_id='dataset_id',
                                   clone_name='dataset_clone_name',
                                   with_metadata=True,
                                   with_items_annotations=False,
                                   with_task_annotations_status=False)
         """
         if filters is None:
-            filters = entities.Filters()
+            filters = entities.Filters(user_query=False)
         elif not isinstance(filters, entities.Filters):
             raise exceptions.PlatformException(
                 error='400',
                 message='"filters" must be a dl.Filters entity. got: {!r}'.format(type(filters)))
 
         copy_filters = copy.deepcopy(filters)
         if copy_filters.has_field('hidden'):
@@ -456,15 +456,16 @@
                 "withItemsAnnotations": with_items_annotations,
                 "withMetadata": with_metadata,
                 "withTaskAnnotationsStatus": with_task_annotations_status
             }
         }
         success, response = self._client_api.gen_request(req_type='post',
                                                          path='/datasets/{}/clone'.format(dataset_id),
-                                                         json_req=payload)
+                                                         json_req=payload,
+                                                         headers={'user_query': filters.user_query})
 
         if not success:
             raise exceptions.PlatformException(response)
 
         command = entities.Command.from_json(_json=response.json(),
                                              client_api=self._client_api)
         command = command.wait()
@@ -790,15 +791,15 @@
                     "projects",
                     dataset.project.name,
                     "datasets",
                     dataset.name,
                 )
 
         if filters is None:
-            filters = entities.Filters()
+            filters = entities.Filters(user_query=False)
         if annotation_filters is not None:
             for annotation_filter_and in annotation_filters.and_filter_list:
                 filters.add_join(field=annotation_filter_and.field,
                                  values=annotation_filter_and.values,
                                  operator=annotation_filter_and.operator,
                                  method=entities.FiltersMethod.AND)
             for annotation_filter_or in annotation_filters.or_filter_list:
@@ -892,15 +893,15 @@
             project.datasets.upload_annotations(dataset='dataset_entity',
                                                  local_path='local_path',
                                                  clean=False,
                                                  export_version=dl.ExportVersion.V1
                                                  )
         """
         if filters is None:
-            filters = entities.Filters()
+            filters = entities.Filters(user_query=False)
         pages = dataset.items.list(filters=filters)
         total_items = pages.items_count
         pbar = tqdm.tqdm(total=total_items, disable=dataset._client_api.verbose.disable_progress_bar,
                          file=sys.stdout, desc='Upload Annotations')
         pool = self._client_api.thread_pools('annotation.upload')
         annotations_uploaded_count = 0
         for item in pages.all():
```

## dtlpy/repositories/downloader.py

```diff
@@ -108,23 +108,24 @@
                     message='Unknown items type to download. Expecting str or Item entities. Got "{}" instead'.format(
                         type(items[0])
                     )
                 )
             # create filters to download annotations
             filters = entities.Filters(field='id',
                                        values=[item.id for item in items],
-                                       operator=entities.FiltersOperations.IN)
+                                       operator=entities.FiltersOperations.IN,
+                                       user_query=False)
 
             # convert to list of list (like pages and page)
             items_to_download = [items]
             num_items = len(items)
         else:
             # filters
             if filters is None:
-                filters = entities.Filters()
+                filters = entities.Filters(user_query=False)
             # file types
             if file_types is not None:
                 filters.add(field='metadata.system.mimetype', values=file_types, operator=entities.FiltersOperations.IN)
             if annotation_filters is not None:
                 for annotation_filter_and in annotation_filters.and_filter_list:
                     filters.add_join(field=annotation_filter_and.field,
                                      values=annotation_filter_and.values,
@@ -132,15 +133,15 @@
                                      method=entities.FiltersMethod.AND)
                 for annotation_filter_or in annotation_filters.or_filter_list:
                     filters.add_join(field=annotation_filter_or.field,
                                      values=annotation_filter_or.values,
                                      operator=annotation_filter_or.operator,
                                      method=entities.FiltersMethod.OR)
             else:
-                annotation_filters = entities.Filters(resource=entities.FiltersResource.ANNOTATION)
+                annotation_filters = entities.Filters(resource=entities.FiltersResource.ANNOTATION, user_query=False)
 
             items_to_download = self.items_repository.list(filters=filters)
             num_items = items_to_download.items_count
 
         if num_items == 0:
             logger.warning('No items found! Nothing was downloaded')
             return list()
@@ -393,15 +394,16 @@
             payload['exportVersion'] = export_version
             if annotation_filters is not None:
                 payload['annotationsQuery'] = annotation_filters.prepare()
                 payload['annotations']['filter'] = filter_output_annotations
 
             success, response = dataset._client_api.gen_request(req_type='post',
                                                                 path='/datasets/{}/export'.format(dataset.id),
-                                                                json_req=payload)
+                                                                json_req=payload,
+                                                                headers={'user_query': filters.user_query})
             if not success:
                 raise exceptions.PlatformException(response)
             command = entities.Command.from_json(_json=response.json(),
                                                  client_api=dataset._client_api)
             command = command.wait(timeout=0)
             if 'outputItemId' not in command.spec:
                 raise exceptions.PlatformException(
```

## dtlpy/repositories/features.py

```diff
@@ -60,15 +60,17 @@
 
         :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
         :return: json response
         """
         # prepare request
         success, response = self._client_api.gen_request(req_type="POST",
                                                          path="{}/query".format(self.URL),
-                                                         json_req=filters.prepare())
+                                                         json_req=filters.prepare(),
+                                                         headers={'user_query': filters.user_query}
+                                                         )
         if not success:
             raise exceptions.PlatformException(response)
         return response.json()
 
     @_api_reference.add(path='/features/vectors', method='post')
     def list(self, filters: entities.Filters = None) -> entities.PagedEntities:
         """
@@ -76,15 +78,15 @@
 
         :param dtlpy.entities.filters.Filters filters: Filters to query the features data
         :return: Pages object
         :rtype: dtlpy.entities.paged_entities.PagedEntities
         """
         # default filters
         if filters is None:
-            filters = entities.Filters(resource=entities.FiltersResource.FEATURE)
+            filters = entities.Filters(resource=entities.FiltersResource.FEATURE, user_query=False)
         # assert type filters
         if not isinstance(filters, entities.Filters):
             raise exceptions.PlatformException(error='400',
                                                message='Unknown filters type: {!r}'.format(type(filters)))
         if filters.resource != entities.FiltersResource.FEATURE:
             raise exceptions.PlatformException(
                 error='400',
```

## dtlpy/repositories/items.py

```diff
@@ -106,15 +106,15 @@
 
         .. code-block:: python
 
             dataset.items.get_all_items()
 
         """
         if filters is None:
-            filters = entities.Filters()
+            filters = entities.Filters(user_query=False)
             filters.add(field='type', values='file')
         pages = self.list(filters=filters)
         num_items = pages.items_count
         items = [None for _ in range(num_items)]
         for i_item, item in enumerate(pages.all()):
             items[i_item] = item
         items = [item for item in items if item is not None]
@@ -144,15 +144,16 @@
 
         :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
         :return: json response
         """
         # prepare request
         success, response = self._client_api.gen_request(req_type="POST",
                                                          path="/datasets/{}/query".format(self.dataset.id),
-                                                         json_req=filters.prepare())
+                                                         json_req=filters.prepare(),
+                                                         headers={'user_query': filters.user_query})
         if not success:
             raise exceptions.PlatformException(response)
         return response.json()
 
     @_api_reference.add(path='/datasets/{id}/query', method='post')
     def list(self,
              filters: entities.Filters = None,
@@ -174,15 +175,15 @@
 
         .. code-block:: python
 
             dataset.items.list(page_offset=0, page_size=100)
         """
         # default filters
         if filters is None:
-            filters = entities.Filters()
+            filters = entities.Filters(user_query=False)
         # assert type filters
         elif not isinstance(filters, entities.Filters):
             raise exceptions.PlatformException(error='400',
                                                message='Unknown filters type: {!r}'.format(type(filters)))
         if filters.resource != entities.FiltersResource.ITEM and filters.resource != entities.FiltersResource.ANNOTATION:
             raise exceptions.PlatformException(
                 error='400',
```

## dtlpy/repositories/models.py

```diff
@@ -531,15 +531,42 @@
         :return:
         """
         payload = dict()
         if service_config is not None:
             payload['serviceConfig'] = service_config
         success, response = self._client_api.gen_request(req_type="post",
                                                          path=f"/ml/models/{model_id}/train",
-                                                         payload=payload)
+                                                         json_req=payload)
+        if not success:
+            raise exceptions.PlatformException(response)
+        return entities.Execution.from_json(_json=response.json(),
+                                            client_api=self._client_api,
+                                            project=self._project)
+
+    def evaluate(self, model_id: str, dataset_id: str, filters: entities.Filters = None, service_config=None):
+        """
+        Evaluate Model, provide data to evaluate the model on You can also provide specific config for the deployed service
+
+        :param str model_id: Model id to predict
+        :param dict service_config : Service object as dict. Contains the spec of the default service to create.
+        :param str dataset_id: ID of the dataset to evaluate
+        :param entities.Filters filters: dl.Filter entity to run the predictions on
+        :return:
+        """
+
+        payload = {'input': {'datasetId': dataset_id}}
+        if service_config is not None:
+            payload['config'] = {'serviceConfig': service_config}
+        if filters is None:
+            filters = entities.Filters()
+        if filters is not None:
+            payload['input']['datasetQuery'] = filters.prepare()
+        success, response = self._client_api.gen_request(req_type="post",
+                                                         path=f"/ml/models/{model_id}/evaluate",
+                                                         json_req=payload)
         if not success:
             raise exceptions.PlatformException(response)
         return entities.Execution.from_json(_json=response.json(),
                                             client_api=self._client_api,
                                             project=self._project)
 
     def predict(self, model, item_ids):
```

## dtlpy/repositories/tasks.py

```diff
@@ -959,49 +959,67 @@
             raise exceptions.PlatformException(response)
         return True
 
     def get_items(self,
                   task_id: str = None,
                   task_name: str = None,
                   dataset: entities.Dataset = None,
-                  filters: entities.Filters = None) -> entities.PagedEntities:
+                  filters: entities.Filters = None,
+                  get_consensus_items: bool = False,
+                  task: entities.Task = None
+                  ) -> entities.PagedEntities:
         """
         Get the task items to use in your code.
 
         **Prerequisites**: You must be in the role of an *owner*, *developer*, or *annotation manager* who has been assigned to be *owner* of the annotation task.
 
         If a filters param is provided, you will receive a PagedEntity output of the task items. If no filter is provided, you will receive a list of the items.
 
-        :param str task_id: the Id of the task
+        :param str task_id: the id of the task
         :param str task_name: the name of the task
+        :param bool get_consensus_items: get the items from the consensus assignment
+        :param dtlpy.entities.Task task: task object
         :param dtlpy.entities.dataset.Dataset dataset: dataset object that refer to the task
         :param dtlpy.entities.filters.Filters filters: Filters entity or a dictionary containing filters parameters
         :return: list of the items or PagedEntity output of items
         :rtype: list or dtlpy.entities.paged_entities.PagedEntities
 
         **Example**:
 
         .. code-block:: python
 
             dataset.tasks.get_items(task_id= 'task_id')
         """
-        if task_id is None and task_name is None:
+        if task is None and task_id is None and task_name is None:
             raise exceptions.PlatformException('400', 'Please provide either task_id or task_name')
+
         if task_id is None:
-            task_id = self.get(task_name=task_name).id
+            if task is None:
+                task = self.get(task_name=task_name)
+            task_id = task.id
 
         if dataset is None and self._dataset is None:
             raise exceptions.PlatformException('400', 'Please provide a dataset entity')
         if dataset is None:
             dataset = self._dataset
 
         if filters is None:
             filters = entities.Filters(use_defaults=False)
         filters.add(field='metadata.system.refs.id', values=[task_id], operator=entities.FiltersOperations.IN)
 
+        if not get_consensus_items:
+            if task is None:
+                task = self.get(task_id=task_id)
+            if task.metadata.get('system', dict()).get('consensusAssignmentId', None):
+                filters.add(
+                    field='metadata.system.refs.id',
+                    values=task.metadata['system']['consensusAssignmentId'],
+                    operator=entities.FiltersOperations.NOT_EQUAL
+                )
+
         return dataset.items.list(filters=filters)
 
     def set_status(self, status: str, operation: str, task_id: str, item_ids: List[str]):
         """
         Update an item status within a task.
 
         **Prerequisites**: You must be in the role of an *owner*, *developer*, or *annotation manager* who has been assigned to be *owner* of the annotation task.
```

## tests/features/environment.py

```diff
@@ -17,14 +17,26 @@
 def after_feature(context, feature):
     if hasattr(feature, 'bot'):
         try:
             feature.bot.delete()
         except Exception:
             logging.exception('Failed to delete bot')
 
+    if hasattr(feature, 'app'):
+        try:
+            context.feature.app.uninstall()
+        except Exception:
+            logging.exception('Failed to delete dpk')
+
+    if hasattr(feature, 'dpk'):
+        try:
+            context.feature.dpk.delete()
+        except Exception:
+            logging.exception('Failed to delete dpk')
+
     if hasattr(feature, 'dataloop_feature_integration'):
         all_deleted = True
         time.sleep(7)  # Wait for drivers to delete
         for integration_id in feature.to_delete_integrations_ids:
             try:
                 feature.dataloop_feature_project.integrations.delete(integrations_id=integration_id, sure=True, really=True)
             except feature.dataloop_feature_dl.exceptions.NotFound:
```

## Comparing `dtlpy-1.78.17.data/scripts/dlp.py` & `dtlpy-1.79.14.data/scripts/dlp.py`

 * *Files identical despite different names*

## Comparing `dtlpy-1.78.17.dist-info/LICENSE` & `dtlpy-1.79.14.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `dtlpy-1.78.17.dist-info/METADATA` & `dtlpy-1.79.14.dist-info/METADATA`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: dtlpy
-Version: 1.78.17
+Version: 1.79.14
 Summary: SDK and CLI for Dataloop platform
 Home-page: https://github.com/dataloop-ai/dtlpy
 Author: Dataloop Team
 Author-email: info@dataloop.ai
 License: Apache License 2.0
 Classifier: Programming Language :: Python
 Classifier: Programming Language :: Python :: 3
```

## Comparing `dtlpy-1.78.17.dist-info/RECORD` & `dtlpy-1.79.14.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 dtlpy/__init__.py,sha256=rcq_EJ5f43HoNbDkt9No2t6OBTbVVkrE6aGhUTK3K34,19847
-dtlpy/__version__.py,sha256=i19AaQ9j8AcTWpkaxaeEl-gd3R5t0NBwh32tCinWk3c,20
+dtlpy/__version__.py,sha256=LH80zp9AHPb2_nicz74-RDrXNmmdFF21pXbYBTWAiSI,20
 dtlpy/exceptions.py,sha256=EQCKs3pwhwZhgMByQN3D3LpWpdxwcKPEEt-bIaDwURM,2871
 dtlpy/new_instance.py,sha256=6dljwr1Zo25ng57PMn8j0jjRD1hJ5LwlvBTBNmyc3Qw,5654
 dtlpy/assets/__init__.py,sha256=D_hAa6NM8Zoy32sF_9b7m0b7I-BQEyBFg8-9Tg2WOeo,976
 dtlpy/assets/lock_open.png,sha256=vXHune4YF__fINPQ2l61G2zI3BeJPX_z5gkwzUNFAxs,24081
 dtlpy/assets/main.py,sha256=N1JUsx79qnXI7Hx22C8JOzHJdGHxvrXeTx5UZAxvJfE,1380
 dtlpy/assets/main_partial.py,sha256=d8Be4Whg9Tb2VFiT85-57_L9IvxRipQXiZ83SxFs0Ro,267
 dtlpy/assets/mock.json,sha256=aByh4XlsFQJM2pOjmd7bd9zT1LSOj5pfutZDHwt8c_8,149
@@ -54,29 +54,29 @@
 dtlpy/entities/app_module.py,sha256=0UiAbBX1q8iEImi3nY7ySWZZHoRRwu0qUXmyXmgVAc4,3645
 dtlpy/entities/artifact.py,sha256=wtLtBuidOPbnba0ok40JyunCCIBGbAl4bP_ebK39Kk4,5711
 dtlpy/entities/assignment.py,sha256=LTUxE2ZB5iZSbSeoroHe5P7Kv6E_K2b5cdY-7TZ4HFE,14346
 dtlpy/entities/base_entity.py,sha256=nGdn9e28ZN1ZqiZXVzxQxpm1qeNvf-K_qVoq5OshyN4,7408
 dtlpy/entities/bot.py,sha256=is3NUCnPg56HSjsHIvFcVkymValMqDV0uHRDC1Ib-ds,3819
 dtlpy/entities/codebase.py,sha256=pwRkAq2GV0wvmzshg89IAmE-0I2Wsy_-QNOu8OV8uqc,8999
 dtlpy/entities/command.py,sha256=SJNGDonOHILhDyJlixt0ZYJngJBTCANjyGmLfkOfjVI,4978
-dtlpy/entities/dataset.py,sha256=QxXKeVUgBa7oYwSkeevLc4E7p1pRKZRJ4fLpeH_rhAg,44074
+dtlpy/entities/dataset.py,sha256=kK925-nf34G3kMGj5SpVqMGce4vHCtHnGHomgS00E-c,44528
 dtlpy/entities/directory_tree.py,sha256=Rni6pLSWytR6yeUPgEdCCRfTg_cqLOdUc9uCqz9KT-Q,1186
-dtlpy/entities/dpk.py,sha256=WgGkABFxSL9Y9pBxA9i0pD4xaI372gU6pbUEUzmNkIs,12627
+dtlpy/entities/dpk.py,sha256=vfrFZh5BdyNHobndzBgAPgx0ntYbcKOol2Re7L_N4_U,12634
 dtlpy/entities/driver.py,sha256=O_QdK1EaLjQyQkmvKsmkNgmvmMb1mPjKnJGxK43KrOA,7197
 dtlpy/entities/execution.py,sha256=e_az-Pln_ZlLsxmVsx3t4Kc034j7MniIm1o1cSO-4Ew,12377
 dtlpy/entities/feature.py,sha256=D7_Kki6oh4tyseg7ufSSyvchwgF4a8cH2vsLHlcv3zA,4192
 dtlpy/entities/feature_set.py,sha256=-WtAIzQBOYd1yBjqEfvm0bHlovdqDdfBitAjg2XLszs,4587
-dtlpy/entities/filters.py,sha256=dGDjLTulqPoqEdx3moCxYMPkBz1ocYLv2HmA0PUP2rA,18485
+dtlpy/entities/filters.py,sha256=3zG7fRBi_7uqvZFmUMwP6zTYHSakKXdLAGl-QjE4ApA,18574
 dtlpy/entities/integration.py,sha256=qXN9cTp4j8uCODOEu8DZ1It-6joRSxIsgucZcCsrPQs,5366
 dtlpy/entities/item.py,sha256=AnNfe5ZQURq5YiougoQRsVxTDkU-jSplzBKWC10mEtw,27922
 dtlpy/entities/label.py,sha256=ycDYavIgKhz806plIX-64c07_TeHpDa-V7LnfFVe4Rg,3869
 dtlpy/entities/links.py,sha256=FAmEwHtsrqKet3c0UHH9u_gHgG6_OwF1-rl4xK7guME,2516
 dtlpy/entities/message.py,sha256=ApJuaKEqxATpXjNYUjGdYPu3ibQzEMo8-LtJ_4xAcPI,5865
-dtlpy/entities/model.py,sha256=US7omMZktHFdSC3KYKwWieY6V6LsZQNURYWCdMTLHiI,18385
-dtlpy/entities/node.py,sha256=-cZPaFHwbU4Cuwd5PYMiskrP4HJncXM51llyNjW9en8,36116
+dtlpy/entities/model.py,sha256=-RkkiJE629d15895_GVnftFWEgOdvBOmgmy-VeSohXY,19134
+dtlpy/entities/node.py,sha256=lrgslUdl9qdaMC-k3c8oTgmR_oA0bymV0-k9gxHw3BQ,36217
 dtlpy/entities/ontology.py,sha256=v1gaVSWwsz-2-HqChPInLBVavcb65NOjDTxixwOXVlw,29275
 dtlpy/entities/organization.py,sha256=AMkx8hNIIIjnu5pYlNjckMRuKt6H3lnOAqtEynkr7wg,9893
 dtlpy/entities/package.py,sha256=jQ2gzaqc8UTLOJ90Hj4jCIPBu59Joo-e2GDY443Syxw,26199
 dtlpy/entities/package_defaults.py,sha256=wTD7Z7rGYjVy8AcUxTFEnkOkviiJaLVZYvduiUBKNZo,211
 dtlpy/entities/package_function.py,sha256=AdXMw5e5a7TT9oaxPqfu2ERDgYTmH_sm6GgpGDu67b8,5917
 dtlpy/entities/package_module.py,sha256=MBaJ5j8eCERsP-s1SIO8_daTU1gEqcaDSpUBu_gUTAk,4035
 dtlpy/entities/package_slot.py,sha256=0dkTUN1wfjyAkOtdoFa6vGZBuZ_UoenpePaSFX9HOYA,5694
@@ -86,15 +86,15 @@
 dtlpy/entities/project.py,sha256=FCGKA-pV-AOaeD5b2S6jEX-TAZSgVM2N-vTSEn0Av-k,14301
 dtlpy/entities/recipe.py,sha256=RzevJdJAeH65Xm0bL2_5rzST_4UodSewUBT-Wj3_UU0,9541
 dtlpy/entities/reflect_dict.py,sha256=2NaSAL-CO0T0FYRYFQlaSpbsoLT2Q18AqdHgQSLX5Y4,3273
 dtlpy/entities/resource_execution.py,sha256=1HuVV__U4jAUOtOkWlWImnM3Yts8qxMSAkMA9sBhArY,5033
 dtlpy/entities/service.py,sha256=AcyTskHUYUvhcUBWTkI8YLzW9UknYFgQQz37VtDxKK4,27221
 dtlpy/entities/setting.py,sha256=lnEyXKgSgpwQDNZAiOs27gguwp7oaXc8XjFAkM4ajpE,8467
 dtlpy/entities/similarity.py,sha256=9GLvkYUK23PLFTGX5AaLmFjVqNZkRKNgJvl7b0CwONE,6067
-dtlpy/entities/task.py,sha256=6sKszmn-lc9-qlj_GQSh-rFDKMeF0GqQR54_96v8n8c,18774
+dtlpy/entities/task.py,sha256=FbBiMJa_za4p6Nkv8VSDbqJC9E41LC6GD65MZ7SRHfc,18850
 dtlpy/entities/time_series.py,sha256=336jWNckjuSn0G29WJFetB7nBoFAKqs4VH9_IB4m4FE,4017
 dtlpy/entities/trigger.py,sha256=3IFPl9hda3MhUiiLsp5nnv7g2y-j9657Xm23SSM98Eg,13984
 dtlpy/entities/user.py,sha256=hqEzwN6rl1oUTpKOV5eXvw9Z7dtpsiC4TAPSNBmkqcM,3865
 dtlpy/entities/webhook.py,sha256=6R06MgLxabvKySInGlSJmaf0AVmAMe3vKusWhqONRyU,3539
 dtlpy/entities/annotation_definitions/__init__.py,sha256=XvormaStOOLPomZNKLUqT-3mHKHFYx23PVxrJUOtAY0,572
 dtlpy/entities/annotation_definitions/base_annotation_definition.py,sha256=Xm6HMLKq-3nQRqg8VR5HMDa_YMEdIvD7WpEWNrtsl_I,2201
 dtlpy/entities/annotation_definitions/box.py,sha256=kNT_Ba7QWKBiyt1uPAmYLyBfPsxvIUNLhVe9042WFnM,8622
@@ -140,51 +140,51 @@
 dtlpy/miscellaneous/__init__.py,sha256=twbvfsKdiNHNR-vUuy8nUlY3vuUVaSnm-wO83yQdeFY,829
 dtlpy/miscellaneous/dict_differ.py,sha256=POJbKR0YyWPf5gFADFpIaNFj9gt2aVBTNof7GJNxTCw,3489
 dtlpy/miscellaneous/git_utils.py,sha256=CT_CCDsqDqu_bY3cLcOSU6k3Zr6w40t8GJULLUtAJ_U,7971
 dtlpy/miscellaneous/json_utils.py,sha256=0P4YTlL6o_L7AUrvAeqkqA46MZZK_hDdTrdnmI59y6g,428
 dtlpy/miscellaneous/list_print.py,sha256=leEg3RodgYfH5t_0JG8VuM8NiesR8sJLK_mRSttL5pY,4808
 dtlpy/miscellaneous/zipping.py,sha256=GMdPhAeHQXeMS5ClaiKWMJWVYQLBLAaJUWxvdYrL4Ro,5337
 dtlpy/ml/__init__.py,sha256=vPkyXpc9kcWWZ_PxyPEOsjKBJdEbowLkZr8FZIb_OBM,799
-dtlpy/ml/base_model_adapter.py,sha256=qtYdi_QuTulJI3dSRwLPj1AMQiiwoP0hFFTpzaP4e0M,35692
+dtlpy/ml/base_model_adapter.py,sha256=s5CUgcJTdh1j1xmgBbncpzu_SFXOoWwsdpw9HOPUaxE,36060
 dtlpy/ml/metrics.py,sha256=BG2E-1Mvjv2e2No9mIJKVmvzqBvLqytKcw3hA7wVUNc,20037
 dtlpy/ml/predictions_utils.py,sha256=He_84U14oS2Ss7T_-Zj5GDiBZwS-GjMPURUh7u7DjF8,12484
 dtlpy/ml/summary_writer.py,sha256=dehDi8zmGC1sAGyy_3cpSWGXoGQSiQd7bL_Thoo8yIs,2784
 dtlpy/ml/train_utils.py,sha256=avvT_TbwJ0Q23mwwHRf0cu6Wt4LU72plEb_lx5oxc1U,2444
 dtlpy/repositories/__init__.py,sha256=tUw86r7hI6f4BSas7_csOCfEso4ABUcdzx8WfR4ipw4,1883
 dtlpy/repositories/analytics.py,sha256=dQPCYTPAIuyfVI_ppR49W7_GBj0033feIm9Gd7LW1V0,2966
-dtlpy/repositories/annotations.py,sha256=aDRphlzfjJbDfoq3gnJIZDn2Eyk1Eptg24ZlGMnkDNU,35175
-dtlpy/repositories/apps.py,sha256=eU43v1CirGir6EnZBoesihkcm95JOzke_P8cFLUAwi4,10299
+dtlpy/repositories/annotations.py,sha256=ESletrU7weEVH7jo678XXxX937hOVg7EQw8acBumHgs,35352
+dtlpy/repositories/apps.py,sha256=tImv7jCuE8dM1NCmO99L-p28nXxEBuKgrbWZJrdeISw,10474
 dtlpy/repositories/artifacts.py,sha256=TsZe2OY_sBhQKUzLBLuz37QrfNi1uMAOQGSlePWBlIU,19554
 dtlpy/repositories/assignments.py,sha256=L5pxRfmiZxHMWPTvGFds4mV9lAdDU_ZzXoTuyJCmMgc,25410
 dtlpy/repositories/bots.py,sha256=x1uOCl-wti83zFZii727-MwufRoChYeuRkOsRyixfPk,8195
 dtlpy/repositories/codebases.py,sha256=fEO9v1nElDZedFfFvSch0Jmby-WF2ZZcXAW9pRJHXo0,25183
 dtlpy/repositories/commands.py,sha256=nfk-KFYArfekyFMsnJwj6IgT7VawVcJQs6GGO8q26_U,5233
-dtlpy/repositories/datasets.py,sha256=1l6IswUtyZ8f9lwLUSrArj_QMVt6hYH-6JR670Zm3is,42482
-dtlpy/repositories/downloader.py,sha256=JpTtDl6acbWDPx469sFg1fhr6VhmJmBuSHxEe-YkJ0U,41101
+dtlpy/repositories/datasets.py,sha256=-XHHbHL0aJywyymephp5U4bb0r_kM3kzGQwp14urMDM,42647
+dtlpy/repositories/downloader.py,sha256=tnvBT1zd2DA1y_UQdRYu1mQAEhCF6JRTCilL08AxYaY,41300
 dtlpy/repositories/dpks.py,sha256=4Cw-avBe1HmVP5I-YSeJOhbRBGcxYUZHD7e-S87DeQI,13793
 dtlpy/repositories/drivers.py,sha256=A7M6QDd5jHf_3KsOaQrJKaK5WhlHZoB5hYEBnJdKsKk,10284
 dtlpy/repositories/executions.py,sha256=xxDC1I8sGD4LmZ87XcNe8gnZSgdDfPVjjJzWAhc92BE,30254
 dtlpy/repositories/feature_sets.py,sha256=o1D40Z1plQmTd0FK8kEic4cTiv82Q8lSSP6TFG3q0C0,7888
-dtlpy/repositories/features.py,sha256=NwjarF3q3HMHTLrxLRv28E9_T3mkulhWVUVeaE2B67k,9166
+dtlpy/repositories/features.py,sha256=Pa6uousXypJQen6FqgXIe03Q6NMEVPeMrw43PP4_Mlw,9343
 dtlpy/repositories/integrations.py,sha256=hufdGFisdgwCekY-gTuST8WTKkR59LRSCnRxdYphkA4,11460
-dtlpy/repositories/items.py,sha256=60MIcLPOZ4WWIEFkvClwky3xClP-xJxCdPavfYO02Jg,37804
+dtlpy/repositories/items.py,sha256=Pn_3s6Svji55t8RTd1lOLNfQErFLQoIk8ngQk7W-PMY,37937
 dtlpy/repositories/messages.py,sha256=CAYQgqwvIZnHU_HVgLhNkwnMCAi4sKsvT7FsSPGnd28,3080
-dtlpy/repositories/models.py,sha256=7hxM6kBb-Jp2vetuqek3Lk2CHceN7hPYmiCFwGMj2Jw,29021
+dtlpy/repositories/models.py,sha256=QoRv5I_G9P-PZFWb0FmkXNWkjgNsOwiWzYNPX0nplLM,30468
 dtlpy/repositories/nodes.py,sha256=xXJm_YA0vDUn0dVvaGeq6ORM0vI3YXvfjuylvGRtkxo,3061
 dtlpy/repositories/ontologies.py,sha256=3Xy30zB6BgdayCdGtNtj1fmhrrVD1WJXoTTeYc0iwQ4,19525
 dtlpy/repositories/organizations.py,sha256=5-l9MXKGm3-gYkaK_-KbJsGuZ98Kf5uV95BWXowRZuI,22957
 dtlpy/repositories/packages.py,sha256=QJuQIngoy43z5brTRJKPg1eyKU7TGmAr7Zcop_KZyiE,86474
 dtlpy/repositories/pipeline_executions.py,sha256=8QepaHRFrGZ4gN0VVsJAp96Ot8Z6OMvWryOswC7L6vQ,11873
 dtlpy/repositories/pipelines.py,sha256=gHPYXHZhVtwtKFeRFd_BNt25B-SmLk29ttWZ_TvlqWQ,23316
 dtlpy/repositories/projects.py,sha256=wO0IKj58WiDzOr5TjkA5rgsmXKZVGCx_klrNuwKQJoo,22184
 dtlpy/repositories/recipes.py,sha256=Jxzt9xOELDepRvxXpvjpuiqHyRSrvTqsoHxhnkWoigQ,15703
 dtlpy/repositories/resource_executions.py,sha256=PyzsbdJxz6jf17Gx13GZmqdu6tZo3TTVv-DypnJ_sY0,5374
 dtlpy/repositories/services.py,sha256=qeb9vt_SDHIyUICablPJOQE_wQABK5Un8Dm5IbnUmEc,63949
 dtlpy/repositories/settings.py,sha256=pvqNse0ANCdU3NSLJEzHco-PZq__OIsPSPVJveB9E4I,12296
-dtlpy/repositories/tasks.py,sha256=4QxWeRqdS9pzVlQuziAIXlKRUIMHkVWIKI91AP9lTEQ,46625
+dtlpy/repositories/tasks.py,sha256=SDMLKHvYQsjgYwXOo_Z8gTdw5Hnj3mcm9ogHOBK68g4,47401
 dtlpy/repositories/times_series.py,sha256=m-bKFEgiZ13yQNelDjBfeXMUy_HgsPD_JAHj1GVx9fU,11420
 dtlpy/repositories/triggers.py,sha256=fvClYSHLO6YcMq9MCaQZXOBfkYzmMS5R7pbDTXHCtPo,21961
 dtlpy/repositories/upload_element.py,sha256=nauOOTniDzawUTbpcZzOqfqDTRVcOMTQJJNHR1Cj7LQ,9251
 dtlpy/repositories/uploader.py,sha256=jeIX0i0oQxB04kyYSfpxhTgazTM-c-2QRUvP9g03Xw0,30682
 dtlpy/repositories/webhooks.py,sha256=IIpxOJ-7KeQp1TY9aJZz-FuycSjAoYx0TDk8z86KAK8,9033
 dtlpy/services/__init__.py,sha256=VfVJy2otIrDra6i7Sepjyez2ujiE6171ChQZp-YgxsM,904
 dtlpy/services/aihttp_retry.py,sha256=tgntZsAY0dW9v08rkjX1T5BLNDdDd8svtgn7nH8DSGU,5022
@@ -212,18 +212,18 @@
 dtlpy/utilities/local_development/local_session.py,sha256=4rcu5T9wD8hJxQNBsY6_Fapp0dBWNozVXjVuVGTeb-s,6451
 dtlpy/utilities/reports/__init__.py,sha256=e4eFLMchZSuo_q593NLAVoSdZ9KECf5W0nQ3TdIpWSg,124
 dtlpy/utilities/reports/figures.py,sha256=FP60Ha0qwyYk24CFa9nVHG2iqDUQ8oAvq-TyuPX1CqM,5927
 dtlpy/utilities/reports/report.py,sha256=3nEsNnIWmdPEsd21nN8vMMgaZVcPKn9iawKTTeOQg2A,2639
 dtlpy/utilities/videos/__init__.py,sha256=SV3w51vfPuGBxaMeNemx6qEMHw_C4lLpWNGXMvdsKSY,734
 dtlpy/utilities/videos/video_player.py,sha256=LCxg0EZ_DeuwcT7U_r7MRC6Q19s0xdFb7x5Gk39PRms,24072
 dtlpy/utilities/videos/videos.py,sha256=Dj916B4TQRIhI7HZVevl3foFrCsPp0eeWwvGbgX3-_A,21875
-dtlpy-1.78.17.data/scripts/dlp,sha256=-F0vSCWuSOOtgERAtsPMPyMmzitjhB7Yeftg_PDlDjw,10
-dtlpy-1.78.17.data/scripts/dlp.bat,sha256=QOvx8Dlx5dUbCTMpwbhOcAIXL1IWmgVRSboQqDhIn3A,37
-dtlpy-1.78.17.data/scripts/dlp.py,sha256=tEokRaDINISXnq8yNx_CBw1qM5uwjYiZoJOYGqWB3RU,4267
+dtlpy-1.79.14.data/scripts/dlp,sha256=-F0vSCWuSOOtgERAtsPMPyMmzitjhB7Yeftg_PDlDjw,10
+dtlpy-1.79.14.data/scripts/dlp.bat,sha256=QOvx8Dlx5dUbCTMpwbhOcAIXL1IWmgVRSboQqDhIn3A,37
+dtlpy-1.79.14.data/scripts/dlp.py,sha256=tEokRaDINISXnq8yNx_CBw1qM5uwjYiZoJOYGqWB3RU,4267
 tests/features/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-tests/features/environment.py,sha256=D2Uhv9j3xZ1aGk8YmkXYcw75q8xE9ukKy6uakNJpaWU,9395
-dtlpy-1.78.17.dist-info/LICENSE,sha256=QwcOLU5TJoTeUhuIXzhdCEEDDvorGiC6-3YTOl4TecE,11356
-dtlpy-1.78.17.dist-info/METADATA,sha256=ocEYVieffR6YCjSpcYqSr99bzWrHJrbGi0lJmnWGnpc,3022
-dtlpy-1.78.17.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-dtlpy-1.78.17.dist-info/entry_points.txt,sha256=C4PyKthCs_no88HU39eioO68oei64STYXC2ooGZTc4Y,43
-dtlpy-1.78.17.dist-info/top_level.txt,sha256=ZWuLmQGUOtWAdgTf4Fbx884w1o0vBYq9dEc1zLv9Mig,12
-dtlpy-1.78.17.dist-info/RECORD,,
+tests/features/environment.py,sha256=VnqE8q8ZN0amNjgO6wW5_ywkPQIaCvzcytve7JnjyWo,9732
+dtlpy-1.79.14.dist-info/LICENSE,sha256=QwcOLU5TJoTeUhuIXzhdCEEDDvorGiC6-3YTOl4TecE,11356
+dtlpy-1.79.14.dist-info/METADATA,sha256=J6L5Thiu3nMtm2mV5vTSmqb3IBr9A861YG9OZOo2los,3022
+dtlpy-1.79.14.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+dtlpy-1.79.14.dist-info/entry_points.txt,sha256=C4PyKthCs_no88HU39eioO68oei64STYXC2ooGZTc4Y,43
+dtlpy-1.79.14.dist-info/top_level.txt,sha256=ZWuLmQGUOtWAdgTf4Fbx884w1o0vBYq9dEc1zLv9Mig,12
+dtlpy-1.79.14.dist-info/RECORD,,
```

