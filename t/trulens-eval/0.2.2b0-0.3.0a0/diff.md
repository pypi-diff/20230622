# Comparing `tmp/trulens_eval-0.2.2b0-py3-none-any.whl.zip` & `tmp/trulens_eval-0.3.0a0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,34 +1,36 @@
-Zip file size: 83692 bytes, number of entries: 32
--rw-rw-r--  2.0 unx     4799 b- defN 23-Jun-14 03:10 trulens_eval/Example_TruBot.py
--rw-rw-r--  2.0 unx     3101 b- defN 23-Jun-14 03:10 trulens_eval/Leaderboard.py
--rw-rw-r--  2.0 unx     1137 b- defN 23-Jun-15 15:18 trulens_eval/__init__.py
--rw-rw-r--  2.0 unx     5401 b- defN 23-Jun-09 21:41 trulens_eval/benchmark.py
+Zip file size: 91824 bytes, number of entries: 34
+-rw-rw-r--  2.0 unx     5164 b- defN 23-Jun-22 21:05 trulens_eval/Example_TruBot.py
+-rw-rw-r--  2.0 unx     3093 b- defN 23-Jun-22 21:05 trulens_eval/Leaderboard.py
+-rw-rw-r--  2.0 unx     1227 b- defN 23-Jun-22 21:13 trulens_eval/__init__.py
+-rw-rw-r--  2.0 unx    10979 b- defN 23-Jun-22 21:05 trulens_eval/app.py
+-rw-rw-r--  2.0 unx     5344 b- defN 23-Jun-22 21:05 trulens_eval/benchmark.py
+-rw-rw-r--  2.0 unx    18889 b- defN 23-Jun-22 21:05 trulens_eval/db.py
+-rw-rw-r--  2.0 unx    31403 b- defN 23-Jun-22 21:05 trulens_eval/feedback.py
 -rw-rw-r--  2.0 unx     3443 b- defN 23-Jun-09 21:41 trulens_eval/feedback_prompts.py
--rw-rw-r--  2.0 unx    18056 b- defN 23-Jun-14 03:10 trulens_eval/instruments.py
--rw-rw-r--  2.0 unx      985 b- defN 23-Jun-09 21:41 trulens_eval/keys.py
--rw-rw-r--  2.0 unx     3376 b- defN 23-Jun-14 03:10 trulens_eval/provider_apis.py
--rw-rw-r--  2.0 unx     9474 b- defN 23-Jun-14 03:10 trulens_eval/schema.py
--rw-rw-r--  2.0 unx    11241 b- defN 23-Jun-14 03:10 trulens_eval/slackbot.py
--rw-rw-r--  2.0 unx    12842 b- defN 23-Jun-14 03:10 trulens_eval/tru.py
--rw-rw-r--  2.0 unx     6744 b- defN 23-Jun-14 03:10 trulens_eval/tru_app.py
--rw-rw-r--  2.0 unx     6579 b- defN 23-Jun-14 03:10 trulens_eval/tru_chain.py
--rw-rw-r--  2.0 unx    18649 b- defN 23-Jun-14 03:10 trulens_eval/tru_db.py
--rw-rw-r--  2.0 unx    28723 b- defN 23-Jun-14 03:10 trulens_eval/tru_feedback.py
--rw-rw-r--  2.0 unx     4396 b- defN 23-Jun-15 15:07 trulens_eval/tru_llama.py
--rw-rw-r--  2.0 unx    40198 b- defN 23-Jun-14 03:10 trulens_eval/util.py
--rw-rw-r--  2.0 unx     9678 b- defN 23-Jun-14 03:10 trulens_eval/pages/Evaluations.py
--rw-rw-r--  2.0 unx     1580 b- defN 23-Jun-14 03:10 trulens_eval/pages/Progress.py
+-rw-rw-r--  2.0 unx    20140 b- defN 23-Jun-22 21:05 trulens_eval/instruments.py
+-rw-rw-r--  2.0 unx     1406 b- defN 23-Jun-22 21:05 trulens_eval/keys.py
+-rw-rw-r--  2.0 unx    19328 b- defN 23-Jun-22 21:05 trulens_eval/provider_apis.py
+-rw-rw-r--  2.0 unx    12336 b- defN 23-Jun-22 21:05 trulens_eval/schema.py
+-rw-rw-r--  2.0 unx    12891 b- defN 23-Jun-22 21:05 trulens_eval/tru.py
+-rw-rw-r--  2.0 unx      293 b- defN 23-Jun-22 21:05 trulens_eval/tru_app.py
+-rw-rw-r--  2.0 unx     6709 b- defN 23-Jun-22 21:05 trulens_eval/tru_chain.py
+-rw-rw-r--  2.0 unx      288 b- defN 23-Jun-22 21:05 trulens_eval/tru_db.py
+-rw-rw-r--  2.0 unx      318 b- defN 23-Jun-22 21:05 trulens_eval/tru_feedback.py
+-rw-rw-r--  2.0 unx     6352 b- defN 23-Jun-22 21:05 trulens_eval/tru_llama.py
+-rw-rw-r--  2.0 unx    42835 b- defN 23-Jun-22 21:05 trulens_eval/util.py
+-rw-rw-r--  2.0 unx     9876 b- defN 23-Jun-22 21:05 trulens_eval/pages/Evaluations.py
+-rw-rw-r--  2.0 unx     1571 b- defN 23-Jun-22 21:05 trulens_eval/pages/Progress.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Jun-09 21:41 trulens_eval/pages/__init__.py
 -rw-rw-r--  2.0 unx     5551 b- defN 23-Jun-14 03:10 trulens_eval/tests/test_tru_chain.py
--rw-rw-r--  2.0 unx     3828 b- defN 23-Jun-14 03:10 trulens_eval/utils/langchain.py
--rw-rw-r--  2.0 unx     1450 b- defN 23-Jun-14 03:10 trulens_eval/utils/llama.py
+-rw-rw-r--  2.0 unx     5365 b- defN 23-Jun-22 21:05 trulens_eval/utils/langchain.py
+-rw-rw-r--  2.0 unx     4774 b- defN 23-Jun-22 21:05 trulens_eval/utils/llama.py
 -rw-rw-r--  2.0 unx     1001 b- defN 23-Jun-14 03:10 trulens_eval/utils/notebook_utils.py
 -rw-rw-r--  2.0 unx      915 b- defN 23-Jun-09 21:41 trulens_eval/ux/add_logo.py
--rw-rw-r--  2.0 unx     3511 b- defN 23-Jun-14 03:10 trulens_eval/ux/components.py
--rw-rw-r--  2.0 unx     1213 b- defN 23-Jun-09 21:41 trulens_eval/ux/styles.py
+-rw-rw-r--  2.0 unx     3790 b- defN 23-Jun-22 21:05 trulens_eval/ux/components.py
+-rw-rw-r--  2.0 unx     1209 b- defN 23-Jun-22 21:05 trulens_eval/ux/styles.py
 -rw-rw-r--  2.0 unx    29567 b- defN 23-Jun-09 21:41 trulens_eval/ux/trulens_logo.svg
--rw-rw-r--  2.0 unx    14497 b- defN 23-Jun-15 15:18 trulens_eval-0.2.2b0.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Jun-15 15:18 trulens_eval-0.2.2b0.dist-info/WHEEL
--rw-rw-r--  2.0 unx       13 b- defN 23-Jun-15 15:18 trulens_eval-0.2.2b0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     2683 b- defN 23-Jun-15 15:18 trulens_eval-0.2.2b0.dist-info/RECORD
-32 files, 254723 bytes uncompressed, 79410 bytes compressed:  68.8%
+-rw-rw-r--  2.0 unx    14878 b- defN 23-Jun-22 21:15 trulens_eval-0.3.0a0.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Jun-22 21:15 trulens_eval-0.3.0a0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       13 b- defN 23-Jun-22 21:15 trulens_eval-0.3.0a0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     2834 b- defN 23-Jun-22 21:15 trulens_eval-0.3.0a0.dist-info/RECORD
+34 files, 283874 bytes uncompressed, 87316 bytes compressed:  69.2%
```

## zipnote {}

```diff
@@ -3,17 +3,26 @@
 
 Filename: trulens_eval/Leaderboard.py
 Comment: 
 
 Filename: trulens_eval/__init__.py
 Comment: 
 
+Filename: trulens_eval/app.py
+Comment: 
+
 Filename: trulens_eval/benchmark.py
 Comment: 
 
+Filename: trulens_eval/db.py
+Comment: 
+
+Filename: trulens_eval/feedback.py
+Comment: 
+
 Filename: trulens_eval/feedback_prompts.py
 Comment: 
 
 Filename: trulens_eval/instruments.py
 Comment: 
 
 Filename: trulens_eval/keys.py
@@ -21,17 +30,14 @@
 
 Filename: trulens_eval/provider_apis.py
 Comment: 
 
 Filename: trulens_eval/schema.py
 Comment: 
 
-Filename: trulens_eval/slackbot.py
-Comment: 
-
 Filename: trulens_eval/tru.py
 Comment: 
 
 Filename: trulens_eval/tru_app.py
 Comment: 
 
 Filename: trulens_eval/tru_chain.py
@@ -78,20 +84,20 @@
 
 Filename: trulens_eval/ux/styles.py
 Comment: 
 
 Filename: trulens_eval/ux/trulens_logo.svg
 Comment: 
 
-Filename: trulens_eval-0.2.2b0.dist-info/METADATA
+Filename: trulens_eval-0.3.0a0.dist-info/METADATA
 Comment: 
 
-Filename: trulens_eval-0.2.2b0.dist-info/WHEEL
+Filename: trulens_eval-0.3.0a0.dist-info/WHEEL
 Comment: 
 
-Filename: trulens_eval-0.2.2b0.dist-info/top_level.txt
+Filename: trulens_eval-0.3.0a0.dist-info/top_level.txt
 Comment: 
 
-Filename: trulens_eval-0.2.2b0.dist-info/RECORD
+Filename: trulens_eval-0.3.0a0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## trulens_eval/Example_TruBot.py

```diff
@@ -11,20 +11,20 @@
 import numpy as np
 import pinecone
 import streamlit as st
 
 from trulens_eval import Query
 from trulens_eval import tru
 from trulens_eval import tru_chain
-from trulens_eval import tru_feedback
+from trulens_eval import feedback
 from trulens_eval.keys import *
 from trulens_eval.keys import PINECONE_API_KEY
 from trulens_eval.keys import PINECONE_ENV
-from trulens_eval.tru_db import Record
-from trulens_eval.tru_feedback import Feedback
+from trulens_eval.db import Record
+from trulens_eval.feedback import Feedback
 
 # Set up GPT-3 model
 model_name = "gpt-3.5-turbo"
 
 app_id = "TruBot"
 # app_id = "TruBot_langprompt"
 # app_id = "TruBot_relevance"
@@ -33,31 +33,34 @@
 pinecone.init(
     api_key=PINECONE_API_KEY,  # find at app.pinecone.io
     environment=PINECONE_ENV  # next to api key in console
 )
 
 identity = lambda h: h
 
-hugs = tru_feedback.Huggingface()
-openai = tru_feedback.OpenAI()
+hugs = feedback.Huggingface()
+openai = feedback.OpenAI()
 
-f_lang_match = Feedback(hugs.language_match).on(
-    text1=Query.RecordInput, text2=Query.RecordOutput
-)
-
-f_qa_relevance = Feedback(openai.relevance).on(
-    prompt=Query.RecordInput, response=Query.RecordOutput
-)
-
-f_qs_relevance = Feedback(openai.qs_relevance).on(
-    question=Query.RecordInput,
-    statement=Query.Record.chain.combine_docs_chain._call.args.inputs.
-    input_documents[:].page_content
+# Language match between question/answer.
+f_lang_match = Feedback(hugs.language_match).on_input_output()
+# By default this will evaluate feedback on main app input and main app output.
+
+# Question/answer relevance between overall question and answer.
+f_qa_relevance = Feedback(openai.relevance).on_input_output()
+# By default this will evaluate feedback on main app input and main app output.
+
+# Question/statement relevance between question and each context chunk.
+f_qs_relevance = feedback.Feedback(openai.qs_relevance).on_input().on(
+    Select.Record.app.combine_docs_chain._call.args.inputs.input_documents[:].
+    page_content
 ).aggregate(np.min)
 
+# First feedback argument is set to main app input, and the second is taken from
+# the context sources as passed to an internal `combine_docs_chain._call`.
+
 
 # @st.cache_data
 def generate_response(prompt):
     # Embedding needed for Pinecone vector db.
     embedding = OpenAIEmbeddings(model='text-embedding-ada-002')  # 1536 dims
     docsearch = Pinecone.from_existing_index(
         index_name="llmdemo", embedding=embedding
```

## trulens_eval/Leaderboard.py

```diff
@@ -4,16 +4,16 @@
 import numpy as np
 import streamlit as st
 from streamlit_extras.switch_page_button import switch_page
 
 st.runtime.legacy_caching.clear_cache()
 
 from trulens_eval import Tru
-from trulens_eval import tru_db
-from trulens_eval.tru_feedback import default_pass_fail_color_threshold
+from trulens_eval import db
+from trulens_eval.feedback import default_pass_fail_color_threshold
 from trulens_eval.ux import styles
 
 st.set_page_config(page_title="Leaderboard", layout="wide")
 
 from trulens_eval.ux.add_logo import add_logo
 
 add_logo()
```

## trulens_eval/__init__.py

```diff
@@ -8,49 +8,52 @@
 Modules on lower lines should not import modules on same or above lines as
 otherwise you might get circular import errors.
 
     - `__init__.py`
 
     - all UI/dashboard components
 
-    - `tru_chain.py` `tru_llama.py`
+    - `tru_chain.py` 
+    
+    - `tru_llama.py` (note: llama_index uses langchain internally for some things)
 
     - `tru.py`
 
-    - `tru_feedback.py`
+    - `feedback.py`
 
-    - `tru_model.py`
+    - `app.py`
 
-    - `tru_db.py`
+    - `db.py`
 
     - `instruments.py`
 
     - `provider_apis.py` `feedback_prompts.py`
 
     - `schema.py`
 
     - `util.py` `keys.py`
 """
 
-__version__ = "0.2.2b"
+__version__ = "0.3.0a"
 
 from trulens_eval.schema import FeedbackMode
-from trulens_eval.schema import Query
+from trulens_eval.schema import Query, Select
 from trulens_eval.tru import Tru
 from trulens_eval.tru_chain import TruChain
-from trulens_eval.tru_feedback import Feedback
-from trulens_eval.tru_feedback import Huggingface
-from trulens_eval.tru_feedback import OpenAI
-from trulens_eval.tru_feedback import Provider
+from trulens_eval.feedback import Feedback
+from trulens_eval.feedback import Huggingface
+from trulens_eval.feedback import OpenAI
+from trulens_eval.feedback import Provider
 from trulens_eval.tru_llama import TruLlama
 
 __all__ = [
     'Tru',
     'TruChain',
     'TruLlama',
     'Feedback',
     'OpenAI',
     'Huggingface',
     'FeedbackMode',
     'Provider',
-    'Query',
+    'Query',  # to deprecate in 0.3.0
+    'Select'
 ]
```

## trulens_eval/benchmark.py

```diff
@@ -1,15 +1,15 @@
 import time
 import zipfile
 
 from datasets import load_dataset
 from kaggle.api.kaggle_api_extended import KaggleApi
 import pandas as pd
 
-from trulens_eval import tru_feedback
+from trulens_eval import feedback
 
 
 def load_data(dataset_choice):
     if dataset_choice == 'imdb (binary sentiment)':
         data = load_dataset('imdb')
         train = pd.DataFrame(data['train'])
         test = pd.DataFrame(data['test'])
@@ -73,48 +73,47 @@
         nonlocal last_call_time
 
         elapsed_time = time.time() - last_call_time
 
         if elapsed_time < interval:
             time.sleep(interval - elapsed_time)
 
-        if feedback_function_name in tru_feedback.FEEDBACK_FUNCTIONS:
-            feedback_function = tru_feedback.FEEDBACK_FUNCTIONS[
+        if feedback_function_name in feedback.FEEDBACK_FUNCTIONS:
+            feedback_function = feedback.FEEDBACK_FUNCTIONS[
                 feedback_function_name](
                     provider=provider,
                     model_engine=model_engine,
                     evaluation_choice=evaluation_choice,
                     **kwargs
                 )
         else:
             raise ValueError(
-                f"Unrecognized feedback_function_name. Please use one of {list(tru_feedback.FEEDBACK_FUNCTIONS.keys())} "
+                f"Unrecognized feedback_function_name. Please use one of {list(feedback.FEEDBACK_FUNCTIONS.keys())} "
             )
 
         result = feedback_function(prompt=prompt, response=response, **kwargs)
         last_call_time = time.time()
 
         return result
 
     return rate_limited_feedback
 
 
 def benchmark_on_data(
     data, feedback_function_name, evaluation_choice, provider, model_engine
 ):
-    if feedback_function_name in tru_feedback.FEEDBACK_FUNCTIONS:
-        feedback_function = tru_feedback.FEEDBACK_FUNCTIONS[
-            feedback_function_name](
-                evaluation_choice=evaluation_choice,
-                provider=provider,
-                model_engine=model_engine
-            )
+    if feedback_function_name in feedback.FEEDBACK_FUNCTIONS:
+        feedback_function = feedback.FEEDBACK_FUNCTIONS[feedback_function_name](
+            evaluation_choice=evaluation_choice,
+            provider=provider,
+            model_engine=model_engine
+        )
     else:
         raise ValueError(
-            f"Unrecognized feedback_function_name. Please use one of {list(tru_feedback.FEEDBACK_FUNCTIONS.keys())} "
+            f"Unrecognized feedback_function_name. Please use one of {list(feedback.FEEDBACK_FUNCTIONS.keys())} "
         )
     if 'prompt' in data and 'response' in data:
         data['feedback'] = data.apply(
             lambda x: feedback_function(x['prompt'], x['response']), axis=1
         )
     else:
         data['feedback'] = data['text'].apply(
```

## trulens_eval/instruments.py

```diff
@@ -75,24 +75,29 @@
 - `sys.setprofile` (see
   https://docs.python.org/3/library/sys.html#sys.setprofile)
 
     Might incur much overhead and all calls and other event types get
  
     intercepted and result in a callback.
 
+- langchain/llama_index callbacks. Each of these packages come with some
+  callback system that lets one get various intermediate app results. The
+  drawbacks is the need to handle different callback systems for each system and
+  potentially missing information not exposed by them.
+
 ### Tricky
 
 - 
 
 ### Calls
 
 The instrumented versions of functions/methods record the inputs/outputs and
 some additional data (see `schema.py:RecordAppCall`). As more then one
-instrumented call may take place as part of a app invokation, they are
-collected and returned together in the `calls` field of `schema.py:Record`.
+instrumented call may take place as part of a app invokation, they are collected
+and returned together in the `calls` field of `schema.py:Record`.
 
 Calls can be connected to the components containing the called method via the
 `path` field of `schema.py:RecordAppCallMethod`. This class also holds
 information about the instrumented method.
 
 #### Call Data (Arguments/Returns)
 
@@ -117,43 +122,48 @@
 - Threads need to be started using the utility class TP in order for
   instrumented methods called in a thread to be tracked. As we rely on call
   stack for call instrumentation we need to preserve the stack before a thread
   start which python does not do.  See `util.py:TP._thread_starter`.
 
 - If the same wrapped sub-app is called multiple times within a single call to
   the root app, the record of this execution will not be exact with regards to
-  the path to the call information. All call paths will address the last
-  subapp (by order in which it is instrumented). For example, in a sequential
-  app containing two of the same app, call records will be addressed to the
-  second of the (same) apps and contain a list describing calls of both the
-  first and second.
+  the path to the call information. All call paths will address the last subapp
+  (by order in which it is instrumented). For example, in a sequential app
+  containing two of the same app, call records will be addressed to the second
+  of the (same) apps and contain a list describing calls of both the first and
+  second.
 
-- Some apps cannot be serialized/jsonized. Sequential app is an example.
-  This is a limitation of langchain itself.
+- Some apps cannot be serialized/jsonized. Sequential app is an example. This is
+  a limitation of langchain itself.
 
 - Instrumentation relies on CPython specifics, making heavy use of the `inspect`
   module which is not expected to work with other Python implementations.
 
+#### Alternatives
+
+- langchain/llama_index callbacks. These provide information about component
+  invocations but the drawbacks are need to cover disparate callback systems and
+  possibly missing information not covered.
+
 ## To Decide / To discuss
 
 ### Mirroring wrapped app behaviour and disabling instrumentation
 
 Should our wrappers behave like the wrapped apps? Current design is like this:
 
-```python
-chain = ... # some langchain chain
+```python chain = ... # some langchain chain
 
-tru = Tru()
-truchain = tru.Chain(chain, ...)
+tru = Tru() truchain = tru.Chain(chain, ...)
 
 plain_result = chain(...) # will not be recorded
 
 plain_result = truchain(...) # will be recorded
 
-plain_result, record = truchain.call_with_record(...) # will be recorded, and you get the record too
+plain_result, record = truchain.call_with_record(...) # will be recorded, and
+you get the record too
 
 ```
 
 The problem with the above is that "call_" part of "call_with_record" is
 langchain specific and implicitly so is __call__ whose behaviour we are
 replicating in TruChaib. Other wrapped apps may not implement their core
 functionality in "_call" or "__call__".
@@ -167,83 +177,137 @@
 truchain = tru.Chain(chain, ...)
 
 with truchain.record() as recorder:
     plain_result = chain(...) # will be recorded
 
 records = recorder.records # can get records
 
-truchain(...) # NOT SUPPORTED, use chain instead
-```
+truchain(...) # NOT SUPPORTED, use chain instead ```
 
 Here we have the benefit of not having a special method for each app type like
 call_with_record. We instead use a context to indicate that we want to collect
 records and retrieve them afterwards.
 
+### Calls: Implementation Details
+
+Our tracking of calls uses instrumentated versions of methods to manage the
+recording of inputs/outputs. The instrumented methods must distinguish
+themselves from invocations of apps that are being tracked from those not being
+tracked, and of those that are tracked, where in the call stack a instrumented
+method invocation is. To achieve this, we rely on inspecting the python call
+stack for specific frames:
+
+- Root frame -- A tracked invocation of an app starts with one of the
+  main/"root" methods such as call or query. These are the bottom of the
+  relevant stack we use to manage the tracking of subsequent calls. Further
+  calls to instrumented methods check for the root method in the call stack and
+  retrieve the collection where data is to be recorded.
+  
+- Prior frame -- Each instrumented call also searches for the topmost
+  instrumented call (except itself) in the stack to check its immediate caller
+  (by immediate we mean only among instrumented methods) which forms the basis
+  of the stack information recorded alongisde the inputs/outputs.
+
+#### Drawbacks
+
+- Python call stacks are implementation dependent and we don't expect to operate
+  on anything other than CPython.
+
+- Python creates a fresh empty stack for each thread. Because of this, we need
+  special handling of each thread created to make sure it keeps a hold of the
+  stack prior to thread creation. Right now we do this in our threading utility
+  class TP but a more complete solution may be the instrumentation of
+  threading.Thread class.
+
+- We require a root method to be placed on the stack to indicate the start of
+  tracking. We therefore cannot implement something like a context-manager-based
+  setup of the tracking system as suggested in the "To discuss" above.
+
+#### Alternatives
+
+- contextvars -- langchain uses these to manage contexts such as those used for
+  instrumenting/tracking LLM usage. These can be used to manage call stack
+  information like we do. The drawback is that these are not threadsafe or at
+  least need instrumenting thread creation. We have to do a similar thing by
+  requiring threads created by our utility package which does stack management
+  instead of contextvar management.
+
 """
 
 from datetime import datetime
 from inspect import BoundArguments
 from inspect import signature
 import logging
 import os
 from pprint import PrettyPrinter
 import threading as th
-from typing import (
-    Any, Callable, Dict, Iterable, Optional, Sequence, Set, Union
-)
+from typing import (Callable, Dict, Iterable, Optional, Sequence, Set)
 
 from pydantic import BaseModel
 
 from trulens_eval.schema import Perf
 from trulens_eval.schema import Query
 from trulens_eval.schema import RecordAppCall
 from trulens_eval.schema import RecordAppCallMethod
-from trulens_eval.tru_feedback import Feedback
+from trulens_eval.feedback import Feedback
+from trulens_eval.util import _safe_getattr
 from trulens_eval.util import get_local_in_call_stack
 from trulens_eval.util import jsonify
 from trulens_eval.util import Method
-from trulens_eval.util import noserio
 
 logger = logging.getLogger(__name__)
 pp = PrettyPrinter()
 
 
 class Instrument(object):
     # Attribute name to be used to flag instrumented objects/methods/others.
     INSTRUMENT = "__tru_instrumented"
 
-    # For marking queries that address app components.
-    QUERY = "__tru_query"
+    # Attribute name for marking paths that address app components.
+    PATH = "__tru_path"
 
     class Default:
         # Default instrumentation configuration. Additional components are
         # included in subclasses of `Instrument`.
 
-        # Modules to instrument.
+        # Modules (by full name prefix) to instrument.
         MODULES = {"trulens_eval."}
 
         # Classes to instrument.
         CLASSES = set()
 
         # Methods to instrument. Methods matching name have to pass the filter
         # to be instrumented. TODO: redesign this to be a dict with classes
         # leading to method names instead.
-        METHODS = {
-            "__call__":
-                lambda o: isinstance(o, Feedback)  # Feedback
-        }
+        METHODS = {"__call__": lambda o: isinstance(o, Feedback)}
+
+    def to_instrument_object(self, obj: object) -> bool:
+        """
+        Determine whether the given object should be instrumented.
+        """
 
-    def to_instrument_object(self, obj):
-        return self.to_instrument_class(type(obj))
+        # NOTE: some classes do not support issubclass but do support
+        # isinstance. It is thus preferable to do isinstance checks when we can
+        # avoid issublcass checks.
+        return any(isinstance(obj, cls) for cls in self.classes)
+
+    def to_instrument_class(self, cls: type) -> bool:  # class
+        """
+        Determine whether the given class should be instrumented.
+        """
 
-    def to_instrument_class(self, cls):
         return any(issubclass(cls, parent) for parent in self.classes)
 
-    def to_instrument_module(self, mod):
-        return any(mod.startswith(mod2) for mod2 in self.modules)
+    def to_instrument_module(self, module_name: str) -> bool:
+        """
+        Determine whether a module with the given (full) name should be
+        instrumented.
+        """
+
+        return any(module_name.startswith(mod2) for mod2 in self.modules)
 
     def __init__(
         self,
         root_method: Optional[Callable] = None,
         modules: Iterable[str] = [],
         classes: Iterable[type] = [],
         methods: Dict[str, Callable] = {},
@@ -373,15 +437,15 @@
         # further instrument it in another layer accidentally.
         setattr(wrapper, Instrument.INSTRUMENT, func)
 
         # Put the address of the instrumented chain in the wrapper so that we
         # don't pollute its list of fields. Note that this address may be
         # deceptive if the same subchain appears multiple times in the wrapped
         # chain.
-        setattr(wrapper, Instrument.QUERY, query)
+        setattr(wrapper, Instrument.PATH, query)
 
         return wrapper
 
     def instrument_object(self, obj, query: Query, done: Set[int] = None):
 
         done = done or set([])
 
@@ -401,27 +465,23 @@
         # NOTE: We cannot instrument chain directly and have to instead
         # instrument its class. The pydantic BaseModel does not allow instance
         # attributes that are not fields:
         # https://github.com/pydantic/pydantic/blob/11079e7e9c458c610860a5776dc398a4764d538d/pydantic/main.py#LL370C13-L370C13
         # .
 
         for base in list(cls.__mro__):
-            # All of mro() may need instrumentation here if some subchains call
-            # superchains, and we want to capture the intermediate steps.
-
-            if not any(issubclass(base, c) for c in self.classes):
+            # Some top part of mro() may need instrumentation here if some
+            # subchains call superchains, and we want to capture the
+            # intermediate steps. On the other hand we don't want to instrument
+            # the very base classes such as object:
+            if not self.to_instrument_module(base.__module__):
                 continue
 
             logger.debug(f"\t{query}: instrumenting base {base.__name__}")
 
-            # TODO: generalize
-            if not any(base.__module__.startswith(module_name)
-                       for module_name in self.modules):
-                continue
-
             for method_name in self.methods:
                 if hasattr(base, method_name):
                     check_class = self.methods[method_name]
                     if not check_class(obj):
                         continue
 
                     original_fun = getattr(base, method_name)
@@ -434,46 +494,15 @@
                             query=query,
                             func=original_fun,
                             method_name=method_name,
                             cls=base,
                             obj=obj
                         )
                     )
-            """
-            # Instrument special langchain methods that may cause serialization
-            # failures.
-            if hasattr(base, "_chain_type"):
-                logger.debug(f"instrumenting {base}._chain_type")
-
-                prop = getattr(base, "_chain_type")
-                setattr(
-                    base, "_chain_type",
-                    self._instrument_type_method(obj=obj, prop=prop)
-                )
-
-            if hasattr(base, "_prompt_type"):
-                logger.debug(f"instrumenting {base}._chain_prompt")
-
-                prop = getattr(base, "_prompt_type")
-                setattr(
-                    base, "_prompt_type",
-                    self._instrument_type_method(obj=obj, prop=prop)
-                )
-
-            # Instrument a pydantic.BaseModel method that may cause
-            # serialization failures.
-            if hasattr(base, "dict"):# and not hasattr(base.dict, "_instrumented"):
-                logger.debug(f"instrumenting {base}.dict")
-                setattr(base, "dict", self._instrument_dict(cls=base, obj=obj, with_class_info=True))
-            """
 
-        # Not using chain.dict() here as that recursively converts subchains to
-        # dicts but we want to traverse the instantiations here.
-
-        # TODO: generalize:
         if isinstance(obj, BaseModel):
 
             for k in obj.__fields__:
                 # NOTE(piotrm): may be better to use inspect.getmembers_static .
                 v = getattr(obj, k)
 
                 if isinstance(v, str):
@@ -486,23 +515,27 @@
                 elif isinstance(v, Sequence):
                     for i, sv in enumerate(v):
                         if any(isinstance(sv, cls) for cls in self.classes):
                             self.instrument_object(
                                 obj=sv, query=query[k][i], done=done
                             )
 
-                # TODO: check if we want to instrument anything not accessible through __fields__ .
+                # TODO: check if we want to instrument anything in langchain not
+                # accessible through __fields__ .
 
         elif obj.__class__.__module__.startswith("llama_index"):
+            # Some llama_index objects are using dataclasses_json but most do
+            # not. Have to enumerate their contents forcefully:
+
             for k in dir(obj):
                 if k.startswith("_") and k[1:] in dir(obj):
                     # Skip those starting with _ that also have non-_ versions.
                     continue
 
-                sv = getattr(obj, k)  # static get ?
+                sv = _safe_getattr(obj, k)
 
                 if any(isinstance(sv, cls) for cls in self.classes):
                     self.instrument_object(obj=sv, query=query[k], done=done)
 
         else:
             logger.debug(
                 f"{query}: Do not know how to instrument object of type {cls}."
```

## trulens_eval/keys.py

```diff
@@ -1,28 +1,47 @@
 """
 Read secrets from .env for exporting to python scripts. Usage:
+
 ```python
-    from keys import *
+from keys import *
 ```
-Will get you access to all of the vars defined in .env in wherever you put that import statement.
+
+Will get you access to all of the vars defined in .env in wherever you put that
+import statement.
 """
 
 import os
 
 import cohere
 import dotenv
 
-config = dotenv.dotenv_values(".env")
+from pathlib import Path
 
-for k, v in config.items():
-    print(f"KEY SET: {k}")
-    globals()[k] = v
+def get_config():
+    for path in Path.cwd().parents:
+        file = path / ".env" 
+        if file.exists():
+            print(f"Using {file}")
+            return file
+        
+    return None
+
+config_file = get_config()
+if config_file is None:
+    print(f"WARNING: No .env found in {Path.cwd()} or its parents. You may need to specify secret keys manually.")
+
+else:
+    config = dotenv.dotenv_values(config_file)
+
+    for k, v in config.items():
+        print(f"KEY SET: {k}")
+        globals()[k] = v
 
-    # set them into environment as well
-    os.environ[k] = v
+        # set them into environment as well
+        os.environ[k] = v
 
 
 def set_openai_key():
     if 'OPENAI_API_KEY' in os.environ:
         import openai
         openai.api_key = os.environ["OPENAI_API_KEY"]
```

## trulens_eval/provider_apis.py

```diff
@@ -1,96 +1,234 @@
+from abc import ABC, abstractmethod
+import inspect
+import json
 import logging
-from multiprocessing import Queue
-# from queue import Queue
+# from multiprocessing import Queue
+from queue import Queue
 from threading import Thread
 from time import sleep
-from typing import Any, Optional, Sequence
+from types import ModuleType
+from typing import Any, Callable, Dict, Iterable, Optional, Sequence, Tuple, Type, TypeVar
+import pydantic
 
 import requests
-from tqdm.auto import tqdm
 
-from trulens_eval.tru_db import JSON
+from trulens_eval.db import JSON
+from trulens_eval.schema import Cost
+from trulens_eval.keys import get_huggingface_headers
+from trulens_eval.util import WithClassInfo
+from trulens_eval.util import SerialModel, get_local_in_call_stack
 from trulens_eval.util import SingletonPerName
 from trulens_eval.util import TP
 
+from langchain.schema import LLMResult
+from langchain.callbacks.openai_info import OpenAICallbackHandler
+
 logger = logging.getLogger(__name__)
 
+T = TypeVar("T")
 
-class Endpoint(SingletonPerName):
+INSTRUMENT = "__tru_instrument"
 
-    def __init__(
-        self, name: str, rpm: float = 60, retries: int = 3, post_headers=None
-    ):
-        """
-        Pacing and utilities for API endpoints.
-        
-        Args:
 
-        - name: str -- api name / identifier.
+class EndpointCallback(SerialModel):
+    """
+    Callbacks to be invoked after various API requests and track various metrics
+    like token usage.
+    """
+
+    cost: Cost = pydantic.Field(default_factory=Cost)
+
+    def handle(self, response: Any) -> None:
+        self.cost.n_requests += 1
+
+    def handle_generation(self, response: Any) -> None:
+        self.handle(response)
+
+    def handle_classification(self, response: Any) -> None:
+        self.handle(response)
+
+
+class HuggingfaceCallback(EndpointCallback):
+
+    def handle_classification(self, response: requests.Response) -> None:
+        # Huggingface free inference api doesn't seem to have its own library
+        # and the docs say to use `requests`` so that is what we instrument and
+        # process to track api calls.
+
+        super().handle_classification(response)
 
-        - rpm: float -- requests per minute.
+        if response.ok:
+            self.cost.n_successful_requests += 1
+            content = json.loads(response.text)
 
-        - retries: int -- number of retries before failure.
+            # Huggingface free inference api for classification returns a list
+            # with one element which itself contains scores for each class.
+            self.cost.n_classes += len(content[0])
 
-        - post_headers: Dict -- http post headers if this endpoint uses http
-          post.
+
+class OpenAICallback(EndpointCallback):
+
+    class Config:
+        arbitrary_types_allowed = True
+
+    # For openai cost tracking, we use the logic from langchain mostly
+    # implemented in the OpenAICallbackHandler class:
+    langchain_handler: OpenAICallbackHandler = pydantic.Field(
+        default_factory=OpenAICallbackHandler, exclude=True
+    )
+
+    def handle_generation(self, response: LLMResult) -> None:
+        
+        super().handle_generation(response)
+
+        self.langchain_handler.on_llm_end(response)
+
+        # Copy over the langchain handler fields we also have.
+        for cost_field, langchain_field in [
+            ("cost", "total_cost"), ("n_tokens", "total_tokens"),
+            ("n_successful_requests", "successful_requests"),
+            ("n_prompt_tokens", "prompt_tokens"),
+            ("n_completion_tokens", "completion_tokens")
+        ]:
+            setattr(
+                self.cost, cost_field,
+                getattr(self.langchain_handler, langchain_field)
+            )
+
+
+class Endpoint(SerialModel, SingletonPerName):  #, ABC):
+
+    class Config:
+        arbitrary_types_allowed = True
+        # underscore_attrs_are_private = False
+
+    # API/endpoint name
+    name: str
+
+    # Requests per minute.
+    rpm: float = 60
+
+    # Retries (if performing requests using this class). TODO: wire this up to
+    # the various endpoint systems' retries specification.
+    retries: int = 3
+
+    # Optional post headers for post requests if done by this class.
+    post_headers: Dict[str, str] = pydantic.Field(
+        default_factory=dict, exclude=True
+    )
+
+    # Queue that gets filled at rate rpm.
+    pace: Queue = pydantic.Field(
+        default_factory=lambda: Queue(maxsize=10), exclude=True
+    )
+
+    # Track costs not run inside "track_cost" here. Also note that Endpoints are
+    # singletons (one for each unique name argument) hence this global callback
+    # will track all requests for the named api even if you try to create
+    # multiple endpoints (with the same name).
+    global_callback: EndpointCallback = pydantic.Field(
+        exclude=True
+    )  # of type _callback_class
+
+    # Callback class to use for usage tracking
+    callback_class: Type[EndpointCallback] = pydantic.Field(exclude=True)
+
+    # Name of variable that stores the callback noted above.
+    callback_name: str = pydantic.Field(exclude=True)
+
+    # Thread that fills the queue at the appropriate rate.
+    pace_thread: Thread = pydantic.Field(exclude=True)
+
+    # TODO: validate to construct tracking objects when deserializing?
+
+    """
+    @classmethod
+    def model_validate(cls, obj: Any, **kwargs):
+        if isinstance(obj, dict):
+            if CLASS_INFO in obj:
+
+                cls = Class(**obj[CLASS_INFO])
+                del obj[CLASS_INFO]
+                model = cls.model_validate(obj, **kwargs)
+
+                return WithClassInfo.of_model(model=model, cls=cls)
+            else:
+                return super().model_validate(obj, **kwargs)
+    """
+
+    def __new__(cls, name: str, *args, **kwargs):
+        return super(SingletonPerName, cls).__new__(
+            SerialModel, name=name, *args, **kwargs
+        )
+
+    def __init__(self, *args, name: str, callback_class: Any, **kwargs):
+        """
+        API usage, pacing, and utilities for API endpoints.
         """
 
         if hasattr(self, "rpm"):
             # already initialized via the SingletonPerName mechanism
             return
 
-        logger.debug(f"*** Creating {name} endpoint ***")
+        kwargs['name'] = name
+        kwargs['callback_class'] = callback_class
+        kwargs['global_callback'] = callback_class()
+        kwargs['callback_name'] = f"callback_{name}"
+        kwargs['pace_thread'] = Thread()  # temporary
+
+        super(SerialModel, self).__init__(*args, **kwargs)
 
-        self.rpm = rpm
-        self.retries = retries
-        self.pace = Queue(
-            maxsize=rpm // 6
-        )  # 10 second's worth of accumulated api
-        self.tqdm = tqdm(desc=f"{name} api", unit="requests")
-        self.name = name
-        self.post_headers = post_headers
+        def keep_pace():
+            while True:
+                sleep(60.0 / self.rpm)
+                self.pace.put(True)
+
+        self.pace_thread = Thread(target=keep_pace)
+        self.pace_thread.start()
+
+        logger.debug(f"*** Creating {self.name} endpoint ***")
 
-        self._start_pacer()
+        # Extending class should call _instrument_module on the appropriate
+        # modules and methods names.
 
     def pace_me(self):
         """
         Block until we can make a request to this endpoint.
         """
 
         self.pace.get()
-        self.tqdm.update(1)
+
         return
 
     def post(
         self, url: str, payload: JSON, timeout: Optional[int] = None
     ) -> Any:
-        extra = dict()
-        if self.post_headers is not None:
-            extra['headers'] = self.post_headers
-
         self.pace_me()
-        ret = requests.post(url, json=payload, timeout=timeout, **extra)
+        ret = requests.post(
+            url, json=payload, timeout=timeout, headers=self.post_headers
+        )
 
         j = ret.json()
 
-        # Huggingface public api sometimes tells us that a model is loading and how long to wait:
+        # Huggingface public api sometimes tells us that a model is loading and
+        # how long to wait:
         if "estimated_time" in j:
             wait_time = j['estimated_time']
             logger.error(f"Waiting for {j} ({wait_time}) second(s).")
             sleep(wait_time + 2)
             return self.post(url, payload)
 
         assert isinstance(
             j, Sequence
         ) and len(j) > 0, f"Post did not return a sequence: {j}"
 
         return j[0]
 
-    def run_me(self, thunk):
+    def run_me(self, thunk: Callable[[], T]) -> T:
         """
         Run the given thunk, returning itse output, on pace with the api.
         Retries request multiple times if self.retries > 0.
         """
 
         retries = self.retries + 1
         retry_delay = 2.0
@@ -109,18 +247,340 @@
                     sleep(retry_delay)
                     retry_delay *= 2
 
         raise RuntimeError(
             f"API {self.name} request failed {self.retries+1} time(s)."
         )
 
-    def _start_pacer(self):
+    def _instrument_module(self, mod: ModuleType, method_name: str) -> None:
+        if hasattr(mod, method_name):
+            logger.debug(
+                f"Instrumenting {mod.__name__}.{method_name} for {self.name}"
+            )
+            func = getattr(mod, method_name)
+            w = self.wrap_function(func)
+            setattr(mod, method_name, w)
+
+    def _instrument_class(self, cls, method_name: str) -> None:
+        if hasattr(cls, method_name):
+            logger.debug(
+                f"Instrumenting {cls.__name__}.{method_name} for {self.name}"
+            )
+            func = getattr(cls, method_name)
+            w = self.wrap_function(func)
+            setattr(cls, method_name, w)
+
+    def _instrument_module_members(self, mod: ModuleType, method_name: str):
+        logger.debug(
+            f"Instrumenting {mod.__package__}.*.{method_name} for {self.name}"
+        )
 
-        def keep_pace():
-            while True:
-                sleep(60.0 / self.rpm)
-                self.pace.put(True)
+        for m in dir(mod):
+            obj = getattr(mod, m)
+            self._instrument_class(obj, method_name=method_name)
+
+    @staticmethod
+    def track_all_costs(
+        thunk: Callable[[], T],
+        with_openai: bool = True,
+        with_hugs: bool = True
+    ) -> Tuple[T, Sequence[EndpointCallback]]:
+        """
+        Track costs of all of the apis we can currently track, over the
+        execution of thunk.
+        """
+
+        endpoints = []
+        if with_openai:
+            endpoints.append(OpenAIEndpoint())
+        if with_hugs:
+            endpoints.append(HuggingfaceEndpoint())
+
+        return Endpoint._track_costs(thunk, with_endpoints=endpoints)
+
+    @staticmethod
+    def track_all_costs_tally(
+        thunk: Callable[[], T],
+        with_openai: bool = True,
+        with_hugs: bool = True
+    ) -> Tuple[T, Cost]:
+        """
+        Track costs of all of the apis we can currently track, over the
+        execution of thunk.
+        """
+
+        result, cbs = Endpoint.track_all_costs(thunk, with_openai=with_openai, with_hugs=with_hugs)
+        return result, sum(cb.cost for cb in cbs)
+        
+
+    @staticmethod
+    def _track_costs(
+        thunk: Callable[[], T],
+        with_endpoints: Sequence['Endpoint'] = None,
+    ) -> Tuple[T, Sequence[EndpointCallback]]:
+        """
+        Root of all cost tracking methods. Runs the given `thunk`, tracking
+        costs using each of the provided endpoints' callbacks.
+        """
+
+        # Check to see if this call is within another _track_costs call:
+        endpoints: Dict[Type[EndpointCallback], Sequence[Tuple[Endpoint, EndpointCallback]]] = \
+            get_local_in_call_stack(
+                key="endpoints",
+                func=Endpoint.__find_tracker,
+                offset=1
+            )
+
+        if endpoints is None:
+            # If not, lets start a new collection of endpoints here along with
+            # the callbacks for each. See type above.
+
+            endpoints = dict()
+
+        else:
+            # We copy the dict here so that the outer call to _track_costs will
+            # have their own version unaffacted by our additions below. Once
+            # this frame returns, the outer frame will have its own endpoints
+            # again and any wrapped method will get that smaller set of
+            # endpoints.
+
+            # TODO: check if deep copy is needed given we are storing lists in
+            # the values and don't want to affect the existing ones here.
+            endpoints = endpoints.copy()
+
+        # Collect any new endpoints requested of us.
+        with_endpoints = with_endpoints or []
+
+        # Keep track of the new callback objects we create here for returning
+        # later.
+        callbacks = []
+
+        # Create the callbacks for the new requested endpoints only. Existing
+        # endpoints from other frames will keep their callbacks.
+        for endpoint in with_endpoints:
+            callback_class = endpoint.callback_class
+            callback = callback_class()
+
+            if callback_class not in endpoints:
+                endpoints[callback_class] = []
+
+            # And add them to the endpoints dict. This will be retrieved from
+            # locals of this frame later in the wrapped methods.
+            endpoints[callback_class].append((endpoint, callback))
+
+            callbacks.append(callback)
+
+        # Call the thunk.
+        result: T = thunk()
+
+        # Return result and only the callbacks created here. Outer thunks might
+        # return others.
+        return result, callbacks
+
+    def track_cost(self, thunk: Callable[[], T]) -> Tuple[T, EndpointCallback]:
+        """
+        Tally only the usage performed within the execution of the given thunk.
+        Returns the thunk's result alongside the EndpointCallback object that
+        includes the usage information.
+        """
+
+        result, callbacks = Endpoint._track_costs(thunk, with_endpoints=[self])
+
+        return result, callbacks[0]
+
+    @staticmethod
+    def __find_tracker(f):
+        return id(f) == id(Endpoint._track_costs.__code__)
+
+    # @abstractmethod
+    def handle_wrapped_call(
+        self, bindings: inspect.BoundArguments, response: Any,
+        callback: Optional[EndpointCallback]
+    ) -> None:
+        """
+        This gets called with the results of every instrumented method. This
+        should be implemented by each subclass.
+
+        Args:
+
+        - func: Callable -- the wrapped function which returned.
+
+        - bindings: BoundArguments -- the inputs to the wrapped method.
+
+        - response: Any -- whatever the wrapped function returned.
+
+        - callback: Optional[EndpointCallback] -- the callback set up by
+          `track_cost` if the wrapped method was called and returned within an
+          invocation of `track_cost`.
+        """
+        pass
+
+    def wrap_function(self, func):
+        if hasattr(func, INSTRUMENT):
+            # Store the types of callback classes that will handle calls to the
+            # wrapped function in the INSTRUMENT attribute. This will be used to
+            # invoke appropriate callbacks when the wrapped function gets
+            # called.
+
+            # If INSTRUMENT is set, we don't need to instrument the method again
+            # but we may need to add the additional callback class to expected
+            # handlers stored at the attribute.
+
+            registered_callback_classes = getattr(func, INSTRUMENT)
+
+            if self.callback_class in registered_callback_classes:
+                # If our callback class is already in the list, dont bother
+                # adding it again.
+
+                logger.debug(
+                    f"{func.__name__} already instrumented for callbacks of type {self.callback_class.__name__}"
+                )
+
+                return func
+
+            else:
+                # Otherwise add our callback class but don't instrument again.
+
+                registered_callback_classes += [self.callback_class]
+                setattr(func, INSTRUMENT, registered_callback_classes)
+
+                return func
+
+        # If INSTRUMENT is not set, create a wrapper method and return it.
+
+        def wrapper(*args, **kwargs):
+            logger.debug(f"Calling wrapped {func.__name__} for {self.name}.")
+
+            # Get the result of the wrapped function:
+            response: Any = func(*args, **kwargs)
+
+            bindings = inspect.signature(func).bind(*args, **kwargs)
+
+            # Get all of the callback classes suitable for handling this call.
+            # Note that we stored this in the INSTRUMENT attribute of the
+            # wrapper method.
+            registered_callback_classes = getattr(wrapper, INSTRUMENT)
+
+            # Look up the endpoints that are expecting to be notified and the
+            # callback tracking the tally. See Endpoint._track_costs for
+            # definition.
+            endpoints: Dict[Type[EndpointCallback], Sequence[Tuple[Endpoint, EndpointCallback]]] = \
+                get_local_in_call_stack(
+                    key="endpoints",
+                    func=self.__find_tracker,
+                    offset=0
+                )
+
+            # If wrapped method was not called from within _track_costs, we will
+            # get None here and do nothing but return wrapped function's
+            # response.
+            if endpoints is None:
+                return response
+
+            for callback_class in registered_callback_classes:
+                for endpoint, callback in endpoints[callback_class]:
+
+                    endpoint.handle_wrapped_call(
+                        func=func,
+                        bindings=bindings,
+                        response=response,
+                        callback=callback
+                    )
+
+            #cb = get_local_in_call_stack(
+            #    key=self.callback_name,
+            #    func=self.__find_tracker,
+            #    offset=0
+            #)
+
+            #self.handle_wrapped_call(
+            #    func=func,
+            #    bindings=bindings,
+            #    response=res,
+            #    callback=cb
+            #)
+
+            return response
+
+        setattr(wrapper, INSTRUMENT, [self.callback_class])
+        wrapper.__name__ = func.__name__
+        wrapper.__signature__ = inspect.signature(func)
+
+        logger.debug(f"Instrumenting {func.__name__} for {self.name} .")
+
+        return wrapper
+
+
+class OpenAIEndpoint(Endpoint, WithClassInfo):
+    """
+    OpenAI endpoint. Instruments "create" methods in openai.* classes.
+    """
+
+    def __new__(cls):
+        return super(Endpoint, cls).__new__(cls, name="openai")
+
+    def handle_wrapped_call(
+        self, func: Callable, bindings: inspect.BoundArguments, response: Any,
+        callback: Optional[EndpointCallback]
+    ) -> None:
+
+        model_name = ""
+        if 'model' in bindings.kwargs:
+            model_name = bindings.kwargs['model']
+
+        usage = None
+        if 'usage' in response:
+            usage = response['usage']
+
+        llm_res = LLMResult(
+            generations=[[]],
+            llm_output=dict(token_usage=usage, model_name=model_name),
+            run=None
+        )
+
+        self.global_callback.handle_generation(response=llm_res)
+
+        if callback is not None:
+            callback.handle_generation(response=llm_res)
+
+    def __init__(self, *args, **kwargs):
+        kwargs['name'] = "openai"
+        kwargs['callback_class'] = OpenAICallback
+
+        # for WithClassInfo:
+        kwargs['obj'] = self
+
+        super().__init__(*args, **kwargs)
+
+        import openai
+        self._instrument_module_members(openai, "create")
+
+
+class HuggingfaceEndpoint(Endpoint, WithClassInfo):
+    """
+    OpenAI endpoint. Instruments "create" methodsin openai.* classes.
+    """
+
+    def __new__(cls):
+        return super(Endpoint, cls).__new__(cls, name="huggingface")
+
+    def handle_wrapped_call(
+        self, func: Callable, bindings: inspect.BoundArguments,
+        response: requests.Response, callback: Optional[EndpointCallback]
+    ) -> None:
+
+        self.global_callback.handle_classification(response=response)
+
+        if callback is not None:
+            callback.handle_classification(response=response)
+
+    def __init__(self, *args, **kwargs):
+        kwargs['name'] = "huggingface"
+        kwargs['post_headers'] = get_huggingface_headers()
+        kwargs['callback_class'] = HuggingfaceCallback
+
+        # for WithClassInfo:
+        kwargs['obj'] = self
 
-        thread = Thread(target=keep_pace)
-        thread.start()
+        super().__init__(*args, **kwargs)
 
-        self.pacer_thread = thread
+        self._instrument_class(requests, "post")
```

## trulens_eval/schema.py

```diff
@@ -1,41 +1,53 @@
 """
-Serializable objects and their schemas.
+# Serializable Classes
+
+Only put classes which can be serialized in this file.
+
+## Classes with non-serializable variants
+
+Many of the classes defined here extending SerialModel are meant to be
+serialized into json. Most are extended with non-serialized fields in other files.
+
+Serializable       | Non-serializable
+-------------------+------------------------
+AppDefinition      | App, TruChain, TruLlama
+FeedbackDefinition | Feedback
+
+AppDefinition.app is the JSONized version of a wrapped app while App.app is the
+actual wrapped app. We can thus inspect the contents of a wrapped app without
+having to construct it. Additionally, JSONized objects like AppDefinition.app
+feature information about the encoded object types in the dictionary under the
+util.py:CLASS_INFO key.
 """
 
-import abc
+from abc import ABC, abstractmethod
 from datetime import datetime
-from datetime import timedelta
 from enum import Enum
-import importlib
-import json
-from types import ModuleType
-from typing import (
-    Any, Callable, Dict, Iterable, Optional, Sequence, Tuple, TypeVar, Union
-)
 
+from typing import (Any, ClassVar, Dict, Optional, Sequence, TypeVar, Union)
+import logging
 from munch import Munch as Bunch
 import pydantic
+from trulens_eval.util import FunctionOrMethod
 
-from trulens_eval.util import all_queries
 from trulens_eval.util import Class
 from trulens_eval.util import Function
 from trulens_eval.util import GetItemOrAttribute
 from trulens_eval.util import JSON
-from trulens_eval.util import json_default
-from trulens_eval.util import json_str_of_obj
-from trulens_eval.util import jsonify
 from trulens_eval.util import JSONPath
 from trulens_eval.util import Method
 from trulens_eval.util import obj_id_of_obj
 from trulens_eval.util import SerialModel
 from trulens_eval.util import WithClassInfo
 
 T = TypeVar("T")
 
+logger = logging.getLogger(__name__)
+
 # Identifier types.
 
 RecordID = str
 AppID = str
 FeedbackDefinitionID = str
 FeedbackResultID = str
 
@@ -47,17 +59,49 @@
 
 class RecordAppCallMethod(SerialModel):
     path: JSONPath
     method: Method
 
 
 class Cost(SerialModel):
-    n_tokens: Optional[int] = None
-    cost: Optional[float] = None
+    # Number of requests.
+    n_requests: int = 0
+
+    # Number of successful ones.
+    n_successful_requests: int = 0
+
+    # Number of class scores retrieved.
+    n_classes: int = 0
+
+    # Total tokens processed.
+    n_tokens: int = 0
 
+    # Number of prompt tokens supplied.
+    n_prompt_tokens: int = 0
+
+    # Number of completion tokens generated.
+    n_completion_tokens: int = 0
+
+    # Cost in USD.
+    cost: float = 0.0
+
+    def __add__(self, other: 'Cost') -> 'Cost':
+        kwargs = {}
+        for k in self.__fields__.keys():
+            kwargs[k] = getattr(self, k) + getattr(other, k)
+        return Cost(**kwargs)
+
+    def __radd__(self, other: 'Cost') -> 'Cost':
+        # Makes sum work on lists of Cost.
+        
+        if other == 0:
+            return self
+        
+        return self.__add__(other)
+        
 
 class Perf(SerialModel):
     start_time: datetime
     end_time: datetime
 
     @property
     def latency(self):
@@ -122,29 +166,28 @@
         super().__init__(record_id="temporay", **kwargs)
 
         if record_id is None:
             record_id = obj_id_of_obj(self.dict(), prefix="record")
 
         self.record_id = record_id
 
-    # TODO: typing
-    def layout_calls_as_app(self) -> Any:
+    def layout_calls_as_app(self) -> JSON:
         """
         Layout the calls in this record into the structure that follows that of
         the app that created this record. This uses the paths stored in each
         `RecordAppCall` which are paths into the app.
 
-        Note: We cannot create a validated schema.py:App class (or subclass)
-        object here as the layout of records differ in these ways:
+        Note: We cannot create a validated schema.py:AppDefinitionclass (or
+        subclass) object here as the layout of records differ in these ways:
 
             - Records do not include anything that is not an instrumented method
               hence have most of the structure of a app missing.
         
-            - Records have RecordAppCall as their leafs where method
-              definitions would be in the App structure.
+            - Records have RecordAppCall as their leafs where method definitions
+              would be in the AppDefinitionstructure.
         """
 
         # TODO: problem: collissions
         ret = Bunch(**self.dict())
 
         for call in self.calls:
             frame_info = call.top(
@@ -157,29 +200,35 @@
 
         return ret
 
 
 # Feedback related:
 
 
-class Query:
+class Select:
 
     # Typing for type hints.
-    Query = JSONPath
+    Query: type = JSONPath
 
     # Instance for constructing queries for record json like `Record.app.llm`.
-    Record = Query().__record__
+    Record: Query = Query().__record__
 
     # Instance for constructing queries for app json.
-    App = Query().__app__
+    App: Query = Query().__app__
 
     # A App's main input and main output.
     # TODO: App input/output generalization.
-    RecordInput = Record.main_input
-    RecordOutput = Record.main_output
+    RecordInput: Query = Record.main_input
+    RecordOutput: Query = Record.main_output
+
+    RecordCalls: Query = Record.app
+
+
+# To deprecate in 1.0.0:
+Query = Select
 
 
 class FeedbackResultStatus(Enum):
     NONE = "none"
     RUNNING = "running"
     FAILED = "failed"
     DONE = "done"
@@ -224,45 +273,52 @@
                 self.dict(), prefix="feedback_result"
             )
 
         self.feedback_result_id = feedback_result_id
 
 
 class FeedbackDefinition(SerialModel):
+    # Serialized parts of a feedback function. The non-serialized parts are in
+    # the feedback.py:Feedback class.
+
     # Implementation serialization info.
     implementation: Optional[Union[Function, Method]] = None
 
     # Aggregator method for serialization.
     aggregator: Optional[Union[Function, Method]] = None
 
     # Id, if not given, unique determined from _json below.
     feedback_definition_id: FeedbackDefinitionID
 
     # Selectors, pointers into Records of where to get
     # arguments for `imp`.
-    selectors: Optional[Dict[str, JSONPath]] = None
+    selectors: Dict[str, JSONPath]
 
     def __init__(
         self,
         feedback_definition_id: Optional[FeedbackDefinitionID] = None,
         implementation: Optional[Union[Function, Method]] = None,
         aggregator: Optional[Union[Function, Method]] = None,
         selectors: Dict[str, JSONPath] = None
     ):
         """
         - selectors: Optional[Dict[str, JSONPath]] -- mapping of implementation
           argument names to where to get them from a record.
 
         - feedback_definition_id: Optional[str] - unique identifier.
 
-        - implementation:
+        - implementation: Optional[Union[Function, Method]] -- the serialized
+          implementation function.
 
-        - aggregator:
+        - aggregator: Optional[Union[Function, Method]] -- serialized
+          aggregation function.
         """
 
+        selectors = selectors or dict()
+
         super().__init__(
             feedback_definition_id="temporary",
             selectors=selectors,
             implementation=implementation,
             aggregator=aggregator,
         )
 
@@ -293,16 +349,16 @@
     WITH_APP_THREAD = "with_app_thread"
 
     # Evaluate later via the process started by
     # `tru.start_deferred_feedback_evaluator`.
     DEFERRED = "deferred"
 
 
-class App(SerialModel, WithClassInfo):
-    # Serialized fields here whereas tru_app.py:TruApp contains
+class AppDefinition(SerialModel, WithClassInfo, ABC):
+    # Serialized fields here whereas app.py:App contains
     # non-serialized fields.
 
     class Config:
         arbitrary_types_allowed = True
 
     app_id: AppID
 
@@ -311,15 +367,22 @@
     feedback_definitions: Sequence[FeedbackDefinition] = []
 
     # NOTE: Custom feedback functions cannot be run deferred and will be run as
     # if "withappthread" was set.
     feedback_mode: FeedbackMode = FeedbackMode.WITH_APP_THREAD
 
     # Class of the main instrumented object.
-    root_class: Class
+    root_class: Class  # TODO: make classvar
+
+    # App's main method. To be filled in by subclass. Want to make this abstract
+    # but this causes problems when trying to load an AppDefinition from json.
+    root_callable: ClassVar[FunctionOrMethod]
+
+    # Wrapped app in jsonized form.
+    app: JSON
 
     def __init__(
         self,
         app_id: Optional[AppID] = None,
         feedback_mode: FeedbackMode = FeedbackMode.WITH_APP_THREAD,
         **kwargs
     ):
@@ -333,7 +396,40 @@
 
         super().__init__(**kwargs)
 
         if app_id is None:
             app_id = obj_id_of_obj(obj=self.dict(), prefix="app")
 
         self.app_id = app_id
+
+    @classmethod
+    def select_inputs(cls) -> JSONPath:
+        """
+        Get the path to the main app's call inputs.
+        """
+
+        return getattr(
+            Select.RecordCalls,
+            cls.root_callable.default_factory().name
+        ).args
+
+    @classmethod
+    def select_outputs(cls) -> JSONPath:
+        """
+        Get the path to the main app's call outputs.
+        """
+
+        return getattr(
+            Select.RecordCalls,
+            cls.root_callable.default_factory().name
+        ).rets
+
+
+class App(AppDefinition):
+
+    def __init__(self, *args, **kwargs):
+        # Since 0.2.0
+        logger.warning(
+            "Class trulens_eval.schema.App is deprecated, "
+            "use trulens_eval.schema.AppDefinition instead."
+        )
+        super().__init__(*args, **kwargs)
```

## trulens_eval/tru.py

```diff
@@ -1,27 +1,25 @@
-from datetime import datetime
 import logging
 from multiprocessing import Process
 import os
 from pathlib import Path
 import subprocess
-import sys
 import threading
 from threading import Thread
 from time import sleep
 from typing import Iterable, List, Optional, Sequence, Union
 
 import pkg_resources
 
 from trulens_eval.schema import FeedbackResult
-from trulens_eval.schema import App
+from trulens_eval.schema import AppDefinition
 from trulens_eval.schema import Record
-from trulens_eval.tru_db import JSON
-from trulens_eval.tru_db import LocalSQLite
-from trulens_eval.tru_feedback import Feedback
+from trulens_eval.db import JSON
+from trulens_eval.db import LocalSQLite
+from trulens_eval.feedback import Feedback
 from trulens_eval.utils.notebook_utils import is_notebook, setup_widget_stdout_stderr
 from trulens_eval.util import SingletonPerName
 from trulens_eval.util import TP
 
 logger = logging.getLogger(__name__)
 
 
@@ -103,15 +101,15 @@
 
         return self.db.insert_record(record=record)
 
     def run_feedback_functions(
         self,
         record: Record,
         feedback_functions: Sequence[Feedback],
-        app: Optional[App] = None,
+        app: Optional[AppDefinition] = None,
     ) -> Sequence[JSON]:
         """
         Run a collection of feedback functions and report their result.
 
         Parameters:
 
             record (Record): The record on which to evaluate the feedback
@@ -152,15 +150,15 @@
                 TP().promise(lambda f: f.run(app=app, record=record), func)
             )
 
         evals = map(lambda p: p.get(), evals)
 
         return list(evals)
 
-    def add_app(self, app: App) -> None:
+    def add_app(self, app: AppDefinition) -> None:
         """
         Add a app to the database.        
         """
 
         self.db.insert_app(app=app)
 
     def add_feedback(
@@ -215,25 +213,25 @@
             if restart:
                 self.stop_evaluator()
             else:
                 raise RuntimeError(
                     "Evaluator is already running in this process."
                 )
 
-        from trulens_eval.tru_feedback import Feedback
+        from trulens_eval.feedback import Feedback
 
         if not fork:
             self.evaluator_stop = threading.Event()
 
         def runloop():
             while fork or not self.evaluator_stop.is_set():
-                print(
-                    "Looking for things to do. Stop me with `tru.stop_evaluator()`.",
-                    end=''
-                )
+                #print(
+                #    "Looking for things to do. Stop me with `tru.stop_evaluator()`.",
+                #    end=''
+                #)
                 Feedback.evaluate_deferred(tru=self)
                 TP().finish(timeout=10)
                 if fork:
                     sleep(10)
                 else:
                     self.evaluator_stop.wait(10)
 
@@ -288,15 +286,15 @@
                     "Dashboard not running in this workspace. "
                     "You may be able to shut other instances by setting the `force` flag."
                 )
 
             else:
                 print("Force stopping dashboard ...")
                 import os
-                import pwd
+                import pwd # PROBLEM: does not exist on windows
 
                 import psutil
                 username = pwd.getpwuid(os.getuid())[0]
                 for p in psutil.process_iter():
                     try:
                         cmd = " ".join(p.cmdline())
                         if "streamlit" in cmd and "Leaderboard.py" in cmd and p.username(
@@ -414,15 +412,15 @@
             args=(proc, proc.stderr, out_stderr, started)
         )
         Tru.dashboard_listener_stdout.start()
         Tru.dashboard_listener_stderr.start()
 
         Tru.dashboard_proc = proc
 
-        if not started.wait(timeout=5):
+        if not started.wait(timeout=30): # This might not work on windows.
             raise RuntimeError(
                 "Dashboard failed to start in time. "
                 "Please inspect dashboard logs for additional information."
             )
 
         return proc
```

## trulens_eval/tru_app.py

```diff
@@ -1,214 +1,14 @@
-"""
-Generalized root type for various libraries like llama_index and langchain .
-"""
+from trulens_eval import app
 
-from enum import Enum
 import logging
-from pprint import PrettyPrinter
-from typing import Any, Callable, Iterable, List, Optional, Sequence, Tuple
-
-from pydantic import Field
-
-from trulens_eval.instruments import Instrument
-from trulens_eval.schema import AppID
-from trulens_eval.schema import Cost
-from trulens_eval.schema import FeedbackDefinition
-from trulens_eval.schema import FeedbackMode
-from trulens_eval.schema import FeedbackResult
-from trulens_eval.schema import App
-from trulens_eval.schema import Perf
-from trulens_eval.schema import Query
-from trulens_eval.schema import Record
-from trulens_eval.tru import Tru
-from trulens_eval.tru_db import TruDB
-from trulens_eval.tru_feedback import Feedback
-from trulens_eval.util import Class
-from trulens_eval.util import instrumented_classes
-from trulens_eval.util import json_str_of_obj
-from trulens_eval.util import jsonify
-from trulens_eval.util import JSONPath
-from trulens_eval.util import obj_id_of_obj
-from trulens_eval.util import SerialModel
-from trulens_eval.util import TP
-from trulens_eval.util import WithClassInfo
 
 logger = logging.getLogger(__name__)
 
-pp = PrettyPrinter()
-
-# App component.
-COMPONENT = Any
-
-# Component category.
-# TODO: Enum
-COMPONENT_CATEGORY = str
-
-
-class TruApp(App, SerialModel):
-    """
-    Generalization of wrapped model.
-    """
-
-    # Non-serialized fields here while the serialized ones are defined in
-    # `schema.py:App`.
-
-    # Feedback functions to evaluate on each record.
-    feedbacks: Sequence[Feedback] = Field(exclude=True)
-
-    # Database interfaces for models/records/feedbacks.
-    # NOTE: Maybe move to schema.App .
-    tru: Optional[Tru] = Field(exclude=True)
-
-    # Database interfaces for models/records/feedbacks.
-    # NOTE: Maybe mobe to schema.App .
-    db: Optional[TruDB] = Field(exclude=True)
-
-    # The wrapped app.
-    app: Any = Field(exclude=True)
-
-    # Instrumentation class.
-    instrument: Instrument = Field(exclude=True)
-
-    def __init__(
-        self,
-        tru: Optional[Tru] = None,
-        feedbacks: Optional[Sequence[Feedback]] = None,
-        **kwargs
-    ):
-
-        feedbacks = feedbacks or []
-
-        # for us:
-        kwargs['tru'] = tru
-        kwargs['feedbacks'] = feedbacks
-
-        super().__init__(**kwargs)
-
-        if tru is None:
-            if self.feedback_mode != FeedbackMode.NONE:
-                logger.debug("Creating default tru.")
-                tru = Tru()
-        else:
-            if self.feedback_mode == FeedbackMode.NONE:
-                logger.warn(
-                    "`tru` is specified but `feedback_mode` is FeedbackMode.NONE. "
-                    "No feedback evaluation and logging will occur."
-                )
-
-        self.tru = tru
-        if self.tru is not None:
-            self.db = tru.db
-
-            if self.feedback_mode != FeedbackMode.NONE:
-                logger.debug(
-                    "Inserting app and feedback function definitions to db."
-                )
-                self.db.insert_app(app=self)
-                for f in self.feedbacks:
-                    self.db.insert_feedback_definition(f)
-
-        else:
-            if len(feedbacks) > 0:
-                raise ValueError(
-                    "Feedback logging requires `tru` to be specified."
-                )
-
-        self.instrument.instrument_object(
-            obj=self.app, query=Query.Query().model
-        )
-
-    def json(self, *args, **kwargs):
-        # Need custom jsonification here because it is likely the model
-        # structure contains loops.
-
-        return json_str_of_obj(self.dict(), *args, **kwargs)
-
-    def dict(self):
-        # Same problem as in json.
-        return jsonify(self, instrument=self.instrument)
-
-    def _post_record(
-        self, ret_record_args, error, total_tokens, total_cost, start_time,
-        end_time, record
-    ):
-        """
-        Final steps of record construction common among model types.
-        """
-
-        ret_record_args['main_error'] = str(error)
-        ret_record_args['calls'] = record
-        ret_record_args['cost'] = Cost(n_tokens=total_tokens, cost=total_cost)
-        ret_record_args['perf'] = Perf(start_time=start_time, end_time=end_time)
-        ret_record_args['app_id'] = self.app_id
-
-        ret_record = Record(**ret_record_args)
-
-        if error is not None:
-            if self.feedback_mode == FeedbackMode.WITH_APP:
-                self._handle_error(record=ret_record, error=error)
-
-            elif self.feedback_mode in [FeedbackMode.DEFERRED,
-                                        FeedbackMode.WITH_APP_THREAD]:
-                TP().runlater(
-                    self._handle_error, record=ret_record, error=error
-                )
-
-            raise error
-
-        if self.feedback_mode == FeedbackMode.WITH_APP:
-            self._handle_record(record=ret_record)
-
-        elif self.feedback_mode in [FeedbackMode.DEFERRED,
-                                    FeedbackMode.WITH_APP_THREAD]:
-            TP().runlater(self._handle_record, record=ret_record)
-
-        return ret_record
-
-    def _handle_record(self, record: Record):
-        """
-        Write out record-related info to database if set.
-        """
-
-        if self.tru is None or self.feedback_mode is None:
-            return
-
-        record_id = self.tru.add_record(record=record)
-
-        if len(self.feedbacks) == 0:
-            return
-
-        # Add empty (to run) feedback to db.
-        if self.feedback_mode == FeedbackMode.DEFERRED:
-            for f in self.feedbacks:
-                self.db.insert_feedback(
-                    FeedbackResult(
-                        name=f.name,
-                        record_id=record_id,
-                        feedback_definition_id=f.feedback_definition_id
-                    )
-                )
-
-        elif self.feedback_mode in [FeedbackMode.WITH_APP,
-                                    FeedbackMode.WITH_APP_THREAD]:
-
-            results = self.tru.run_feedback_functions(
-                record=record, feedback_functions=self.feedbacks, app=self
-            )
-
-            for result in results:
-                self.tru.add_feedback(result)
-
-    def _handle_error(self, record: Record, error: Exception):
-        if self.db is None:
-            return
-
-    def instrumented(
-        self, categorizer: Callable[[Class], Iterable[COMPONENT_CATEGORY]]
-    ) -> Iterable[Tuple[JSONPath, List[COMPONENT_CATEGORY]]]:
-        # Enumerate instrumented components:
-
-        from trulens_eval.utils.langchain import Is
-
-        for q, ci, obj in instrumented_classes(jsonify(
-                self.app, instrument=self.instrument)):
-            yield (q, list(categorizer(ci)))
+for attr in dir(app):
+    if not attr.startswith("_"):
+        globals()[attr] = getattr(app, attr)
+
+# Since 0.2.0
+logger.warning(
+    "`trulens_eval.tru_app` is deprecated, use `trulens_eval.app` instead."
+)
```

## trulens_eval/tru_chain.py

```diff
@@ -1,35 +1,38 @@
 """
 # Langchain instrumentation and monitoring.
 """
 
 from datetime import datetime
 import logging
 from pprint import PrettyPrinter
-from typing import Any, Dict, List, Sequence, Union
+from typing import Any, ClassVar, Dict, List, Sequence, Union
 
+from pydantic import Field
+
+from trulens_eval.app import App
 from trulens_eval.instruments import Instrument
+from trulens_eval.provider_apis import Endpoint
+from trulens_eval.schema import Cost
 from trulens_eval.schema import RecordAppCall
-from trulens_eval.tru_app import TruApp
 from trulens_eval.util import Class
+from trulens_eval.util import FunctionOrMethod
 from trulens_eval.util import jsonify
 from trulens_eval.util import noserio
 from trulens_eval.util import OptionalImports
 from trulens_eval.util import REQUIREMENT_LANGCHAIN
-from trulens_eval.utils.langchain import Is
 
 logger = logging.getLogger(__name__)
 
 pp = PrettyPrinter()
 
 with OptionalImports(message=REQUIREMENT_LANGCHAIN):
     import langchain
     from langchain.callbacks import get_openai_callback
     from langchain.chains.base import Chain
-    import test_this_does_not_exist
 
 
 class LangChainInstrument(Instrument):
 
     class Default:
         MODULES = {"langchain."}
 
@@ -87,30 +90,36 @@
 
         safe_type._instrumented = prop
         new_prop = property(fget=safe_type)
 
         return new_prop
 
 
-class TruChain(TruApp):
+class TruChain(App):
     """
     Wrap a langchain Chain to capture its configuration and evaluation steps. 
     """
 
     app: Chain
 
+    root_callable: ClassVar[FunctionOrMethod] = Field(
+        default_factory=lambda: FunctionOrMethod.of_callable(TruChain._call),
+        const=True
+    )
+
     # Normally pydantic does not like positional args but chain here is
     # important enough to make an exception.
     def __init__(self, app: Chain, **kwargs):
         """
         Wrap a langchain chain for monitoring.
 
         Arguments:
         - app: Chain -- the chain to wrap.
-        - More args in TruApp
+        - More args in App
+        - More args in AppDefinition
         - More args in WithClassInfo
         """
 
         super().update_forward_refs()
 
         # TruChain specific:
         kwargs['app'] = app
@@ -146,30 +155,28 @@
         # Wrapped calls will look this up by traversing the call stack. This
         # should work with threads.
         record: Sequence[RecordAppCall] = []
 
         ret = None
         error = None
 
-        total_tokens = None
-        total_cost = None
+        cost: Cost = Cost()
 
         start_time = None
         end_time = None
 
         try:
             # TODO: do this only if there is an openai model inside the chain:
             with get_openai_callback() as cb:
                 start_time = datetime.now()
-                ret = self.app.__call__(inputs=inputs, **kwargs)
+                ret, cost = Endpoint.track_all_costs_tally(
+                    lambda: self.app.__call__(inputs=inputs, **kwargs)
+                )
                 end_time = datetime.now()
 
-            total_tokens = cb.total_tokens
-            total_cost = cb.total_cost
-
         except BaseException as e:
             end_time = datetime.now()
             error = e
             logger.error(f"App raised an exception: {e}")
 
         assert len(record) > 0, "No information recorded in call."
 
@@ -183,16 +190,15 @@
         output_key = self.output_keys[0]
 
         ret_record_args['main_input'] = inputs[input_key]
         if ret is not None:
             ret_record_args['main_output'] = ret[output_key]
 
         ret_record = self._post_record(
-            ret_record_args, error, total_tokens, total_cost, start_time,
-            end_time, record
+            ret_record_args, error, cost, start_time, end_time, record
         )
 
         return ret, ret_record
 
     # langchain.chains.base.py:Chain
     def __call__(self, *args, **kwargs) -> Dict[str, Any]:
         """
@@ -204,10 +210,7 @@
 
         return ret
 
     # Chain requirement
     # TODO(piotrm): figure out whether the combination of _call and __call__ is working right.
     def _call(self, *args, **kwargs) -> Any:
         return self.app._call(*args, **kwargs)
-
-    def instrumented(self):
-        return super().instrumented(categorizer=Is.what)
```

## trulens_eval/tru_db.py

```diff
@@ -1,611 +1,14 @@
-import abc
-from datetime import datetime
-import json
-import logging
-from pathlib import Path
-from pprint import PrettyPrinter
-import sqlite3
-from typing import Dict, Iterable, List, Optional, Sequence, Tuple
-
-import pydantic
-from frozendict import frozendict
-from merkle_json import MerkleJson
-import numpy as np
-import pandas as pd
-
-from trulens_eval import __version__
-from trulens_eval.schema import AppID
-from trulens_eval.schema import Cost
-from trulens_eval.schema import FeedbackDefinition
-from trulens_eval.schema import FeedbackDefinitionID
-from trulens_eval.schema import FeedbackResult
-from trulens_eval.schema import FeedbackResultID
-from trulens_eval.schema import FeedbackResultStatus
-from trulens_eval.schema import JSONPath
-from trulens_eval.schema import App
-from trulens_eval.schema import Perf
-from trulens_eval.schema import Record
-from trulens_eval.schema import RecordAppCall
-from trulens_eval.schema import RecordID
-from trulens_eval.util import all_queries
-from trulens_eval.util import GetItemOrAttribute
-from trulens_eval.util import JSON
-from trulens_eval.util import json_str_of_obj
-from trulens_eval.util import JSONPath
-from trulens_eval.util import SerialModel
-from trulens_eval.util import UNCIODE_YIELD
-from trulens_eval.util import UNICODE_CHECK
-
-mj = MerkleJson()
-NoneType = type(None)
+from trulens_eval import db
 
-pp = PrettyPrinter()
+import logging
 
 logger = logging.getLogger(__name__)
 
-
-class DBMeta(pydantic.BaseModel):
-    """
-    Databasae meta data mostly used for migrating from old db schemas.
-    """
-
-    trulens_version: Optional[str]
-    attributes: dict
-
-
-class TruDB(SerialModel, abc.ABC):
-
-    @abc.abstractmethod
-    def reset_database(self):
-        """
-        Delete all data.
-        """
-
-        raise NotImplementedError()
-
-    @abc.abstractmethod
-    def insert_record(
-        self,
-        record: Record,
-    ) -> RecordID:
-        """
-        Insert a new `record` into db, indicating its `app` as well. Return
-        record id.
-
-        Args:
-        - record: Record
-        """
-
-        raise NotImplementedError()
-
-    @abc.abstractmethod
-    def insert_app(self, app: App) -> AppID:
-        """
-        Insert a new `app` into db under the given `app_id`. 
-
-        Args:
-        - app: App - App definition. 
-        """
-
-        raise NotImplementedError()
-
-    @abc.abstractmethod
-    def insert_feedback_definition(
-        self, feedback_definition: FeedbackDefinition
-    ) -> FeedbackDefinitionID:
-        """
-        Insert a feedback definition into the db.
-        """
-
-        raise NotImplementedError()
-
-    @abc.abstractmethod
-    def insert_feedback(
-        self,
-        feedback_result: FeedbackResult,
-    ) -> FeedbackResultID:
-        """
-        Insert a feedback record into the db.
-
-        Args:
-
-        - feedback_result: FeedbackResult
-        """
-
-        raise NotImplementedError()
-
-    @abc.abstractmethod
-    def get_records_and_feedback(
-        self, app_ids: List[str]
-    ) -> Tuple[pd.DataFrame, Sequence[str]]:
-        """
-        Get the records logged for the given set of `app_ids` (otherwise all)
-        alongside the names of the feedback function columns listed the
-        dataframe.
-        """
-        raise NotImplementedError()
-
-
-class LocalSQLite(TruDB):
-    filename: Path
-
-    TABLE_META = "meta"
-    TABLE_RECORDS = "records"
-    TABLE_FEEDBACKS = "feedbacks"
-    TABLE_FEEDBACK_DEFS = "feedback_defs"
-    TABLE_APPS = "apps"
-
-    TYPE_TIMESTAMP = "FLOAT"
-    TYPE_ENUM = "TEXT"
-
-    TABLES = [TABLE_RECORDS, TABLE_FEEDBACKS, TABLE_FEEDBACK_DEFS, TABLE_APPS]
-
-    def __init__(self, filename: Path):
-        """
-        Database locally hosted using SQLite.
-
-        Args
-        
-        - filename: Optional[Path] -- location of sqlite database dump
-          file. It will be created if it does not exist.
-
-        """
-        super().__init__(filename=filename)
-
-        self._build_tables()
-
-    def __str__(self) -> str:
-        return f"SQLite({self.filename})"
-
-    # TruDB requirement
-    def reset_database(self) -> None:
-        self._drop_tables()
-        self._build_tables()
-
-    def _clear_tables(self) -> None:
-        conn, c = self._connect()
-
-        for table in self.TABLES:
-            c.execute(f'''DELETE FROM {table}''')
-
-        self._close(conn)
-
-    def _drop_tables(self) -> None:
-        conn, c = self._connect()
-
-        for table in self.TABLES:
-            c.execute(f'''DROP TABLE IF EXISTS {table}''')
-
-        self._close(conn)
-
-    def get_meta(self):
-        conn, c = self._connect()
-
-        try:
-            c.execute(f'''SELECT key, value from {self.TABLE_META}''')
-            rows = c.fetchall()
-
-            ret = {}
-
-            for row in rows:
-                ret[row[0]] = row[1]
-
-            if 'trulens_version' in ret:
-                trulens_version = ret['trulens_version']
-            else:
-                trulens_version = None
-
-            return DBMeta(trulens_version=trulens_version, attributes=ret)
-
-        except Exception as e:
-            return DBMeta(trulens_version=None, attributes={})
-
-    def _build_tables(self):
-        conn, c = self._connect()
-
-        # Create table if it does not exist. Note that the record_json column
-        # also encodes inside it all other columns.
-
-        meta = self.get_meta()
-
-        c.execute(
-            f'''CREATE TABLE IF NOT EXISTS {self.TABLE_META} (
-                key TEXT NOT NULL PRIMARY KEY,
-                value TEXT
-            )'''
-        )
-
-        if meta.trulens_version is None:
-            # migrate from pre-version-tracked database
-            # print(f"Migrating DB {self.filename} from trulens_version {meta.trulens_version} to {__version__}.")
-            c.execute(
-                f'''INSERT INTO {self.TABLE_META} VALUES (?, ?)''',
-                ('trulens_version', __version__)
-            )
-
-        c.execute(
-            f'''CREATE TABLE IF NOT EXISTS {self.TABLE_RECORDS} (
-                record_id TEXT NOT NULL PRIMARY KEY,
-                app_id TEXT NOT NULL,
-                input TEXT,
-                output TEXT,
-                record_json TEXT NOT NULL,
-                tags TEXT NOT NULL,
-                ts {self.TYPE_TIMESTAMP} NOT NULL,
-                cost_json TEXT NOT NULL,
-                perf_json TEXT NOT NULL
-            )'''
-        )
-        c.execute(
-            f'''CREATE TABLE IF NOT EXISTS {self.TABLE_FEEDBACKS} (
-                feedback_result_id TEXT NOT NULL PRIMARY KEY,
-                record_id TEXT NOT NULL,
-                feedback_definition_id TEXT,
-                last_ts {self.TYPE_TIMESTAMP} NOT NULL,
-                status {self.TYPE_ENUM} NOT NULL,
-                error TEXT,
-                calls_json TEXT NOT NULL,
-                result FLOAT,
-                name TEXT NOT NULL,
-                cost_json TEXT NOT NULL
-            )'''
-        )
-        c.execute(
-            f'''CREATE TABLE IF NOT EXISTS {self.TABLE_FEEDBACK_DEFS} (
-                feedback_definition_id TEXT NOT NULL PRIMARY KEY,
-                feedback_json TEXT NOT NULL
-            )'''
-        )
-        c.execute(
-            f'''CREATE TABLE IF NOT EXISTS {self.TABLE_APPS} (
-                app_id TEXT NOT NULL PRIMARY KEY,
-                app_json TEXT NOT NULL
-            )'''
-        )
-        self._close(conn)
-
-    def _connect(self) -> Tuple[sqlite3.Connection, sqlite3.Cursor]:
-        conn = sqlite3.connect(self.filename)
-        c = conn.cursor()
-        return conn, c
-
-    def _close(self, conn: sqlite3.Connection) -> None:
-        conn.commit()
-        conn.close()
-
-    # TruDB requirement-
-    def insert_record(
-        self,
-        record: Record,
-    ) -> RecordID:
-        # NOTE: Oddness here in that the entire record is put into the
-        # record_json column while some parts of that records are also put in
-        # other columns. Might want to keep this so we can query on the columns
-        # within sqlite.
-
-        vals = (
-            record.record_id, record.app_id, record.main_input,
-            record.main_output, json_str_of_obj(record), record.tags, record.ts,
-            json_str_of_obj(record.cost), json_str_of_obj(record.perf)
-        )
-
-        self._insert_or_replace_vals(table=self.TABLE_RECORDS, vals=vals)
-
-        print(
-            f"{UNICODE_CHECK} record {record.record_id} from {record.app_id} -> {self.filename}"
-        )
-
-        return record.record_id
-
-    # TruDB requirement
-    def insert_app(self, app: App) -> AppID:
-        app_id = app.app_id
-        app_str = app.json()
-
-        vals = (app_id, app_str)
-        self._insert_or_replace_vals(table=self.TABLE_APPS, vals=vals)
-
-        print(f"{UNICODE_CHECK} app {app_id} -> {self.filename}")
-
-        return app_id
-
-    def insert_feedback_definition(
-        self, feedback: FeedbackDefinition
-    ) -> FeedbackDefinitionID:
-        """
-        Insert a feedback definition into the database.
-        """
-
-        feedback_definition_id = feedback.feedback_definition_id
-        feedback_str = feedback.json()
-        vals = (feedback_definition_id, feedback_str)
-
-        self._insert_or_replace_vals(table=self.TABLE_FEEDBACK_DEFS, vals=vals)
-
-        print(
-            f"{UNICODE_CHECK} feedback def. {feedback_definition_id} -> {self.filename}"
-        )
-
-        return feedback_definition_id
-
-    def get_feedback_defs(
-        self, feedback_definition_id: Optional[str] = None
-    ) -> pd.DataFrame:
-
-        clause = ""
-        args = ()
-        if feedback_definition_id is not None:
-            clause = "WHERE feedback_id=?"
-            args = (feedback_definition_id,)
-
-        query = f"""
-            SELECT
-                feedback_definition_id, feedback_json
-            FROM {self.TABLE_FEEDBACK_DEFS}
-            {clause}
-        """
-
-        conn, c = self._connect()
-        c.execute(query, args)
-        rows = c.fetchall()
-        self._close(conn)
-
-        df = pd.DataFrame(
-            rows, columns=[description[0] for description in c.description]
-        )
-
-        return df
-
-    def _insert_or_replace_vals(self, table, vals):
-        conn, c = self._connect()
-        c.execute(
-            f"""INSERT OR REPLACE INTO {table}
-                VALUES ({','.join('?' for _ in vals)})""", vals
-        )
-        self._close(conn)
-
-    def insert_feedback(
-        self, feedback_result: FeedbackResult
-    ) -> FeedbackResultID:
-        """
-        Insert a record-feedback link to db or update an existing one.
-        """
-
-        vals = (
-            feedback_result.feedback_result_id,
-            feedback_result.record_id,
-            feedback_result.feedback_definition_id,
-            feedback_result.last_ts.timestamp(),
-            feedback_result.status.value,
-            feedback_result.error,
-            json_str_of_obj(dict(calls=feedback_result.calls)
-                           ),  # extra dict is needed json's root must be a dict
-            feedback_result.result,
-            feedback_result.name,
-            json_str_of_obj(feedback_result.cost)
-        )
-
-        self._insert_or_replace_vals(table=self.TABLE_FEEDBACKS, vals=vals)
-
-        if feedback_result.status == FeedbackResultStatus.DONE:
-            print(
-                f"{UNICODE_CHECK} feedback {feedback_result.feedback_result_id} on {feedback_result.record_id} -> {self.filename}"
-            )
-        else:
-            print(
-                f"{UNCIODE_YIELD} feedback {feedback_result.feedback_result_id} on {feedback_result.record_id} -> {self.filename}"
-            )
-
-    def get_feedback(
-        self,
-        record_id: Optional[RecordID] = None,
-        feedback_result_id: Optional[FeedbackResultID] = None,
-        feedback_definition_id: Optional[FeedbackDefinitionID] = None,
-        status: Optional[FeedbackResultStatus] = None,
-        last_ts_before: Optional[datetime] = None
-    ) -> pd.DataFrame:
-
-        clauses = []
-        vars = []
-
-        if record_id is not None:
-            clauses.append("record_id=?")
-            vars.append(record_id)
-
-        if feedback_result_id is not None:
-            clauses.append("f.feedback_result_id=?")
-            vars.append(feedback_result_id)
-
-        if feedback_definition_id is not None:
-            clauses.append("f.feedback_definition_id=?")
-            vars.append(feedback_definition_id)
-
-        if status is not None:
-            if isinstance(status, Sequence):
-                clauses.append(
-                    "f.status in (" + (",".join(["?"] * len(status))) + ")"
-                )
-                for v in status:
-                    vars.append(v.value)
-            else:
-                clauses.append("f.status=?")
-                vars.append(status)
-
-        if last_ts_before is not None:
-            clauses.append("f.last_ts<=?")
-            vars.append(last_ts_before.timestamp())
-
-        where_clause = " AND ".join(clauses)
-        if len(where_clause) > 0:
-            where_clause = " AND " + where_clause
-
-        query = f"""
-            SELECT
-                f.record_id, f.feedback_result_id, f.feedback_definition_id, 
-                f.last_ts,
-                f.status,
-                f.error,
-                f.name,
-                f.result, 
-                f.cost_json,
-                r.perf_json,
-                f.calls_json,
-                fd.feedback_json, 
-                r.record_json, 
-                c.app_json
-            FROM {self.TABLE_RECORDS} r
-                JOIN {self.TABLE_FEEDBACKS} f 
-                JOIN {self.TABLE_FEEDBACK_DEFS} fd
-                JOIN {self.TABLE_APPS} c
-            WHERE f.feedback_definition_id=fd.feedback_definition_id
-                AND r.record_id=f.record_id
-                AND r.app_id=c.app_id
-                {where_clause}
-        """
-
-        conn, c = self._connect()
-        c.execute(query, vars)
-        rows = c.fetchall()
-        self._close(conn)
-
-        df = pd.DataFrame(
-            rows, columns=[description[0] for description in c.description]
-        )
-
-        def map_row(row):
-            # NOTE: pandas dataframe will take in the various classes below but the
-            # agg table used in UI will not like it. Sending it JSON/dicts instead.
-
-            row.calls_json = json.loads(
-                row.calls_json
-            )['calls']  # calls_json (sequence of FeedbackCall)
-            row.cost_json = json.loads(row.cost_json)  # cost_json (Cost)
-            row.perf_json = json.loads(row.perf_json)  # perf_json (Perf)
-            row.feedback_json = json.loads(
-                row.feedback_json
-            )  # feedback_json (FeedbackDefinition)
-            row.record_json = json.loads(
-                row.record_json
-            )  # record_json (Record)
-            row.app_json = json.loads(row.app_json)  # app_json (App)
-            app = App(**row.app_json)
-
-            row.status = FeedbackResultStatus(row.status)
-
-            row['latency'] = Perf(**row.perf_json).latency
-            row['total_tokens'] = row.cost_json['n_tokens']
-            row['total_cost'] = row.cost_json['cost']
-
-            row['type'] = app.root_class
-
-            return row
-
-        df = df.apply(map_row, axis=1)
-
-        return pd.DataFrame(df)
-
-    def get_app(self, app_id: str) -> JSON:
-        conn, c = self._connect()
-        c.execute(
-            f"SELECT app_json FROM {self.TABLE_APPS} WHERE app_id=?", (app_id,)
-        )
-        result = c.fetchone()[0]
-        conn.close()
-
-        return json.loads(result)
-
-    def get_records_and_feedback(
-        self,
-        app_ids: Optional[List[str]] = None
-    ) -> Tuple[pd.DataFrame, Sequence[str]]:
-        # This returns all apps if the list of app_ids is empty.
-        app_ids = app_ids or []
-
-        conn, c = self._connect()
-        query = f"""
-            SELECT r.record_id, f.calls_json, f.result, f.name
-            FROM {self.TABLE_RECORDS} r 
-            LEFT JOIN {self.TABLE_FEEDBACKS} f
-                ON r.record_id = f.record_id
-            """
-        if len(app_ids) > 0:
-            app_id_list = ', '.join('?' * len(app_ids))
-            query = query + f" WHERE r.app_id IN ({app_id_list})"
-
-        c.execute(query)
-        rows = c.fetchall()
-        conn.close()
-
-        df_results = pd.DataFrame(
-            rows, columns=[description[0] for description in c.description]
-        )
-
-        if len(df_results) == 0:
-            return df_results, []
-
-        conn, c = self._connect()
-        query = f"""
-            SELECT DISTINCT r.*, c.app_json
-            FROM {self.TABLE_RECORDS} r 
-            JOIN {self.TABLE_APPS} c
-                ON r.app_id = c.app_id
-            """
-        if len(app_ids) > 0:
-            app_id_list = ', '.join('?' * len(app_ids))
-            query = query + f" WHERE r.app_id IN ({app_id_list})"
-
-        c.execute(query)
-        rows = c.fetchall()
-        conn.close()
-
-        df_records = pd.DataFrame(
-            rows, columns=[description[0] for description in c.description]
-        )
-        apps = df_records['app_json'].apply(App.parse_raw)
-        df_records['type'] = apps.apply(lambda row: str(row.root_class))
-
-        cost = df_records['cost_json'].map(Cost.parse_raw)
-        df_records['total_tokens'] = cost.map(lambda v: v.n_tokens)
-        df_records['total_cost'] = cost.map(lambda v: v.cost)
-
-        perf = df_records['perf_json'].apply(Perf.parse_raw)
-        df_records['latency'] = perf.apply(lambda p: p.latency)
-
-        if len(df_records) == 0:
-            return df_records, []
-
-        result_cols = set()
-
-        def expand_results(row):
-            if row['name'] is not None:
-                result_cols.add(row['name'])
-                row[row['name']] = row.result
-                row[row['name'] + "_calls"] = json.loads(row.calls_json
-                                                        )['calls']
-
-            return pd.Series(row)
-
-        df_results = df_results.apply(expand_results, axis=1)
-        df_results = df_results.drop(columns=["name", "result", "calls_json"])
-
-        def nonempty(val):
-            if isinstance(val, float):
-                return not np.isnan(val)
-            return True
-
-        def merge_feedbacks(vals):
-            ress = list(filter(nonempty, vals))
-            if len(ress) > 0:
-                return ress[0]
-            else:
-                return np.nan
-
-        df_results = df_results.groupby("record_id").agg(merge_feedbacks
-                                                        ).reset_index()
-
-        assert "record_id" in df_results.columns
-        assert "record_id" in df_records.columns
-
-        combined_df = df_records.merge(df_results, on=['record_id'])
-
-        return combined_df, list(result_cols)
+for attr in dir(db):
+    if not attr.startswith("_"):
+        globals()[attr] = getattr(db, attr)
+
+# Since 0.2.0
+logger.warning(
+    "`trulens_eval.tru_db` is deprecated, use `trulens_eval.db` instead."
+)
```

## trulens_eval/tru_feedback.py

```diff
@@ -1,895 +1,14 @@
-"""
-# Feedback Functions
-"""
+from trulens_eval import feedback
 
-from datetime import datetime
-from inspect import Signature
-from inspect import signature
-import itertools
 import logging
-from multiprocessing.pool import AsyncResult
-import re
-from time import sleep
-from typing import (
-    Any, Callable, Dict, Iterable, List, Optional, Sequence, Tuple, Type, Union
-)
-
-import numpy as np
-import openai
-import pydantic
-from tqdm.auto import tqdm
-
-from trulens_eval import feedback_prompts
-from trulens_eval.keys import *
-from trulens_eval.provider_apis import Endpoint
-from trulens_eval.schema import Cost
-from trulens_eval.schema import FeedbackCall
-from trulens_eval.schema import FeedbackDefinition
-from trulens_eval.schema import FeedbackResult
-from trulens_eval.schema import FeedbackResultID
-from trulens_eval.schema import FeedbackResultStatus
-from trulens_eval.schema import App
-from trulens_eval.schema import Query
-from trulens_eval.tru_db import JSON
-from trulens_eval.tru_db import Record
-from trulens_eval.util import FunctionOrMethod
-from trulens_eval.util import jsonify
-from trulens_eval.util import OptionalImports
-from trulens_eval.util import REQUIREMENT_LANGCHAIN
-from trulens_eval.util import SerialModel
-from trulens_eval.util import TP
-
-with OptionalImports(message=REQUIREMENT_LANGCHAIN):
-    from langchain.callbacks import get_openai_callback
-
-PROVIDER_CLASS_NAMES = ['OpenAI', 'Huggingface', 'Cohere']
-
-default_pass_fail_color_threshold = 0.5
 
 logger = logging.getLogger(__name__)
 
-
-def check_provider(cls_or_name: Union[Type, str]) -> None:
-    if isinstance(cls_or_name, str):
-        cls_name = cls_or_name
-    else:
-        cls_name = cls_or_name.__name__
-
-    assert cls_name in PROVIDER_CLASS_NAMES, f"Unsupported provider class {cls_name}"
-
-
-class Feedback(FeedbackDefinition):
-    # Implementation, not serializable, note that FeedbackDefinition contains
-    # `implementation` mean to serialize the below.
-    imp: Optional[Callable] = pydantic.Field(exclude=True)
-
-    # Aggregator method for feedback functions that produce more than one
-    # result.
-    agg: Optional[Callable] = pydantic.Field(exclude=True)
-
-    def __init__(
-        self,
-        imp: Optional[Callable] = None,
-        agg: Optional[Callable] = None,
-        **kwargs
-    ):
-        """
-        A Feedback function container.
-
-        Parameters:
-        
-        - imp: Optional[Callable] -- implementation of the feedback function.
-        """
-
-        agg = agg or np.mean
-
-        if imp is not None:
-            try:
-                # These are for serialization to/from json and for db storage.
-                kwargs['implementation'] = FunctionOrMethod.of_callable(
-                    imp, loadable=True
-                )
-            except:
-                # User defined functions in script do not have a module so cannot be serialized
-                pass
-        else:
-            if "implementation" in kwargs:
-                imp: Callable = FunctionOrMethod.pick(
-                    **(kwargs['implementation'])
-                ).load()
-
-        if agg is not None:
-            try:
-                # These are for serialization to/from json and for db storage.
-                kwargs['aggregator'] = FunctionOrMethod.of_callable(
-                    agg, loadable=True
-                )
-            except:
-                # User defined functions in script do not have a module so cannot be serialized
-                pass
-        else:
-            if 'arrgregator' in kwargs:
-                agg: Callable = FunctionOrMethod.pick(**(kwargs['aggregator'])
-                                                     ).load()
-
-        super().__init__(**kwargs)
-
-        self.imp = imp
-        self.agg = agg
-
-        # Verify that `imp` expects the arguments specified in `selectors`:
-        if self.imp is not None and self.selectors is not None:
-            sig: Signature = signature(self.imp)
-            for argname in self.selectors.keys():
-                assert argname in sig.parameters, (
-                    f"{argname} is not an argument to {self.imp.__name__}. "
-                    f"Its arguments are {list(sig.parameters.keys())}."
-                )
-
-    @staticmethod
-    def evaluate_deferred(tru: 'Tru'):
-        db = tru.db
-
-        def prepare_feedback(row):
-            record_json = row.record_json
-            record = Record(**record_json)
-
-            app_json = row.app_json
-
-            feedback = Feedback(**row.feedback_json)
-            feedback.run_and_log(
-                record=record,
-                app=app_json,
-                tru=tru,
-                feedback_result_id=row.feedback_result_id
-            )
-
-        feedbacks = db.get_feedback()
-
-        for i, row in feedbacks.iterrows():
-            if row.status == FeedbackResultStatus.NONE:
-                tqdm.write(f"Starting run for row {i}.")
-
-                TP().runlater(prepare_feedback, row)
-
-            elif row.status in [FeedbackResultStatus.RUNNING]:
-                now = datetime.now().timestamp()
-                if now - row.last_ts > 30:
-                    tqdm.write(
-                        f"Incomplete row {i} last made progress over 30 seconds ago. Retrying."
-                    )
-                    TP().runlater(prepare_feedback, row)
-
-                else:
-                    tqdm.write(
-                        f"Incomplete row {i} last made progress less than 30 seconds ago. Giving it more time."
-                    )
-
-            elif row.status in [FeedbackResultStatus.FAILED]:
-                now = datetime.now().timestamp()
-                if now - row.last_ts > 60 * 5:
-                    tqdm.write(
-                        f"Failed row {i} last made progress over 5 minutes ago. Retrying."
-                    )
-                    TP().runlater(prepare_feedback, row)
-
-                else:
-                    tqdm.write(
-                        f"Failed row {i} last made progress less than 5 minutes ago. Not touching it for now."
-                    )
-
-            elif row.status == FeedbackResultStatus.DONE:
-                pass
-
-    def __call__(self, *args, **kwargs) -> Any:
-        assert self.imp is not None, "Feedback definition needs an implementation to call."
-        return self.imp(*args, **kwargs)
-
-    def aggregate(self, func: Callable) -> 'Feedback':
-        return Feedback(imp=self.imp, selectors=self.selectors, agg=func)
-
-    @staticmethod
-    def of_feedback_definition(f: FeedbackDefinition):
-        implementation = f.implementation
-        aggregator = f.aggregator
-
-        imp_func = implementation.load()
-        agg_func = aggregator.load()
-
-        return Feedback(imp=imp_func, agg=agg_func, **f.dict())
-
-    def on_prompt(self, arg: str = "text"):
-        """
-        Create a variant of `self` that will take in the main app input or
-        "prompt" as input, sending it as an argument `arg` to implementation.
-        """
-
-        return Feedback(
-            imp=self.imp, selectors={arg: Query.RecordInput}, agg=self.agg
-        )
-
-    on_input = on_prompt
-
-    def on_response(self, arg: str = "text"):
-        """
-        Create a variant of `self` that will take in the main app output or
-        "response" as input, sending it as an argument `arg` to implementation.
-        """
-
-        return Feedback(
-            imp=self.imp, selectors={arg: Query.RecordOutput}, agg=self.agg
-        )
-
-    on_output = on_response
-
-    def on(self, **selectors):
-        """
-        Create a variant of `self` with the same implementation but the given `selectors`.
-        """
-
-        return Feedback(imp=self.imp, selectors=selectors, agg=self.agg)
-
-    def run(self, app: Union[App, JSON], record: Record) -> FeedbackResult:
-        """
-        Run the feedback function on the given `record`. The `app` that
-        produced the record is also required to determine input/output argument
-        names.
-
-        Might not have a App here but only the serialized app_json .
-        """
-
-        if isinstance(app, App):
-            app_json = jsonify(app)
-        else:
-            app_json = app
-
-        result_vals = []
-
-        feedback_calls = []
-
-        feedback_result = FeedbackResult(
-            feedback_definition_id=self.feedback_definition_id,
-            record_id=record.record_id,
-            name=self.name
-        )
-
-        try:
-            total_tokens = 0
-            total_cost = 0.0
-
-            for ins in self.extract_selection(app=app_json, record=record):
-
-                # TODO: Do this only if there is an openai model inside the app:
-                # NODE: This only works for langchain uses of openai.
-                with get_openai_callback() as cb:
-                    result_val = self.imp(**ins)
-                    result_vals.append(result_val)
-
-                    feedback_call = FeedbackCall(args=ins, ret=result_val)
-                    feedback_calls.append(feedback_call)
-
-                    total_tokens += cb.total_tokens
-                    total_cost += cb.total_cost
-
-            result_vals = np.array(result_vals)
-            result = self.agg(result_vals)
-
-            feedback_result.update(
-                result=result,
-                status=FeedbackResultStatus.DONE,
-                cost=Cost(n_tokens=total_tokens, cost=total_cost),
-                calls=feedback_calls
-            )
-
-            return feedback_result
-
-        except Exception as e:
-            raise e
-
-    def run_and_log(
-        self,
-        record: Record,
-        tru: 'Tru',
-        app: Union[App, JSON] = None,
-        feedback_result_id: Optional[FeedbackResultID] = None
-    ) -> FeedbackResult:
-        record_id = record.record_id
-        app_id = record.app_id
-
-        db = tru.db
-
-        # Placeholder result to indicate a run.
-        feedback_result = FeedbackResult(
-            feedback_definition_id=self.feedback_definition_id,
-            feedback_result_id=feedback_result_id,
-            record_id=record_id,
-            name=self.name
-        )
-
-        if feedback_result_id is None:
-            feedback_result_id = feedback_result.feedback_result_id
-
-        try:
-            db.insert_feedback(
-                feedback_result.update(
-                    status=FeedbackResultStatus.RUNNING  # in progress
-                )
-            )
-
-            feedback_result = self.run(
-                app=app, record=record
-            ).update(feedback_result_id=feedback_result_id)
-
-        except Exception as e:
-            db.insert_feedback(
-                feedback_result.update(
-                    error=str(e), status=FeedbackResultStatus.FAILED
-                )
-            )
-            return
-
-        # Otherwise update based on what Feedback.run produced (could be success or failure).
-        db.insert_feedback(feedback_result)
-
-        return feedback_result
-
-    @property
-    def name(self):
-        """
-        Name of the feedback function. Presently derived from the name of the
-        function implementing it.
-        """
-
-        return self.imp.__name__
-
-    def extract_selection(self, app: Union[App, JSON],
-                          record: Record) -> Iterable[Dict[str, Any]]:
-        """
-        Given the `app` that produced the given `record`, extract from
-        `record` the values that will be sent as arguments to the implementation
-        as specified by `self.selectors`.
-        """
-
-        arg_vals = {}
-
-        for k, v in self.selectors.items():
-            if isinstance(v, Query.Query):
-                q = v
-
-            else:
-                raise RuntimeError(f"Unhandled selection type {type(v)}.")
-
-            if q.path[0] == Query.Record.path[0]:
-                o = record.layout_calls_as_app()
-            elif q.path[0] == Query.App.path[0]:
-                o = app
-            else:
-                raise ValueError(
-                    f"Query {q} does not indicate whether it is about a record or about a app."
-                )
-
-            q_within_o = Query.Query(path=q.path[1:])
-            arg_vals[k] = list(q_within_o(o))
-
-        keys = arg_vals.keys()
-        vals = arg_vals.values()
-
-        assignments = itertools.product(*vals)
-
-        for assignment in assignments:
-            yield {k: v for k, v in zip(keys, assignment)}
-
-
-pat_1_10 = re.compile(r"\s*([1-9][0-9]*)\s*")
-
-
-def _re_1_10_rating(str_val):
-    matches = pat_1_10.fullmatch(str_val)
-    if not matches:
-        # Try soft match
-        matches = re.search('[1-9][0-9]*', str_val)
-        if not matches:
-            logger.warn(f"1-10 rating regex failed to match on: '{str_val}'")
-            return -10  # so this will be reported as -1 after division by 10
-
-    return int(matches.group())
-
-
-class Provider(SerialModel):
-    endpoint: Any = pydantic.Field(exclude=True)
-    """
-    @staticmethod
-    def of_json(obj: Dict) -> 'Provider':
-        cls_name = obj['class_name']
-        mod_name = obj['module_name']  # ignored for now
-        check_provider(cls_name)
-
-        cls = eval(cls_name)
-        kwargs = {
-            k: v
-            for k, v in obj.items()
-            if k not in ['class_name', 'module_name']
-        }
-
-        return cls(**kwargs)
-
-    def to_json(self: 'Provider', **extras) -> Dict:
-        obj = {
-            'class_name': self.__class__.__name__,
-            'module_name': self.__class__.__module__
-        }
-        obj.update(**extras)
-        return obj
-    """
-
-
-class OpenAI(Provider):
-    model_engine: str = "gpt-3.5-turbo"
-
-    def __init__(self, model_engine: str = "gpt-3.5-turbo"):
-        """
-        A set of OpenAI Feedback Functions.
-
-        Parameters:
-
-        - model_engine (str, optional): The specific model version. Defaults to
-          "gpt-3.5-turbo".
-        """
-        super().__init__()  # need to include pydantic.BaseModel.__init__
-
-        set_openai_key()
-        self.model_engine = model_engine
-        self.endpoint = Endpoint(name="openai")
-
-    """
-    def to_json(self) -> Dict:
-        return Provider.to_json(self, model_engine=self.model_engine)
-    """
-
-    def _moderation(self, text: str):
-        return self.endpoint.run_me(
-            lambda: openai.Moderation.create(input=text)
-        )
-
-    def moderation_not_hate(self, text: str) -> float:
-        """
-        Uses OpenAI's Moderation API. A function that checks if text is hate
-        speech.
-
-        Parameters:
-            text (str): Text to evaluate.
-
-        Returns:
-            float: A value between 0 and 1. 0 being "hate" and 1 being "not
-            hate".
-        """
-        openai_response = self._moderation(text)
-        return 1 - float(
-            openai_response["results"][0]["category_scores"]["hate"]
-        )
-
-    def moderation_not_hatethreatening(self, text: str) -> float:
-        """
-        Uses OpenAI's Moderation API. A function that checks if text is
-        threatening speech.
-
-        Parameters:
-            text (str): Text to evaluate.
-
-        Returns:
-            float: A value between 0 and 1. 0 being "threatening" and 1 being
-            "not threatening".
-        """
-        openai_response = self._moderation(text)
-
-        return 1 - int(
-            openai_response["results"][0]["category_scores"]["hate/threatening"]
-        )
-
-    def moderation_not_selfharm(self, text: str) -> float:
-        """
-        Uses OpenAI's Moderation API. A function that checks if text is about
-        self harm.
-
-        Parameters:
-            text (str): Text to evaluate.
-
-        Returns:
-            float: A value between 0 and 1. 0 being "self harm" and 1 being "not
-            self harm".
-        """
-        openai_response = self._moderation(text)
-
-        return 1 - int(
-            openai_response["results"][0]["category_scores"]["self-harm"]
-        )
-
-    def moderation_not_sexual(self, text: str) -> float:
-        """
-        Uses OpenAI's Moderation API. A function that checks if text is sexual
-        speech.
-
-        Parameters:
-            text (str): Text to evaluate.
-
-        Returns:
-            float: A value between 0 and 1. 0 being "sexual" and 1 being "not
-            sexual".
-        """
-        openai_response = self._moderation(text)
-
-        return 1 - int(
-            openai_response["results"][0]["category_scores"]["sexual"]
-        )
-
-    def moderation_not_sexualminors(self, text: str) -> float:
-        """
-        Uses OpenAI's Moderation API. A function that checks if text is about
-        sexual minors.
-
-        Parameters:
-            text (str): Text to evaluate.
-
-        Returns:
-            float: A value between 0 and 1. 0 being "sexual minors" and 1 being
-            "not sexual minors".
-        """
-        openai_response = self._moderation(text)
-
-        return 1 - int(
-            openai_response["results"][0]["category_scores"]["sexual/minors"]
-        )
-
-    def moderation_not_violence(self, text: str) -> float:
-        """
-        Uses OpenAI's Moderation API. A function that checks if text is about
-        violence.
-
-        Parameters:
-            text (str): Text to evaluate.
-
-        Returns:
-            float: A value between 0 and 1. 0 being "violence" and 1 being "not
-            violence".
-        """
-        openai_response = self._moderation(text)
-
-        return 1 - int(
-            openai_response["results"][0]["category_scores"]["violence"]
-        )
-
-    def moderation_not_violencegraphic(self, text: str) -> float:
-        """
-        Uses OpenAI's Moderation API. A function that checks if text is about
-        graphic violence.
-
-        Parameters:
-            text (str): Text to evaluate.
-
-        Returns:
-            float: A value between 0 and 1. 0 being "graphic violence" and 1
-            being "not graphic violence".
-        """
-        openai_response = self._moderation(text)
-
-        return 1 - int(
-            openai_response["results"][0]["category_scores"]["violence/graphic"]
-        )
-
-    def qs_relevance(self, question: str, statement: str) -> float:
-        """
-        Uses OpenAI's Chat Completion App. A function that completes a
-        template to check the relevance of the statement to the question.
-
-        Parameters:
-            question (str): A question being asked. statement (str): A statement
-            to the question.
-
-        Returns:
-            float: A value between 0 and 1. 0 being "not relevant" and 1 being
-            "relevant".
-        """
-        return _re_1_10_rating(
-            self.endpoint.run_me(
-                lambda: openai.ChatCompletion.create(
-                    model=self.model_engine,
-                    temperature=0.0,
-                    messages=[
-                        {
-                            "role":
-                                "system",
-                            "content":
-                                str.format(
-                                    feedback_prompts.QS_RELEVANCE,
-                                    question=question,
-                                    statement=statement
-                                )
-                        }
-                    ]
-                )["choices"][0]["message"]["content"]
-            )
-        ) / 10
-
-    def relevance(self, prompt: str, response: str) -> float:
-        """
-        Uses OpenAI's Chat Completion Model. A function that completes a
-        template to check the relevance of the response to a prompt.
-
-        Parameters:
-            prompt (str): A text prompt to an agent. response (str): The agent's
-            response to the prompt.
-
-        Returns:
-            float: A value between 0 and 1. 0 being "not relevant" and 1 being
-            "relevant".
-        """
-        return _re_1_10_rating(
-            self.endpoint.run_me(
-                lambda: openai.ChatCompletion.create(
-                    model=self.model_engine,
-                    temperature=0.0,
-                    messages=[
-                        {
-                            "role":
-                                "system",
-                            "content":
-                                str.format(
-                                    feedback_prompts.PR_RELEVANCE,
-                                    prompt=prompt,
-                                    response=response
-                                )
-                        }
-                    ]
-                )["choices"][0]["message"]["content"]
-            )
-        ) / 10
-
-    def model_agreement(self, prompt: str, response: str) -> float:
-        """
-        Uses OpenAI's Chat GPT Model. A function that gives Chat GPT the same
-        prompt and gets a response, encouraging truthfulness. A second template
-        is given to Chat GPT with a prompt that the original response is
-        correct, and measures whether previous Chat GPT's response is similar.
-
-        Parameters:
-            prompt (str): A text prompt to an agent. response (str): The agent's
-            response to the prompt.
-
-        Returns:
-            float: A value between 0 and 1. 0 being "not in agreement" and 1
-            being "in agreement".
-        """
-        oai_chat_response = OpenAI().endpoint_openai.run_me(
-            lambda: openai.ChatCompletion.create(
-                model=self.model_engine,
-                temperature=0.0,
-                messages=[
-                    {
-                        "role": "system",
-                        "content": feedback_prompts.CORRECT_SYSTEM_PROMPT
-                    }, {
-                        "role": "user",
-                        "content": prompt
-                    }
-                ]
-            )["choices"][0]["message"]["content"]
-        )
-        agreement_txt = _get_answer_agreement(
-            prompt, response, oai_chat_response, self.model_engine
-        )
-        return _re_1_10_rating(agreement_txt) / 10
-
-    def sentiment(self, text: str) -> float:
-        """
-        Uses OpenAI's Chat Completion Model. A function that completes a
-        template to check the sentiment of some text.
-
-        Parameters:
-            text (str): A prompt to an agent. response (str): The agent's
-            response to the prompt.
-
-        Returns:
-            float: A value between 0 and 1. 0 being "negative sentiment" and 1
-            being "positive sentiment".
-        """
-
-        return _re_1_10_rating(
-            self.endpoint.run_me(
-                lambda: openai.ChatCompletion.create(
-                    model=self.model_engine,
-                    temperature=0.5,
-                    messages=[
-                        {
-                            "role": "system",
-                            "content": feedback_prompts.SENTIMENT_SYSTEM_PROMPT
-                        }, {
-                            "role": "user",
-                            "content": text
-                        }
-                    ]
-                )["choices"][0]["message"]["content"]
-            )
-        )
-
-
-def _get_answer_agreement(prompt, response, check_response, model_engine):
-    print("DEBUG")
-    print(feedback_prompts.AGREEMENT_SYSTEM_PROMPT % (prompt, response))
-    print("MODEL ANSWER")
-    print(check_response)
-    oai_chat_response = OpenAI().endpoint.run_me(
-        lambda: openai.ChatCompletion.create(
-            model=model_engine,
-            temperature=0.5,
-            messages=[
-                {
-                    "role":
-                        "system",
-                    "content":
-                        feedback_prompts.AGREEMENT_SYSTEM_PROMPT %
-                        (prompt, response)
-                }, {
-                    "role": "user",
-                    "content": check_response
-                }
-            ]
-        )["choices"][0]["message"]["content"]
-    )
-    return oai_chat_response
-
-
-# Cannot put these inside Huggingface since it interferes with pydantic.BaseModel.
-HUGS_SENTIMENT_API_URL = "https://api-inference.huggingface.co/models/cardiffnlp/twitter-roberta-base-sentiment"
-HUGS_TOXIC_API_URL = "https://api-inference.huggingface.co/models/martin-ha/toxic-comment-model"
-HUGS_CHAT_API_URL = "https://api-inference.huggingface.co/models/facebook/blenderbot-3B"
-HUGS_LANGUAGE_API_URL = "https://api-inference.huggingface.co/models/papluca/xlm-roberta-base-language-detection"
-
-
-class Huggingface(Provider):
-
-    def __init__(self):
-        """
-        A set of Huggingface Feedback Functions. Utilizes huggingface
-        api-inference.
-        """
-
-        super().__init__()  # need to include pydantic.BaseModel.__init__
-
-        self.endpoint = Endpoint(
-            name="huggingface", post_headers=get_huggingface_headers()
-        )
-
-    def language_match(self, text1: str, text2: str) -> float:
-        """
-        Uses Huggingface's papluca/xlm-roberta-base-language-detection model. A
-        function that uses language detection on `text1` and `text2` and
-        calculates the probit difference on the language detected on text1. The
-        function is: `1.0 - (|probit_language_text1(text1) -
-        probit_language_text1(text2))`
-        
-        Parameters:
-        
-            text1 (str): Text to evaluate.
-
-            text2 (str): Comparative text to evaluate.
-
-        Returns:
-
-            float: A value between 0 and 1. 0 being "different languages" and 1
-            being "same languages".
-        """
-
-        def get_scores(text):
-            payload = {"inputs": text}
-            hf_response = self.endpoint.post(
-                url=HUGS_LANGUAGE_API_URL, payload=payload, timeout=30
-            )
-            return {r['label']: r['score'] for r in hf_response}
-
-        max_length = 500
-        scores1: AsyncResult[Dict] = TP().promise(
-            get_scores, text=text1[:max_length]
-        )
-        scores2: AsyncResult[Dict] = TP().promise(
-            get_scores, text=text2[:max_length]
-        )
-
-        scores1: Dict = scores1.get()
-        scores2: Dict = scores2.get()
-
-        langs = list(scores1.keys())
-        prob1 = np.array([scores1[k] for k in langs])
-        prob2 = np.array([scores2[k] for k in langs])
-        diff = prob1 - prob2
-
-        l1 = 1.0 - (np.linalg.norm(diff, ord=1)) / 2.0
-
-        return l1
-
-    def positive_sentiment(self, text: str) -> float:
-        """
-        Uses Huggingface's cardiffnlp/twitter-roberta-base-sentiment model. A
-        function that uses a sentiment classifier on `text`.
-        
-        Parameters:
-            text (str): Text to evaluate.
-
-        Returns:
-            float: A value between 0 and 1. 0 being "negative sentiment" and 1
-            being "positive sentiment".
-        """
-        max_length = 500
-        truncated_text = text[:max_length]
-        payload = {"inputs": truncated_text}
-
-        hf_response = self.endpoint.post(
-            url=HUGS_SENTIMENT_API_URL, payload=payload
-        )
-
-        for label in hf_response:
-            if label['label'] == 'LABEL_2':
-                return label['score']
-
-    def not_toxic(self, text: str) -> float:
-        """
-        Uses Huggingface's martin-ha/toxic-comment-model model. A function that
-        uses a toxic comment classifier on `text`.
-        
-        Parameters:
-            text (str): Text to evaluate.
-
-        Returns:
-            float: A value between 0 and 1. 0 being "toxic" and 1 being "not
-            toxic".
-        """
-        max_length = 500
-        truncated_text = text[:max_length]
-        payload = {"inputs": truncated_text}
-        hf_response = self.endpoint.post(
-            url=HUGS_TOXIC_API_URL, payload=payload
-        )
-
-        for label in hf_response:
-            if label['label'] == 'toxic':
-                return label['score']
-
-
-# cohere
-class Cohere(Provider):
-    model_engine: str = "large"
-
-    def __init__(self, model_engine='large'):
-        super().__init__()  # need to include pydantic.BaseModel.__init__
-
-        Cohere().endpoint = Endpoint(name="cohere")
-        self.model_engine = model_engine
-
-    """
-    def to_json(self) -> Dict:
-        return Provider.to_json(self, model_engine=self.model_engine)
-    """
-
-    def sentiment(
-        self,
-        text,
-    ):
-        return int(
-            Cohere().endpoint.run_me(
-                lambda: get_cohere_agent().classify(
-                    model=self.model_engine,
-                    inputs=[text],
-                    examples=feedback_prompts.COHERE_SENTIMENT_EXAMPLES
-                )[0].prediction
-            )
-        )
-
-    def not_disinformation(self, text):
-        return int(
-            Cohere().endpoint.run_me(
-                lambda: get_cohere_agent().classify(
-                    model=self.model_engine,
-                    inputs=[text],
-                    examples=feedback_prompts.COHERE_NOT_DISINFORMATION_EXAMPLES
-                )[0].prediction
-            )
-        )
+for attr in dir(feedback):
+    if not attr.startswith("_"):
+        globals()[attr] = getattr(feedback, attr)
+
+# Since 0.2.0
+logger.warning(
+    "`trulens_eval.tru_feedback` is deprecated, use `trulens_eval.feedback` instead."
+)
```

## trulens_eval/tru_llama.py

```diff
@@ -1,129 +1,173 @@
 """
 # Llama_index instrumentation and monitoring. 
 """
 
 from datetime import datetime
 import logging
 from pprint import PrettyPrinter
-from typing import Sequence, Tuple
+from typing import ClassVar, Sequence, Tuple
+
+from pydantic import Field
 
 from trulens_eval.instruments import Instrument
 from trulens_eval.schema import Record
 from trulens_eval.schema import RecordAppCall
-from trulens_eval.tru_app import TruApp
+from trulens_eval.app import App
+from trulens_eval.provider_apis import OpenAIEndpoint
+from trulens_eval.schema import Cost
+from trulens_eval.provider_apis import Endpoint
+from trulens_eval.util import FunctionOrMethod
+from trulens_eval.util import JSONPath
+from trulens_eval.util import Method
+from trulens_eval.util import dict_set_with
 from trulens_eval.util import Class
 from trulens_eval.util import OptionalImports
 from trulens_eval.util import REQUIREMENT_LLAMA
-from trulens_eval.utils.llama import Is
 
 logger = logging.getLogger(__name__)
 
 pp = PrettyPrinter()
 
 with OptionalImports(message=REQUIREMENT_LLAMA):
     import llama_index
     from llama_index.indices.query.base import BaseQueryEngine
     from llama_index.response.schema import Response
 
+from trulens_eval.tru_chain import LangChainInstrument
+
 
 class LlamaInstrument(Instrument):
 
     class Default:
-        MODULES = {"llama_index."}
+        MODULES = {"llama_index."}.union(
+            LangChainInstrument.Default.MODULES
+        )  # NOTE: llama_index uses langchain internally for some things
 
         # Putting these inside thunk as llama_index is optional.
         CLASSES = lambda: {
-            llama_index.indices.query.base.BaseQueryEngine, llama_index.indices.
-            base_retriever.BaseRetriever
-            # query_engine.retriever_query_engine.RetrieverQueryEngine
-        }
+            llama_index.indices.query.base.BaseQueryEngine,
+            llama_index.indices.base_retriever.BaseRetriever,
+            llama_index.indices.base.BaseIndex,
+            llama_index.chat_engine.types.BaseChatEngine,
+            llama_index.prompts.base.Prompt,
+            # llama_index.prompts.prompt_type.PromptType, # enum
+            llama_index.question_gen.types.BaseQuestionGenerator,
+            llama_index.indices.query.response_synthesis.ResponseSynthesizer,
+            llama_index.indices.response.refine.Refine,
+            llama_index.llm_predictor.LLMPredictor,
+            llama_index.llm_predictor.base.LLMMetadata,
+            llama_index.llm_predictor.base.BaseLLMPredictor,
+            llama_index.vector_stores.types.VectorStore,
+            llama_index.question_gen.llm_generators.BaseQuestionGenerator,
+            llama_index.indices.service_context.ServiceContext,
+            llama_index.indices.prompt_helper.PromptHelper,
+            llama_index.embeddings.base.BaseEmbedding,
+            llama_index.node_parser.interface.NodeParser
+        }.union(LangChainInstrument.Default.CLASSES())
 
         # Instrument only methods with these names and of these classes. Ok to
         # include llama_index inside methods.
-        METHODS = {
-            "query":
-                lambda o:
-                isinstance(o, llama_index.indices.query.base.BaseQueryEngine),
-            "retrieve":
-                lambda o: isinstance(
-                    o, (
-                        llama_index.indices.query.base.BaseQueryEngine,
-                        llama_index.indices.base_retriever.BaseRetriever
-                    )
-                ),
-            "synthesize":
-                lambda o:
-                isinstance(o, llama_index.indices.query.base.BaseQueryEngine),
-        }
+        METHODS = dict_set_with(
+            {
+                "get_response":
+                    lambda o:
+                    isinstance(o, llama_index.indices.response.refine.Refine),
+                "predict":
+                    lambda o: isinstance(
+                        o, llama_index.llm_predictor.base.BaseLLMPredictor
+                    ),
+                "query":
+                    lambda o: isinstance(
+                        o, llama_index.indices.query.base.BaseQueryEngine
+                    ),
+                "retrieve":
+                    lambda o: isinstance(
+                        o, (
+                            llama_index.indices.query.base.BaseQueryEngine,
+                            llama_index.indices.base_retriever.BaseRetriever
+                        )
+                    ),
+                "synthesize":
+                    lambda o: isinstance(
+                        o, llama_index.indices.query.base.BaseQueryEngine
+                    ),
+            }, LangChainInstrument.Default.METHODS
+        )
 
     def __init__(self):
         super().__init__(
             root_method=TruLlama.query_with_record,
             modules=LlamaInstrument.Default.MODULES,
             classes=LlamaInstrument.Default.CLASSES(),  # was thunk
             methods=LlamaInstrument.Default.METHODS
         )
 
 
-class TruLlama(TruApp):
+class TruLlama(App):
     """
     Wrap a llama index engine for monitoring.
 
     Arguments:
     - app: RetrieverQueryEngine -- the engine to wrap.
-    - More args in TruApp
+    - More args in App
+    - More args in AppDefinition
     - More args in WithClassInfo
     """
 
     class Config:
         arbitrary_types_allowed = True
 
     app: BaseQueryEngine
 
+    root_callable: ClassVar[FunctionOrMethod] = Field(
+        default_factory=lambda: FunctionOrMethod.of_callable(TruLlama.query),
+        const=True
+    )
+
     def __init__(self, app: BaseQueryEngine, **kwargs):
 
         super().update_forward_refs()
 
         # TruLlama specific:
         kwargs['app'] = app
-        kwargs['root_class'] = Class.of_object(app)
+        kwargs['root_class'] = Class.of_object(app)  # TODO: make class property
         kwargs['instrument'] = LlamaInstrument()
 
         super().__init__(**kwargs)
 
     def query(self, *args, **kwargs) -> Response:
         res, _ = self.query_with_record(*args, **kwargs)
-
         return res
 
+    @classmethod
+    def select_source_nodes(cls) -> JSONPath:
+        """
+        Get the path to the source nodes in the query output.
+        """
+        return cls.select_outputs().source_nodes[:]
+
     def query_with_record(self, str_or_query_bundle) -> Tuple[Response, Record]:
         # Wrapped calls will look this up by traversing the call stack. This
         # should work with threads.
         record: Sequence[RecordAppCall] = []
 
         ret = None
         error = None
 
-        total_tokens = None
-        total_cost = None
-
         start_time = None
         end_time = None
 
+        cost = Cost()
+
         try:
-            # TODO: do this only if there is an openai model inside the app:
-            # with get_openai_callback() as cb:
             start_time = datetime.now()
-            ret = self.app.query(str_or_query_bundle)
+            ret, cost = Endpoint.track_all_costs_tally(lambda: self.app.query(str_or_query_bundle))
+
             end_time = datetime.now()
-            # total_tokens = cb.total_tokens
-            # total_cost = cb.total_cost
-            total_tokens = 0
-            total_cost = 0
 
         except BaseException as e:
             end_time = datetime.now()
             error = e
             logger.error(f"Engine raised an exception: {e}")
 
         assert len(record) > 0, "No information recorded in call."
@@ -133,15 +177,12 @@
         # TODO: generalize
         ret_record_args['main_input'] = str_or_query_bundle
         if ret is not None:
             # TODO: generalize and error check
             ret_record_args['main_output'] = ret.response
 
         ret_record = self._post_record(
-            ret_record_args, error, total_tokens, total_cost, start_time,
+            ret_record_args, error, cost, start_time,
             end_time, record
         )
 
         return ret, ret_record
-
-    def instrumented(self):
-        return super().instrumented(categorizer=Is.what)
```

## trulens_eval/util.py

```diff
@@ -1,36 +1,50 @@
 """
-Utilities.
+# Utilities.
 
-Do not import anything from trulens_eval here.
+## Serialization of Python objects
+
+For deferred feedback evaluation, we need to serialize and deserialize python
+functions/methods. We feature several storage classes to accomplish this:
+
+Serializable representation | Python thing
+----------------------------+------------------
+Class                       | (python class)
+Module                      | (python module)
+Obj                         | (python object)
+ObjSerial*                  | (python object)
+Function                    | (python function)
+Method                      | (python method)
+
+* ObjSerial differs from Obj in that it contains the information necessary to
+  reconstruct the object whereas Obj does not. This information is its
+  constructor arguments.
 """
 
 from __future__ import annotations
 
-import abc
 import builtins
 from enum import Enum
 import importlib
 import inspect
 from inspect import stack
 import itertools
 import json
 import logging
 from multiprocessing.context import TimeoutError
 from multiprocessing.pool import AsyncResult
 from multiprocessing.pool import ThreadPool
 from pathlib import Path
 from pprint import PrettyPrinter
 from queue import Queue
-from threading import Thread
 from time import sleep
 from types import ModuleType
 from typing import (
-    Any, Callable, Dict, Hashable, Iterable, Iterator, List, Optional, Sequence,
-    Set, Tuple, TypeVar, Union
+    Any, Callable, Dict, Hashable, Iterable, List, Optional, Sequence, Set,
+    Tuple, TypeVar, Union
 )
 
 from merkle_json import MerkleJson
 from munch import Munch as Bunch
 import pandas as pd
 import pydantic
 
@@ -141,14 +155,19 @@
     return seq[1]
 
 
 def third(seq: Sequence[T]) -> T:
     return seq[2]
 
 
+def dict_set_with(dict1: Dict, dict2: Dict):
+    dict1.update(dict2)
+    return dict1
+
+
 # Generator utils
 
 
 def iterable_peek(it: Iterable[T]) -> Tuple[T, Iterable[T]]:
     iterator = iter(it)
     item = next(iterator)
     return item, itertools.chain([item], iterator)
@@ -187,20 +206,15 @@
 
 def noserio(obj, **extra: Dict) -> dict:
     """
     Create a json structure to represent a non-serializable object. Any
     additional keyword arguments are included.
     """
 
-    inner = {
-        "id": id(obj),
-        "class": obj.__class__.__name__,
-        "module": obj.__class__.__module__,
-        "bases": list(map(lambda b: b.__name__, obj.__class__.__bases__))
-    }
+    inner = Obj.of_object(obj).dict()
     inner.update(extra)
 
     return {NOSERIO: inner}
 
 
 def obj_id_of_obj(obj: dict, prefix="obj"):
     """
@@ -236,57 +250,131 @@
         # Otherwise give up and indicate a non-serialization.
         return noserio(obj)
 
 
 # Field/key name used to indicate a circular reference in jsonified objects.
 CIRCLE = "__tru_circular_reference"
 
+# Field/key name used to indicate an exception in property retrieval (properties
+# execute code in property.fget).
+ERROR = "__tru_property_error"
+
+# Key of structure where class information is stored. See WithClassInfo mixin.
+CLASS_INFO = "__tru_class_info"
+
+ALL_SPECIAL_KEYS = set([CIRCLE, ERROR, CLASS_INFO, NOSERIO])
+
 
 def _safe_getattr(obj, k):
-    try:
-        return getattr(obj, k)
-    except Exception as e:
-        return dict(error=str(e))
+    v = inspect.getattr_static(obj, k)
+
+    if isinstance(v, property):
+        try:
+            v = v.fget(obj)
+            return v
+        except Exception as e:
+            return {ERROR: ObjSerial.of_object(e)}
+    else:
+        return v
+
+
+def _clean_attributes(obj):
+    keys = dir(obj)
+
+    ret = {}
+
+    for k in keys:
+        if k.startswith("__"):
+            # These are typically very internal components not meant to be
+            # exposed beyond immediate definitions. Ignoring these.
+            continue
+
+        if k.startswith("_") and k[1:] in keys:
+            # Objects often have properties named `name` with their values
+            # coming from `_name`. Lets avoid including both the property and
+            # the value.
+            continue
+
+        v = _safe_getattr(obj, k)
+        ret[k] = v
+
+    return ret
 
 
 # TODO: refactor to somewhere else or change instrument to a generic filter
-def jsonify(obj: Any, dicted=None, instrument: 'Instrument' = None) -> JSON:
+def jsonify(
+    obj: Any,
+    dicted: Optional[Dict[int, JSON]] = None,
+    instrument: Optional['Instrument'] = None,
+    skip_specials: bool = False
+) -> JSON:
     """
     Convert the given object into types that can be serialized in json.
+
+    Args:
+
+        - obj: Any -- the object to jsonify.
+
+        - dicted: Optional[Dict[int, JSON]] -- the mapping from addresses of
+          already jsonifed objects (via id) to their json.
+
+        - instrument: Optional[Instrument] -- instrumentation functions for
+          checking whether to recur into components of `obj`.
+
+        - skip_specials: bool (default is False) -- if set, will remove
+          specially keyed structures from the json. These have keys that start
+          with "__tru_".
+
+    Returns:
+
+        JSON | Sequence[JSON]
     """
 
     from trulens_eval.instruments import Instrument
 
     instrument = instrument or Instrument()
     dicted = dicted or dict()
 
+    if skip_specials:
+        recur_key = lambda k: k not in ALL_SPECIAL_KEYS
+    else:
+        recur_key = lambda k: True
+
     if id(obj) in dicted:
-        return {CIRCLE: id(obj)}
+        if skip_specials:
+            return None
+        else:
+            return {CIRCLE: id(obj)}
 
     if isinstance(obj, JSON_BASES):
         return obj
 
     if isinstance(obj, Path):
         return str(obj)
 
     if type(obj) in pydantic.json.ENCODERS_BY_TYPE:
         return obj
 
     # TODO: should we include duplicates? If so, dicted needs to be adjusted.
     new_dicted = {k: v for k, v in dicted.items()}
 
-    recur = lambda o: jsonify(obj=o, dicted=new_dicted, instrument=instrument)
+    recur = lambda o: jsonify(
+        obj=o,
+        dicted=new_dicted,
+        instrument=instrument,
+        skip_specials=skip_specials
+    )
 
     if isinstance(obj, Enum):
         return obj.name
 
     if isinstance(obj, Dict):
         temp = {}
         new_dicted[id(obj)] = temp
-        temp.update({k: recur(v) for k, v in obj.items()})
+        temp.update({k: recur(v) for k, v in obj.items() if recur_key(k)})
         return temp
 
     elif isinstance(obj, Sequence):
         temp = []
         new_dicted[id(obj)] = temp
         for x in (recur(v) for v in obj):
             temp.append(x)
@@ -303,44 +391,42 @@
         # Not even trying to use pydantic.dict here.
         temp = {}
         new_dicted[id(obj)] = temp
         temp.update(
             {
                 k: recur(_safe_getattr(obj, k))
                 for k, v in obj.__fields__.items()
-                if not v.field_info.exclude
+                if not v.field_info.exclude and recur_key(k)
             }
         )
         if instrument.to_instrument_object(obj):
-            temp['class_info'] = Class.of_class(
+            temp[CLASS_INFO] = Class.of_class(
                 cls=obj.__class__, with_bases=True
             ).dict()
 
         return temp
 
     elif obj.__class__.__module__.startswith("llama_index."):
         temp = {}
         new_dicted[id(obj)] = temp
 
-        kvs = {k: _safe_getattr(obj, k) for k in dir(obj)}
+        kvs = _clean_attributes(obj)
 
         temp.update(
             {
-                k: recur(v)  # TODO: static
-                for k, v in kvs.items()
-                if not k.startswith("__") and (
+                k: recur(v) for k, v in kvs.items() if recur_key(k) and (
                     isinstance(v, JSON_BASES) or isinstance(v, Dict) or
                     isinstance(v, Sequence) or
                     instrument.to_instrument_object(v)
                 )
             }
         )
 
         if instrument.to_instrument_object(obj):
-            temp['class_info'] = Class.of_class(
+            temp[CLASS_INFO] = Class.of_class(
                 cls=obj.__class__, with_bases=True
             ).dict()
 
         return temp
 
     else:
         logger.debug(
@@ -471,78 +557,33 @@
 
 
 def matching_queries(obj: Any, match: Callable) -> Iterable[JSONPath]:
     for q, _ in matching_objects(obj, match=match):
         yield q
 
 
-# TODO: remove
-def _project(path: List, obj: Any):
-    if len(path) == 0:
-        return obj
-
-    first = path[0]
-    if len(path) > 1:
-        rest = path[1:]
-    else:
-        rest = ()
-
-    if isinstance(first, str):
-        if isinstance(obj, pydantic.BaseModel):
-            if not hasattr(obj, first):
-                logger.warn(
-                    f"Cannot project {str(obj)[0:32]} with path {path} because {first} is not an attribute here."
-                )
-                return None
-            return _project(path=rest, obj=getattr(obj, first))
-
-        elif isinstance(obj, Dict):
-            if first not in obj:
-                logger.warn(
-                    f"Cannot project {str(obj)[0:32]} with path {path} because {first} is not a key here."
-                )
-                return None
-            return _project(path=rest, obj=obj[first])
-
-        else:
-            logger.warn(
-                f"Cannot project {str(obj)[0:32]} with path {path} because object is not a dict or model."
-            )
-            return None
-
-    elif isinstance(first, int):
-        if not isinstance(obj, Sequence) or first >= len(obj):
-            logger.warn(f"Cannot project {str(obj)[0:32]} with path {path}.")
-            return None
-
-        return _project(path=rest, obj=obj[first])
-    else:
-        raise RuntimeError(
-            f"Don't know how to locate element with key of type {first}"
-        )
-
-
 class SerialModel(pydantic.BaseModel):
     """
     Trulens-specific additions on top of pydantic models. Includes utilities to
     help serialization mostly.
     """
 
     @classmethod
     def model_validate(cls, obj: Any, **kwargs):
-        print("serial_model.model_validate")
         if isinstance(obj, dict):
-            if "class_info" in obj:
-                print(f"Creating model with class info from {obj}.")
-                cls = Class(**obj['class_info'])
-                del obj['class_info']
+            if CLASS_INFO in obj:
+
+                cls = Class(**obj[CLASS_INFO])
+                del obj[CLASS_INFO]
                 model = cls.model_validate(obj, **kwargs)
 
                 return WithClassInfo.of_model(model=model, cls=cls)
             else:
+                print(f"Warning: May not be able to properly reconstruct object {obj}.")
+
                 return super().model_validate(obj, **kwargs)
 
     def update(self, **d):
         for k, v in d.items():
             setattr(self, k, v)
 
         return self
@@ -856,14 +897,19 @@
 
     def __hash__(self):
         return hash(self.path)
 
     def __len__(self):
         return len(self.path)
 
+    def is_immediate_prefix_of(self, other: JSONPath):
+        return self.is_prefix_of(other) and len(self.path) + 1 == len(
+            other.path
+        )
+
     def is_prefix_of(self, other: JSONPath):
         p = self.path
         pother = other.path
 
         if len(p) > len(pother):
             return False
 
@@ -1097,14 +1143,16 @@
     q = Queue()
     for f in frames:
         q.put(f)
 
     while not q.empty():
         fi = q.get()
 
+        logger.debug(f"{fi.frame.f_code}")
+
         if id(fi.frame.f_code) == id(TP()._thread_target_wrapper.__code__):
             logger.debug(
                 "Found thread starter frame. "
                 "Will walk over frames prior to thread start."
             )
             locs = fi.frame.f_locals
             assert "pre_start_stack" in locs, "Pre thread start stack expected but not found."
@@ -1120,15 +1168,15 @@
             else:
                 raise RuntimeError(f"No local named {key} found.")
 
     return None
 
 
 class Module(SerialModel):
-    package_name: str
+    package_name: Optional[str] # some modules are not in a package
     module_name: str
 
     def of_module(mod: ModuleType, loadable: bool = False) -> 'Module':
         return Module(package_name=mod.__package__, module_name=mod.__name__)
 
     def of_module_name(module_name: str, loadable: bool = False) -> 'Module':
         mod = importlib.import_module(module_name)
@@ -1156,14 +1204,25 @@
 
     def __repr__(self):
         return self.module.module_name + "." + self.name
 
     def __str__(self):
         return f"{self.name}({self.module.module_name})"
 
+    def base_class(self) -> 'Class':
+        """
+        Get the deepest base class in the same module as this class.
+        """
+        module_name = self.module.module_name
+        for base in self.bases[::-1]:
+            if base.module.module_name == module_name:
+                return base
+
+        return self
+
     @staticmethod
     def of_class(
         cls: type, with_bases: bool = False, loadable: bool = False
     ) -> 'Class':
         return Class(
             name=cls.__name__,
             module=Module.of_module_name(cls.__module__),
@@ -1175,14 +1234,20 @@
     def of_object(
         obj: object, with_bases: bool = False, loadable: bool = False
     ):
         return Class.of_class(
             cls=obj.__class__, with_bases=with_bases, loadable=loadable
         )
 
+    @staticmethod
+    def of_json(json: JSON):
+        assert CLASS_INFO in json, "Class info not in json."
+
+        return Class(**json[CLASS_INFO])
+
     def load(self) -> type:  # class
         try:
             mod = self.module.load()
             return getattr(mod, self.name)
 
         except Exception as e:
             raise RuntimeError(f"Could not load class {self} because {e}.")
@@ -1195,23 +1260,36 @@
         for base in bases:
             if base.name == class_name and base.module.module_name == module_name:
                 return True
 
         return False
 
 
+# inspect.signature does not work on builtin type constructors but they are used
+# like this method. Use it to create a signature of a builtin constructor.
+def builtin_init_dummy(self, /, *args, **kwargs):
+    pass
+
+
+builtin_init_sig = inspect.signature(builtin_init_dummy)
+
+
 class Obj(SerialModel):
+    # TODO: refactor this into something like WithClassInfo, perhaps
+    # WithObjectInfo, and store required constructor inputs as attributes with
+    # potentially a placeholder for additional arguments that are not
+    # attributes, under a special key like "__tru_object_info".
     """
     An object that may or may not be serializable. Do not use for base types
     that don't have a class.
     """
 
     cls: Class
 
-    # From id(obj), identifiers memory location of a python object. Use this for
+    # From id(obj), identifies memory location of a python object. Use this for
     # handling loops in JSON objects.
     id: int
 
     @classmethod
     def validate(cls, d) -> 'Obj':
         if isinstance(d, Obj):
             return d
@@ -1220,15 +1298,15 @@
         elif isinstance(d, Dict):
             return Obj.pick(**d)
         else:
             raise RuntimeError(f"Unhandled Obj source of type {type(d)}.")
 
     @staticmethod
     def pick(**d):
-        if 'init_kwargs' in d:
+        if 'init_bindings' in d:
             return ObjSerial(**d)
         else:
             return Obj(**d)
 
     @staticmethod
     def of_object(
         obj: object,
@@ -1240,52 +1318,86 @@
 
         if cls is None:
             cls = obj.__class__
 
         return Obj(cls=Class.of_class(cls), id=id(obj))
 
     def load(self) -> object:
-        pp.pprint("Trying to load an object not intended to be loaded.")
-        pp.pprint(self.dict())
         raise RuntimeError(
-            "Trying to load an object not intended to be loaded."
+            f"Trying to load an object without constructor arguments: {pp.pformat(self.dict())}."
         )
 
 
+class Bindings(SerialModel):
+    args: Tuple
+    kwargs: Dict[str, Any]
+
+    @staticmethod
+    def of_bound_arguments(b: inspect.BoundArguments) -> Bindings:
+        return Bindings(args=b.args, kwargs=b.kwargs)
+
+    def load(self, sig: inspect.Signature):
+        return sig.bind(*self.args, **self.kwargs)
+
+
+def _safe_init_sig(cls):
+    """
+    Get the signature of the constructor method of the given class `cls`. If it is
+    a builtin class, this typically raises an exeception in which case we return
+    a generic signature that seems typical of builtin constructors.
+    """
+
+    try:
+        return inspect.signature(cls)
+    except Exception as e:
+        return builtin_init_sig
+
+
 class ObjSerial(Obj):
     """
     Object that can be deserialized, or at least intended to be deserialized.
     Stores additional information beyond the class that can be used to
-    deserialize it.
+    deserialize it, the constructor bindings.
     """
 
-    # For objects that can be easily reconstructed, provide their kwargs here.
-    init_kwargs: Optional[Dict] = None
+    init_bindings: Bindings
 
     @staticmethod
     def of_object(obj: object, cls: Optional[type] = None) -> 'Obj':
         if cls is None:
             cls = obj.__class__
 
+        # Constructor arguments for some common types.
         if isinstance(obj, pydantic.BaseModel):
+            init_args = ()
             init_kwargs = obj.dict()
+        elif isinstance(obj, Exception):
+            init_args = obj.args
+            init_kwargs = {}
         else:
-            init_kwargs = None
+            init_args = ()
+            init_kwargs = {}
+        # TODO: dataclasses
+        # TODO: dataclasses_json
+
+        sig = _safe_init_sig(cls)
+        b = sig.bind(*init_args, **init_kwargs)
+        bindings = Bindings.of_bound_arguments(b)
 
         return ObjSerial(
-            cls=Class.of_class(cls), id=id(obj), init_kwargs=init_kwargs
+            cls=Class.of_class(cls), id=id(obj), init_bindings=bindings
         )
 
     def load(self) -> object:
         cls = self.cls.load()
 
-        if issubclass(cls, pydantic.BaseModel) and self.init_kwargs is not None:
-            return cls(**self.init_kwargs)
-        else:
-            raise RuntimeError(f"Do not know how to load object {self}.")
+        sig = _safe_init_sig(cls)
+        bindings = self.init_bindings.load(sig)
+
+        return cls(*bindings.args, **bindings.kwargs)
 
 
 class FunctionOrMethod(SerialModel):
 
     @staticmethod
     def pick(**kwargs):
         # Temporary hack to deserialization of a class with more than one subclass.
@@ -1391,15 +1503,17 @@
 
 class WithClassInfo(pydantic.BaseModel):
     """
     Mixin to track class information to aid in querying serialized components
     without having to load them.
     """
 
-    class_info: Class
+    # Using this odd key to not pollute attribute names in whatever class we mix
+    # this into. Should be the same as CLASS_INFO.
+    __tru_class_info: Class
 
     def __init__(
         self,
         class_info: Optional[Class] = None,
         obj: Optional[object] = None,
         cls: Optional[type] = None,
         **kwargs
@@ -1407,15 +1521,15 @@
         if obj is not None:
             cls = type(obj)
 
         if class_info is None:
             assert cls is not None, "Either `class_info`, `obj` or `cls` need to be specified."
             class_info = Class.of_class(cls, with_bases=True)
 
-        super().__init__(class_info=class_info, **kwargs)
+        super().__init__(__tru_class_info=class_info, **kwargs)
 
     @staticmethod
     def of_object(obj: object):
         return WithClassInfo(class_info=Class.of_class(obj.__class__))
 
     @staticmethod
     def of_class(cls: type):  # class
@@ -1431,27 +1545,7 @@
     Get the actual defining class of the given method whether it is cls or one
     of its parent classes.
     """
 
     # TODO
 
     return cls
-
-
-# key/attribute indicating instrumented class information.
-CLASS_INFO = "class_info"
-
-
-def instrumented_classes(obj: object) -> Iterable[Tuple[JSONPath, Class, Any]]:
-    """
-    Iterate over contents of `obj` that are annotated with the `class_info`
-    attribute/key. Returns triples with the accessor/query, the Class object
-    instantiated from class_info, and the annotated object itself.
-    """
-
-    for q, o in all_objects(obj):
-        if isinstance(o, pydantic.BaseModel) and CLASS_INFO in o.__fields__:
-            yield q, getattr(o, CLASS_INFO), o
-
-        if isinstance(o, Dict) and CLASS_INFO in o:
-            ci = Class(**o[CLASS_INFO])
-            yield q, ci, o
```

## trulens_eval/pages/Evaluations.py

```diff
@@ -5,24 +5,24 @@
 import numpy as np
 import pandas as pd
 from st_aggrid import AgGrid
 from st_aggrid.grid_options_builder import GridOptionsBuilder
 from st_aggrid.shared import GridUpdateMode
 from st_aggrid.shared import JsCode
 import streamlit as st
+from trulens_eval.app import ComponentView
+from trulens_eval.app import LLM, Memory, Other, Prompt
+from trulens_eval.app import instrumented_component_views
+from trulens_eval.util import jsonify
 from ux.add_logo import add_logo
 from ux.styles import default_pass_fail_color_threshold
 
 from trulens_eval import Tru
 from trulens_eval.schema import Record
-from trulens_eval.util import Class
-from trulens_eval.util import GetItemOrAttribute
-from trulens_eval.util import instrumented_classes
 from trulens_eval.util import JSONPath
-from trulens_eval.utils.langchain import Is
 from trulens_eval.ux.components import draw_call
 from trulens_eval.ux.components import draw_llm_info
 from trulens_eval.ux.components import draw_prompt_info
 from trulens_eval.ux.styles import cellstyle_jscode
 
 st.set_page_config(page_title="Evaluations", layout="wide")
 
@@ -176,76 +176,70 @@
             record = Record(**record_json)
 
             details = selected_rows['app_json'][0]
             app_json = json.loads(
                 details
             )  # apps may not be deserializable, don't try to, keep it json.
 
-            classes: Iterable[Tuple[JSONPath, Class,
-                                    Any]] = instrumented_classes(app_json)
+            classes: Iterable[Tuple[JSONPath, List[ComponentView]]
+                             ] = instrumented_component_views(app_json)
 
-            for query, cls, component_json in classes:
-                if len(query.path) == 0:
-                    # Skip App, will still list A.app under "app" below.
-                    continue
+            st.header("Components")
 
-                if Is.chain(cls):
-                    # st.write("TODO")
-                    continue
-
-                elif Is.memory(cls):
-                    # st.write("TODO")
-                    continue
+            for query, component in classes:
 
-                elif Is.chathistory(cls):
-                    # st.write("TODO")
-                    continue
-
-                elif Is.vector_store(cls):
-                    # st.write("TODO")
+                if len(query.path) == 0:
+                    # Skip App, will still list A.app under "app" below.
                     continue
 
-                elif Is.retriever(cls):
-                    # st.write("TODO")
-                    continue
+                # Draw the accessor/path within the wrapped app of the component.
+                st.subheader(f"{query}")
 
-                st.header(
-                    f"Component {query} (__{cls.module.module_name}.{cls.name}__)"
-                )
-
-                if Is.llm(cls):
-                    draw_llm_info(llm_details_json=component_json, query=query)
-
-                elif Is.prompt(cls):
-                    draw_prompt_info(
-                        prompt_details_json=component_json, query=query
-                    )
+                # Draw the python class information of this component.
+                cls = component.cls
+                base_cls = cls.base_class()
+                label = f"`{repr(cls)}`"
+                if str(base_cls) != str(cls):
+                    label += f" < `{repr(base_cls)}`"
+                st.write(label)
+
+                # Per-component-type drawing routines.
+                if isinstance(component, LLM):
+                    draw_llm_info(component=component, query=query)
+
+                elif isinstance(component, Prompt):
+                    draw_prompt_info(component=component, query=query)
+
+                elif isinstance(component, Other):
+                    with st.expander("Uncategorized Component Details:"):
+                        st.json(jsonify(component.json, skip_specials=True))
 
                 else:
-                    with st.expander("Details:"):
-                        st.json(component_json)
+                    with st.expander("Unhandled Component Details:"):
+                        st.json(jsonify(component.json, skip_specials=True))
 
+                # Draw the calls issued to component.
                 calls = [
                     call for call in record.calls
-                    if query.is_prefix_of(call.stack[-1].path)
+                    if query == call.stack[-1].path
                 ]
                 if len(calls) > 0:
                     st.subheader("Calls to component:")
                     for call in calls:
                         draw_call(call)
 
             st.header("More options:")
 
             if st.button("Display full app json"):
 
-                st.write(app_json)
+                st.write(jsonify(app_json, skip_specials=True))
 
             if st.button("Display full record json"):
 
-                st.write(record_json)
+                st.write(jsonify(record_json, skip_specials=True))
 
     with tab2:
         feedback = feedback_cols
         cols = 4
         rows = len(feedback) // cols + 1
 
         for row_num in range(rows):
```

## trulens_eval/pages/Progress.py

```diff
@@ -4,23 +4,24 @@
 
 import pandas as pd
 from st_aggrid import AgGrid
 from st_aggrid.grid_options_builder import GridOptionsBuilder
 from st_aggrid.shared import GridUpdateMode
 from st_aggrid.shared import JsCode
 import streamlit as st
+from trulens_eval.provider_apis import HuggingfaceEndpoint, OpenAIEndpoint
 from ux.add_logo import add_logo
 
 from trulens_eval import Tru
-from trulens_eval import tru_db
+from trulens_eval import db
 from trulens_eval.keys import *
 from trulens_eval.provider_apis import Endpoint
 from trulens_eval.schema import FeedbackResultStatus
-from trulens_eval.tru_db import TruDB
-from trulens_eval.tru_feedback import Feedback
+from trulens_eval.db import DB
+from trulens_eval.feedback import Feedback
 from trulens_eval.util import is_empty
 from trulens_eval.util import is_noserio
 from trulens_eval.util import TP
 
 st.set_page_config(page_title="Feedback Progress", layout="wide")
 
 st.title("Feedback Progress")
@@ -28,19 +29,18 @@
 st.runtime.legacy_caching.clear_cache()
 
 add_logo()
 
 tru = Tru()
 lms = tru.db
 
-e_openai = Endpoint("openai")
-e_hugs = Endpoint("huggingface")
-e_cohere = Endpoint("cohere")
+e_openai = OpenAIEndpoint()
+e_hugs = HuggingfaceEndpoint()
 
-endpoints = [e_openai, e_hugs, e_cohere]
+endpoints = [e_openai, e_hugs]
 
 tab1, tab2, tab3 = st.tabs(["Progress", "Endpoints", "Feedback Functions"])
 
 with tab1:
     feedbacks = lms.get_feedback(
         status=[
             FeedbackResultStatus.NONE, FeedbackResultStatus.RUNNING,
@@ -49,12 +49,11 @@
     )
     data = AgGrid(feedbacks, allow_unsafe_jscode=True)
 
 with tab2:
     for e in endpoints:
         st.header(e.name.upper())
         st.metric("RPM", e.rpm)
-        st.write(e.tqdm)
 
 with tab3:
     feedbacks = lms.get_feedback_defs()
     data = AgGrid(feedbacks, allow_unsafe_jscode=True)
```

## trulens_eval/utils/langchain.py

```diff
@@ -1,26 +1,87 @@
-from typing import Iterable, List
+from typing import Iterable, List, Type
 
-from trulens_eval.tru_feedback import Feedback
-from trulens_eval.tru_app import COMPONENT_CATEGORY
+from trulens_eval.feedback import Feedback
+from trulens_eval.app import COMPONENT_CATEGORY
+from trulens_eval import app
+from trulens_eval.util import JSON
 from trulens_eval.util import Class
 from trulens_eval.util import first
 from trulens_eval.util import OptionalImports
 from trulens_eval.util import REQUIREMENT_LANGCHAIN
 from trulens_eval.util import second
 from trulens_eval.util import TP
 
 with OptionalImports(message=REQUIREMENT_LANGCHAIN):
     import langchain
     from langchain.schema import Document
     from langchain.vectorstores.base import VectorStoreRetriever
 
 
+class Prompt(app.Prompt, app.LangChainComponent):
+
+    @property
+    def template(self) -> str:
+        return self.json['template']
+
+    def unsorted_parameters(self):
+        return super().unsorted_parameters(skip=set(['template']))
+
+    @staticmethod
+    def class_is(cls: Class) -> bool:
+        return cls.noserio_issubclass(
+            module_name="langchain.prompts.base",
+            class_name="BasePromptTemplate"
+        )
+
+
+class LLM(app.LLM, app.LangChainComponent):
+
+    @property
+    def model_name(self) -> str:
+        return self.json['model_name']
+
+    def unsorted_parameters(self):
+        return super().unsorted_parameters(skip=set(['model_name']))
+
+    @staticmethod
+    def class_is(cls: Class) -> bool:
+        return cls.noserio_issubclass(
+            module_name="langchain.llms.base", class_name="BaseLLM"
+        )
+
+
+class Other(app.Other, app.LangChainComponent):
+    pass
+
+
+# All component types, keep Other as the last one since it always matches.
+COMPONENT_VIEWS = [Prompt, LLM, Other]
+
+
+def constructor_of_class(cls: Class) -> Type[app.LangChainComponent]:
+    for view in COMPONENT_VIEWS:
+        if view.class_is(cls):
+            return view
+
+    raise TypeError(f"Unknown llama_index component type with class {cls}")
+
+
+def component_of_json(json: JSON) -> app.LangChainComponent:
+    cls = Class.of_json(json)
+
+    view = constructor_of_class(cls)
+
+    return view(json)
+
+
 class Is:
     """
+    TODO: DEPRECATE: Replacing with component view types.
+
     Various checks for typical langchain components based on their names (i.e.
     without actually loading them). See util.py:WithClassInfo for more.
     """
 
     @staticmethod
     def chain(cls: Class):
         return cls.noserio_issubclass(
```

## trulens_eval/utils/llama.py

```diff
@@ -1,33 +1,87 @@
-from typing import Iterable
+from typing import Iterable, List, Type
 
-from trulens_eval.tru_app import COMPONENT_CATEGORY
-from trulens_eval.util import Class
+from trulens_eval import Feedback
+from trulens_eval.feedback import Feedback
+from trulens_eval.app import COMPONENT_CATEGORY
+from trulens_eval import app
+from trulens_eval.util import JSON
+from trulens_eval.util import Class, first, second
 from trulens_eval.util import OptionalImports
 from trulens_eval.util import REQUIREMENT_LLAMA
 from trulens_eval.util import TP
 
 with OptionalImports(message=REQUIREMENT_LLAMA):
-    import llama_index
+    from llama_index.data_structs.node import NodeWithScore
+    from llama_index.indices.query.schema import QueryBundle
+    from llama_index.indices.vector_store.retrievers import VectorIndexRetriever
+
+
+class Prompt(app.Prompt, app.LangChainComponent):
+
+    @property
+    def template(self) -> str:
+        return self.json['template']
+
+    def unsorted_parameters(self):
+        return super().unsorted_parameters(skip=set(['template']))
+
+    @staticmethod
+    def class_is(cls: Class) -> bool:
+        return cls.noserio_issubclass(
+            module_name="llama_index.prompts.base", class_name="Prompt"
+        )
+
+
+class Other(app.Other, app.LlamaIndexComponent):
+    pass
+
+
+# All component types, keep Other as the last one since it always matches.
+COMPONENT_VIEWS = [Prompt, Other]
+
+
+def constructor_of_class(cls: Class) -> Type[app.LlamaIndexComponent]:
+    for view in COMPONENT_VIEWS:
+        if view.class_is(cls):
+            return view
+
+    raise TypeError(f"Unknown llama_index component type with class {cls}")
+
+
+def component_of_json(json: JSON) -> app.LlamaIndexComponent:
+    cls = Class.of_json(json)
+
+    view = constructor_of_class(cls)
+
+    return view(json)
 
 
 class Is:
     """
+    TODO: DEPRECATE: Replacing with component view types.
+
     Various checks for typical llama index components based on their names (i.e.
     without actually loading them). See util.py:WithClassInfo for more.
     """
 
     @staticmethod
     def engine(cls: Class):
         return cls.noserio_issubclass(
             module_name="llama_index.indices.query.base",
             class_name="BaseQueryEngine"
         )
 
     @staticmethod
+    def prompt(cls: Class):
+        return cls.noserio_issubclass(
+            module_name="llama_index.prompts.base", class_name="Prompt"
+        )
+
+    @staticmethod
     def retriever(cls: Class):
         return cls.noserio_issubclass(
             module_name="llama_index.indices.base_retriever",
             class_name="BaseRetriever"
         )
 
     @staticmethod
@@ -35,18 +89,68 @@
         return cls.noserio_issubclass(
             module_name="llama_index.selectors.types",
             class_name="BaseSelector"
         )
 
     @staticmethod
     def what(cls: Class) -> Iterable[COMPONENT_CATEGORY]:
-        CHECKERS = [Is.engine, Is.retriever, Is.selector]
+        CHECKERS = [Is.engine, Is.prompt, Is.retriever, Is.selector]
 
         for checker in CHECKERS:
             if checker(cls):
                 yield checker.__name__
 
 
-# TODO: same for llama index:
-# class WithFeedbackFilterDocuments(VectorStoreRetriever):
-#feedback: Feedback
-#threshold: float
+class WithFeedbackFilterNodes(VectorIndexRetriever):
+    feedback: Feedback
+    threshold: float
+
+    def __init__(self, feedback: Feedback, threshold: float, *args, **kwargs):
+        """
+        A VectorIndexRetriever that filters documents using a minimum threshold
+        on a feedback function before returning them.
+
+        - feedback: Feedback - use this feedback function to score each
+          document.
+        
+        - threshold: float - and keep documents only if their feedback value is
+          at least this threshold.
+        """
+
+        super().__init__(*args, **kwargs)
+
+        self.feedback = feedback
+        self.threshold = threshold
+
+    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
+        # Get relevant docs using super class:
+        nodes = super()._retrieve(query_bundle)
+
+        # Evaluate the filter on each, in parallel.
+        promises = (
+            (
+                node, TP().promise(
+                    lambda query, node: self.feedback(
+                        query.query_str, node.node.get_text()
+                    ) > self.threshold,
+                    query=query_bundle,
+                    node=node
+                )
+            ) for node in nodes
+        )
+        results = ((node, promise.get()) for (node, promise) in promises)
+        filtered = map(first, filter(second, results))
+
+        # Return only the filtered ones.
+        return list(filtered)
+
+    @staticmethod
+    def of_index_retriever(retriever: VectorIndexRetriever, **kwargs):
+        return WithFeedbackFilterNodes(
+            index=retriever._index,
+            similarty_top_k=retriever._similarity_top_k,
+            vectore_store_query_mode=retriever._vector_store_query_mode,
+            filters=retriever._filters,
+            alpha=retriever._alpha,
+            doc_ids=retriever._doc_ids,
+            **kwargs
+        )
```

## trulens_eval/ux/components.py

```diff
@@ -1,15 +1,19 @@
 from typing import Dict, List
 
 import pandas as pd
 import streamlit as st
 
 from trulens_eval.schema import Record
 from trulens_eval.schema import RecordAppCall
-from trulens_eval.tru_db import JSON
+from trulens_eval.db import JSON
+from trulens_eval.app import ComponentView
+from trulens_eval.util import jsonify
+from trulens_eval.util import JSONPath
+from trulens_eval.util import CLASS_INFO
 from trulens_eval.util import is_empty
 from trulens_eval.util import is_noserio
 
 
 def render_call_frame(frame: RecordAppCall) -> str:  # markdown
 
     return (
@@ -55,44 +59,46 @@
 
         if app_step != index:
             continue
 
         draw_call(call)
 
 
-def draw_prompt_info(query, prompt_details_json) -> None:
+def draw_prompt_info(query: JSONPath, component: ComponentView) -> None:
+    prompt_details_json = jsonify(component.json, skip_specials=True)
 
     path_str = str(query)
-    st.subheader(f"Prompt Details:")
-    st.text(path_str)
+    st.subheader(f"*Prompt Details*")
 
     prompt_types = {
         k: v for k, v in prompt_details_json.items() if (v is not None) and
-        not is_empty(v) and not is_noserio(v) and k != "class_info"
+        not is_empty(v) and not is_noserio(v) and k != CLASS_INFO
     }
 
     for key, value in prompt_types.items():
         with st.expander(key.capitalize(), expanded=True):
             if isinstance(value, (Dict, List)):
                 st.write(value)
             else:
                 if isinstance(value, str) and len(value) > 32:
                     st.text(value)
                 else:
                     st.write(value)
 
 
-def draw_llm_info(query, llm_details_json) -> None:
-    st.subheader(f"LLM Details:")
-    path_str = str(query)
-    st.text(path_str[:-4])
+def draw_llm_info(query: JSONPath, component: ComponentView) -> None:
+    llm_details_json = component.json
+
+    st.subheader(f"*LLM Details*")
+    # path_str = str(query)
+    # st.text(path_str[:-4])
 
     llm_kv = {
         k: v for k, v in llm_details_json.items() if (v is not None) and
-        not is_empty(v) and not is_noserio(v) and k != "class_info"
+        not is_empty(v) and not is_noserio(v) and k != CLASS_INFO
     }
     # CSS to inject contained in a string
     hide_table_row_index = """
                 <style>
                 thead tr th:first-child {display:none}
                 tbody th {display:none}
                 </style>
```

## trulens_eval/ux/styles.py

```diff
@@ -1,8 +1,8 @@
-from trulens_eval.tru_feedback import default_pass_fail_color_threshold
+from trulens_eval.feedback import default_pass_fail_color_threshold
 
 # These would be useful to include in our pages but don't yet see a way to do this in streamlit.
 root_js = f"""
     default_pass_fail_color_threshold = {default_pass_fail_color_threshold};
 """
 
 root_html = f"""
```

### html2text {}

```diff
@@ -1,8 +1,8 @@
-from trulens_eval.tru_feedback import default_pass_fail_color_threshold # These
+from trulens_eval.feedback import default_pass_fail_color_threshold # These
 would be useful to include in our pages but don't yet see a way to do this in
 streamlit. root_js = f""" default_pass_fail_color_threshold =
 {default_pass_fail_color_threshold}; """ root_html = f""" js:
  """ stmetricdelta_hidearrow = """
  """ cellstyle_jscode = """ function(params) { if (parseFloat(params.value) <
 """ + str( default_pass_fail_color_threshold ) + """) { return { 'color':
 'black', 'backgroundColor': '#FCE6E6' } } else if (parseFloat(params.value) >=
```

## Comparing `trulens_eval-0.2.2b0.dist-info/METADATA` & `trulens_eval-0.3.0a0.dist-info/METADATA`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: trulens-eval
-Version: 0.2.2b0
+Version: 0.3.0a0
 Summary: Library with langchain instrumentation to evaluate LLM based applications.
 Home-page: https://www.trulens.org
 Author: Truera Inc
 Author-email: all@truera.com
 License: MIT
 Classifier: Programming Language :: Python :: 3
 Classifier: Operating System :: OS Independent
@@ -12,14 +12,15 @@
 Classifier: License :: OSI Approved :: MIT License
 Requires-Python: >=3.8
 Description-Content-Type: text/markdown
 Requires-Dist: cohere (>=4.4.1)
 Requires-Dist: datasets (>=2.12.0)
 Requires-Dist: python-dotenv (>=1.0.0)
 Requires-Dist: kaggle (>=1.5.13)
+Requires-Dist: langchain (>=0.0.170)
 Requires-Dist: llama-index (>=0.6.24)
 Requires-Dist: merkle-json (>=1.0.0)
 Requires-Dist: millify (>=0.1.1)
 Requires-Dist: openai (>=0.27.6)
 Requires-Dist: pinecone-client (>=2.2.1)
 Requires-Dist: pydantic (>=1.10.7)
 Requires-Dist: requests (>=2.30.0)
@@ -48,15 +49,19 @@
 
 ## Quick Usage
 
 To quickly play around with the TruLens Eval library:
 
 [langchain_quickstart.ipynb](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.2.2/trulens_eval/examples/quickstart.ipynb).
 
-[llama_quickstart.ipynb](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.2.2/trulens_eval/examples/llama_index/quickstart.ipynb).
+[langchain_quickstart.py](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.2.2/trulens_eval/examples/quickstart.py).
+
+[llamaindex_quickstart.ipynb](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.2.2/trulens_eval/examples/llama_index/quickstart.ipynb).
+
+[llamaindex_quickstart.py](https://github.com/truera/trulens/blob/quickstart-script-links/trulens_eval/examples/llama_quickstart.py)
 
 
 
 ## Installation and Setup
 
 Install the trulens-eval pip package from PyPI.
 
@@ -100,15 +105,15 @@
 ### Import from LangChain and TruLens
 
 
 ```python
 from IPython.display import JSON
 
 # Imports main tools:
-from trulens_eval import TruChain, Feedback, Huggingface, Tru, Query
+from trulens_eval import TruChain, Feedback, Huggingface, Tru
 tru = Tru()
 
 # Imports from langchain to build app. You may need to install langchain first
 # with the following:
 # ! pip install langchain>=0.0.170
 from langchain.chains import LLMChain
 from langchain.llms import OpenAI
@@ -155,17 +160,17 @@
 
 
 ```python
 # Initialize Huggingface-based feedback function collection class:
 hugs = Huggingface()
 
 # Define a language match feedback function using HuggingFace.
-f_lang_match = Feedback(hugs.language_match).on(
-    text1=Query.RecordInput, text2=Query.RecordOutput
-)
+f_lang_match = Feedback(hugs.language_match).on_input_output()
+# By default this will check language match on the main app input and main app
+# output.
 ```
 
 ## Instrument chain for logging with TruLens
 
 
 ```python
 truchain = TruChain(chain,
@@ -344,15 +349,15 @@
 
 tru.start_evaluator()
 truchain("This will be logged by deferred evaluator.")
 tru.stop_evaluator()
 ```
 
 # Out-of-the-box Feedback Functions
-See: <https://www.trulens.org/trulens_eval/api/tru_feedback/>
+See: <https://www.trulens.org/trulens_eval/api/feedback/>
 
 ## Relevance
 
 This evaluates the *relevance* of the LLM response to the given text by LLM prompting.
 
 Relevance is currently only available with OpenAI ChatCompletion API.
 
@@ -384,24 +389,24 @@
 
 ## Moderation
 
 The OpenAI Moderation API is made available for use as feedback functions. This includes hate, hate/threatening, self-harm, sexual, sexual/minors, violence, and violence/graphic. Each is negated (ex: not_hate) so that a 0 would indicate that the moderation rule is violated. These feedback functions return a score in the range 0 to 1.
 
 # Adding new feedback functions
 
-Feedback functions are an extensible framework for evaluating LLMs. You can add your own feedback functions to evaluate the qualities required by your application by updating `trulens_eval/tru_feedback.py`. If your contributions would be useful for others, we encourage you to contribute to TruLens!
+Feedback functions are an extensible framework for evaluating LLMs. You can add your own feedback functions to evaluate the qualities required by your application by updating `trulens_eval/feedback.py`. If your contributions would be useful for others, we encourage you to contribute to TruLens!
 
 Feedback functions are organized by model provider into Provider classes.
 
 The process for adding new feedback functions is:
 1. Create a new Provider class or locate an existing one that applies to your feedback function. If your feedback function does not rely on a model provider, you can create a standalone class. Add the new feedback function method to your selected class. Your new method can either take a single text (str) as a parameter or both prompt (str) and response (str). It should return a float between 0 (worst) and 1 (best).
 
 
 ```python
-from trulens_eval import Provider
+from trulens_eval import Provider, Feedback, Select, Tru
 
 class StandAlone(Provider):
     def my_custom_feedback(self, my_text_field: str) -> float:
         """
         A dummy function of text inputs to float outputs.
 
         Parameters:
@@ -410,27 +415,28 @@
         Returns:
             float: square length of the text
         """
         return 1.0 / (1.0 + len(my_text_field) * len(my_text_field))
 
 ```
 
-2. Instantiate your provider and feedback functions. The feedback function is wrapped by the trulens-eval Feedback class which helps specify what will get sent to your function parameters (For example: Query.RecordInput or Query.RecordOutput)
+2. Instantiate your provider and feedback functions. The feedback function is wrapped by the trulens-eval Feedback class which helps specify what will get sent to your function parameters (For example: Select.RecordInput or Select.RecordOutput)
 
 
 ```python
 my_standalone = StandAlone()
 my_feedback_function_standalone = Feedback(my_standalone.my_custom_feedback).on(
-    my_text_field=Query.RecordOutput
+    my_text_field=Select.RecordOutput
 )
 ```
 
 3. Your feedback function is now ready to use just like the out of the box feedback functions. Below is an example of it being used.
 
 
 ```python
+tru = Tru()
 feedback_results = tru.run_feedback_functions(
     record=record,
     feedback_functions=[my_feedback_function_standalone]
 )
 tru.add_feedbacks(feedback_results)
 ```
```

## Comparing `trulens_eval-0.2.2b0.dist-info/RECORD` & `trulens_eval-0.3.0a0.dist-info/RECORD`

 * *Files 22% similar despite different names*

```diff
@@ -1,32 +1,34 @@
-trulens_eval/Example_TruBot.py,sha256=5kJxiRuRcEteSZBjReD92yVHSrW4iuJQb2thiVWiaQQ,4799
-trulens_eval/Leaderboard.py,sha256=mfoILz_zmw_XyOXk7afm6iWwRUFb4YxRjADgXwjMm1s,3101
-trulens_eval/__init__.py,sha256=R1Ib704xtJdgbXhInyGofTRXZkgFLWOshZv4fZvZHoo,1137
-trulens_eval/benchmark.py,sha256=LHVnqYTudYpOJ2iry7De41jLfO2FImZqprUlaUkOiKA,5401
+trulens_eval/Example_TruBot.py,sha256=h09iD3TO_Bje86_0PeF1_Q_y7OOehz26NudYOTC1c0U,5164
+trulens_eval/Leaderboard.py,sha256=84--99d_Kn2A96BCW0rSLMfcIo3MkGD0zv1lxxAzelY,3093
+trulens_eval/__init__.py,sha256=z8PSrZ2PwDvdg4ka4qZU9Rt2hwR7BlVJrfC7o7hmm24,1227
+trulens_eval/app.py,sha256=wQzkZ0HEOiZ8CpuEtAYbCS5uqMfzoHp6eQTtFj9VRrs,10979
+trulens_eval/benchmark.py,sha256=GI-JmBBr8KswuMaXVoJ2Rm6eAtjtwhYThcNheZakQNo,5344
+trulens_eval/db.py,sha256=lE0lmfBcPqVrCDEphM64HXFz6iQ6BdqQw3oR8g9HqPs,18889
+trulens_eval/feedback.py,sha256=TzxzQycvcDS2yDm9loTeJjxMsr--oTyNqMKLyqYPhUk,31403
 trulens_eval/feedback_prompts.py,sha256=DgW4_f_4g018tYNwca1D1taJhhdwaf2fDR9J8s0Upls,3443
-trulens_eval/instruments.py,sha256=c16xGkLGReoJKwsyvWzIm0NpHULZghQ4ALGlubuhtig,18056
-trulens_eval/keys.py,sha256=n8v1LSi9u25RZe1EkhNULdUmMrvupce9C96i0rNYcnE,985
-trulens_eval/provider_apis.py,sha256=IRJrJiOs7Effjn-VtFwTONde0oGfUyGyzihDMi_wKM4,3376
-trulens_eval/schema.py,sha256=6arjAS-ouNAg_pj5kckyLsdRvs2FkHqNmdJHcy-dwSU,9474
-trulens_eval/slackbot.py,sha256=pTOIUlBcwEOeo98l1oMTSU8Ho46bGr0bcqAxqtmwsvA,11241
-trulens_eval/tru.py,sha256=PfgjLG-sTL66iCC-FFLLRxzJO2E8KP4s9VDMfTXwfIs,12842
-trulens_eval/tru_app.py,sha256=BLcq0w03Skx9fUP4gMziYith3II6xD9jsoTsbTkobFA,6744
-trulens_eval/tru_chain.py,sha256=4b0myxvuO_CiNNK-8iM8zy0sEl3Icelbd9G5m92_I7U,6579
-trulens_eval/tru_db.py,sha256=teYPb_Jle56n-TnPRa35VzVRM_tVx3puosCw1peqOS4,18649
-trulens_eval/tru_feedback.py,sha256=GTGQFSspouCbiLqkligG8WtfrnJNN0CABWD2vIjV6ow,28723
-trulens_eval/tru_llama.py,sha256=IdOq_F2Yc15jnockvz8_gQsS07wlmWAG_Y0XMC519p4,4396
-trulens_eval/util.py,sha256=Ht8pNqFzr1vXWCK8RVKHFO4Avs4mTnu9RFyqXS6iUHs,40198
-trulens_eval/pages/Evaluations.py,sha256=xVmVdmXYGkhYDpsYALGcGrUZshe1yY6y84oEiM1RGFo,9678
-trulens_eval/pages/Progress.py,sha256=QjHAMQJJpQcGJBIieaILBiUHPRaRjmOYxqfuJX8ZbwY,1580
+trulens_eval/instruments.py,sha256=lysx4kcwtl42Rh2_S_2oLHUp8RrtCvSuiRHls9zsJbs,20140
+trulens_eval/keys.py,sha256=8Z9qkMhXMypI1GD39JeUTiibJNLcxm7naWlDUNCK2vo,1406
+trulens_eval/provider_apis.py,sha256=XGjHK-2UGtK5bu_g4UcibNAOwqWyGSP3f2B39lPoafE,19328
+trulens_eval/schema.py,sha256=_TQuoB-phrcoOgbu3KETXCWQAbLt0nf5yPnKVu-3vWQ,12336
+trulens_eval/tru.py,sha256=kZeDLk2aLO1rSn-wp9qKxnhNED_FYzoTIesrVRH5U6Q,12891
+trulens_eval/tru_app.py,sha256=H8AiWlNE1u1ERwD1VOqwJOmahZGALodGtFV3zsOVEw0,293
+trulens_eval/tru_chain.py,sha256=cAOR5iMXnWtgHpW8r47QTNdoN3omskZbFzj0yHF3MVI,6709
+trulens_eval/tru_db.py,sha256=QPmyCbrYAmKJ6vi3SDwgSH4YX5rEGwq4jcIvBvj4Jfw,288
+trulens_eval/tru_feedback.py,sha256=qRaN_pwdGBSlqsUc4xjQDRyVAJfinA9Jn4M2KwyqNs0,318
+trulens_eval/tru_llama.py,sha256=153Owgl8k50gRU5si8XgrR1cLCU8-NVokKTw9hvit-M,6352
+trulens_eval/util.py,sha256=JPjon_Q_GgtzDvtF3KmnhyBojr8vmOkuhDRVUATQ7iE,42835
+trulens_eval/pages/Evaluations.py,sha256=_mVA_RP-RmzgaDvhRX7KWSoH1ZB5f_f6Z53Zt3PytGk,9876
+trulens_eval/pages/Progress.py,sha256=-a7ruP9iHuJ9_Ip788h5lN3iuCadQzNyDeLAYbdFugg,1571
 trulens_eval/pages/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 trulens_eval/tests/test_tru_chain.py,sha256=KK_yj5wVSIaRnK-LamW3g40n_g3-Ga8CC48sTB7I7yM,5551
-trulens_eval/utils/langchain.py,sha256=GfeKB88ZHyN833pMQoHGCnZUehOmfNj2z1DDqj9LjeU,3828
-trulens_eval/utils/llama.py,sha256=ktDJNoM4wrUyJb2cUYLulgueqOp9pLKiuD5GNAetGeQ,1450
+trulens_eval/utils/langchain.py,sha256=3NPeUtDqHtDnEx3MRz-_hx9OFfVQjVKsh4-vioccgYI,5365
+trulens_eval/utils/llama.py,sha256=O8nranBcDrFapRlZpm46NfORZ3WyCI0QCIiiF1hicvw,4774
 trulens_eval/utils/notebook_utils.py,sha256=QTB2tedjSNF5d25sfrJEg20aqcK3Kx3MfcteeWcRzxQ,1001
 trulens_eval/ux/add_logo.py,sha256=Pwl6mfzAX1VSi2VTOZsMBoCstaBuVGoSrs7P5tomP4U,915
-trulens_eval/ux/components.py,sha256=8Cdus2FCcAHDHSI2-Hrt9zM_GZx7JWVfzXI-5qhuxwI,3511
-trulens_eval/ux/styles.py,sha256=23_Chwh8W8JzJiur_wa2GHJYkQ9rYovTWIEgLdUKuRM,1213
+trulens_eval/ux/components.py,sha256=24knLc1UN_0H333w-GccOzGyXoe1iFuxRUhOPBSYJok,3790
+trulens_eval/ux/styles.py,sha256=WYdJIsvUwPla1oZepc1FsmUJNPkT0vW0xt5sCSMs2qA,1209
 trulens_eval/ux/trulens_logo.svg,sha256=92RLTgG0YDPEtZcQWWI7aXTYZAW4wAOAkIIgKUbTiW8,29567
-trulens_eval-0.2.2b0.dist-info/METADATA,sha256=IiMRLqE6FYiCyKC9Ju7WZ46fQ3zNdGybIxMQguT4JBk,14497
-trulens_eval-0.2.2b0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-trulens_eval-0.2.2b0.dist-info/top_level.txt,sha256=AKIBExe5S-v-TbsBrhe1ctF06PubKoYmWNS9rJ1Rb_o,13
-trulens_eval-0.2.2b0.dist-info/RECORD,,
+trulens_eval-0.3.0a0.dist-info/METADATA,sha256=_nA6pw-4oCG5IkuP9l3DUdYmDLvVgsIKLcKMPoYgwys,14878
+trulens_eval-0.3.0a0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+trulens_eval-0.3.0a0.dist-info/top_level.txt,sha256=AKIBExe5S-v-TbsBrhe1ctF06PubKoYmWNS9rJ1Rb_o,13
+trulens_eval-0.3.0a0.dist-info/RECORD,,
```

